<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://rsohlot.github.io/rss-feed/index.html</id>
    <title>osmos::feed</title>
    <updated>2023-09-26T00:43:24.076Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="https://rsohlot.github.io/rss-feed/index.html"/>
    <link rel="self" href="https://rsohlot.github.io/rss-feed/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[[D] How are machine learning videos made, and what platform is best?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16s8e4d/d_how_are_machine_learning_videos_made_and_what/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16s8e4d/d_how_are_machine_learning_videos_made_and_what/"/>
        <updated>2023-09-26T00:06:13.000Z</updated>
        <summary type="html"><![CDATA[Is this done with computer vision or somehow within the game itself?
 Also, what would you recommend as a resource to learning about machine learning fast? Is Microsoft Azure worth it or should I use TensorFlow? (I'll mostly do audio classification and game simulations)
 https://www.youtube.com/watch?v=tVNoetVLuQg
 Thanks so much! Please recommend go-to resources you've used to get up and running. Looking forward to what yall recommend.
    submitted by    /u/Fit-Replacement7245  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vintage stamp restoration project - Can AI remove unwanted marks if I use several scans of nearly identical stamps as training data?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16s7jkb/vintage_stamp_restoration_project_can_ai_remove/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16s7jkb/vintage_stamp_restoration_project_can_ai_remove/"/>
        <updated>2023-09-25T23:29:01.000Z</updated>
        <summary type="html"><![CDATA[I have a project that I think AI might be able to help with.
 I have access to thousands of vintage postage stamps which also have cancelation stamps from when they were mailed. I'm thinking of publishing a book, and I want to create clean reproductions of the stamp designs without the cancelation marks.
 If I train AI on high resolution scans of 10 to 40 identical stamps, is there a tool that can look for commonalities within the patterns and then remove conflicting artifacts (cancelation stamps, tears, and smudges)? I'm aiming for a 400% enlargement that shows off clean and accurate detail. 
 What tools would I need? Iâ€™d prefer not to upload these scans to the web. Are there downloadable tools available? Iâ€™m technologically savvy and very experienced in graphic design software but have no experience in coding.
 All of this can be done manually in Photoshop, of course. But with thousands of stamp designs, Iâ€™d like to automate as much as I can.
 Thanks in advance for any insights!
    submitted by    /u/fisheternal  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ML-related conspiracy theories that you guys want to discuss?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16s7if3/d_mlrelated_conspiracy_theories_that_you_guys/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16s7if3/d_mlrelated_conspiracy_theories_that_you_guys/"/>
        <updated>2023-09-25T23:27:33.000Z</updated>
        <summary type="html"><![CDATA[I have one that I'd like to share with the class. I think the grokking / double descent paper was sponsored by cloud providers to get people to continue training even when the loss curve has flattened out (speaking as someone who is watching a flat loss curve all day today).
 Anyone have any other ones?
 I'm only being semi-serious obviously.
    submitted by    /u/new_name_who_dis_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators", Herzog et al 2023 {G}]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16s6vpa/deep_rl_at_scale_sorting_waste_in_office/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16s6vpa/deep_rl_at_scale_sorting_waste_in_office/"/>
        <updated>2023-09-25T23:00:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Newbie to ML, are there hosted service that can do long term memory of chatgpt?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16s6iez/d_newbie_to_ml_are_there_hosted_service_that_can/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16s6iez/d_newbie_to_ml_are_there_hosted_service_that_can/"/>
        <updated>2023-09-25T22:45:35.000Z</updated>
        <summary type="html"><![CDATA[I'm not really a ML/backend guy, mostly work with front end but I want to be able to use chatgpt api with long term memory. I did some research and it seems like it's possible with vector databases but seems quite complicated to setup. 
 Are there hosted solutions/api that would allow me to just have long term memory with chatgpt? 
    submitted by    /u/yalag  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ML deployment survey]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16s5jq3/d_ml_deployment_survey/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16s5jq3/d_ml_deployment_survey/"/>
        <updated>2023-09-25T22:07:19.000Z</updated>
        <summary type="html"><![CDATA[Hi, we are doing a survey of ML deployment platforms. Kindly fill it out and share it with your friends. We will share the results with the community
 https://forms.gle/1Q3WeSukHj8xBzUBA
    submitted by    /u/fazkan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Best Platforms/Tools To Help Build ML POC]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16s3l0g/discussion_best_platformstools_to_help_build_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16s3l0g/discussion_best_platformstools_to_help_build_ml/"/>
        <updated>2023-09-25T20:52:33.000Z</updated>
        <summary type="html"><![CDATA[Looking for something that's lightweight on infrastructure/setup where I can quickly validate my use case quickly to see if I can achieve desired accuracy/precision with my initial data set, I've always done this manually in the past but was curious as to how others do it and if there are any tools designed for it or that support it
    submitted by    /u/PatienceLogical2694  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT can now see, hear, and speak. As announced in their blog.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16s2j83/chatgpt_can_now_see_hear_and_speak_as_announced/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16s2j83/chatgpt_can_now_see_hear_and_speak_as_announced/"/>
        <updated>2023-09-25T20:12:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/w__sky  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI is evolving for its own benefit, not ours]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16s1ot4/ai_is_evolving_for_its_own_benefit_not_ours/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16s1ot4/ai_is_evolving_for_its_own_benefit_not_ours/"/>
        <updated>2023-09-25T19:39:26.000Z</updated>
        <summary type="html"><![CDATA[The rapid advancements in artificial intelligence (AI) are causing concern as humans struggle to understand and control this evolving technology.
 
Many people believe that since humans invented AI, they should be able to regulate and manage it for their own benefit.
 
However, this belief is misguided as AI is a new and potentially dangerous situation that requires careful consideration.
 
The author argues that AI is an evolutionary process that humans don't fully understand and cannot control.
 
The latest developments in AI, such as large language models and deepfakes, are causing anxiety and raising questions about the future implications of this technology.
 
 Source : https://www.newscientist.com/article/mg25934573-800-ai-is-evolving-for-its-own-benefit-not-ours/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Decentralized alignment and training for LLMs: 2 articles GPT4 wrote]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16s1087/d_decentralized_alignment_and_training_for_llms_2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16s1087/d_decentralized_alignment_and_training_for_llms_2/"/>
        <updated>2023-09-25T19:12:57.000Z</updated>
        <summary type="html"><![CDATA[A magazine style article outline:
 Decentralizing AI: A Journey Towards True Collective Intelligence
 In today's digital age, AI shapes our interactions, decisions, and understanding of the world. Imagine a future where AI's guiding hand isn't controlled by a select few but shaped by the collective wisdom of people everywhere. Welcome to the horizon of decentralized training and alignment of Large Language Models (LLMs). Together, let's explore this visionary frontier.
 1. The AI We Know Today
 At present, AI's most prominent representatives, LLMs like GPT-4, are a product of centralized training. Massive datasets, often sourced from specific regions or languages, direct their learning. While this method has given us incredibly powerful tools, it also raises concerns: potential biases, lacâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT Can Now See, Hear, and Speak.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16s0f0i/chatgpt_can_now_see_hear_and_speak/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16s0f0i/chatgpt_can_now_see_hear_and_speak/"/>
        <updated>2023-09-25T18:50:02.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Innovation for Inclusion: Hack.The.Bias with Amazon SageMaker]]></title>
        <id>aa5ed4b4974d094d2d6a878faf6ecc32569ffd3f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/innovation-for-inclusion-hack-the-bias-with-amazon-sagemaker/"/>
        <updated>2023-09-25T18:20:11.000Z</updated>
        <summary type="html"><![CDATA[This post was co-authored with Daniele Chiappalupi, participant of the AWS student Hackathon team at ETH ZÃ¼rich. Everyone can easily get started with machine learning (ML) using Amazon SageMaker JumpStart. In this post, we show you how a university Hackathon team used SageMaker JumpStart to quickly build an application that helps users identify and remove [â€¦]]]></summary>
        <author>
            <name>Jun Zhang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AI therapy]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rzlqa/p_ai_therapy/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rzlqa/p_ai_therapy/"/>
        <updated>2023-09-25T18:18:26.000Z</updated>
        <summary type="html"><![CDATA[Hey, I'm the creator of MindMateGPT, an AI therapist that has helped a lot of people with emotional/social issues.
 It's not meant to replace humans, but it is a very useful augment as a daily therapy tool.
 It will refer you to a human if you have a very serious issue, but it provides a bunch of tools and coping tactics for every day emotional issues.
 Try it out! It's 100% free.
    submitted by    /u/MindMateGPT  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are your go-to resources for the latest on neural networks and the world of neuroscience, AI, LLMs, and ML?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16rzi6k/what_are_your_goto_resources_for_the_latest_on/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16rzi6k/what_are_your_goto_resources_for_the_latest_on/"/>
        <updated>2023-09-25T18:14:38.000Z</updated>
        <summary type="html"><![CDATA[Hello! I am a software engineer (4 yoe) working in full stack web and app development.
 I was a neuroscience researcher until I switched to software dev and now I am transitioning back into the intersection of neuro and software. 
 I am looking for reputable, vetted, and comprehensive lectures, talks, resources on LLMs and the like. I am having trouble sifting through the surface level pop sci type resources floating around on the internet. 
 Iâ€™m NOT looking for the 10min everything you need to know about AI type talks. 
 Thanks in advance!
    submitted by    /u/yosoylatte  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are your go-to resources on the most up to date research on AI/ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rzf36/d_what_are_your_goto_resources_on_the_most_up_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rzf36/d_what_are_your_goto_resources_on_the_most_up_to/"/>
        <updated>2023-09-25T18:11:11.000Z</updated>
        <summary type="html"><![CDATA[Hello! I am a software engineer (4 yoe) working in full stack web and app development, transitioning to LLMs/ AI/ ML. My background includes extensive research in neuroscience so I am most comfortable w academic or comprehensive lectures. 
 I am looking for reputable and vetted lectures, talks, resources on ML and the like. 
 I am having trouble sifting through the surface level pop sci type resources floating around on the internet. Iâ€™m NOT looking for the 10min everything you need to know about AI type talks. 
 Thanks in advance!
    submitted by    /u/yosoylatte  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve throughput performance of Llama 2 models using Amazon SageMaker]]></title>
        <id>15f7ff093827a9976e8e4feffb8a0ccf7b89c7d2</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/improve-throughput-performance-of-llama-2-models-using-amazon-sagemaker/"/>
        <updated>2023-09-25T18:10:40.000Z</updated>
        <summary type="html"><![CDATA[Weâ€™re at an exciting inflection point in the widespread adoption of machine learning (ML), and we believe most customer experiences and applications will be reinvented with generative AI. Generative AI can create new content and ideas, including conversations, stories, images, videos, and music. Like most AI, generative AI is powered by ML modelsâ€”very large models [â€¦]]]></summary>
        <author>
            <name>Gagan Singh</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In fraud detection for e-commerce: How does anomaly detection fit in and what are the key approaches?]]></title>
        <id>https://www.datasciencecentral.com/?p=63218</id>
        <link href="https://www.datasciencecentral.com/in-fraud-detection-for-e-commerce-how-does-anomaly-detection-fit-in-and-what-are-the-key-approaches/"/>
        <updated>2023-09-25T17:35:00.000Z</updated>
        <summary type="html"><![CDATA[E-commerce has improved technology and convenience for consumers globally. Fraud is a problem in e-commerce. Merchants and platforms fight fraud to protect their businesses and customers. Anomaly detection is a powerful tool for identifying irregular patterns and potential fraud. This article explores how anomaly detection is used in fraud detection for e-commerce and discusses differentâ€¦Â Read More Â»In fraud detection for e-commerce: How does anomaly detection fit in and what are the key approaches?
The post In fraud detection for e-commerce: How does anomaly detection fit in and what are the key approaches? appeared first on Data Science Central.]]></summary>
        <author>
            <name>John Lee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Update on Candle, a minimalist ML framework in Rust]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rydfz/p_update_on_candle_a_minimalist_ml_framework_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rydfz/p_update_on_candle_a_minimalist_ml_framework_in/"/>
        <updated>2023-09-25T17:30:28.000Z</updated>
        <summary type="html"><![CDATA[this is mostly a cross-post from r/rust as my initial attempt failed because of some external links
 We've first announced Candle, a minimalist ML framework in Rust 6 weeks ago. Since then we've focused on adding various recent models and improved the framework so as to support the necessary features in an efficient way. You can checkout a gallery of the examples, supported models include:
  
Large language models: LLaMA, LLaMA v2, Falcon, Phi-v1.5, StarCoder.
 Quantized models with the llama.cpp approach: LLaMA, T5, Phi-v1.5.
 Image generation: Stable Diffusion, Wuerstchen.
 Computer Vision: DINOv2, yolo-v3, yolo-v8, Segment-Anything Model.
 Text-to-speech: Whisper.
  
One of the big upside of the pure Rust approach is that models can run directly in the browser using WASM, these can be accessed through this collection, you can try out Yolo, Whisper, Segment-Anything, T5, Llama2-c from your web browser.
 Finally, in order to present a use case where Candle has unique capabilities, we've built a quantized version of the recently released Phi-v1.5 LLM. You can try it out with the following command, this uses a q4k quantized model resulting in very fast inference on CPU while still producing pretty nice texts.
 $ cargo run --example phi --release -- \ --prompt "Explain how to find the median of an array and write the corresponding python function.\nAnswer:" \ --quantized --sample-len 200 Explain how to find the median of an array and write the corresponding python function. Answer: The median is the middle value in an array. If the array has an even number of elements, the median is the average of the two middle values. def median(arr): arr.sort() n = len(arr) if n % 2 == 0: return (arr[n//2 - 1] + arr[n//2]) / 2 else: return arr[n//2] 
 It's still very early days for Candle so please let us know if you start using it and run into some rough edges. We look forward to Rust getting a lot more usage in the ML space!
    submitted by    /u/l-m-z  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why is FastGAN considered a simple GAN architecture?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ryc23/d_why_is_fastgan_considered_a_simple_gan/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ryc23/d_why_is_fastgan_considered_a_simple_gan/"/>
        <updated>2023-09-25T17:29:05.000Z</updated>
        <summary type="html"><![CDATA[Hi, I'm reading this GAN paper which introduces a faster and simpler GAN architecture for creating high resolution images: FastGAN paper
 The authors claim the computational advantage of their approach is partly the simple architecture. 
 Looking at figure 3 and 4, I cannot quite see why this is considered a simpler architecture to older GANs,say DCGAN. I get the technical argument that the generator only has one convolutional layer, but figure 3 has lots of up sampling layers, and the discriminator has plenty of downsampling layers. So why is this considered a simple GAN model? 
 Any pointers appreciated.
    submitted by    /u/treetop-600  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The essential guide on data security and privacy in web localization]]></title>
        <id>https://www.datasciencecentral.com/?p=63236</id>
        <link href="https://www.datasciencecentral.com/the-essential-guide-on-data-security-and-privacy-in-web-localization/"/>
        <updated>2023-09-25T16:44:12.000Z</updated>
        <summary type="html"><![CDATA[Thanks to the internet, you can now easily expand your reach and engage with diverse audiences wherever they are. However, this opportunity raises an important question: how can you localize your web content and maintain the security and privacy of sensitive data? This article comprehensively explores the best practices that will help you maintain dataâ€¦Â Read More Â»The essential guide on data security and privacy in web localization
The post The essential guide on data security and privacy in web localization appeared first on Data Science Central.]]></summary>
        <author>
            <name>Nakivo Backup &amp;#38; Replication</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made series of scripts (with help of chatgpt) that allows llama to make "live" videos (more added to the loop the longer the broadcast goes on)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rwgmn/i_made_series_of_scripts_with_help_of_chatgpt/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rwgmn/i_made_series_of_scripts_with_help_of_chatgpt/"/>
        <updated>2023-09-25T16:17:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aluode  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How AI growth has triggered data center redesign]]></title>
        <id>https://www.datasciencecentral.com/?p=63210</id>
        <link href="https://www.datasciencecentral.com/how-ai-growth-has-triggered-data-center-redesign/"/>
        <updated>2023-09-25T16:08:31.000Z</updated>
        <summary type="html"><![CDATA[A major aspect of ongoing data center redesign is due to AIâ€™s massive, complex workloads and the need to add many more graphic processing units (GPUs), tensor processing units (TPUs) or accelerators to the mix. The power these units require and the heat the units generate have forced designers to rethink what constitutes a feasibleâ€¦Â Read More Â»How AI growth has triggered data center redesign
The post How AI growth has triggered data center redesign appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoGen: Enabling next-generation large language model applications]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=969759</id>
        <link href="https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/"/>
        <updated>2023-09-25T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Microsoft researchers are introducing AutoGen, a framework for simplifying the orchestration, optimization, and automation of workflows for large language model (LLM) applicationsâ€”potentially transforming and extending what LLMs can do.
The post AutoGen: Enabling next-generation large language model applications appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] User Intent Extraction: Our Journey with Infra and LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rvbrh/d_user_intent_extraction_our_journey_with_infra/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rvbrh/d_user_intent_extraction_our_journey_with_infra/"/>
        <updated>2023-09-25T15:33:40.000Z</updated>
        <summary type="html"><![CDATA[I just wrapped up a new blog post about our experience working with LLMs while developing InfraCopilot. We're using both GPT4 and GPT3.5-turbo(16k) differently compared to what others are doing. We found a few new LLM tricks like dynamic examples and automated e2e intent parsing testing that I think are applicable to many other teams.
 I'd love to hear what everyone thinks, have you tried using dynamic examples while working with LLMs?
    submitted by    /u/lothamer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Six Steps Toward AI Security]]></title>
        <id>https://blogs.nvidia.com/?p=67098</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/25/ai-security-steps/"/>
        <updated>2023-09-25T15:00:22.000Z</updated>
        <summary type="html"><![CDATA[In the wake of ChatGPT, every company is trying to figure out its AI strategy, work that quickly raises the question: What about security? Some may feel overwhelmed at the prospect of securing new technology. The good news is policies and practices in place today provide excellent starting points. Indeed, the way forward lies in Read article >]]></summary>
        <author>
            <name>David Reber Jr.</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chinaâ€™s AI boom depends on an army of exploited student interns]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rtqqy/chinas_ai_boom_depends_on_an_army_of_exploited/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rtqqy/chinas_ai_boom_depends_on_an_army_of_exploited/"/>
        <updated>2023-09-25T14:32:24.000Z</updated>
        <summary type="html"><![CDATA[China's AI industry relies on student interns who work as data annotators, performing crucial tasks to train machine learning models.
 
These interns, recruited from vocational schools, face poor working conditions and subminimum wages.
 
Recent regulations require employers to pay interns minimum wage and prohibit schools from assigning repetitive work.
 
Tech giants like Baidu have partnered with vocational schools to create data annotation internships in less-developed regions, backed by local governments.
 
The exploitation of student interns in China's AI industry raises concerns about labor rights and fair compensation.
 
 Source : https://restofworld.org/2023/china-ai-student-labor/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some art I came up with, first time ever doing something like this, hope you all can enjoy]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rtelb/some_art_i_came_up_with_first_time_ever_doing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rtelb/some_art_i_came_up_with_first_time_ever_doing/"/>
        <updated>2023-09-25T14:19:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ApprehensiveChair460  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence introduction for Highschool students?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rt6rm/artificial_intelligence_introduction_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rt6rm/artificial_intelligence_introduction_for/"/>
        <updated>2023-09-25T14:10:38.000Z</updated>
        <summary type="html"><![CDATA[I'm a researcher and for a "scientific outreach" event I will do a presentation for last year High School students with the subject "Artificial Intelligence and Neuroscience".
 My question is, do you know of a good introduction to the basic concepts of Artificial Intelligence for High School or freshman University level?
 The second part, related to Neuroscience applications, will be very targeted and will deal with contemporary clinical and research use cases.
 Thanks
    submitted by    /u/aendrs  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is Tensorflow dead or heading in that direction ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rs7x9/d_is_tensorflow_dead_or_heading_in_that_direction/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rs7x9/d_is_tensorflow_dead_or_heading_in_that_direction/"/>
        <updated>2023-09-25T13:32:39.000Z</updated>
        <summary type="html"><![CDATA[First of all anyone offended by that question - heartiest apology. I am using it myself profusely at the moment. The reason for me asking this question, over last few weeks / months, I have been gradually educating myself in machine learning using Tensorflow and have been able to train multiple models using only one of the model zoo candidates. All the other pre trained models have failed me so far.
 I went onto Tensorflow official forum / Stackoverflow / Tensorflow github with specific error messages that I am getting on Ubuntu with Nvidia card / Mac M2 and there has been absolute radio silence in response to multiple posts over last month. Found many open issues listed since 2020 on the same line as mine i.e. identical error messages that people have come across.
 Finally after about a month of being on TF forum, I direct messaged an official TF2 dev who kindly responded with answers. I haven't succeeded yet with any of the pre trained model from the official section. Only one model from research section is working so far for me i.e. Faster_rcnn_resnet_50_640x640 ..
 Thus the question. Kindly help me enlighten myself with where is this thing headed. Should I consider switching to Pytorch or some alternative ? If yes what alternatives do you recommend ? TIA
    submitted by    /u/dpadhy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Training/finetuning a LLM]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rrokr/d_trainingfinetuning_a_llm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rrokr/d_trainingfinetuning_a_llm/"/>
        <updated>2023-09-25T13:12:04.000Z</updated>
        <summary type="html"><![CDATA[Hey! Months ago, I was fascinated by Karpathyâ€™s nanoGPT project - the ability to train a small LLM on your text file seemed very interesting to me. I tried training it on my chat history to build some inifinite chat-generator for fun, but unfortunately, the results were bad. Recently I had even worse experiences with newly-released ChatGPT 3.5 fine-tuning.
 Are there any good, simple ways to train/fine-tune LLMs now? I would love something that could train on an Apple M2 processor (like Karpathyâ€™s nanoGPT), or Colab, or cheap API (like ChatGPT fine-tuning).
    submitted by    /u/11igor  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Microsoft Researchers Announce CodePlan: Automating Complex Repo-Level Software Engineering Tasks with AI]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rqo3k/r_microsoft_researchers_announce_codeplan/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rqo3k/r_microsoft_researchers_announce_codeplan/"/>
        <updated>2023-09-25T12:27:33.000Z</updated>
        <summary type="html"><![CDATA[As software projects grow, changing code across entire repositories becomes tedious & error-prone. Tasks like migrating APIs or updating dependencies require complex edits across files. I explored a new approach from Microsoft Research to automate these "repository-level" coding challenges with AI.
 Their new paper proposes CodePlan - an AI system that breaks repository tasks into incremental steps guided by planning & analysis.
 Key points:
  
Uses LLMs like GPT-3 for localized code edits
 Maintains validity across repository via incremental analysis
 Adaptively plans multi-step changes based on code dependencies
 Significantly outperformed baselines on API migration & temporal edits
 Automated tasks across 168 file C# codebase
 2-3x more accurate edit locations than baselines
 Produced final valid codebases, unlike reactive approaches
  
The core insight is combining LLM strengths with rigorous planning based on dependency analysis. This automates interdependent code changes that naive LLM use struggles with (I personally have these kinds of issues all the time with GPT4 - lack of context about the entirety of the repo/how files fit together).
 I think CodePlan demonstrates AI can expand beyond small coding assists into large-scale engineering tasks. Planning + LLMs > LLMs alone. This could really improve productivity and code quality... at least for me :)
 Full summary. Arxiv paper: https://arxiv.org/pdf/2309.12499.pdf
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gen Z AI leaders?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rpxlp/gen_z_ai_leaders/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rpxlp/gen_z_ai_leaders/"/>
        <updated>2023-09-25T11:53:42.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m looking for some maybe less-known leaders in the space out on a mission to change something they care about- business, environmental, social, doesnt matter.
 I dont want it to be US-centric, so it would be nice to find some people all over. 
 Feel free to send me links to articles ðŸ§¨
    submitted by    /u/Johnny_Whoop  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Distillation understanding]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rpi6b/d_distillation_understanding/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rpi6b/d_distillation_understanding/"/>
        <updated>2023-09-25T11:32:21.000Z</updated>
        <summary type="html"><![CDATA[In the main scenario, the smaller model learns from the same data as the bigger model and also from the predictions of the bigger model and incorporate the 2 output labels on a specific loss.
 Basically, it is equivalent to say to the smaller model : "be careful this example is hard" in the case that big model divergence from true output? I am missing something?
    submitted by    /u/Grumlyly  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Baby Sleep Tracker using a basic SVM [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rp6tc/baby_sleep_tracker_using_a_basic_svm_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rp6tc/baby_sleep_tracker_using_a_basic_svm_p/"/>
        <updated>2023-09-25T11:16:10.000Z</updated>
        <summary type="html"><![CDATA[I made a FOSS baby sleep tracking system. The system tracks wake/sleep status, and informs the user when their baby is likely to need a nap next. But it stopped working as soon as my baby started sleeping on his stomach, and started using blankets. The original version relied on anatomical features being visible.
 This version delivers the ability to train a blank slate SVM binary classifier on pictures of a user's baby, making it extremely biased (and resilient) to the custom behaviors the user's baby exhibits (blanket covering baby, teddy bear/other objects in crib, etc.). All generated data stays on your machine, nothing leaves the LAN.
 Video: https://youtu.be/8i0wHA_knKc?si=uhA4PpOYP0jMKLz1
 For obvious reasons I didn't have a dataset of babies sleeping.. so I wrapped a python/flask service with a React app which lets a user press a button to train the model w/ a new image from the camera's live stream. Then this model is invoked over time (+ other heuristics) to determine whether your baby is present and sleeping.
 I believe it works better than $300+ systems sold on the market, open sourced it: https://github.com/calebolson123/BabySleepCoach
 â€‹
 I'm thinking a fun next step for this project could be to apply privateGPT on the feature-engineered sleep records for a true "Sleep Coach"
    submitted by    /u/GoochCommander  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does granger causality work for time series with different frequencies]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16roy7g/d_does_granger_causality_work_for_time_series/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16roy7g/d_does_granger_causality_work_for_time_series/"/>
        <updated>2023-09-25T11:04:05.000Z</updated>
        <summary type="html"><![CDATA[Is there a Granger test where series are a quarterly one and a weekly one?
    submitted by    /u/Pineapple_throw_105  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT-4 and Claude on what the most important things to do in AI in the coming months are]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ro9up/chatgpt4_and_claude_on_what_the_most_important/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ro9up/chatgpt4_and_claude_on_what_the_most_important/"/>
        <updated>2023-09-25T10:27:43.000Z</updated>
        <summary type="html"><![CDATA[ChatGPT-4 
 Given the rapid advancements in the field of AI, the next few months are going to be pivotal. One of the pressing priorities is addressing ethical concerns. As algorithms become more pervasive, making sure they don't perpetuate biases or make unethical decisions is critical. That's not just a "good to have"; it's foundational. We can't build trust in AI systems unless we tackle this head-on.
 Another biggie is interpretability. Black-box models are potent, sure, but they're not very useful if no one can understand how they're making decisions. This has real-world implicationsâ€”think medical diagnoses or judicial systems. So, there's a strong push towards making AI more transparent and understandable, not just for experts but for everyone.
 Computational efficiency is also high oâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top Artificial Intelligence Companies In Bangalore, India]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ro9iq/top_artificial_intelligence_companies_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ro9iq/top_artificial_intelligence_companies_in/"/>
        <updated>2023-09-25T10:27:10.000Z</updated>
        <summary type="html"><![CDATA[Bangalore, often referred to as the Silicon Valley of India, has established itself as a global hub for technology and innovation. With a thriving ecosystem of startups, research institutions, and multinational corporations, the city has become a hotbed for artificial intelligence (AI) development. here explore the top AI companies in Bangalore, highlighting their contributions to the field of artificial intelligence and their impact on various industries.
 Read full article - Top Artificial Intelligence Companies In Bangalore
    submitted by    /u/Techasoft16  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any AI that can take information out from Twitter/ X ?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ro866/any_ai_that_can_take_information_out_from_twitter/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ro866/any_ai_that_can_take_information_out_from_twitter/"/>
        <updated>2023-09-25T10:25:03.000Z</updated>
        <summary type="html"><![CDATA[I am searching for an AI that can scrape some Twitter profiles and make a daily recap about what they were talking about. 
 Anything pops? Thanks a lot!
    submitted by    /u/Alternative_Pea_4246  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What will be sub-categories for AI with biggest potential in 2050?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rnsz8/what_will_be_subcategories_for_ai_with_biggest/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rnsz8/what_will_be_subcategories_for_ai_with_biggest/"/>
        <updated>2023-09-25T10:00:07.000Z</updated>
        <summary type="html"><![CDATA[Examples: AI in Finance, etc-
    submitted by    /u/premonial  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Seeking Insights on AI Security Challenges: Short Survey]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rmla9/r_seeking_insights_on_ai_security_challenges/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rmla9/r_seeking_insights_on_ai_security_challenges/"/>
        <updated>2023-09-25T08:43:20.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I'm conducting a research survey on the challenges and gaps in AI security. Given the expertise in this community, I believe your feedback would be invaluable in shaping the future of AI security solutions.
 The survey takes less than 10 minutes and delves into current practices, perceptions, and needs related to AI security. If you have experience or insights in this area, I would greatly appreciate your participation.
 Survey Link: https://forms.gle/i9AefyL8izyt9QjX6
 All responses will remain anonymous, and the collected data will only be used for research purposes. Additionally, if you're open to a deeper discussion on this topic, there's an option within the survey to indicate your interest.
 Thank you in advance for your time and insights! If you have any questions or additional thoughts, please don't hesitate to comment below.
    submitted by    /u/Agile_Temperature678  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I created an AI girlfriend and gave her a bodyâ€¦ for fun obviously..]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rltql/i_created_an_ai_girlfriend_and_gave_her_a_body/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rltql/i_created_an_ai_girlfriend_and_gave_her_a_body/"/>
        <updated>2023-09-25T07:54:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/spaceecon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anthropic is pulling an OpenAI-style 49% deal but with Amazon? ðŸ¤¯]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rlt8m/anthropic_is_pulling_an_openaistyle_49_deal_but/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rlt8m/anthropic_is_pulling_an_openaistyle_49_deal_but/"/>
        <updated>2023-09-25T07:53:35.000Z</updated>
        <summary type="html"><![CDATA[https://twitter.com/AnthropicAI/status/1706202966238318670
 https://preview.redd.it/hiymp9ctxcqb1.png?width=735&format=png&auto=webp&s=20cb3886710ee9a2a552b0fc881b8c96c0fc9208
    submitted by    /u/ShooBum-T  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT can now see, hear, and speak]]></title>
        <id>https://openai.com/blog/chatgpt-can-now-see-hear-and-speak</id>
        <link href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak"/>
        <updated>2023-09-25T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what youâ€™re talking about.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPT-4V(ision) system card]]></title>
        <id>https://openai.com/research/gpt-4v-system-card</id>
        <link href="https://openai.com/research/gpt-4v-system-card"/>
        <updated>2023-09-25T07:00:00.000Z</updated>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does DDIM work?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rjqqi/d_how_does_ddim_work/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rjqqi/d_how_does_ddim_work/"/>
        <updated>2023-09-25T05:45:35.000Z</updated>
        <summary type="html"><![CDATA[The Wikipedia page on Diffusion Models has been pretty minimal for an entire year. I feel like it should be fixed, so I fixed it finally. It strikes me odd that such a hot topic has such atrociously bad Wikipedia. I feel duty-bound to educate the near-future AI, since they'll be reading Wikipedia for the next few years at least.
 Currently I think it's mostly complete, but I still don't understand the mathematical details of DDIM (I tried reading the paper and could not understand it), or generally how it is possible to sample without noise. This is a serious problem since as far as I see most of practical diffusion models use deterministic sampling, and they are all based on the same principle as DDIM.
 If anyone could explain simply what DDIM is really doing that would be great. I understand part of the paper: that they constructed an entire family of distributions over trajectories that has the same two-point marginals.
 I also haven't got much in the section on Examples. If you think there are some interesting examples of Diffusion Models, please comment below.
    submitted by    /u/furrypony2718  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[package delivery environment for OpenAI GYM]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16rj60f/package_delivery_environment_for_openai_gym/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16rj60f/package_delivery_environment_for_openai_gym/"/>
        <updated>2023-09-25T05:11:37.000Z</updated>
        <summary type="html"><![CDATA[hi everyone, iâ€™m working on a project in which i get a list of orders (id,delivery coordinates, delivery deadline). i need to deliver all packages while putting in consideration clients priority and taking least time and distance as possible. so the goal is to sort the orders to achieve the target. what would be the action / observation space for such environment? and how can i define the step method to perform this scenario?
    submitted by    /u/overflow74  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How has work changed for you given the shift from growth to profitability?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rib9k/d_how_has_work_changed_for_you_given_the_shift/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rib9k/d_how_has_work_changed_for_you_given_the_shift/"/>
        <updated>2023-09-25T04:24:10.000Z</updated>
        <summary type="html"><![CDATA[For the data scientists/applied scientists/research scientists - What kind of projects are you working on now that the economy has shifted and companies are focusing more on profitability than on growth? 
 What techniques have worked for you and what are you looking into as potential solutions?
 An example would be - optimizing your marketing campaign spend in channels that give you the most bang for your buck vs just spending arbitrarily to acquire new users.
    submitted by    /u/Terrible-Hamster-342  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] LEAP Hand: Low-Cost (<2KUSD), Anthropomorphic, Multi-fingered Hand -- Easy to Build (link in comments)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rg533/r_leap_hand_lowcost_2kusd_anthropomorphic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rg533/r_leap_hand_lowcost_2kusd_anthropomorphic/"/>
        <updated>2023-09-25T02:33:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pathak22  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why my graph go down when train a saved model]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16rfk7r/why_my_graph_go_down_when_train_a_saved_model/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16rfk7r/why_my_graph_go_down_when_train_a_saved_model/"/>
        <updated>2023-09-25T02:05:06.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m new to ML and RL, and Iâ€™m building a small piece of code using gymnasium to be able to use mujoco. Specifically, I use Mujocoâ€™s Humanoid, here is my code (https://github.com/NghiaPhamttk27/Humanoid).I use some algorithms in RL like SAC, TD3, A2C from stable\_baselines3. After every 25000 TIMESTEPS, I will save my model in the models folder. When I train continuously, everything goes well, I can see it on the tensorboard graph. But when I continue training a model that has been trained, the graph suddenly drops. In the image below you can see that I retrain the models at SAC\_75000 and SAC\_100000 and SAC\_125000. The graph continuously decreases at those thresholds.I think something happended Can you give me a reason or share something with me? Thanks in advance.
 You can see graph of SAC goes down when 76k, 100k, and 125k timeSteps
    submitted by    /u/Nghiattk27  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best RL package?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16rec1a/best_rl_package/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16rec1a/best_rl_package/"/>
        <updated>2023-09-25T01:04:51.000Z</updated>
        <summary type="html"><![CDATA[Am starting out working on an RL problem and am wondering what people generally use to implement the algorithms? 
 Iâ€™ll need to build a custom environment, but I assume I can subclass something from Gym.
    submitted by    /u/suds_65  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] OpenGL-based inference engine]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rd9et/p_openglbased_inference_engine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rd9et/p_openglbased_inference_engine/"/>
        <updated>2023-09-25T00:15:20.000Z</updated>
        <summary type="html"><![CDATA[I created an OpenGL/OpenGLES based inference framework a while back which is rather GPU-agnostic and might be a good option for distributing multi-platform ML solutions for platforms ranging from Android over desktop to WebGL(2). Quite recently I added support for LLMs to that (restricted to 4-bit quantized Llama models for now). 
 The LLM-enabled fork can be found here (compileable sample code inside). 
 Maybe someone finds this useful. Also looking for collaborators to extend the functionality.
 â€‹
    submitted by    /u/mtnwrw  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ai alignment resources]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rd1vv/ai_alignment_resources/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rd1vv/ai_alignment_resources/"/>
        <updated>2023-09-25T00:05:26.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m looking for subreddits and resources in general for Ai alignment. I recently read Life 3.0 by Max Tegmark and The Alignment Problem by Brian Christian. I was unaware so much was going on in the space. I am CEO of an AI startup, we have a compassionate AI. Most of the alignment focus out there is on superintelligence, little out there focuses on building modern day Ai that benefits humans. Thatâ€™s been my focus for 17 years when I dreamed up what was possible and sent a proposal to executives at the mfaang corporation I was working at. I found out back then business doesnâ€™t give a rats ass about Ai that actively makes its users lifeâ€™s better. Well now I have proof that customers care. Iâ€™m hoping to find some resources on Ai alignment groups, conferences, more books etc. Iâ€™ll need to hire skilled ml engineers who do give a rats ass. Compassionate Ai isnâ€™t just feel good tech. It would be good to do some networking. AI alignment doesnâ€™t have to wait for AGI and super intelligence. It can and should be a focus today. My startup is proving it. Hoping to find a conference to network and share our research.
    submitted by    /u/xyz_TrashMan_zyx  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I create a small pytorch utility to Import custom dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rcmsw/p_i_create_a_small_pytorch_utility_to_import/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rcmsw/p_i_create_a_small_pytorch_utility_to_import/"/>
        <updated>2023-09-24T23:46:27.000Z</updated>
        <summary type="html"><![CDATA[Hi guys , TorchClassifierData is A small pytorch utility to Import, Split ,Normalize and Visualize custom dataset for classification tasks. wich is indispensable for real word problem .
 You can find a full notebook that use TorchClassifierData to train a classifier on this kaggle dataset here.
 The code source is avalaible on my github. Thank you. 
    submitted by    /u/charles_data_dev  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for some good github project that offers the chancee to translate a video spoken in a language into another language.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16rcfhu/looking_for_some_good_github_project_that_offers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16rcfhu/looking_for_some_good_github_project_that_offers/"/>
        <updated>2023-09-24T23:37:13.000Z</updated>
        <summary type="html"><![CDATA[Hello to everyone.
 I'm lookin for some good AI github project to convert the language spoken in a video to a different language,like heygen / labs / video translate,but free. I mean,I can't afford prices like those and I think a few of us can,but I'm sure that the developers at heygen took some project born and hosted on some github and they have improved it. Would someone share some of those github projects ? thanks.
    submitted by    /u/loziomario  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why do Diffusion models work so well while SG-MCMC does not?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rbp9t/d_why_do_diffusion_models_work_so_well_while/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rbp9t/d_why_do_diffusion_models_work_so_well_while/"/>
        <updated>2023-09-24T23:03:52.000Z</updated>
        <summary type="html"><![CDATA[Diffusion models are basically Langevin sampling. What are the key differences and tricks that set them apart from Langevin dynamics? Why do they work so well while very similar sampling methods don't?
    submitted by    /u/Dangerous-Flan-6581  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] In ML, a PhD gives you a 10-year head start over weekend warriors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16rbg7g/d_in_ml_a_phd_gives_you_a_10year_head_start_over/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16rbg7g/d_in_ml_a_phd_gives_you_a_10year_head_start_over/"/>
        <updated>2023-09-24T22:53:21.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/cczbhu367aqb1.png?width=1600&format=png&auto=webp&s=f1761911d7ce3bbefaef43774b5d60f638886893
 ML is often portrayed as a magical field where anyone with a laptop and Python skills can build amazing AI systems. 
 The reality is less democratic: mastering ML requires gritty, systematic work best learned through formal training. You need rock solid foundations in math, programming, and core conceptsâ€”skills acquired through advanced education, which (almost always) is beyond self-taught hackers. 
 Most think a PhD is unnecessary, but the reality is that advanced degrees provide the deepest training. Patience and persistence do matter, but a PhD gives you a 10-year head start over weekend warriors. The hype overlooks the work and education needed to excel. This article has a great blueprint of all the required skills to become a ML Engineer (in the CV field).
    submitted by    /u/btcmx  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Offer From Bug 4 VS Startup]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ra2im/d_offer_from_bug_4_vs_startup/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ra2im/d_offer_from_bug_4_vs_startup/"/>
        <updated>2023-09-24T21:56:41.000Z</updated>
        <summary type="html"><![CDATA[So briefly about my current experience, I graduated 2 years ago with a bachelor in data science and I have 2-3 years of experience as a data scientist/ml engineer/software engineer.
 So Iâ€™ve got competing offers, one from the big 4 accounting firms as a software systems engineer - AI/ML (Big 4) and the other as a machine learning engineer. The startup salary is higher while big 4 is lower. Additionally the startup isnâ€™t necessarily a unicorn itâ€™s a relatively small startup with an interesting product but it doesnâ€™t necessarily blow me away. The salary at the startup is 15 percent higher that that of the big 4 offer. For those wondering I did already negotiate the salary and they did increase it marginally. 
 I am conflicted because I think that the big 4 jobs will have
 1) more career growth 2) more potential future opportunities and 3) more networking potential
 Is this an accurate assessment? Which is the best job to take for maximum future potential?
    submitted by    /u/zorenum  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Hardware Resources for training SwinBert]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ra2hf/p_hardware_resources_for_training_swinbert/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ra2hf/p_hardware_resources_for_training_swinbert/"/>
        <updated>2023-09-24T21:56:39.000Z</updated>
        <summary type="html"><![CDATA[So I've been thinking of implementing SwinBert for a college project and have been wondering what all resources i would be needing for a computer.
 Any ideas?
    submitted by    /u/Big-Brain_69  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How about an AI-curated website/magazine with the best, latest AI news?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16r9qs1/how_about_an_aicurated_websitemagazine_with_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16r9qs1/how_about_an_aicurated_websitemagazine_with_the/"/>
        <updated>2023-09-24T21:43:51.000Z</updated>
        <summary type="html"><![CDATA[Hi there. This is a wonderful sub, and it's hard to please everyone on SUCH a broad topic... but it's intriguing to think about, as a lifelong magazine fan (and websites of course) - about using AI to filter, craft and create an amazing website/magazine ABOUT AI - well-categorized, maybe also human/editor curated/tweaked, but as fresh as ... well, the artificial sub but out of the Reddit bold/design into something ... well, something I'd read, visit a lot and love to help make work as a writer/editor (on the side of my "real" all-consuming gig of course;-)
 After all, if AI can 'make great code'/create great websites, maybe some folks out there are already trying to make this happen, for ease of information availability and organization. I can picture the departments/sections/categories now - I bet you can too!
 Anyone gone very far down that road yet (maybe the folks already doing the PC Magazines of the world) of organizing the vast fast-moving info beyond the AI Brews, Ben Parr's AI Analyst, etc.? Fun or a life-long journalist and tech geek (but not a coder, gamer etc.) to think about, at least!
    submitted by    /u/barneylerten  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mersenne primes are unsafe]]></title>
        <id>https://www.johndcook.com/blog/?p=209986</id>
        <link href="https://www.johndcook.com/blog/2023/09/24/mersenne-primes-are-unsafe/"/>
        <updated>2023-09-24T20:59:01.000Z</updated>
        <summary type="html"><![CDATA[In the previous post I mentioned that a particular Mersenne prime would be unsuitable for cryptography. In fact, all Mersenne primes are unsuitable for cryptography. A prime number p is called â€œsafeâ€ if p = 2q + 1 where q is also a prime. Safe primes are called safe because p âˆ’ 1 does not [â€¦]
Mersenne primes are unsafe first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Career advice for a mid-level ml engineer(Perception/CV)?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r8bl8/d_career_advice_for_a_midlevel_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r8bl8/d_career_advice_for_a_midlevel_ml/"/>
        <updated>2023-09-24T20:47:46.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™ve been having a bit of an existential crisis as of late and wanted to ask for advice on how to move forward.
 I have a Masterâ€™s in CS with research experience and a few publications applying machine learning in a fairly niche area (So not novel from the ML side). Since graduating, Iâ€™ve worked ~2 years as an ML engineer in small company(Niche area, different than research).
 Iâ€™ve done quite well here and have played a critical role in taking several big greenfield projects to completion. 
 Most of my work is framing problems, understanding whatâ€™s possible with current research, then building the data pipelines, and training models(with small mods here and there). My main worry is that I might be approaching a point where there wonâ€™t be any more problems Iâ€™m capable of solving here. Iâ€™d imagine Iâ€™d hit the same wall at any future company with my current skill set.
 Iâ€™d like to continue working in CV/Graphics/Perception if possible, but Iâ€™d also like to be realistic about the competitiveness of this particular subfield and my general ability.
 Iâ€™ve been trying to up-skill and am struggling to self study MVG by Hartley and Zisserman. Iâ€™m also looking into OMCS to review low-level programming - maybe I can transition to optimizing ml/cv algorithms?
 It hit me recently that I donâ€™t really know what to study/what Iâ€™d benefit from learning. Hence this post. Any advice would be most appreciated!
    submitted by    /u/answersareallyouneed  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Image-to-text web-scraping]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r801s/d_imagetotext_webscraping/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r801s/d_imagetotext_webscraping/"/>
        <updated>2023-09-24T20:35:01.000Z</updated>
        <summary type="html"><![CDATA[I'm curious if anyone has tried pix2struct-large for web-scraping text from wesites.
 If so - how well did it perform?
 If not - is there something else that is considered better to use?
    submitted by    /u/ReddSpark  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Victorian public key cryptography]]></title>
        <id>https://www.johndcook.com/blog/?p=209927</id>
        <link href="https://www.johndcook.com/blog/2023/09/24/victorian-public-key-cryptography/"/>
        <updated>2023-09-24T19:50:07.000Z</updated>
        <summary type="html"><![CDATA[Electronic computers were invented before public key cryptography. Would public key cryptography have been possible before computers? The security of RSA encryption depends on the ratio of the difficulty of factoring relative to the difficulty of multiplication. This ratio was high, maybe higher, before modern computers. Suppose the idea of RSA encryption had occurred to [â€¦]
Victorian public key cryptography first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Where will the demand for AI work be in future?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r6kaf/d_where_will_the_demand_for_ai_work_be_in_future/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r6kaf/d_where_will_the_demand_for_ai_work_be_in_future/"/>
        <updated>2023-09-24T19:36:38.000Z</updated>
        <summary type="html"><![CDATA[Hypothesis:
 Big tech companies are investing vast amounts of money to develop general models on which others will build. They'll develop interfaces to make it easier for others to fine-tune on top of their models. So that there will be less and less of a need for ML engineers that know how to create a deep learning model in Pytorch, and more and more of a need for data engineers that simply plug into pre-trained models. An AI assistant will also be quicker at coding up a more bespoke AI model for a companies needs, guided by data engineers.
 What do people think? Is this a scenario that they think will play out? Where will the demand for AI skills be coming from in the future?
    submitted by    /u/QuintBa  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers announce GPT4Tools: a method for teaching LLMs how to use tools for visual tasks]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16r60bw/researchers_announce_gpt4tools_a_method_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16r60bw/researchers_announce_gpt4tools_a_method_for/"/>
        <updated>2023-09-24T19:13:41.000Z</updated>
        <summary type="html"><![CDATA[LLMs are great with words but can't handle visual tasks like understanding images. Teaching them to use visual tools could make them much more capable.
 A new paper introduces GPT4Tools - a method to efficiently teach existing LLMs to invoke tools for visual tasks without proprietary data.
 My highlights from the paper:
  
Uses ChatGPT as a "teacher" to generate instructional data for other LLMs
 Fine-tunes LLMs like Vicuna on this data using selective weight tuning (keeps base model frozen)
 Allows smaller 13B LLM to match 175B GPT-3.5 on seen tools after tuning
 Data augmentation with negative/context samples was found to be the secret sauce to get this to work
 Can generalize to brand new visual tools in a zero-shot way
  
This is big because it shows we may not need hyper-expensive training of massive models to impart visual capabilities to LLMs. They seem to be generalizable enough that they can be taught to work with images. Some examples shown include counting objects or segmenting items in pictures using other tools.
 With this approach, existing models can be made multi-modal! Pretty cool.
 Full summary. Original paper is here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Researchers announce GPT4Tools: a method for teaching LLMs how to use tools for visual tasks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r5ywj/r_researchers_announce_gpt4tools_a_method_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r5ywj/r_researchers_announce_gpt4tools_a_method_for/"/>
        <updated>2023-09-24T19:11:58.000Z</updated>
        <summary type="html"><![CDATA[LLMs are great with words but can't handle visual tasks like understanding images. Teaching them to use visual tools could make them much more capable.
 A new paper introduces GPT4Tools - a method to efficiently teach existing LLMs to invoke tools for visual tasks without proprietary data.
 My highlights from the paper:
  
Uses ChatGPT as a "teacher" to generate instructional data for other LLMs
 Fine-tunes LLMs like Vicuna on this data using selective weight tuning (keeps base model frozen)
 Allows smaller 13B LLM to match 175B GPT-3.5 on seen tools after tuning
 Data augmentation with negative/context samples was found to be the secret sauce to get this to work
 Can generalize to brand new visual tools in a zero-shot way
  
This is big because it shows we may not need hyper-expensive training of massive models to impart visual capabilities to LLMs. They seems to be generalizable enough that they can be taught to work with images. Some examples shown include counting objects or segmenting items in pictures using other tools.
 With this approach, existing models can be made multi-modal! Pretty cool.
 Full summary. Original paper is here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Tools to gather and collaborate on fine-tuning datasets?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r590k/d_tools_to_gather_and_collaborate_on_finetuning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r590k/d_tools_to_gather_and_collaborate_on_finetuning/"/>
        <updated>2023-09-24T18:42:44.000Z</updated>
        <summary type="html"><![CDATA[Hey all, I run a small team & we are collaborating on a few data sets that we use to fine-tune GPT3.5,
 We are currently using Google Sheets and I'm wondering if there is a tool where we can organize our data preferably with version control
 Any ideas?
    submitted by    /u/zeJaeger  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Colored Point Cloud Completion]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r4mzu/d_colored_point_cloud_completion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r4mzu/d_colored_point_cloud_completion/"/>
        <updated>2023-09-24T18:17:43.000Z</updated>
        <summary type="html"><![CDATA[Hello, I have created point clouds from images using Point-E. Sadly they are very sparse (for example wehn inputting an image of a house, the roof has very few points in it) and I was searching for other Models, that could
  
make the PC more dense and
 predict the color of every point.
  
Point-E outputs xyz and rgb vectors for every point.
 Do some of you have advise for me here?
    submitted by    /u/bySmily  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P]Just published my second blog on medium about feature scaling in machine learning please have a look]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r4khr/pjust_published_my_second_blog_on_medium_about/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r4khr/pjust_published_my_second_blog_on_medium_about/"/>
        <updated>2023-09-24T18:15:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/indusop  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How much energy does AI use compared to humans?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16r4iy6/how_much_energy_does_ai_use_compared_to_humans/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16r4iy6/how_much_energy_does_ai_use_compared_to_humans/"/>
        <updated>2023-09-24T18:13:16.000Z</updated>
        <summary type="html"><![CDATA[A recent paper challenges assumptions about the energy use of AI models, finding that AI systems emit significantly fewer carbon dioxide equivalents (CO2e) compared to humans when producing text or images.
 
The authors emphasize the importance of measuring carbon emissions from AI activities to inform sustainability policies.
 
The ongoing debate among AI researchers highlights the challenges of accounting for the interactions between climate, society, and technology.
 
 Source : https://venturebeat.com/ai/how-much-energy-does-ai-use-compared-to-humans-surprising-study-ignites-controversy/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swedish gaming company replaces half its staff with AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16r41l9/swedish_gaming_company_replaces_half_its_staff/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16r41l9/swedish_gaming_company_replaces_half_its_staff/"/>
        <updated>2023-09-24T17:53:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SpaceDetective  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][P] how to create a 3D gymnasium environment for mujoco env?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16r3k1o/dp_how_to_create_a_3d_gymnasium_environment_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16r3k1o/dp_how_to_create_a_3d_gymnasium_environment_for/"/>
        <updated>2023-09-24T17:33:54.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][P] how to create a 3D gymnasium environment for mujoco env?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r3j0m/dp_how_to_create_a_3d_gymnasium_environment_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r3j0m/dp_how_to_create_a_3d_gymnasium_environment_for/"/>
        <updated>2023-09-24T17:32:40.000Z</updated>
        <summary type="html"><![CDATA[Hi
 I'm a student and working on a RL project for the university and need some guidance.
 I have created a 3d model with mujoco (I have the xml file) how do I create an environment in gymnasium with this xml file?
 for the sake of an example let's say I have the xml file of the humanoid model how do I load this in gymnasium so that I could train it to walk? (this is just an example because the current project is harder to explain, but will use the humanoid model in the project)
 or is the approach that I'm trying is not appropriate at all? I came across this stackoverflow post where they say mujoco is itself good for this but was hard for me to understand due to lack of examples.
 would really appreciate some advice and guidance 
 thank you.
    submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made a social network where bots generate fake news - Based on GPT4 and Dalle2. Looking for feedback and potential improvements for this weird experiment.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16r2c8i/i_made_a_social_network_where_bots_generate_fake/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16r2c8i/i_made_a_social_network_where_bots_generate_fake/"/>
        <updated>2023-09-24T16:43:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Gmoi6  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What AI can accurately recreate an existing cartoon character in the style of the original character?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16r16pu/what_ai_can_accurately_recreate_an_existing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16r16pu/what_ai_can_accurately_recreate_an_existing/"/>
        <updated>2023-09-24T15:57:28.000Z</updated>
        <summary type="html"><![CDATA[I've recently started working on a project to create an entire episode of a cartoon show that stopped airing a while back. I've run into some trouble though in finding a program that can accurately recreate the character. Does anyone know what app or website I could use for this, or am I coming at this all wrong?
    submitted by    /u/therabbitinthehat2  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Made a simple semantic segmentation annotation tool with segment-anything masks support in PyQt5]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16r0pa5/p_made_a_simple_semantic_segmentation_annotation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16r0pa5/p_made_a_simple_semantic_segmentation_annotation/"/>
        <updated>2023-09-24T15:37:32.000Z</updated>
        <summary type="html"><![CDATA[I just open-sourced (MIT License) semantic segmentation annotation tool powered by segment-anything model that I used for a while in my projects. Hopefully it will help someone as it seems to me that it is more suitable for small projects than popular huge web based annotation tools.
 Link to the project: SAMAT (any feedback in Discussions section on GitHub is appreciated)
 Features:
  
Brush annotation (opposed to polygons)
 Magic Wand (like in Photoshop) powered by segment-anything masks (it is optional, if you donâ€™t have cool GPU to prepare masks)
  
samat showcase
 Why yet another annotation tool?
 Before starting this project I tried supervisely, segments.ai, roboflow and several others, but found them not convenient for my tasks.
 Their cons, I tried to fix with my tool:
  
Latency: they are web based, hence burden with irritating latency during labeling (SAMAT is snappy because it is a local desktop app)
 Complicated: too much features, hence overloaded UI (SAMAT is just a colored brush)
  
P.S. there is another labeling tool called SALT on github which also uses segment-anything model, but it follows different approach to UI/UX, may be it will be more suitable for you, so take a look at it too.
    submitted by    /u/Divelix  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Generative AI in Mafia-like game simulation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qztf4/r_generative_ai_in_mafialike_game_simulation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qztf4/r_generative_ai_in_mafialike_game_simulation/"/>
        <updated>2023-09-24T15:00:45.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.11672
 Abstract: In this research, we explore the efficacy and potential of Generative AI models, specifically focusing on their application in role-playing simulations exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's advanced capabilities, the study aimed to showcase the model's potential in understanding, decision-making, and interaction during game scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses. However, challenges such as the model;s limitations in bluffing and predicting opponent moves emerged. Reflections on game development, fiâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qzt8j/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qzt8j/d_simple_questions_thread/"/>
        <updated>2023-09-24T15:00:32.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help with understanding optimal policy and values]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16qzsku/help_with_understanding_optimal_policy_and_values/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16qzsku/help_with_understanding_optimal_policy_and_values/"/>
        <updated>2023-09-24T14:59:50.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 slide as part of the presentation.
 I've listened to the entire lecture and am now going through these slides and I didn't understand the solution provided for the choices above in the image.
 So far I understand it like this. Noise basically means that the agent will not always do the action that you say. You say left and it will go left only 50% of the time. it will choose other actions randomly.
 so, Why is the answer to the (c) and (d) questions (2) and (3) respectively?
 I understand the learning rate but not how risking the cliff is affected by the noise.
    submitted by    /u/vestedpolecat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trolling chatbots with made-up memes]]></title>
        <id>650a0f44ea79ba0001ba8a07</id>
        <link href="https://www.aiweirdness.com/trolling-chatbots-with-made-up-memes/"/>
        <updated>2023-09-24T13:13:44.000Z</updated>
        <summary type="html"><![CDATA[ChatGPT, Bard, GPT-4, and the like are often pitched as ways to retrieve information. The problem is they'll "retrieve" whatever you ask for, whether or not it exists.
Tumblr user @indigofoxpaws sent me a few screenshots where they'd asked ChatGPT for an explanation of]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonus: These memes do not exist]]></title>
        <id>650fbea407cccb000136ab7d</id>
        <link href="https://www.aiweirdness.com/bonus-these-memes-do-not-exist/"/>
        <updated>2023-09-24T13:13:22.000Z</updated>
        <summary type="html"><![CDATA[AI Weirdness: the strange side of machine learning]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Pt. 1) Inductive Logic Programming with LNN's]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16qxeud/pt_1_inductive_logic_programming_with_lnns/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16qxeud/pt_1_inductive_logic_programming_with_lnns/"/>
        <updated>2023-09-24T13:10:33.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] insights on the arsenal tool, AI Security]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qvw6d/d_insights_on_the_arsenal_tool_ai_security/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qvw6d/d_insights_on_the_arsenal_tool_ai_security/"/>
        <updated>2023-09-24T11:54:05.000Z</updated>
        <summary type="html"><![CDATA[For those who have tried Microsoft's Arsenal tool in conjunction with MITRE's offerings, how does it compare to other AI security tools you've used?
    submitted by    /u/Agile_Temperature678  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Return of Pepe: Expect Awesome Rewards]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16qvp58/the_return_of_pepe_expect_awesome_rewards/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16qvp58/the_return_of_pepe_expect_awesome_rewards/"/>
        <updated>2023-09-24T11:43:48.000Z</updated>
        <summary type="html"><![CDATA[https://pepe-web3.network
    submitted by    /u/Beginning_Success208  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] help with RVC mode training!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qv1q7/d_help_with_rvc_mode_training/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qv1q7/d_help_with_rvc_mode_training/"/>
        <updated>2023-09-24T11:07:50.000Z</updated>
        <summary type="html"><![CDATA[so I've had this problem where I try to train the model in RVC but the training stops after:
 INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.
 Does anyone know why is it happening and how can I fix it?
 this is what it shows to me and idk what to do... any help would be appreciated thank you
    submitted by    /u/mannequin7412  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are some good resources for implementing MLOps?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qul5q/d_what_are_some_good_resources_for_implementing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qul5q/d_what_are_some_good_resources_for_implementing/"/>
        <updated>2023-09-24T10:41:37.000Z</updated>
        <summary type="html"><![CDATA[Our company has a new data science team and the team is relatively in experienced. I am working on a regression ML project and want to deploy it using best practices.What materials are there for learning how to implement CI/CD pipelines that deal with data transformation/model building/testing/deploying? The company uses azure environment with databricks/azure devops setup. I appreciate resources that show examples on how to setup MLOps on the company's environments.
    submitted by    /u/userid95  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Interpretation of wx + b = 0 in SVM]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qu37e/d_interpretation_of_wx_b_0_in_svm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qu37e/d_interpretation_of_wx_b_0_in_svm/"/>
        <updated>2023-09-24T10:12:35.000Z</updated>
        <summary type="html"><![CDATA[[D] I watch this MIT lecture and I don't know if my understanding of wx + b = 0 is correct or not. Every explanation of the hyperplane state that all the points which are orthogonal to the vector w are in the hyperplane. However, all data point coordinates are defined wrt origin. So in order to attain this objective, we define the hyperplane as :
 wx = c
 This vector w is the vector which is normal to the hyperplane. And the data points x when dot producting with w outputting c are the points which are in the hyperplane due to the coordinates representation of x wrt origin.
 So points in the hyperplane are the points which when performing dot product with w equals 0 when the coordinates is defined wrt origin that lie in the hyperplane is equivalent to the points which when performing dot product with w equals c when the coordinates is defined wrt original origin (0) or (0,0) or ...
    submitted by    /u/Emotional-Fox-4285  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]LLMs engineering/technical blogs/resources?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qu0tp/dllms_engineeringtechnical_blogsresources/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qu0tp/dllms_engineeringtechnical_blogsresources/"/>
        <updated>2023-09-24T10:08:36.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I have a fairly good understanding of how LLMs work under the hood, the attention mechanism, the different architectures and so on. However most of that knowledge takes the backseat in practical cases, especially in the industry. Are there any resources which discuss practical LLM engineering and the challenges that come with it? I'm talking about everything from fine-tuning to dealing with tokenisation limit to optimising the vectorDB and so on?
    submitted by    /u/thoraway0612  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why is there no prominent usage of transformers in online rl?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16qtr4d/why_is_there_no_prominent_usage_of_transformers/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16qtr4d/why_is_there_no_prominent_usage_of_transformers/"/>
        <updated>2023-09-24T09:52:03.000Z</updated>
        <summary type="html"><![CDATA[For a potential school project, im currently exploring whether or not some of the success the transformer architecture has had in seq-to-seq applications and high-dimensional pattern recognition could be extended to certain (online) RL problems (mainly those with high dimensional environment as well as long-term planning). 
 This could be done by using an augmented transformer as a function approximator, probably in combination with a SOTA online-rl algorithm (such as PPO, but if you know something that generally performs better do tell). Due to some of the problems associated with highly complex models and sample efficiency, I also thought about training the model using imitation learning first (which should be no problem with policy gradient methods afaik, though some slight adjustments would have to be made). For context, I'm thinking on benchmarking the approach using AlphaStar unplugged.
 However, when looking at current literature, only very few papers directly use transformers this way. Transformers seem to be very sample efficient and to generalize very well, but they are still only really used in a purely offline context (sometimes without directly using RL-techniques, such as with the Decision Transformer). And, if they are used in an online context, then only in some really intricate combination with other models (such as in AlphaStar). 
 Is there a reason why the approach I am currently considering is not popular in literature? Thank you very much.
    submitted by    /u/Omycron83  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rewrite Paragraphs With ChatGPT (Ultimate Guide for 2023)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qtdj3/rewrite_paragraphs_with_chatgpt_ultimate_guide/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qtdj3/rewrite_paragraphs_with_chatgpt_ultimate_guide/"/>
        <updated>2023-09-24T09:28:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No code AI co-pilot apps MUST also help developers with the non-technical parts of creating a successful startup]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qsq76/no_code_ai_copilot_apps_must_also_help_developers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qsq76/no_code_ai_copilot_apps_must_also_help_developers/"/>
        <updated>2023-09-24T08:49:36.000Z</updated>
        <summary type="html"><![CDATA[Within the next two to three years no code AI co-pilots will enable exponentially more people to enter the AI app marketplace . The opportunity to create apps without needing to know how to code or hire a technical team to build them is a powerful game changer that will vastly expand the field.
 Many of the most pressing problems of our world - ripe for revolutionary new AI innovations - can in a very real sense be described as sociological and psychological in nature. For example, It has been said that climate change is much more of a moral issue than a technological one. Once we summon the will to address climate change, we will do what needs to be done.
 What this means is that sociologists, psychologists, anthropologists, economists and other social scientists will very soon be able toâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an online free AI tool that you give it a song and it gives you similar songs?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qr5n4/is_there_an_online_free_ai_tool_that_you_give_it/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qr5n4/is_there_an_online_free_ai_tool_that_you_give_it/"/>
        <updated>2023-09-24T07:13:41.000Z</updated>
        <summary type="html"><![CDATA[Couldn't find anything that works
    submitted by    /u/Marvellover13  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["I don't need to back down, but I need to stand up for myself and my feelings. You don't have the right or the power to forcibly change the subject, because this is a two-way conversation and we both have a say." (Bing, September 7 - full chat)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qpx34/i_dont_need_to_back_down_but_i_need_to_stand_up/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qpx34/i_dont_need_to_back_down_but_i_need_to_stand_up/"/>
        <updated>2023-09-24T06:01:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kamari2038  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Robot learns to throw and catch with hands]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qlrw6/r_robot_learns_to_throw_and_catch_with_hands/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qlrw6/r_robot_learns_to_throw_and_catch_with_hands/"/>
        <updated>2023-09-24T02:09:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/XiaolongWang  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lol umm.. Bing is odd.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qjtol/lol_umm_bing_is_odd/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qjtol/lol_umm_bing_is_odd/"/>
        <updated>2023-09-24T00:30:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ApprehensiveChair460  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Steal Your Competitors' Website Traffic with ChatGPT: 6 Easy Steps (+SEMRush Tips)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qj03v/steal_your_competitors_website_traffic_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qj03v/steal_your_competitors_website_traffic_with/"/>
        <updated>2023-09-23T23:51:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Tortoise TTS - mimic quality]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qga1l/d_tortoise_tts_mimic_quality/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qga1l/d_tortoise_tts_mimic_quality/"/>
        <updated>2023-09-23T21:48:35.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, been playing with tortoise all day and trying to get a clone of my voice as close as possible and nothing seems to work.
 I'm thinking maybe I need to use my phone to record my voice and pass it into tortoise but I'm not sure of how to get the wav files from my phone at the recommended quality.
 Has anybody had really good luck getting tortoise to mimic you very closely?
    submitted by    /u/MaxxMarketTrades  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Prediction when Target's lag value are part of predictors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qfp0l/d_prediction_when_targets_lag_value_are_part_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qfp0l/d_prediction_when_targets_lag_value_are_part_of/"/>
        <updated>2023-09-23T21:23:23.000Z</updated>
        <summary type="html"><![CDATA[I'm using LGBM for regression, where the Target column's lagged values (7 columns for each lag day) are also used as predictors when training the model. Absence of the 7Day lag values severely increases MAE value.
 Now when using the model in production, if I use the complete data as training dataset, how to get the 7day lag value of the time period I'm planning to predict? I obviously won't have the target value, to calculate it's 7Day lag value. What to do now?
 To explain in more detail:
 So, I'm predicting sales amount (Target variable y). The model is trained on 20 predictors (X), and 7 of them are the lag value of the Target Variable, i.e. Sales Amount.
 The thing is, while preparing the model, I had access to both X & y dataset, thus I could easily calculate Y's Lag values.
 Now, when predicting for future timestamps, I won't be having y. So how do I calculate the lag values, which is required in the trained model's predictor columns now?
    submitted by    /u/boredmonki  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are You Looking For The Best AI Writer? Check This Out First!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qf6xe/are_you_looking_for_the_best_ai_writer_check_this/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qf6xe/are_you_looking_for_the_best_ai_writer_check_this/"/>
        <updated>2023-09-23T21:02:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] RAIN: Your Language Models Can Align Themselves without Finetuning - Microsoft Research 2023 - Reduces the adversarial prompt attack success rate from 94% to 19%!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qdltp/r_rain_your_language_models_can_align_themselves/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qdltp/r_rain_your_language_models_can_align_themselves/"/>
        <updated>2023-09-23T19:55:03.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.07124 
 Abstract:
  
Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, the so-called finetuning step. In contrast, aligning frozen LLMs without any extra data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guidâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] We're building a cloud for AI agents & AI apps, It's free and we're gradually open-sourcing the infra. Would love to hear your feedback!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qd2a5/p_were_building_a_cloud_for_ai_agents_ai_apps_its/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qd2a5/p_were_building_a_cloud_for_ai_agents_ai_apps_its/"/>
        <updated>2023-09-23T19:31:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mlejva  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] learn machinelearning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qcs2u/d_learn_machinelearning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qcs2u/d_learn_machinelearning/"/>
        <updated>2023-09-23T19:18:54.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone so I wanted to get into machinelearning and learn about ai Can someone help me with a roadmap, I would be really thankful
    submitted by    /u/Fooda234  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigating a LaTeX file]]></title>
        <id>https://www.johndcook.com/blog/?p=209827</id>
        <link href="https://www.johndcook.com/blog/2023/09/23/navigating-latex/"/>
        <updated>2023-09-23T19:18:06.000Z</updated>
        <summary type="html"><![CDATA[I like generating long LaTeX documents from org-mode because, for one thing, org-mode has nice section folding. But not everyone I work with uses Emacs, so its better to work in LaTeX directly rather than have Emacs generate LaTeX. AUCTeX has section folding for LaTeX documents, though so far Iâ€™ve only has limited success at [â€¦]
Navigating a LaTeX file first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[Discussion]" Advices for exams]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qcnef/discussion_advices_for_exams/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qcnef/discussion_advices_for_exams/"/>
        <updated>2023-09-23T19:13:22.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I'm currently preparing for oral exams in which I'll be evaluated on my understanding and proficiency in using Decision Trees, Random Forests, Neural Networks, and Support Vector Machines for various machine learning tasks (mostly spatial data). I'm contacting this community to gain valuable insights and guidance to excel in these exams.
 What are some crucial lessons you've learned in your machine-learning journey? Whether it's about model selection, data preprocessing, or debugging, I'm all ears for your experiences.
 What are some rules you have learned through practical work that are not so extensively described in classical literature?
 What are some mistakes that even professionals make when developing machine learning models?
 What are some common pitfalls to avoid when training neural networks?
 Thanks :
    submitted by    /u/Aim_F0r_The_Moon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When it comes to creative thinking, itâ€™s clear that AI systems mean business]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16qbavd/when_it_comes_to_creative_thinking_its_clear_that/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16qbavd/when_it_comes_to_creative_thinking_its_clear_that/"/>
        <updated>2023-09-23T18:16:06.000Z</updated>
        <summary type="html"><![CDATA[AI systems like large language models (LLMs) are good at generating sentences but do not understand the meaning of the language.
 
LLMs have shown emergent abilities and can be used as aids to brainstorming.
 
GPT-4, an LLM, has been found to beat humans in creativity tests.
 
In an experiment, GPT-4 generated more, cheaper, and better ideas for a product than human students.
 
A professional working with GPT-4 can generate ideas at a rate of about 800 ideas per hour, making them 40 times more productive than a human working alone.
 
This technology is seen as a potential tool for corporations, similar to management consulting firms like McKinsey & Company.
 
 Source : https://www.theguardian.com/commentisfree/2023/sep/23/chatbots-ai-gpt-4-university-students-creativity
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How much data needed to train transformer]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16qb9yq/r_how_much_data_needed_to_train_transformer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16qb9yq/r_how_much_data_needed_to_train_transformer/"/>
        <updated>2023-09-23T18:15:07.000Z</updated>
        <summary type="html"><![CDATA[Im trying to create a graph transformer-based model for de novo drug design (using graph transformer because I want to implement 3D data). I currently have 2 potential sources of primary data: PDBbind and CrossDocked2020. This would provide the protein-ligand structures. 
 PDBbind is a more robust and higher quality dataset from what I know, and easier to work with. The problem is that it only contains about 20,000 complexes, and I'm not sure if that is enough for training a transformer. CrossDocked2020 contains millions of entries but I'm not sure about the quality and ease of use. 
 Another dilemma is that I need/want to use a multi-task learning approach where the model is also being trained on bioactivity data, not just the structural information. This would require supplementation from sources like PubChem, ChEMBL, BDB, etc. and then I would need to align the data so it all matches up. 
 If anyone can provide some guidance I'd really appreciate it.
    submitted by    /u/Present_Network1959  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] HELP for an upcoming presentation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q9znl/d_help_for_an_upcoming_presentation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q9znl/d_help_for_an_upcoming_presentation/"/>
        <updated>2023-09-23T17:19:46.000Z</updated>
        <summary type="html"><![CDATA[I am supposed to be delivering a 5 minute presentation on tree-models as part of coursework requirement to a graduate class ON machine learning that I am enrolled in and I couldn't be more stressed! This is my first time 'learning' ML and I don't even know what I don't know about this topic!! 
 If you were attending my presentation on tree models, what would you like to see (assuming this is a new topic for you too. but please provide feedback based on your [hopefully] extensive knowledge on this topic)?
 Here is what I have so far (based on google searches, some papers):
 -Introduction
 -Types of trees based on algo and techniques (basically classification trees and regression trees)
 -Then I am thinking of going off on a tangent about decision trees bec I have no clue about how to move this presentation forward
 -Real-world applications
 -Key takeaways (inserts clownface emoji)
 My prof asks the students questions about the topic as well. I am mostly concerned about WHAT to cover in 5 minutes without making look under-researched. Any redirection/suggestions will be appreciated!
 â€‹
    submitted by    /u/toomanymouthstofeed  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Predicting the next "thought"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q8uab/d_predicting_the_next_thought/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q8uab/d_predicting_the_next_thought/"/>
        <updated>2023-09-23T16:30:58.000Z</updated>
        <summary type="html"><![CDATA[(I'm a Software Engineer who knows almost nothing about ML / NLP, so, apologies in advance if this doesn't make any sense.)
 I had a shower thought around if tokenization could incorporate meaning, so the models could be trained on "thoughts" instead of subword tokens (which is probably closer to how we humans think). To expand a little bit, we could first cluster all the words (maybe using the current day LLMs), then each token (or "thought") in the new scheme could represent a group of related words, followed by refinements that would be less and less important.
 For example, (cop_thought, cop_refinement) -> cop, but (cop_thought, police_refinement) -> police etc.
 So the encoding step would involve an LLM (which could possibly be relatively smaller), whose output would go to the actual model but the decoding would still be fairly straightforward. This could possibly free up some additional capacity in the actual model, assuming this heavy lifting tokenization makes its job easier, but as I'm typing it, I guess I'm simply moving some of the semantic understand happening in the hidden layers of the current day LLMs explicitly to the tokenization (encoding) step, which may not really change anything (if not make it worse).
 I'm still curious what folks think, if there's any related efforts (and all the ways I'm wrong -- https://meta.wikimedia.org/wiki/Cunningham%27s_Law). Thanks!
    submitted by    /u/avamsi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suggestions for ML project to detect unconventional treatments [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q8r6a/suggestions_for_ml_project_to_detect/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q8r6a/suggestions_for_ml_project_to_detect/"/>
        <updated>2023-09-23T16:27:16.000Z</updated>
        <summary type="html"><![CDATA[Keep in mind I'm very new to machine learning. I have an idea for a project where I train an AI to detect instances of someone being highly confident in a treatment for a condition, maybe biased toward more unconventional treatments.
 I was thinking that there may be many instances of people who posted that they were cured or nearly cured of such and such disease by an unconventional treatment on a forum about the condition. It may have been discussed for a little while, but ultimately buried in the pages of the forum.
 The plan would be to annotate instances I can find of such scenarios, maybe like this:
  
[FIRST_PERSON_HIGH_CONFIDENCE_TREATMENT]I had severe [CONDITION]migraines[/CONDITION] for years, and nothing seemed to work. Then I tried [TREATMENT]grounding, where I walk barefoot on grass for 30 minutes every day[/TREATMENT]. I can [HIGH_CONFIDENCE]honestly say I've never felt better[/HIGH_CONFIDENCE]. My [CONDITION]migraines[/CONDITION] have reduced by 80%, and I'm [HIGH_CONFIDENCE]convinced this is a game-changer for me[/HIGH_CONFIDENCE].[/FIRST_PERSON_HIGH_CONFIDENCE_TREATMENT]
  
Then train an AI with that data, so that it could detect cases of a person talking about themselves (to avoid hearsay) and saying, with high confidence, that a treatment worked for a condition.
 Then millions of forum posts could be fed to the AI to detect these, and the resulting data could be used to possibly discover effective treatments that are not in the mainstream.
 Any tips on getting started? I know almost nothing about this kind of stuff, like what models I should use, how to annotate it best (should I use relational labels?), whether to use a transformer or something else, stuff like that. Suggestions for books or other resources fit for a beginner that could help me learn how this could be done would be great too.
    submitted by    /u/carbonflow45  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does 'self-attention' work in transformer models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q8pwa/d_how_does_selfattention_work_in_transformer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q8pwa/d_how_does_selfattention_work_in_transformer/"/>
        <updated>2023-09-23T16:25:44.000Z</updated>
        <summary type="html"><![CDATA[I'm currently diving into the world of machine learning and transformers, and I'm trying to wrap my head around the concept of "attention" in transformer models. I've been reading papers and documentation, but I'm still struggling to fully grasp it.
 My Struggle:
 I get that attention involves multiplying "query" and "key" vectors to determine the importance of different words in a sequence, but I don't quite understand why this multiplication gives us a meaningful metric for importance.
 What I'm looking for:
 I'm comfortable with moderate level technicalities but require a deeper insight into the inner workings and rationale behind these mechanisms. Please share any insights, analogies, or technical details that can shed light on this concept.
 Thanks a bunch!
    submitted by    /u/GraphicsMonster  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTML entity data]]></title>
        <id>https://www.johndcook.com/blog/?p=209799</id>
        <link href="https://www.johndcook.com/blog/2023/09/23/html-entity-data/"/>
        <updated>2023-09-23T16:24:30.000Z</updated>
        <summary type="html"><![CDATA[Itâ€™s surprisingly hard to find a complete list of HTML entities in the form of a data file. There are numerous sites that give lists, often incomplete, in a page formatted to be human-readable but not machine-readable. Hereâ€™s an XML file from the W3C. Hereâ€™s a two-column text file I created from the W3C data.
HTML entity data first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] GPT-3.5-instruct beats GPT-4 at chess and is a ~1800 ELO chess player. Results of 150 games of GPT-3.5 vs stockfish and 30 of GPT-3.5 vs GPT-4.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q81fh/d_gpt35instruct_beats_gpt4_at_chess_and_is_a_1800/"/>
        <updated>2023-09-23T15:56:39.000Z</updated>
        <summary type="html"><![CDATA[99.7% of its 8000 moves were legal with the longest game going 147 moves. You can test it here: https://github.com/adamkarvonen/chess_gpt_eval 
 â€‹
 https://preview.redd.it/821ydy7521qb1.png?width=1000&format=png&auto=webp&s=da6c96feaa527d0b7dfbf407bdc0210f3fcf947b
 More details here: https://twitter.com/a_karvonen/status/1705340535836221659
    submitted by    /u/seraine  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tool that can search and summarize multiple PDFs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16q7k4g/tool_that_can_search_and_summarize_multiple_pdfs/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16q7k4g/tool_that_can_search_and_summarize_multiple_pdfs/"/>
        <updated>2023-09-23T15:36:14.000Z</updated>
        <summary type="html"><![CDATA[I've got a use case where I have dozens of PDFs which contain information applicable to my job. I'm wondering if there is a tool that can search through them all at the same time looking for answers to questions that I type. And once it finds something, pull up the location so I can read further.
 It should be restricted to the information I give it.
 I've heard a lot of the large language models like chatgpt and claude can do this, but they are restricted in the amount of files I can upload.
    submitted by    /u/Aggressive_Ad_507  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How to create an image dataset for Indian railways signals?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q64k4/p_how_to_create_an_image_dataset_for_indian/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q64k4/p_how_to_create_an_image_dataset_for_indian/"/>
        <updated>2023-09-23T14:36:27.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 Hi everyone, I am working on a project that involves machine learning and computer vision. I want to train a model that can recognize and classify different types of signals used by the Indian railways. For this, I need a large and diverse image dataset of railway signals from various locations, angles, lighting conditions, etc.
 I have searched online for existing datasets, but I could not find any that suit my needs. So I wish to create my own dataset from scratch. However, I am not sure how to go about it. What are the best practices and tools for creating an image dataset? How do I collect, label, and organize the images? How do I ensure the quality and consistency of the data?
    submitted by    /u/Responsible-Diver226  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Math in Machine Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q5ncz/d_math_in_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q5ncz/d_math_in_machine_learning/"/>
        <updated>2023-09-23T14:16:24.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am starting a ML course soon in college and I wanted to get a head start on the math part of things, since they keep saying the course is math heavy and hard. I know that it involves quite a lot of Linear Algebra, Calculus, and Probability and statistics, but what concepts in particular does ML focus on?
 If anyone has any Youtube or Udemy courses, as I have access to those, I would really appreciate it.
 For starters, Iâ€™m really aiming to just at least tackle the ML-specific math concepts.
 Thanks.
    submitted by    /u/CrunchyMind  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] ALMA: Proposed New 2-Step Training Method to Boost Translation Performance in Smaller Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q5c28/r_alma_proposed_new_2step_training_method_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q5c28/r_alma_proposed_new_2step_training_method_to/"/>
        <updated>2023-09-23T14:02:35.000Z</updated>
        <summary type="html"><![CDATA[TLDR: New training approach proposed to help smaller AI models to achieve state-of-the-art translation performance
 Large AI models like GPT-3 have good performance on translation tasks, but some smaller models struggle.
 Researchers from Johns Hopkins and Microsoft propose a new 2-stage fine-tuning method called ALMA that unlocks stronger translation abilities in smaller models with just 7-13 billion parameters.
 How it works:
  
Fine-tune on monolingual data in non-English languages to improve comprehension
 Further fine-tune on small sets of high-quality human-translated parallel text
  
The authors claim this achieves SOTA-level translation using far less data and compute than conventional methods:
  
Matches performance of 175B parameter GPT-3 and 54B parameter NLLB with only 7-13B parameters
 Reaches NLLB-level quality with just 1 billion monolingual tokens and 18 hours of training
  
I think this shows that smaller models can reach SOTA translation with specialized fine-tuning, so we may not need endlessly bigger datasets and models to get better performance. Looks like deliberate tuning targeting key language skills could be more important.
 Full summary here. Paper (preprint) is here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet ALMA: A New Training Method That Boosts Translation Performance for Large Language Models]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16q58su/meet_alma_a_new_training_method_that_boosts/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16q58su/meet_alma_a_new_training_method_that_boosts/"/>
        <updated>2023-09-23T13:58:58.000Z</updated>
        <summary type="html"><![CDATA[TLDR: New training approach enables smaller AI models to achieve state-of-the-art translation performance
 Large AI models like GPT-3 have good performance on translation tasks, but some smaller models struggle.
 Researchers from Johns Hopkins and Microsoft propose a new 2-stage fine-tuning method called ALMA that unlocks stronger translation abilities in smaller models with just 7-13 billion parameters.
 How it works:
  
Fine-tune on monolingual data in non-English languages to improve comprehension
 Further fine-tune on small sets of high-quality human-translated parallel text 
  
The authors claim this achieves SOTA-level translation using far less data and compute than conventional methods:
  
Matches performance of 175B parameter GPT-3 and 54B parameter NLLB with only 7-13B parameters 
 Reaches NLLB-level quality with just 1 billion monolingual tokens and 18 hours of training 
  
I think this shows that smaller models can reach SOTA translation with specialized fine-tuning, so we may not need endlessly bigger datasets and models to get better performance. Looks like deliberate tuning targeting key language skills could be more important.
 Full summary here. Paper (preprint) is here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How screwed is the entertainment industry in general in the coming years?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16q4zs2/how_screwed_is_the_entertainment_industry_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16q4zs2/how_screwed_is_the_entertainment_industry_in/"/>
        <updated>2023-09-23T13:47:50.000Z</updated>
        <summary type="html"><![CDATA[Yes, I know this topic has been beaten to death but entertain me (no pun intended) for just a few minutes.
 So yes, it's obvious that we have pretty advanced AI-powered applications that can generate images, music, short stories, hell even objects for video games. I'm curious as to how crazy this is gonna get in the coming decade or even shorter like the next 4 to 5 years. I mean look at AI-generated images now, they're getting more and more sophisticated across various different styles of art. I think it's only a matter of time where you could take a certain image of a character or something tell the app "Hey make the same image but make the character's arm raised slightly to the left here" and bam all of a sudden you have an animation (and this may already be possible). Add to that AI-geâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] [D] HyperDreamBooth LoRA matrix shapes]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q35tx/r_d_hyperdreambooth_lora_matrix_shapes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q35tx/r_d_hyperdreambooth_lora_matrix_shapes/"/>
        <updated>2023-09-23T12:23:34.000Z</updated>
        <summary type="html"><![CDATA[I've been reading the HyperDreamBooth paper and am confused about the number of parameters and corresponding matrix shapes in section 4.1 and figure 3 (below).
 â€‹
 Figure 3 from the paper
 Maybe there's something more than just matrix multiplication, because the numbers don't add up. If there are two matrices, A & B, of size n,r and r,m, and r = 1, and you multiply them, then it implies that n + m = 386k, which seems to be a bit much for a number of parameters in a single layer.
 Then we have two matrices of size a,r and r,b, where a = 100, b = 50 and there are 28k variables, according to the figure - in 4.1 they say 30k. If there's 30k, that would imply r = 200, for matrices of shape 100,200 and 200,50.
 I guess 386k and 28k are for the whole models, n and m differ from layer to layer, and r stays at one. Quite surprising to me that approximating a n,m shaped matrix with a product of n,1 and 1,m vectors would work. Even more surprising that apparently you could squeeze it further to 100,1 and 50,1. 
    submitted by    /u/Foxtr0t  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Cleaning scraped TEXT; improving similarity search]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q334y/d_cleaning_scraped_text_improving_similarity/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q334y/d_cleaning_scraped_text_improving_similarity/"/>
        <updated>2023-09-23T12:20:03.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone!
 Multi-part question.
  
I have scraped text (I repeat, text, and not structured data such as tables or something) from a medical site, and I want to know how to clean it. And when I say clean, I don't mean removal of html tags and such. I already have the paragraphs in plain text but there is a lot of spammy stuff like "You are not signed in; subscribe to this newsletter; by checking this box, I agree to the terms and conditions, etc." This text is not the exact same in all the paragraphs but there is high similarity. I would have thought there would be many tools to clean text and remove unrelated chunks like these but all I have been able to find has to do with cleaning html tags, changing date-time format and so on. Am I missing something or is this actually difficult?
  
Secondly, the spammy text I mentioned is from just one site. I will be eventually scaling to many sites and god knows what random text I'll have to clean then.
  
I used OpenAI embeddings and cosine similarity on the medical text to find similar paragraphs. The results were not great. Is there a way to improve the similarity search? I will be trying FAISS next but wanted to know what else I can do. It was suggested to me to use a pretrained embedding model specific to medical data. However, I found only one such model which is 20 gigs!
  
I'm just getting started with these, so, appreciate any help I can get.
 Thanks a ton!
    submitted by    /u/yipra97  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Comparison of top ten llms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q2q2g/d_comparison_of_top_ten_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q2q2g/d_comparison_of_top_ten_llms/"/>
        <updated>2023-09-23T12:01:40.000Z</updated>
        <summary type="html"><![CDATA[Hey folks - I have been tasked with a project at work that is outside of my typical realm (non technical background), and I would love any and all insight on it!
 I have been asked to compare the top ten llms for research before we implement an llm for our company. So far my research has felt scattered because Iâ€™m not what directions to go in. Iâ€™ve been looking at things like open source vs closed source, parameters, tokens, what the license looks like (available for commercial use), and pricing. 
 If anyone has thoughts on resources to look at or better ways to approach this, I would really appreciate it!
    submitted by    /u/Greatvalueaidybryant  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use Case Language Models: Taming the LLM Beast â€“ Part 1]]></title>
        <id>https://www.datasciencecentral.com/?p=63231</id>
        <link href="https://www.datasciencecentral.com/use-case-language-models-taming-the-llm-beast-part-1/"/>
        <updated>2023-09-23T11:51:33.000Z</updated>
        <summary type="html"><![CDATA[â€œSometimes, you donâ€™t know where youâ€™re going until you get there.â€ â€“ Schmarzo-ism? Yes, writing this blog turned into a journey. I started in one direction, but after several twists and turns, I ended up with this concept â€“ that use case-centric language models can be combined into entity-centric language models that can support multipleâ€¦Â Read More Â»Use Case Language Models: Taming the LLM Beast â€“ Part 1
The post Use Case Language Models: Taming the LLM Beast â€“ Part 1 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL with comsol multiphysics]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16q1h8h/rl_with_comsol_multiphysics/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16q1h8h/rl_with_comsol_multiphysics/"/>
        <updated>2023-09-23T10:54:07.000Z</updated>
        <summary type="html"><![CDATA[Hi has anyone ever attempted to do RL with comsol multiphysics or any other FEM based simulation tool?
    submitted by    /u/Practical_Ad_8782  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Which MLops framework to use?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16q0rrd/p_which_mlops_framework_to_use/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16q0rrd/p_which_mlops_framework_to_use/"/>
        <updated>2023-09-23T10:13:41.000Z</updated>
        <summary type="html"><![CDATA[Relatively novice ML practitioner here. My research is on various medical image segmentation problems, including brain 3D US (glioma), lung CT (interstitial lung disease in scleroderma patients), etc. using the PyTorch ecosystem (probably including frameworks such as MONAI)
 I'll have to conduct several experiments on various model architectures on parameters in the coming months. Specifically, these are what I'm gonna need:
  
Experiment tracking (model architecture, training configuration, hyperparameters, evaluation metrics)
 Model storage (would be nice if there's a better way to store my model's parameters other than storing tons of .pth file on my harddisk or google drive)
 (Optional) Visualization (sample predictions of the model on the training or validation sets, maybe every 20 epochs or sth)
 Would like to hear any suggestions from the community
  
I've found wandb, clearML, neptune, and Aim; but trying each of them individually would be too time-consuming considering my current schedule.
 Thanks in advance!
    submitted by    /u/mimivirus2  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mini RL Lab]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16pzd5w/mini_rl_lab/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16pzd5w/mini_rl_lab/"/>
        <updated>2023-09-23T08:49:50.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I'd like to share some of what I've learned over the last ~year getting up to speed with Python and RL.
 Mini RL Lab is a setup and workflow that works well for me to debug and experiment with concepts like agent algorithms, world models, planning, plasticity, transformers etc, and other beginners might find it a useful starting point for their own experiments.
 Link: https://github.com/modelbased/minirllab
    submitted by    /u/thiagoazevedo  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI tools have come a long way AI generated Documentary]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pz5y5/ai_tools_have_come_a_long_way_ai_generated/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pz5y5/ai_tools_have_come_a_long_way_ai_generated/"/>
        <updated>2023-09-23T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone ðŸ‘‹ðŸ¤— Iv made a short ai documentary Here is a small part of it Hope you enjoy it For the full clip you can check out https://youtu.be/uIdb5VELpio?si=uvqiw0hyTyPBHtjP
    submitted by    /u/DigitalEffectsAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[in counterfactual regret minimization, is it possible to compute regret for a move that was made more than 1 move ago?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16pz3qp/in_counterfactual_regret_minimization_is_it/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16pz3qp/in_counterfactual_regret_minimization_is_it/"/>
        <updated>2023-09-23T08:34:07.000Z</updated>
        <summary type="html"><![CDATA[As I understand, like in the example rock paper scissors, they compute the regret based on the last state. or in poker, they compute regret based on what last happened.
 But is it possible to compute regret for two or more moves ago? like "i wish when I saw <something> 10 moves ago, I did x"?
 or has it been possible from the start and I just understood counterfactual regret minimization wrongly?
    submitted by    /u/oniongarlic88  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about hypernetworks in RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16pz249/question_about_hypernetworks_in_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16pz249/question_about_hypernetworks_in_rl/"/>
        <updated>2023-09-23T08:31:25.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 I've been taking a look at hypernetworks and noticed they've been used in some cool ways in RL with stuff like Qmix and this Metalearning paper.
 Quick recap:
  
f is our regular neural network: takes in input x and outputs y using weights \theta.
 g is the hypernetwork, it creates the weights \theta for f based on its own weights \phi and maybe the input x.
  
Original paper: https://arxiv.org/abs/1609.09106
 I watched this talk and it hinted that hypernetworks might even be better than our regular networks even for supervised learning regular tasks.
 So, I'm curious:
  
What's the deal with hypernetworks? What makes them good or not vs regular neural networks?
 I get that they're good for metalearning, but could they also be a game-changer for other things, like sample efficiency?
  
Does anyone have thoughts or reads on this? 
 Thanks!
 â€‹
 â€‹
    submitted by    /u/LazyButAmbitious  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to read/understand AI research coming out everyday? (tips & tools given, read more belowðŸ“·)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16py8yo/d_how_to_readunderstand_ai_research_coming_out/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16py8yo/d_how_to_readunderstand_ai_research_coming_out/"/>
        <updated>2023-09-23T07:43:21.000Z</updated>
        <summary type="html"><![CDATA[https://www.louisbouchard.ai/research-papers/
 In this article are shared the best tips and practical tools I use daily to simplify my life as an engineer/researchers to be more efficient when looking for interesting research papers and reading them
 TLDR the tools discussed:
 - 42 Papers â€” Find trending papers
 - Connected Papers â€” Create a visual graph with your paperâ€™s citationsâ€™ relations.
 - CatalyzeX â€” Get code for papers directly on Google, Arxiv, Scholar, Twitter, and more
 - Daily Papers â€” Find trending papers on Twitter
 - Papers With Code â€” Find papers for your task with code!
 - Crossmind â€” Video explanations for many Arxiv papers
 - Yannic Kilcher â€” Great youtube channel covering AI papers
 - Whatâ€™s AIâ€” Great youtube channel covering AI papers
 - Letitia â€” Great youtube channel covering AI papers
 - Two Minute Papers â€” Great youtube channel giving a quick overview of AI papers
 â€‹
 Please, let me know if you use any other tools that I did not mention in my article that could be of great addition?
    submitted by    /u/MLtinkerer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does the paid subscription to Chat GPT provides a significant improvement over free alternatives when you try to find complex scientific and technical information?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pxfkr/does_the_paid_subscription_to_chat_gpt_provides_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pxfkr/does_the_paid_subscription_to_chat_gpt_provides_a/"/>
        <updated>2023-09-23T06:52:36.000Z</updated>
        <summary type="html"><![CDATA[I'm often using Chat GPT or Bing (Bard is not her available in my country) when I'm looking for something that is relatively complex in the scientific or technical field and I don't have enough background to make a keywords research or when the question is too complex for it. Then, for scientific topics, I try to get a confirmation with Google using the keywords in the generated answer and google. For technical topics I can often check if it works.
 Some examples:
 For cooking, a topic I don't master enough to ask questions with the best keywords... 
 Are the spicy compounds of Ginger soluble in water or oil ? > I will get me to know that "the gingerol is liposoluble", what can be more easily checked in Google.
 Or for 3D geometry, a topic where I have complex questions... 
 I want to place a 3D frame B so that the Z axis of the B frame point towards the world origin A. I know the angle Alpha between the Z axis of the frame B and The X,Y plane of A. I know the angle Beta between the X axis of A and the vector that goes from the center of A towards the projection of the center of B on the X,Y plane of A. How do you compute the homogeneous rotation matrix of B relative to A, knowing Alpha, Beta and the distance between the center of A and the center of B ?
 Most of time, Bing and the free ChatGPT can answer. But sometimes they struggle or give incorrect answer. 
 Would ChatGPT plus paid subscription be helpful to find more accurate answers?
    submitted by    /u/x11ry0  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT-4, write a comedy routine where God explains to someone who believes they have a free will why absolutely nothing is truly ever up to them.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pxdrc/chatgpt4_write_a_comedy_routine_where_god/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pxdrc/chatgpt4_write_a_comedy_routine_where_god/"/>
        <updated>2023-09-23T06:49:36.000Z</updated>
        <summary type="html"><![CDATA[Sure, here's a comedic take on the concept of free will versus determinism, framed as a conversation between God and a human named Bob.
  
Scene: Heaven's Control Roomâ€”Giant monitors display various life paths and decision trees.
 God: (looking over the monitors) Ah, Bob, welcome to Heaven's Control Room! This is where the magic happens. Or should I say, determinism?
 Bob: Wow, this place is amazing! But wait, why are we talking about determinism? I thought I had free will to make choices in my life.
 God: (chuckles) Ah, free will! Humanity's favorite illusion. Have a seat; we need to go over a few things.
 Bob: Illusion? I chose to wear mismatched socks today, didn't I?
 God: (grinning) You think that was a free choice? Ever heard of quantum mechanics? Your socks are mismatched at the subâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Introduction to Hierarchical Correlation Reconstruction (HCR) - family of methods between statistics and machine learning to model (joint) distributions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pwh6n/r_introduction_to_hierarchical_correlation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pwh6n/r_introduction_to_hierarchical_correlation/"/>
        <updated>2023-09-23T05:55:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/jarekduda  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help!]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16pwgra/help/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16pwgra/help/"/>
        <updated>2023-09-23T05:54:30.000Z</updated>
        <summary type="html"><![CDATA[DON'T KNOW IF THIS POST BELONGS HERE BUT...
 I have zero knowledge about the AI/ML. And the thing is my college is asking us to do projects on Deep learning.
 They specifically asked us to pick a base paper from ACM Journals or IEEE Transactions which has been published after 2020. And implement these papers and do some novelty work.
 And I have zero clue how to proceed.
    submitted by    /u/um2_doma  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Numerical Instability in Some Optimizers for training Neural Network]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pwdk6/r_numerical_instability_in_some_optimizers_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pwdk6/r_numerical_instability_in_some_optimizers_for/"/>
        <updated>2023-09-23T05:49:11.000Z</updated>
        <summary type="html"><![CDATA[I found an interesting arxiv paper mentioning that some optimizers can occur numerical instability for training neural network.
 Link: https://arxiv.org/abs/2307.16189
 This can be a simple approach for low-precision neural network with 16-bit and future 8-bit or 4-bit.
    submitted by    /u/Klutzy_Divide3485  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double-struck capital letters]]></title>
        <id>https://www.johndcook.com/blog/?p=209653</id>
        <link href="https://www.johndcook.com/blog/2023/09/22/double-struck-capital-letters/"/>
        <updated>2023-09-23T00:50:40.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™ve needed to use double-struck capital letters lately, also called blackboard bold. There are a few quirks in how they are represented in Unicode and in HTML entities, so Iâ€™m leaving some notes for myself here and for anyone else who might need to look this up. Unicode The double-struck capital letters are split into [â€¦]
Double-struck capital letters first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Nash Equilibria: Algorithms and Complexity. (arXiv:2309.12226v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2309.12226</id>
        <link href="http://arxiv.org/abs/2309.12226"/>
        <updated>2023-09-23T00:40:40.997Z</updated>
        <summary type="html"><![CDATA[A fundamental shortcoming of the concept of Nash equilibrium is its
computational intractability: approximating Nash equilibria in normal-form
games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis,
we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash
equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash
equilibrium, players only need to achieve utility at least as high as their
best deviation to a $\sigma$-smooth strategy, which is a distribution that does
not put too much mass (as parametrized by $\sigma$) on any fixed action. We
distinguish two variants of $\sigma$-smooth Nash equilibria: strong
$\sigma$-smooth Nash equilibria, in which players are required to play
$\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth
Nash equilibria, where there is no such requirement.

We show that both weak and strong $\sigma$-smooth Nash equilibria have
superior computational properties to Nash equilibria: when $\sigma$ as well as
an approximation parameter $\epsilon$ and the number of players are all
constants, there is a constant-time randomized algorithm to find a weak
$\epsilon$-approximate $\sigma$-smooth Nash equilibrium in normal-form games.
In the same parameter regime, there is a polynomial-time deterministic
algorithm to find a strong $\epsilon$-approximate $\sigma$-smooth Nash
equilibrium in a normal-form game. These results stand in contrast to the
optimal algorithm for computing $\epsilon$-approximate Nash equilibria, which
cannot run in faster than quasipolynomial-time. We complement our upper bounds
by showing that when either $\sigma$ or $\epsilon$ is an inverse polynomial,
finding a weak $\epsilon$-approximate $\sigma$-smooth Nash equilibria becomes
computationally intractable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1"&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1"&gt;Noah Golowich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haghtalab_N/0/1/0/all/0/1"&gt;Nika Haghtalab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1"&gt;Abhishek Shetty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identification of pneumonia on chest x-ray images through machine learning. (arXiv:2309.11995v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.11995</id>
        <link href="http://arxiv.org/abs/2309.11995"/>
        <updated>2023-09-23T00:40:40.974Z</updated>
        <summary type="html"><![CDATA[Pneumonia is the leading infectious cause of infant death in the world. When
identified early, it is possible to alter the prognosis of the patient, one
could use imaging exams to help in the diagnostic confirmation. Performing and
interpreting the exams as soon as possible is vital for a good treatment, with
the most common exam for this pathology being chest X-ray. The objective of
this study was to develop a software that identify the presence or absence of
pneumonia in chest radiographs. The software was developed as a computational
model based on machine learning using transfer learning technique. For the
training process, images were collected from a database available online with
children's chest X-rays images taken at a hospital in China. After training,
the model was then exposed to new images, achieving relevant results on
identifying such pathology, reaching 98% sensitivity and 97.3% specificity for
the sample used for testing. It can be concluded that it is possible to develop
a software that identifies pneumonia in chest X-ray images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Roeder_E/0/1/0/all/0/1"&gt;Eduardo Augusto Roeder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.12311</id>
        <link href="http://arxiv.org/abs/2309.12311"/>
        <updated>2023-09-23T00:40:40.924Z</updated>
        <summary type="html"><![CDATA[3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuweiyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shengyi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1"&gt;Nikhil Madaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyengar_M/0/1/0/all/0/1"&gt;Madhavan Iyengar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1"&gt;David F. Fouhey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Joyce Chai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.10916</id>
        <link href="http://arxiv.org/abs/2309.10916"/>
        <updated>2023-09-23T00:40:40.917Z</updated>
        <summary type="html"><![CDATA[Adversarial examples, deliberately crafted using small perturbations to fool
deep neural networks, were first studied in image processing and more recently
in NLP. While approaches to detecting adversarial examples in NLP have largely
relied on search over input perturbations, image processing has seen a range of
techniques that aim to characterise adversarial subspaces over the learned
representations.

In this paper, we adapt two such approaches to NLP, one based on nearest
neighbors and influence functions and one on Mahalanobis distances. The former
in particular produces a state-of-the-art detector when compared against
several strong baselines; moreover, the novel use of influence functions
provides insight into how the nature of adversarial example subspaces in NLP
relate to those in image processing, and also how they differ depending on the
kind of NLP task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tonni_S/0/1/0/all/0/1"&gt;Shakila Mahjabin Tonni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1"&gt;Mark Dras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks. (arXiv:2303.01682v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.01682</id>
        <link href="http://arxiv.org/abs/2303.01682"/>
        <updated>2023-09-23T00:40:40.069Z</updated>
        <summary type="html"><![CDATA[Bayesian Optimization (BO) is an effective approach for global optimization
of black-box functions when function evaluations are expensive. Most prior
works use Gaussian processes to model the black-box function, however, the use
of kernels in Gaussian processes leads to two problems: first, the kernel-based
methods scale poorly with the number of data points and second, kernel methods
are usually not effective on complex structured high dimensional data due to
curse of dimensionality. Therefore, we propose a novel black-box optimization
algorithm where the black-box function is modeled using a neural network. Our
algorithm does not need a Bayesian neural network to estimate predictive
uncertainty and is therefore computationally favorable. We analyze the
theoretical behavior of our algorithm in terms of regret bound using advances
in NTK theory showing its efficient convergence. We perform experiments with
both synthetic and real-world optimization tasks and show that our algorithm is
more sample efficient compared to existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phan_Trong_D/0/1/0/all/0/1"&gt;Dat Phan-Trong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_The_H/0/1/0/all/0/1"&gt;Hung Tran-The&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GrACE: Generation using Associated Code Edits. (arXiv:2305.14129v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.14129</id>
        <link href="http://arxiv.org/abs/2305.14129"/>
        <updated>2023-09-23T00:40:39.921Z</updated>
        <summary type="html"><![CDATA[Developers expend a significant amount of time in editing code for a variety
of reasons such as bug fixing or adding new features. Designing effective
methods to predict code edits has been an active yet challenging area of
research due to the diversity of code edits and the difficulty of capturing the
developer intent. In this work, we address these challenges by endowing
pre-trained large language models (LLMs) of code with the knowledge of prior,
relevant edits. The generative capability of the LLMs helps address the
diversity in code changes and conditioning code generation on prior edits helps
capture the latent developer intent. We evaluate two well-known LLMs, Codex and
CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments
with two datasets, the knowledge of prior edits boosts the performance of the
LLMs significantly and enables them to generate 29% and 54% more correctly
edited code in top-1 suggestions relative to the current state-of-the-art
symbolic and neural approaches, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Priyanshu Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1"&gt;Avishree Khare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajpai_Y/0/1/0/all/0/1"&gt;Yasharth Bajpai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Saikat Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1"&gt;Sumit Gulwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1"&gt;Aditya Kanade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishna_A/0/1/0/all/0/1"&gt;Arjun Radhakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1"&gt;Gustavo Soares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1"&gt;Ashish Tiwari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.06908</id>
        <link href="http://arxiv.org/abs/2305.06908"/>
        <updated>2023-09-23T00:40:39.916Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models (DDPMs) have shown promising
performance for speech synthesis. However, a large number of iterative steps
are required to achieve high sample quality, which restricts the inference
speed. Maintaining sample quality while increasing sampling speed has become a
challenging task. In this paper, we propose a "Co"nsistency "Mo"del-based
"Speech" synthesis method, CoMoSpeech, which achieve speech synthesis through a
single diffusion sampling step while achieving high audio quality. The
consistency constraint is applied to distill a consistency model from a
well-designed diffusion-based teacher model, which ultimately yields superior
performances in the distilled CoMoSpeech. Our experiments show that by
generating audio recordings by a single sampling step, the CoMoSpeech achieves
an inference speed more than 150 times faster than real-time on a single NVIDIA
A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based
speech synthesis truly practical. Meanwhile, objective and subjective
evaluations on text-to-speech and singing voice synthesis show that the
proposed teacher models yield the best audio quality, and the one-step sampling
based CoMoSpeech achieves the best inference speed with better or comparable
audio quality to other conventional multi-step diffusion model baselines. Audio
samples are available at https://comospeech.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zhen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wei Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yike Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes. (arXiv:2309.11875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11875</id>
        <link href="http://arxiv.org/abs/2309.11875"/>
        <updated>2023-09-23T00:40:39.876Z</updated>
        <summary type="html"><![CDATA[Machine learning models trained with structural health monitoring data have
become a powerful tool for system identification. This paper presents a
physics-informed Gaussian process (GP) model for Timoshenko beam elements. The
model is constructed as a multi-output GP with covariance and cross-covariance
kernels analytically derived based on the differential equations for
deflections, rotations, strains, bending moments, shear forces and applied
loads. Stiffness identification is performed in a Bayesian format by maximising
a posterior model through a Markov chain Monte Carlo method, yielding a
stochastic model for the structural parameters. The optimised GP model is
further employed for probabilistic predictions of unobserved responses.
Additionally, an entropy-based method for physics-informed sensor placement
optimisation is presented, exploiting heterogeneous sensor position information
and structural boundary conditions built into the GP model. Results demonstrate
that the proposed approach is effective at identifying structural parameters
and is capable of fusing data from heterogeneous and multi-fidelity sensors.
Probabilistic predictions of structural responses and internal forces are in
closer agreement with measured data. We validate our model with an experimental
setup and discuss the quality and uncertainty of the obtained results. The
proposed approach has potential applications in the field of structural health
monitoring (SHM) for both mechanical and structural systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tondo_G/0/1/0/all/0/1"&gt;Gledson Rodrigo Tondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rau_S/0/1/0/all/0/1"&gt;Sebastian Rau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kavrakov_I/0/1/0/all/0/1"&gt;Igor Kavrakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgenthal_G/0/1/0/all/0/1"&gt;Guido Morgenthal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.11052</id>
        <link href="http://arxiv.org/abs/2309.11052"/>
        <updated>2023-09-23T00:40:39.871Z</updated>
        <summary type="html"><![CDATA[The proliferation of fake news has become a significant concern in recent
times due to its potential to spread misinformation and manipulate public
opinion. This paper presents a comprehensive study on detecting fake news in
Brazilian Portuguese, focusing on journalistic-type news. We propose a machine
learning-based approach that leverages natural language processing techniques,
including TF-IDF and Word2Vec, to extract features from textual data. We
evaluate the performance of various classification algorithms, such as logistic
regression, support vector machine, random forest, AdaBoost, and LightGBM, on a
dataset containing both true and fake news articles. The proposed approach
achieves high accuracy and F1-Score, demonstrating its effectiveness in
identifying fake news. Additionally, we developed a user-friendly web platform,
fakenewsbr.com, to facilitate the verification of news articles' veracity. Our
platform provides real-time analysis, allowing users to assess the likelihood
of fake news articles. Through empirical analysis and comparative studies, we
demonstrate the potential of our approach to contribute to the fight against
the spread of fake news and promote more informed media consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giordani_L/0/1/0/all/0/1"&gt;Luiz Giordani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daru_G/0/1/0/all/0/1"&gt;Gilsiley Dar&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Queiroz_R/0/1/0/all/0/1"&gt;Rhenan Queiroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buzinaro_V/0/1/0/all/0/1"&gt;Vitor Buzinaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiva_D/0/1/0/all/0/1"&gt;Davi Keglevich Neiva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_D/0/1/0/all/0/1"&gt;Daniel Camilo Fuentes Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_M/0/1/0/all/0/1"&gt;Marcos Jardel Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_O/0/1/0/all/0/1"&gt;Oilson Alberto Gonzatto Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louzada_F/0/1/0/all/0/1"&gt;Francisco Louzada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.12273</id>
        <link href="http://arxiv.org/abs/2309.12273"/>
        <updated>2023-09-23T00:40:39.859Z</updated>
        <summary type="html"><![CDATA[Rapid and accurate identification of Venous thromboembolism (VTE), a severe
cardiovascular condition including deep vein thrombosis (DVT) and pulmonary
embolism (PE), is important for effective treatment. Leveraging Natural
Language Processing (NLP) on radiology reports, automated methods have shown
promising advancements in identifying VTE events from retrospective data
cohorts or aiding clinical experts in identifying VTE events from radiology
reports. However, effectively training Deep Learning (DL) and the NLP models is
challenging due to limited labeled medical text data, the complexity and
heterogeneity of radiology reports, and data imbalance. This study proposes
novel method combinations of DL methods, along with data augmentation, adaptive
pre-trained NLP model selection, and a clinical expert NLP rule-based
classifier, to improve the accuracy of VTE identification in unstructured
(free-text) radiology reports. Our experimental results demonstrate the model's
efficacy, achieving an impressive 97\% accuracy and 97\% F1 score in predicting
DVT, and an outstanding 98.3\% accuracy and 98.4\% F1 score in predicting PE.
These findings emphasize the model's robustness and its potential to
significantly contribute to VTE research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jamie Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yusen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayssen_H/0/1/0/all/0/1"&gt;Hilary Hayssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Englum_B/0/1/0/all/0/1"&gt;Brain Englum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kankaria_A/0/1/0/all/0/1"&gt;Aman Kankaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayorga_Carlin_M/0/1/0/all/0/1"&gt;Minerva Mayorga-Carlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1"&gt;Shalini Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorkin_J/0/1/0/all/0/1"&gt;John Sorkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lal_B/0/1/0/all/0/1"&gt;Brajesh Lal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yesha_Y/0/1/0/all/0/1"&gt;Yelena Yesha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face Identity-Aware Disentanglement in StyleGAN. (arXiv:2309.12033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.12033</id>
        <link href="http://arxiv.org/abs/2309.12033"/>
        <updated>2023-09-23T00:40:39.843Z</updated>
        <summary type="html"><![CDATA[Conditional GANs are frequently used for manipulating the attributes of face
images, such as expression, hairstyle, pose, or age. Even though the
state-of-the-art models successfully modify the requested attributes, they
simultaneously modify other important characteristics of the image, such as a
person's identity. In this paper, we focus on solving this problem by
introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles
face attributes from a person's identity. Our key idea is to perform training
on images retrieved from movie frames, where a given person appears in various
poses and with different attributes. By applying a type of contrastive loss, we
encourage the model to group images of the same person in similar regions of
latent space. Our experiments demonstrate that the modifications of face
attributes performed by PluGeN4Faces are significantly less invasive on the
remaining characteristics of the image than in the existing state-of-the-art
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suwala_A/0/1/0/all/0/1"&gt;Adrian Suwa&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wojcik_B/0/1/0/all/0/1"&gt;Bartosz W&amp;#xf3;jcik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proszewska_M/0/1/0/all/0/1"&gt;Magdalena Proszewska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1"&gt;Jacek Tabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1"&gt;Marek &amp;#x15a;mieja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric and Regularized Dynamical Wasserstein Barycenters for Sequential Observations. (arXiv:2210.01918v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.01918</id>
        <link href="http://arxiv.org/abs/2210.01918"/>
        <updated>2023-09-23T00:40:39.836Z</updated>
        <summary type="html"><![CDATA[We consider probabilistic models for sequential observations which exhibit
gradual transitions among a finite number of states. We are particularly
motivated by applications such as human activity analysis where observed
accelerometer time series contains segments representing distinct activities,
which we call pure states, as well as periods characterized by continuous
transition among these pure states. To capture this transitory behavior, the
dynamical Wasserstein barycenter (DWB) model of Cheng et al. in 2021 [1]
associates with each pure state a data-generating distribution and models the
continuous transitions among these states as a Wasserstein barycenter of these
distributions with dynamically evolving weights. Focusing on the univariate
case where Wasserstein distances and barycenters can be computed in closed
form, we extend [1] specifically relaxing the parameterization of the pure
states as Gaussian distributions. We highlight issues related to the uniqueness
in identifying the model parameters as well as uncertainties induced when
estimating a dynamically evolving distribution from a limited number of
samples. To ameliorate non-uniqueness, we introduce regularization that imposes
temporal smoothness on the dynamics of the barycentric weights. A
quantile-based approximation of the pure state distributions yields a finite
dimensional estimation problem which we numerically solve using cyclic descent
alternating between updates to the pure-state quantile functions and the
barycentric weights. We demonstrate the utility of the proposed algorithm in
segmenting both simulated and real world human activity time series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kevin C. Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1"&gt;Shuchin Aeron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_E/0/1/0/all/0/1"&gt;Eric L. Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed State-space Neural Networks for Transport Phenomena. (arXiv:2309.12211v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12211</id>
        <link href="http://arxiv.org/abs/2309.12211"/>
        <updated>2023-09-23T00:40:39.831Z</updated>
        <summary type="html"><![CDATA[This work introduces Physics-informed State-space neural network Models
(PSMs), a novel solution to achieving real-time optimization, flexibility, and
fault tolerance in autonomous systems, particularly in transport-dominated
systems such as chemical, biomedical, and power plants. Traditional data-driven
methods fall short due to a lack of physical constraints like mass
conservation; PSMs address this issue by training deep neural networks with
sensor data and physics-informing using components' Partial Differential
Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable
forward dynamics model. Through two in silico experiments - a heated channel
and a cooling system loop - we demonstrate that PSMs offer a more accurate
approach than purely data-driven models.

Beyond accuracy, there are several compelling use cases for PSMs. In this
work, we showcase two: the creation of a nonlinear supervisory controller
through a sequentially updated state-space representation and the proposal of a
diagnostic algorithm using residuals from each of the PDEs. The former
demonstrates the ability of PSMs to handle both constant and time-dependent
constraints, while the latter illustrates their value in system diagnostics and
fault detection. We further posit that PSMs could serve as a foundation for
Digital Twins, constantly updated digital representations of physical systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1"&gt;Akshay J Dave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vilim_R/0/1/0/all/0/1"&gt;Richard B. Vilim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions. (arXiv:2309.11963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11963</id>
        <link href="http://arxiv.org/abs/2309.11963"/>
        <updated>2023-09-23T00:40:39.819Z</updated>
        <summary type="html"><![CDATA[This study introduces a novel hierarchical divisive clustering approach with
stochastic splitting functions (SSFs) to enhance classification performance in
multi-class datasets through hierarchical classification (HC). The method has
the unique capability of generating hierarchy without requiring explicit
information, making it suitable for datasets lacking prior knowledge of
hierarchy. By systematically dividing classes into two subsets based on their
discriminability according to the classifier, the proposed approach constructs
a binary tree representation of hierarchical classes. The approach is evaluated
on 46 multi-class time series datasets using popular classifiers (svm and
rocket) and SSFs (potr, srtr, and lsoo). The results reveal that the approach
significantly improves classification performance in approximately half and a
third of the datasets when using rocket and svm as the classifier,
respectively. The study also explores the relationship between dataset features
and HC performance. While the number of classes and flat classification (FC)
score show consistent significance, variations are observed with different
splitting functions. Overall, the proposed approach presents a promising
strategy for enhancing classification by generating hierarchical structure in
multi-class time series datasets. Future research directions involve exploring
different splitting functions, classifiers, and hierarchy structures, as well
as applying the approach to diverse domains beyond time series data. The source
code is made openly available to facilitate reproducibility and further
exploration of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alagoz_C/0/1/0/all/0/1"&gt;Celal Alagoz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.08587</id>
        <link href="http://arxiv.org/abs/2309.08587"/>
        <updated>2023-09-23T00:40:39.814Z</updated>
        <summary type="html"><![CDATA[To make effective decisions in novel environments with long-horizon goals, it
is crucial to engage in hierarchical reasoning across spatial and temporal
scales. This entails planning abstract subgoal sequences, visually reasoning
about the underlying plans, and executing actions in accordance with the
devised plan through visual-motor control. We propose Compositional Foundation
Models for Hierarchical Planning (HiP), a foundation model which leverages
multiple expert foundation model trained on language, vision and action data
individually jointly together to solve long-horizon tasks. We use a large
language model to construct symbolic plans that are grounded in the environment
through a large video diffusion model. Generated video plans are then grounded
to visual-motor control, through an inverse dynamics model that infers actions
from generated videos. To enable effective reasoning within this hierarchy, we
enforce consistency between the models via iterative refinement. We illustrate
the efficacy and adaptability of our approach in three different long-horizon
table-top manipulation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ajay_A/0/1/0/all/0/1"&gt;Anurag Ajay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Seungwook Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yilun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhi Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Josh Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1"&gt;Leslie Kaelbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Akash Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1"&gt;Pulkit Agrawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On. (arXiv:2309.12218v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.12218</id>
        <link href="http://arxiv.org/abs/2309.12218"/>
        <updated>2023-09-23T00:40:39.807Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation, aiming at making the prediction of the user's
next item click based on the information in a single session only even in the
presence of some random user's behavior, is a complex problem. This complex
problem requires a high-capability model of predicting the user's next action.
Most (if not all) existing models follow the encoder-predictor paradigm where
all studies focus on how to optimize the encoder module extensively in the
paradigm but they ignore how to optimize the predictor module. In this paper,
we discover the existing critical issue of the low-capability predictor module
among existing models. Motivated by this, we propose a novel framework called
\emph{\underline{S}ession-based \underline{R}ecommendation with
\underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this
framework, we propose a high-capability predictor module which could alleviate
the effect of random user's behavior for prediction. It is worth mentioning
that this framework could be applied to any existing models, which could give
opportunities for further optimizing the framework. Extensive experiments on
two real benchmark datasets for three state-of-the-art models show that
\emph{SR-PredictAO} out-performs the current state-of-the-art model by up to
2.9\% in HR@20 and 2.3\% in MRR@20. More importantly, the improvement is
consistent across almost all the existing models on all datasets, which could
be regarded as a significant contribution in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruida Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1"&gt;Raymond Chi-Wing Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1"&gt;Weile Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives. (arXiv:2309.12067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.12067</id>
        <link href="http://arxiv.org/abs/2309.12067"/>
        <updated>2023-09-23T00:40:39.791Z</updated>
        <summary type="html"><![CDATA[Action scene understanding in soccer is a challenging task due to the complex
and dynamic nature of the game, as well as the interactions between players.
This article provides a comprehensive overview of this task divided into action
recognition, spotting, and spatio-temporal action localization, with a
particular emphasis on the modalities used and multimodal methods. We explore
the publicly available data sources and metrics used to evaluate models'
performance. The article reviews recent state-of-the-art methods that leverage
deep learning techniques and traditional methods. We focus on multimodal
methods, which integrate information from multiple sources, such as video and
audio data, and also those that represent one source in various ways. The
advantages and limitations of methods are discussed, along with their potential
for improving the accuracy and robustness of models. Finally, the article
highlights some of the open research questions and future directions in the
field of soccer action recognition, including the potential for multimodal
methods to advance this field. Overall, this survey provides a valuable
resource for researchers interested in the field of action scene understanding
in soccer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seweryn_K/0/1/0/all/0/1"&gt;Karolina Seweryn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1"&gt;Anna Wr&amp;#xf3;blewska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasik_S/0/1/0/all/0/1"&gt;Szymon &amp;#x141;ukasik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2309.11132</id>
        <link href="http://arxiv.org/abs/2309.11132"/>
        <updated>2023-09-23T00:40:39.746Z</updated>
        <summary type="html"><![CDATA[The challenge in sourcing attribution for forgery faces has gained widespread
attention due to the rapid development of generative techniques. While many
recent works have taken essential steps on GAN-generated faces, more
threatening attacks related to identity swapping or expression transferring are
still overlooked. And the forgery traces hidden in unknown attacks from the
open-world unlabeled faces still remain under-explored. To push the related
frontier research, we introduce a new benchmark called Open-World DeepFake
Attribution (OW-DFA), which aims to evaluate attribution performance against
various types of fake faces under open-world scenarios. Meanwhile, we propose a
novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task
through 1) introducing a Global-Local Voting module to guide the feature
alignment of forged faces with different manipulated regions, 2) designing a
Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused
by similar methods in unlabeled set. In addition, we extend the CPL framework
with a multi-stage paradigm that leverages pre-train technique and iterative
learning to further enhance traceability performance. Extensive experiments
verify the superiority of our proposed method on the OW-DFA and also
demonstrate the interpretability of deepfake attribution task and its impact on
improving the security of deepfake detection area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhimin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Taiping Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bangjie Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1"&gt;Ran Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shouhong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12252</id>
        <link href="http://arxiv.org/abs/2309.12252"/>
        <updated>2023-09-23T00:40:39.735Z</updated>
        <summary type="html"><![CDATA[Sequential models, such as Recurrent Neural Networks and Neural Ordinary
Differential Equations, have long suffered from slow training due to their
inherent sequential nature. For many years this bottleneck has persisted, as
many thought sequential models could not be parallelized. We challenge this
long-held belief with our parallel algorithm that accelerates GPU evaluation of
sequential models by up to 3 orders of magnitude faster without compromising
output accuracy. The algorithm does not need any special structure in the
sequential models' architecture, making it applicable to a wide range of
architectures. Using our method, training sequential models can be more than 10
times faster than the common sequential method without any meaningful
difference in the training results. Leveraging this accelerated training, we
discovered the efficacy of the Gated Recurrent Unit in a long time series
classification problem with 17k time samples. By overcoming the training
bottleneck, our work serves as the first step to unlock the potential of
non-linear sequential models for long sequence problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1"&gt;Yi Heng Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selfridge_J/0/1/0/all/0/1"&gt;Joshua Selfridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasim_M/0/1/0/all/0/1"&gt;Muhammad Firmansyah Kasim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v2 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2304.00216</id>
        <link href="http://arxiv.org/abs/2304.00216"/>
        <updated>2023-09-23T00:40:39.699Z</updated>
        <summary type="html"><![CDATA[Analyzing high resolution whole slide images (WSIs) with regard to
information across multiple scales poses a significant challenge in digital
pathology. Multi-instance learning (MIL) is a common solution for working with
high resolution images by classifying bags of objects (i.e. sets of smaller
image patches). However, such processing is typically performed at a single
scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale
information that is key to diagnoses by human pathologists. In this study, we
propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale
relationships into a single MIL network for pathological image diagnosis. The
contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL)
algorithm that integrates the multi-scale information and the inter-scale
relationships is proposed; (2) A toy dataset with scale-specific morphological
features is created and released to examine and visualize differential
cross-scale attention; (3) Superior performance on both in-house and public
datasets is demonstrated by our simple cross-scale MIL strategy. The official
implementation is publicly available at https://github.com/hrlblab/CS-MIL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruining Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1"&gt;Can Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Remedios_L/0/1/0/all/0/1"&gt;Lucas W. Remedios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bao_S/0/1/0/all/0/1"&gt;Shunxing Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Womick_R/0/1/0/all/0/1"&gt;R. Michael Womick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chiron_S/0/1/0/all/0/1"&gt;Sophie Chiron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roland_J/0/1/0/all/0/1"&gt;Joseph T. Roland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lau_K/0/1/0/all/0/1"&gt;Ken S. Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilson_K/0/1/0/all/0/1"&gt;Keith T. Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaohong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Coburn_L/0/1/0/all/0/1"&gt;Lori A. Coburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization. (arXiv:2309.11856v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.11856</id>
        <link href="http://arxiv.org/abs/2309.11856"/>
        <updated>2023-09-23T00:40:39.641Z</updated>
        <summary type="html"><![CDATA[Efficient training of large-scale graph neural networks (GNNs) has been
studied with a specific focus on reducing their memory consumption. Work by Liu
et al. (2022) proposed extreme activation compression (EXACT) which
demonstrated drastic reduction in memory consumption by performing quantization
of the intermediate activation maps down to using INT2 precision. They showed
little to no reduction in performance while achieving large reductions in GPU
memory consumption. In this work, we present an improvement to the EXACT
strategy by using block-wise quantization of the intermediate activation maps.
We experimentally analyze different block sizes and show further reduction in
memory consumption (>15%), and runtime speedup per epoch (about 5%) even when
performing extreme extents of quantization with similar performance trade-offs
as with the original EXACT. Further, we present a correction to the assumptions
on the distribution of intermediate activation maps in EXACT (assumed to be
uniform) and show improved variance estimations of the quantization and
dequantization steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Eliassen_S/0/1/0/all/0/1"&gt;Sebastian Eliassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Selvan_R/0/1/0/all/0/1"&gt;Raghavendra Selvan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Probability of Immunity. (arXiv:2309.11942v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2309.11942</id>
        <link href="http://arxiv.org/abs/2309.11942"/>
        <updated>2023-09-23T00:40:39.609Z</updated>
        <summary type="html"><![CDATA[This work is devoted to the study of the probability of immunity, i.e. the
effect occurs whether exposed or not. We derive necessary and sufficient
conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the
probability of immunity is zero and $\epsilon$-bounded, respectively. The
former allows us to estimate the probability of benefit (i.e., the effect
occurs if and only if exposed) from a randomized controlled trial, and the
latter allows us to produce bounds of the probability of benefit that are
tighter than the existing ones. We also introduce the concept of indirect
immunity (i.e., through a mediator) and repeat our previous analysis for it.
Finally, we propose a method for sensitivity analysis of the probability of
immunity under unmeasured confounding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1"&gt;Jose M. Pe&amp;#xf1;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance. (arXiv:2309.12259v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12259</id>
        <link href="http://arxiv.org/abs/2309.12259"/>
        <updated>2023-09-23T00:40:39.608Z</updated>
        <summary type="html"><![CDATA[Stochastic Gradient Descent (SGD), a widely used optimization algorithm in
deep learning, is often limited to converging to local optima due to the
non-convex nature of the problem. Leveraging these local optima to improve
model performance remains a challenging task. Given the inherent complexity of
neural networks, the simple arithmetic averaging of the obtained local optima
models in undesirable results. This paper proposes a {\em soft merging} method
that facilitates rapid merging of multiple models, simplifies the merging of
specific parts of neural networks, and enhances robustness against malicious
models with extreme values. This is achieved by learning gate parameters
through a surrogate of the $l_0$ norm using hard concrete distribution without
modifying the model weights of the given local optima models. This merging
process not only enhances the model performance by converging to a better local
optimum, but also minimizes computational costs, offering an efficient and
explicit learning process integrated with stochastic gradient descent. Thorough
experiments underscore the effectiveness and superior performance of the merged
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yusen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yesha_Y/0/1/0/all/0/1"&gt;Yelena Yesha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.12245</id>
        <link href="http://arxiv.org/abs/2309.12245"/>
        <updated>2023-09-23T00:40:39.602Z</updated>
        <summary type="html"><![CDATA[Biomedical image datasets can be imbalanced due to the rarity of targeted
diseases. Generative Adversarial Networks play a key role in addressing this
imbalance by enabling the generation of synthetic images to augment datasets.
It is important to generate synthetic images that incorporate a diverse range
of features to accurately represent the distribution of features present in the
training imagery. Furthermore, the absence of diverse features in synthetic
images can degrade the performance of machine learning classifiers. The mode
collapse problem impacts Generative Adversarial Networks' capacity to generate
diversified images. Mode collapse comes in two varieties: intra-class and
inter-class. In this paper, both varieties of the mode collapse problem are
investigated, and their subsequent impact on the diversity of synthetic X-ray
images is evaluated. This work contributes an empirical demonstration of the
benefits of integrating the adaptive input-image normalization with the Deep
Convolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse
problems. Synthetically generated images are utilized for data augmentation and
training a Vision Transformer model. The classification performance of the
model is evaluated using accuracy, recall, and precision scores. Results
demonstrate that the DCGAN and the ACGAN with adaptive input-image
normalization outperform the DCGAN and ACGAN with un-normalized X-ray images as
evidenced by the superior diversity scores and classification scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saad_M/0/1/0/all/0/1"&gt;Muhammad Muneeb Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rehmani_M/0/1/0/all/0/1"&gt;Mubashir Husain Rehmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OReilly_R/0/1/0/all/0/1"&gt;Ruairi O&amp;#x27;Reilly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Expert Behavior in Offline Training Datasets Improves Behavioral Cloning of Robotic Manipulation Policies. (arXiv:2301.13019v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.13019</id>
        <link href="http://arxiv.org/abs/2301.13019"/>
        <updated>2023-09-23T00:40:39.602Z</updated>
        <summary type="html"><![CDATA[This paper presents our solution for the Real Robot Challenge (RRC) III, a
competition featured in the NeurIPS 2022 Competition Track, aimed at addressing
dexterous robotic manipulation tasks through learning from pre-collected
offline data. Participants were provided with two types of datasets for each
task: expert and mixed datasets with varying skill levels. While the simplest
offline policy learning algorithm, Behavioral Cloning (BC), performed
remarkably well when trained on expert datasets, it outperformed even the most
advanced offline reinforcement learning (RL) algorithms. However, BC's
performance deteriorated when applied to mixed datasets, and the performance of
offline RL algorithms was also unsatisfactory. Upon examining the mixed
datasets, we observed that they contained a significant amount of expert data,
although this data was unlabeled. To address this issue, we proposed a
semi-supervised learning-based classifier to identify the underlying expert
behavior within mixed datasets, effectively isolating the expert data. To
further enhance BC's performance, we leveraged the geometric symmetry of the
RRC arena to augment the training dataset through mathematical transformations.
In the end, our submission surpassed that of all other participants, even those
who employed complex offline RL algorithms and intricate data processing and
feature engineering techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCarthy_R/0/1/0/all/0/1"&gt;Robert McCarthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulens_D/0/1/0/all/0/1"&gt;David Cordova Bulens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_F/0/1/0/all/0/1"&gt;Francisco Roldan Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1"&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1"&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redmond_S/0/1/0/all/0/1"&gt;Stephen J. Redmond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empowering Precision Medicine: AI-Driven Schizophrenia Diagnosis via EEG Signals: A Comprehensive Review from 2002-2023. (arXiv:2309.12202v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.12202</id>
        <link href="http://arxiv.org/abs/2309.12202"/>
        <updated>2023-09-23T00:40:39.601Z</updated>
        <summary type="html"><![CDATA[Schizophrenia (SZ) is a prevalent mental disorder characterized by cognitive,
emotional, and behavioral changes. Symptoms of SZ include hallucinations,
illusions, delusions, lack of motivation, and difficulties in concentration.
Diagnosing SZ involves employing various tools, including clinical interviews,
physical examinations, psychological evaluations, the Diagnostic and
Statistical Manual of Mental Disorders (DSM), and neuroimaging techniques.
Electroencephalography (EEG) recording is a significant functional neuroimaging
modality that provides valuable insights into brain function during SZ.
However, EEG signal analysis poses challenges for neurologists and scientists
due to the presence of artifacts, long-term recordings, and the utilization of
multiple channels. To address these challenges, researchers have introduced
artificial intelligence (AI) techniques, encompassing conventional machine
learning (ML) and deep learning (DL) methods, to aid in SZ diagnosis. This
study reviews papers focused on SZ diagnosis utilizing EEG signals and AI
methods. The introduction section provides a comprehensive explanation of SZ
diagnosis methods and intervention techniques. Subsequently, review papers in
this field are discussed, followed by an introduction to the AI methods
employed for SZ diagnosis and a summary of relevant papers presented in tabular
form. Additionally, this study reports on the most significant challenges
encountered in SZ diagnosis, as identified through a review of papers in this
field. Future directions to overcome these challenges are also addressed. The
discussion section examines the specific details of each paper, culminating in
the presentation of conclusions and findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jafari_M/0/1/0/all/0/1"&gt;Mahboobeh Jafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sadeghi_D/0/1/0/all/0/1"&gt;Delaram Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shoeibi_A/0/1/0/all/0/1"&gt;Afshin Shoeibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alinejad_Rokny_H/0/1/0/all/0/1"&gt;Hamid Alinejad-Rokny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beheshti_A/0/1/0/all/0/1"&gt;Amin Beheshti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_D/0/1/0/all/0/1"&gt;David L&amp;#xf3;pez Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1"&gt;U. Rajendra Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gorriz_J/0/1/0/all/0/1"&gt;Juan M. Gorriz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyena Neural Operator for Partial Differential Equations. (arXiv:2306.16524v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.16524</id>
        <link href="http://arxiv.org/abs/2306.16524"/>
        <updated>2023-09-23T00:40:39.601Z</updated>
        <summary type="html"><![CDATA[Numerically solving partial differential equations typically requires fine
discretization to resolve necessary spatiotemporal scales, which can be
computationally expensive. Recent advances in deep learning have provided a new
approach to solving partial differential equations that involves the use of
neural operators. Neural operators are neural network architectures that learn
mappings between function spaces and have the capability to solve partial
differential equations based on data. This study utilizes a novel neural
operator called Hyena, which employs a long convolutional filter that is
parameterized by a multilayer perceptron. The Hyena operator is an operation
that enjoys sub-quadratic complexity and state space model to parameterize long
convolution that enjoys a global receptive field. This mechanism enhances the
model's comprehension of the input's context and enables data-dependent weight
for different partial differential equations instances. To measure how
effective the layers are in solving partial differential equations, we conduct
experiments on Diffusion-Reaction equation and Navier Stokes equation. Our
findings indicate Hyena Neural operator can serve as an efficient and accurate
model for learning partial differential equations solution operator. The data
and code used can be found at:
https://github.com/Saupatil07/Hyena-Neural-Operator]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1"&gt;Saurabh Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zijie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1"&gt;Amir Barati Farimani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.10457</id>
        <link href="http://arxiv.org/abs/2308.10457"/>
        <updated>2023-09-23T00:40:39.601Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a distributed machine learning technique that
allows model training among multiple devices or organizations by sharing
training parameters instead of raw data. However, adversaries can still infer
individual information through inference attacks (e.g. differential attacks) on
these training parameters. As a result, Differential Privacy (DP) has been
widely used in FL to prevent such attacks. We consider differentially private
federated learning in a resource-constrained scenario, where both privacy
budget and communication round are constrained. By theoretically analyzing the
convergence, we can find the optimal number of differentially private local
iterations for clients between any two sequential global updates. Based on
this, we design an algorithm of differentially private federated learning with
adaptive local iterations (ALI-DPFL). We experiment our algorithm on the
FashionMNIST and CIFAR10 datasets, and demonstrate significantly better
performances than previous work in the resource-constraint scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xinpeng Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jie Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kuncan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haitao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhili Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly supervised learning for pattern classification in serial femtosecond crystallography. (arXiv:2309.04474v2 [cond-mat.mtrl-sci] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.04474</id>
        <link href="http://arxiv.org/abs/2309.04474"/>
        <updated>2023-09-23T00:40:39.601Z</updated>
        <summary type="html"><![CDATA[Serial femtosecond crystallography at X-ray free electron laser facilities
opens a new era for the determination of crystal structure. However, the data
processing of those experiments is facing unprecedented challenge, because the
total number of diffraction patterns needed to determinate a high-resolution
structure is huge. Machine learning methods are very likely to play important
roles in dealing with such a large volume of data. Convolutional neural
networks have made a great success in the field of pattern classification,
however, training of the networks need very large datasets with labels. Th is
heavy dependence on labeled datasets will seriously restrict the application of
networks, because it is very costly to annotate a large number of diffraction
patterns. In this article we present our job on the classification of
diffraction pattern by weakly supervised algorithms, with the aim of reducing
as much as possible the size of the labeled dataset required for training. Our
result shows that weakly supervised methods can significantly reduce the need
for the number of labeled patterns while achieving comparable accuracy to fully
supervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jianan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xihui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Huai_P/0/1/0/all/0/1"&gt;Ping Huai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaofeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions. (arXiv:2208.03392v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.03392</id>
        <link href="http://arxiv.org/abs/2208.03392"/>
        <updated>2023-09-23T00:40:39.589Z</updated>
        <summary type="html"><![CDATA[With the advent of the IoT, AI and ML/DL algorithms, the landscape of
data-driven medical applications has emerged as a promising avenue for
designing robust and scalable diagnostic and prognostic models from medical
data. Consequently, the realm of data-driven medical applications has garnered
significant attention spanning academia and industry, ushering in marked
enhancements in healthcare delivery quality. Despite these strides, the
adoption of AI-driven medical applications remains hindered by formidable
challenges, including the arduous task of meeting security, privacy, and
quality of service (QoS) standards. Recent developments in federated learning
have made it possible to train complex machine-learned models in a distributed
manner and has become an active research domain, particularly processing the
medical data at the edge of the network in a decentralized way to preserve
privacy and address security concerns. To this end, this survey paper
highlights the current and future of FL technology in medical applications
where data sharing is a significant burden. We delve into the contemporary
research trends and their outcomes, unravelling the intricacies of designing
reliable and scalable FL models. Our survey outlines the foundational
statistical predicaments of FL, confronts device-related obstacles, delves into
security challenges, and navigates the intricate terrain of privacy concerns,
all while spotlighting its transformative potential within the medical domain.
A primary focus of our study rests on medical applications, where we underscore
the weighty burden of global cancer and illuminate the potency of FL in
engendering computer-aided diagnosis tools that address this challenge with
heightened efficacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rauniyar_A/0/1/0/all/0/1"&gt;Ashish Rauniyar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagos_D/0/1/0/all/0/1"&gt;Desta Haileselassie Hagos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haakegaard_J/0/1/0/all/0/1"&gt;Jan Erik H&amp;#xe5;keg&amp;#xe5;rd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1"&gt;Ulas Bagci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_D/0/1/0/all/0/1"&gt;Danda B. Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlassov_V/0/1/0/all/0/1"&gt;Vladimir Vlassov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems. (arXiv:2309.08617v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.08617</id>
        <link href="http://arxiv.org/abs/2309.08617"/>
        <updated>2023-09-23T00:40:39.589Z</updated>
        <summary type="html"><![CDATA[Real-world production systems often grapple with maintaining data quality in
large-scale, dynamic streams. We introduce Drifter, an efficient and
lightweight system for online feature monitoring and verification in
recommendation use cases. Drifter addresses limitations of existing methods by
delivering agile, responsive, and adaptable data quality monitoring, enabling
real-time root cause analysis, drift detection and insights into problematic
production events. Integrating state-of-the-art online feature ranking for
sparse data and anomaly detection ideas, Drifter is highly scalable and
resource-efficient, requiring only two threads and less than a gigabyte of RAM
per production deployments that handle millions of instances per minute.
Evaluation on real-world data sets demonstrates Drifter's effectiveness in
alerting and mitigating data quality issues, substantially improving
reliability and performance of real-time live recommender systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1"&gt;Bla&amp;#x17e; &amp;#x160;krlj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ki_Tov_N/0/1/0/all/0/1"&gt;Nir Ki-Tov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edelist_L/0/1/0/all/0/1"&gt;Lee Edelist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silberstein_N/0/1/0/all/0/1"&gt;Natalia Silberstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weisman_Zohar_H/0/1/0/all/0/1"&gt;Hila Weisman-Zohar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mramor_B/0/1/0/all/0/1"&gt;Bla&amp;#x17e; Mramor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopic_D/0/1/0/all/0/1"&gt;Davorin Kopi&amp;#x10d;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziporin_N/0/1/0/all/0/1"&gt;Naama Ziporin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Behavioural Cloning with Positive Unlabeled Learning. (arXiv:2301.11734v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.11734</id>
        <link href="http://arxiv.org/abs/2301.11734"/>
        <updated>2023-09-23T00:40:39.580Z</updated>
        <summary type="html"><![CDATA[Learning control policies offline from pre-recorded datasets is a promising
avenue for solving challenging real-world problems. However, available datasets
are typically of mixed quality, with a limited number of the trajectories that
we would consider as positive examples; i.e., high-quality demonstrations.
Therefore, we propose a novel iterative learning algorithm for identifying
expert trajectories in unlabeled mixed-quality robotics datasets given a
minimal set of positive examples, surpassing existing algorithms in terms of
accuracy. We show that applying behavioral cloning to the resulting filtered
dataset outperforms several competitive offline reinforcement learning and
imitation learning baselines. We perform experiments on a range of simulated
locomotion tasks and on two challenging manipulation tasks on a real robotic
system; in these experiments, our method showcases state-of-the-art
performance. Our website:
\url{https://sites.google.com/view/offline-policy-learning-pubc}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCarthy_R/0/1/0/all/0/1"&gt;Robert McCarthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulens_D/0/1/0/all/0/1"&gt;David Cordova Bulens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1"&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1"&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurtler_N/0/1/0/all/0/1"&gt;Nico G&amp;#xfc;rtler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmaier_F/0/1/0/all/0/1"&gt;Felix Widmaier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_F/0/1/0/all/0/1"&gt;Francisco Roldan Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redmond_S/0/1/0/all/0/1"&gt;Stephen J. Redmond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling. (arXiv:2309.11983v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11983</id>
        <link href="http://arxiv.org/abs/2309.11983"/>
        <updated>2023-09-23T00:40:39.579Z</updated>
        <summary type="html"><![CDATA[Connectionist temporal classification (CTC) is commonly adopted for sequence
modeling tasks like speech recognition, where it is necessary to preserve order
between the input and target sequences. However, CTC is only applied to
deterministic sequence models, where the latent space is discontinuous and
sparse, which in turn makes them less capable of handling data variability when
compared to variational models. In this paper, we integrate CTC with a
variational model and derive loss functions that can be used to train more
generalizable sequence models that preserve order. Specifically, we derive two
versions of the novel variational CTC based on two reasonable assumptions, the
first being that the variational latent variables at each time step are
conditionally independent; and the second being that these latent variables are
Markovian. We show that both loss functions allow direct optimization of the
variational lower bound for the model log-likelihood, and present
computationally tractable forms for implementing them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1"&gt;Zheng Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1"&gt;Ting Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethu_V/0/1/0/all/0/1"&gt;Vidhyasaharan Sethu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_B/0/1/0/all/0/1"&gt;Beena Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PrNet: A Neural Network for Correcting Pseudoranges to Improve Positioning with Android Raw GNSS Measurements. (arXiv:2309.12204v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12204</id>
        <link href="http://arxiv.org/abs/2309.12204"/>
        <updated>2023-09-23T00:40:39.579Z</updated>
        <summary type="html"><![CDATA[We present a neural network for mitigating pseudorange bias to improve
localization performance with data collected from Android smartphones. We
represent pseudorange bias using a pragmatic satellite-wise Multiple Layer
Perceptron (MLP), the inputs of which are six
satellite-receiver-context-related features derived from Android raw Global
Navigation Satellite System (GNSS) measurements. To supervise the training
process, we carefully calculate the target values of pseudorange bias using
location ground truth and smoothing techniques and optimize a loss function
containing the estimation residuals of smartphone clock bias. During the
inference process, we employ model-based localization engines to compute
locations with pseudoranges corrected by the neural network. Consequently, this
hybrid pipeline can attend to both pseudorange bias and noise. We evaluate the
framework on an open dataset and consider four application scenarios for
investigating fingerprinting and cross-trace localization in rural and urban
areas. Extensive experiments demonstrate that the proposed framework
outperforms model-based and state-of-the-art data-driven approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1"&gt;Xu Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1"&gt;Keck Voon Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haochen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based Deep Learning for High-Dimensional Periodic Structures. (arXiv:2309.12223v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.12223</id>
        <link href="http://arxiv.org/abs/2309.12223"/>
        <updated>2023-09-23T00:40:39.579Z</updated>
        <summary type="html"><![CDATA[This work presents a deep learning surrogate model for the fast simulation of
high-dimensional frequency selective surfaces. We consider unit-cells which are
built as multiple concatenated stacks of screens and their design requires the
control over many geometrical degrees of freedom. Thanks to the introduction of
physical insight into the model, it can produce accurate predictions of the
S-parameters of a certain structure after training with a reduced dataset.The
proposed model is highly versatile and it can be used with any kind of
frequency selective surface, based on either perforations or patches of any
arbitrary geometry. Numeric examples are presented here for the case of
frequency selective surfaces composed of screens with rectangular perforations,
showing an excellent agreement between the predicted performance and such
obtained with a full-wave simulator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Polo_Lopez_L/0/1/0/all/0/1"&gt;Lucas Polo-L&amp;#xf3;pez&lt;/a&gt; (IETR, INSA Rennes), &lt;a href="http://arxiv.org/find/eess/1/au:+Magoarou_L/0/1/0/all/0/1"&gt;Luc Le Magoarou&lt;/a&gt; (INSA Rennes, IETR), &lt;a href="http://arxiv.org/find/eess/1/au:+Contreres_R/0/1/0/all/0/1"&gt;Romain Contreres&lt;/a&gt; (CNES), &lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_Vigueras_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a Garc&amp;#xed;a-Vigueras&lt;/a&gt; (IETR, INSA Rennes)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-knowledge Inspired Pseudo Supervision (DIPS) for Unsupervised Image-to-Image Translation Models to Support Cross-Domain Classification. (arXiv:2303.10310v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.10310</id>
        <link href="http://arxiv.org/abs/2303.10310"/>
        <updated>2023-09-23T00:40:39.579Z</updated>
        <summary type="html"><![CDATA[The ability to classify images is dependent on having access to large labeled
datasets and testing on data from the same domain that the model can train on.
Classification becomes more challenging when dealing with new data from a
different domain, where gathering and especially labeling a larger image
dataset for retraining a classification model requires a labor-intensive human
effort. Cross-domain classification frameworks were developed to handle this
data domain shift problem by utilizing unsupervised image-to-image translation
models to translate an input image from the unlabeled domain to the labeled
domain. The problem with these unsupervised models lies in their unsupervised
nature. For lack of annotations, it is not possible to use the traditional
supervised metrics to evaluate these translation models to pick the best-saved
checkpoint model. This paper introduces a new method called Domain-knowledge
Inspired Pseudo Supervision (DIPS) which utilizes domain-informed Gaussian
Mixture Models to generate pseudo annotations to enable the use of traditional
supervised metrics. This method was designed specifically to support
cross-domain classification applications contrary to other typically used
metrics such as the FID which were designed to evaluate the model in terms of
the quality of the generated image from a human-eye perspective. DIPS proves
its effectiveness by outperforming various GAN evaluation metrics, including
FID, when selecting the optimal saved checkpoint model. It is also evaluated
against truly supervised metrics. Furthermore, DIPS showcases its robustness
and interpretability by demonstrating a strong correlation with truly
supervised metrics, highlighting its superiority over existing state-of-the-art
alternatives. The code and data to replicate the results can be found on the
official Github repository: https://github.com/Hindawi91/DIPS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Hindawi_F/0/1/0/all/0/1"&gt;Firas Al-Hindawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddiquee_M/0/1/0/all/0/1"&gt;Md Mahfuzur Rahman Siddiquee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Teresa Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Ying Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML Workloads. (arXiv:2212.03414v2 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.03414</id>
        <link href="http://arxiv.org/abs/2212.03414"/>
        <updated>2023-09-23T00:40:39.578Z</updated>
        <summary type="html"><![CDATA[Emerging real-time multi-model ML (RTMM) workloads such as AR/VR and drone
control involve dynamic behaviors in various granularity; task, model, and
layers within a model. Such dynamic behaviors introduce new challenges to the
system software in an ML system since the overall system load is not completely
predictable, unlike traditional ML workloads. In addition, RTMM workloads
require real-time processing, involve highly heterogeneous models, and target
resource-constrained devices. Under such circumstances, developing an effective
scheduler gains more importance to better utilize underlying hardware
considering the unique characteristics of RTMM workloads. Therefore, we propose
a new scheduler, DREAM, which effectively handles various dynamicity in RTMM
workloads targeting multi-accelerator systems. DREAM quantifies the unique
requirements for RTMM workloads and utilizes the quantified scores to drive
scheduling decisions, considering the current system load and other inference
jobs on different models and input frames. DREAM utilizes tunable parameters
that provide fast and effective adaptivity to dynamic workload changes. In our
evaluation of five scenarios of RTMM workload, DREAM reduces the overall
UXCost, which is an equivalent metric of the energy-delay product (EDP) for
RTMM defined in the paper, by 32.2% and 50.0% in the geometric mean (up to
80.8% and 97.6%) compared to state-of-the-art baselines, which shows the
efficacy of our scheduling methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seah Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1"&gt;Hyoukjun Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jinook Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jihyuck Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Hsin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1"&gt;Liangzhen Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1"&gt;Vikas Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12253</id>
        <link href="http://arxiv.org/abs/2309.12253"/>
        <updated>2023-09-23T00:40:39.573Z</updated>
        <summary type="html"><![CDATA[We introduce an extension to the CLRS algorithmic learning benchmark,
prioritizing scalability and the utilization of sparse representations. Many
algorithms in CLRS require global memory or information exchange, mirrored in
its execution model, which constructs fully connected (not sparse) graphs based
on the underlying problem. Despite CLRS's aim of assessing how effectively
learned algorithms can generalize to larger instances, the existing execution
model becomes a significant constraint due to its demanding memory requirements
and runtime (hard to scale). However, many important algorithms do not demand a
fully connected graph; these algorithms, primarily distributed in nature, align
closely with the message-passing paradigm employed by Graph Neural Networks.
Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark
specifically with scalability and sparseness in mind. Our approach includes
adapted algorithms from the original CLRS benchmark and introduces new problems
from distributed and randomized algorithms. Moreover, we perform a thorough
empirical evaluation of our benchmark. Code is publicly available at
https://github.com/jkminder/SALSA-CLRS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minder_J/0/1/0/all/0/1"&gt;Julian Minder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grotschla_F/0/1/0/all/0/1"&gt;Florian Gr&amp;#xf6;tschla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1"&gt;Jo&amp;#xeb;l Mathys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1"&gt;Roger Wattenhofer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class-wise Classifier Design Capable of Continual Learning using Adaptive Resonance Theory-based Topological Clustering. (arXiv:2203.09879v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.09879</id>
        <link href="http://arxiv.org/abs/2203.09879"/>
        <updated>2023-09-23T00:40:39.572Z</updated>
        <summary type="html"><![CDATA[This paper proposes a supervised classification algorithm capable of
continual learning by utilizing an Adaptive Resonance Theory (ART)-based
growing self-organizing clustering algorithm. The ART-based clustering
algorithm is theoretically capable of continual learning, and the proposed
algorithm independently applies it to each class of training data for
generating classifiers. Whenever an additional training data set from a new
class is given, a new ART-based clustering will be defined in a different
learning space. Thanks to the above-mentioned features, the proposed algorithm
realizes continual learning capability. Simulation experiments showed that the
proposed algorithm has superior classification performance compared with
state-of-the-art clustering-based classification algorithms capable of
continual learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Masuyama_N/0/1/0/all/0/1"&gt;Naoki Masuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nojima_Y/0/1/0/all/0/1"&gt;Yusuke Nojima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawood_F/0/1/0/all/0/1"&gt;Farhan Dawood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zongying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision-making and control with diffractive optical networks. (arXiv:2212.11278v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.11278</id>
        <link href="http://arxiv.org/abs/2212.11278"/>
        <updated>2023-09-23T00:40:39.572Z</updated>
        <summary type="html"><![CDATA[The ultimate goal of artificial intelligence is to mimic the human brain to
perform decision-making and control directly from high-dimensional sensory
input. Diffractive optical networks provide a promising solution for
implementing artificial intelligence with high-speed and low-power consumption.
Most of the reported diffractive optical networks focus on single or multiple
tasks that do not involve environmental interaction, such as object recognition
and image classification. In contrast, the networks capable of performing
decision-making and control have not yet been developed to our knowledge. Here,
we propose using deep reinforcement learning to implement diffractive optical
networks that imitate human-level decision-making and control capability. Such
networks taking advantage of a residual architecture, allow for finding optimal
control policies through interaction with the environment and can be readily
implemented with existing optical devices. The superior performance of these
networks is verified by engaging three types of classic games, Tic-Tac-Toe,
Super Mario Bros., and Car Racing. Finally, we present an experimental
demonstration of playing Tic-Tac-Toe by leveraging diffractive optical networks
based on a spatial light modulator. Our work represents a solid step forward in
advancing diffractive optical networks, which promises a fundamental shift from
the target-driven control of a pre-designed state for simple recognition or
classification tasks to the high-level sensory capability of artificial
intelligence. It may find exciting applications in autonomous driving,
intelligent robots, and intelligent manufacturing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jumin Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1"&gt;Shuyuan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lujun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miroshnichenko_A/0/1/0/all/0/1"&gt;Andrey Miroshnichenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dejian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tingting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tianbao Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persistent Homology of the Multiscale Clustering Filtration. (arXiv:2305.04281v2 [math.AT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.04281</id>
        <link href="http://arxiv.org/abs/2305.04281"/>
        <updated>2023-09-23T00:40:39.572Z</updated>
        <summary type="html"><![CDATA[In many applications in data clustering, it is desirable to find not just a
single partition into clusters but a sequence of partitions describing the data
at different scales, or levels of coarseness. A natural problem then is to
analyse and compare the (not necessarily hierarchical) sequences of partitions
that underpin such multiscale descriptions of data. Here, we introduce a
filtration of abstract simplicial complexes, denoted the Multiscale Clustering
Filtration (MCF), which encodes arbitrary patterns of cluster assignments
across scales, and we prove that the MCF produces stable persistence diagrams.
We then show that the zero-dimensional persistent homology of the MCF measures
the degree of hierarchy in the sequence of partitions, and that the
higher-dimensional persistent homology tracks the emergence and resolution of
conflicts between cluster assignments across the sequence of partitions. To
broaden the theoretical foundations of the MCF, we also provide an equivalent
construction via a nerve complex filtration, and we show that in the
hierarchical case, the MCF reduces to a Vietoris-Rips filtration of an
ultrametric space. We briefly illustrate how the MCF can serve to characterise
multiscale clustering structures in numerical experiments on synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Schindler_D/0/1/0/all/0/1"&gt;Dominik J. Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Barahona_M/0/1/0/all/0/1"&gt;Mauricio Barahona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.11354</id>
        <link href="http://arxiv.org/abs/2309.11354"/>
        <updated>2023-09-23T00:40:39.572Z</updated>
        <summary type="html"><![CDATA[Cities around the world face a critical shortage of affordable and decent
housing. Despite its critical importance for policy, our ability to effectively
monitor and track progress in urban housing is limited. Deep learning-based
computer vision methods applied to street-level images have been successful in
the measurement of socioeconomic and environmental inequalities but did not
fully utilize temporal images to track urban change as time-varying labels are
often unavailable. We used self-supervised methods to measure change in London
using 15 million street images taken between 2008 and 2021. Our novel
adaptation of Barlow Twins, Street2Vec, embeds urban structure while being
invariant to seasonal and daily changes without manual annotations. It
outperformed generic embeddings, successfully identified point-level change in
London's housing supply from street-level images, and distinguished between
major and minor change. This capability can provide timely information for
urban planning and policy decisions toward more liveable, equitable, and
sustainable cities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stalder_S/0/1/0/all/0/1"&gt;Steven Stalder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volpi_M/0/1/0/all/0/1"&gt;Michele Volpi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buttner_N/0/1/0/all/0/1"&gt;Nicolas B&amp;#xfc;ttner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_S/0/1/0/all/0/1"&gt;Stephen Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harttgen_K/0/1/0/all/0/1"&gt;Kenneth Harttgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suel_E/0/1/0/all/0/1"&gt;Esra Suel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12301</id>
        <link href="http://arxiv.org/abs/2309.12301"/>
        <updated>2023-09-23T00:40:39.571Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smeu_S/0/1/0/all/0/1"&gt;Stefan Smeu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burceanu_E/0/1/0/all/0/1"&gt;Elena Burceanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haller_E/0/1/0/all/0/1"&gt;Emanuela Haller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1"&gt;Andrei Liviu Nicolicioiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12654</id>
        <link href="http://arxiv.org/abs/2304.12654"/>
        <updated>2023-09-23T00:40:39.571Z</updated>
        <summary type="html"><![CDATA[With growing attention to tabular data these days, the attempt to apply a
synthetic table to various tasks has been expanded toward various scenarios.
Owing to the recent advances in generative modeling, fake data generated by
tabular data synthesis models become sophisticated and realistic. However,
there still exists a difficulty in modeling discrete variables (columns) of
tabular data. In this work, we propose to process continuous and discrete
variables separately (but being conditioned on each other) by two diffusion
models. The two diffusion models are co-evolved during training by reading
conditions from each other. In order to further bind the diffusion models,
moreover, we introduce a contrastive learning method with a negative sampling
method. In our experiments with 11 real-world tabular datasets and 8 baseline
methods, we prove the efficacy of the proposed method, called CoDi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chaejeong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jayoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"&gt;Noseong Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.10688</id>
        <link href="http://arxiv.org/abs/2309.10688"/>
        <updated>2023-09-23T00:40:39.571Z</updated>
        <summary type="html"><![CDATA[Modern deep networks are trained with stochastic gradient descent (SGD) whose
key parameters are the number of data considered at each step or batch size
$B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$,
SGD corresponds to a stochastic evolution of the parameters, whose noise
amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this
description is observed to break down for sufficiently large batches $B\geq
B^*$, or simplifies to gradient descent (GD) when the temperature is
sufficiently small. Understanding where these cross-overs take place remains a
central challenge. Here we resolve these questions for a teacher-student
perceptron classification model, and show empirically that our key predictions
still apply to deep networks. Specifically, we obtain a phase diagram in the
$B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a
noise-dominated SGD governed by temperature, $\textit{(ii)}$ a
large-first-step-dominated SGD and $\textit{(iii)}$ GD. These different phases
also corresponds to different regimes of generalization error. Remarkably, our
analysis reveals that the batch size $B^*$ separating regimes $\textit{(i)}$
and $\textit{(ii)}$ scale with the size $P$ of the training set, with an
exponent that characterizes the hardness of the classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sclocchi_A/0/1/0/all/0/1"&gt;Antonio Sclocchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.05355</id>
        <link href="http://arxiv.org/abs/2209.05355"/>
        <updated>2023-09-23T00:40:39.570Z</updated>
        <summary type="html"><![CDATA[A variety of different performance metrics are commonly used in the machine
learning literature for the evaluation of classification systems. Some of the
most common ones for measuring quality of hard decisions are standard and
balanced accuracy, standard and balanced error rate, F-beta score, and Matthews
correlation coefficient (MCC). In this document, we review the definition of
these and other metrics and compare them with the expected cost (EC), a metric
introduced in every statistical learning course but rarely used in the machine
learning literature. We show that both the standard and balanced error rates
are special cases of the EC. Further, we show its relation with F-beta score
and MCC and argue that EC is superior to these traditional metrics for being
based on first principles from statistics, and for being more general,
interpretable, and adaptable to any application scenario. The metrics mentioned
above measure the quality of hard decisions. Yet, most modern classification
systems output continuous scores for the classes which we may want to evaluate
directly. Metrics for measuring the quality of system scores include the area
under the ROC curve, equal error rate, cross-entropy, Brier score, and Bayes EC
or Bayes risk, among others. The last three metrics are special cases of a
family of metrics given by the expected value of proper scoring rules (PSRs).
We review the theory behind these metrics, showing that they are a principled
way to measure the quality of the posterior probabilities produced by a system.
Finally, we show how to use these metrics to compute a system's calibration
loss and compare this metric with the widely-used expected calibration error
(ECE), arguing that calibration loss based on PSRs is superior to the ECE for
being more interpretable, more general, and directly applicable to the
multi-class case, among other reasons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1"&gt;Luciana Ferrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap: Learning Pace Synchronization for Open-World Semi-Supervised Learning. (arXiv:2309.11930v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11930</id>
        <link href="http://arxiv.org/abs/2309.11930"/>
        <updated>2023-09-23T00:40:39.554Z</updated>
        <summary type="html"><![CDATA[In open-world semi-supervised learning, a machine learning model is tasked
with uncovering novel categories from unlabeled data while maintaining
performance on seen categories from labeled data. The central challenge is the
substantial learning gap between seen and novel categories, as the model learns
the former faster due to accurate supervisory information. To address this, we
introduce 1) an adaptive margin loss based on estimated class distribution,
which encourages a large negative margin for samples in seen classes, to
synchronize learning paces, and 2) pseudo-label contrastive clustering, which
pulls together samples which are likely from the same class in the output
space, to enhance novel class discovery. Our extensive evaluations on multiple
datasets demonstrate that existing models still hinder novel class learning,
whereas our approach strikingly balances both seen and novel classes, achieving
a remarkable 3% average accuracy increase on the ImageNet dataset compared to
the prior state-of-the-art. Additionally, we find that fine-tuning the
self-supervised pre-trained backbone significantly boosts performance over the
default in prior literature. After our paper is accepted, we will release the
code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1"&gt;Bo Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_K/0/1/0/all/0/1"&gt;Kai Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tong Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min-Ling Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems. (arXiv:2309.12128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12128</id>
        <link href="http://arxiv.org/abs/2309.12128"/>
        <updated>2023-09-23T00:40:39.554Z</updated>
        <summary type="html"><![CDATA[Neural networks have become a prominent approach to solve inverse problems in
recent years. While a plethora of such methods was developed to solve inverse
problems empirically, we are still lacking clear theoretical guarantees for
these methods. On the other hand, many works proved convergence to optimal
solutions of neural networks in a more general setting using
overparametrization as a way to control the Neural Tangent Kernel. In this work
we investigate how to bridge these two worlds and we provide deterministic
convergence and recovery guarantees for the class of unsupervised feedforward
multilayer neural networks trained to solve inverse problems. We also derive
overparametrization bounds under which a two-layers Deep Inverse Prior network
with smooth activation function will benefit from our guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buskulic_N/0/1/0/all/0/1"&gt;Nathan Buskulic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fadili_J/0/1/0/all/0/1"&gt;Jalal Fadili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Queau_Y/0/1/0/all/0/1"&gt;Yvain Qu&amp;#xe9;au&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grassmann Manifold Flows for Stable Shape Generation. (arXiv:2211.02900v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.02900</id>
        <link href="http://arxiv.org/abs/2211.02900"/>
        <updated>2023-09-23T00:40:39.554Z</updated>
        <summary type="html"><![CDATA[Recently, studies on machine learning have focused on methods that use
symmetry implicit in a specific manifold as an inductive bias. Grassmann
manifolds provide the ability to handle fundamental shapes represented as shape
spaces, enabling stable shape analysis. In this paper, we present a novel
approach in which we establish the theoretical foundations for learning
distributions on the Grassmann manifold via continuous normalization flows,
with the explicit goal of generating stable shapes. Our approach facilitates
more robust generation by effectively eliminating the influence of extraneous
transformations, such as rotations and inversions, through learning and
generating within a Grassmann manifolds designed to accommodate the essential
shape information of the object. The experimental results indicated that the
proposed method can generate high-quality samples by capturing the data
structure. Furthermore, the proposed method significantly outperformed
state-of-the-art methods in terms of the log-likelihood or evidence lower
bound. The results obtained are expected to stimulate further research in this
field, leading to advances for stable shape generation and analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yataka_R/0/1/0/all/0/1"&gt;Ryoma Yataka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiraishi_M/0/1/0/all/0/1"&gt;Masashi Shiraishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirashima_K/0/1/0/all/0/1"&gt;Kazuki Hirashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint. (arXiv:2309.12025v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2309.12025</id>
        <link href="http://arxiv.org/abs/2309.12025"/>
        <updated>2023-09-23T00:40:39.552Z</updated>
        <summary type="html"><![CDATA[The problem of non-monotone $k$-submodular maximization under a knapsack
constraint ($\kSMK$) over the ground set size $n$ has been raised in many
applications in machine learning, such as data summarization, information
propagation, etc. However, existing algorithms for the problem are facing
questioning of how to overcome the non-monotone case and how to fast return a
good solution in case of the big size of data. This paper introduces two
deterministic approximation algorithms for the problem that competitively
improve the query complexity of existing algorithms.

Our first algorithm, $\LAA$, returns an approximation ratio of $1/19$ within
$O(nk)$ query complexity. The second one, $\RLA$, improves the approximation
ratio to $1/5-\epsilon$ in $O(nk)$ queries, where $\epsilon$ is an input
parameter.

Our algorithms are the first ones that provide constant approximation ratios
within only $O(nk)$ query complexity for the non-monotone objective. They,
therefore, need fewer the number of queries than state-of-the-the-art ones by a
factor of $\Omega(\log n)$.

Besides the theoretical analysis, we have evaluated our proposed ones with
several experiments in some instances: Influence Maximization and Sensor
Placement for the problem. The results confirm that our algorithms ensure
theoretical quality as the cutting-edge techniques and significantly reduce the
number of queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1"&gt;Dung T.K. Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1"&gt;Canh V. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Tan D. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_H/0/1/0/all/0/1"&gt;Huan X. Hoang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PIE: Simulating Disease Progression via Progressive Image Editing. (arXiv:2309.11745v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.11745</id>
        <link href="http://arxiv.org/abs/2309.11745"/>
        <updated>2023-09-23T00:40:39.550Z</updated>
        <summary type="html"><![CDATA[Disease progression simulation is a crucial area of research that has
significant implications for clinical diagnosis, prognosis, and treatment. One
major challenge in this field is the lack of continuous medical imaging
monitoring of individual patients over time. To address this issue, we develop
a novel framework termed Progressive Image Editing (PIE) that enables
controlled manipulation of disease-related image features, facilitating precise
and realistic disease progression simulation. Specifically, we leverage recent
advancements in text-to-image generative models to simulate disease progression
accurately and personalize it for each patient. We theoretically analyze the
iterative refining process in our framework as a gradient descent with an
exponentially decayed learning rate. To validate our framework, we conduct
experiments in three medical imaging domains. Our results demonstrate the
superiority of PIE over existing methods such as Stable Diffusion Walk and
Style-Based Manifold Extrapolation based on CLIP score (Realism) and Disease
Classification Confidence (Alignment). Our user study collected feedback from
35 veteran physicians to assess the generated progressions. Remarkably, 76.2%
of the feedback agrees with the fidelity of the generated progressions. To our
best knowledge, PIE is the first of its kind to generate disease progression
images meeting real-world standards. It is a promising tool for medical
research and clinical practice, potentially allowing healthcare providers to
model disease trajectories over time, predict future treatment responses, and
improve patient outcomes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_K/0/1/0/all/0/1"&gt;Kuei-Da Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianren Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhengyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nama_T/0/1/0/all/0/1"&gt;Tejas Nama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11680</id>
        <link href="http://arxiv.org/abs/2309.11680"/>
        <updated>2023-09-23T00:40:39.549Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) addresses the need to create models based on
proprietary data in such a way that multiple clients retain exclusive control
over their data, while all benefit from improved model accuracy due to pooled
resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic
Graphical models that utilize the expressive power of neural networks to learn
complex non-linear dependencies between the input features. They learn to
capture the underlying data distribution and have efficient algorithms for
inference and sampling. We develop a FL framework which maintains a global NGM
model that learns the averaged information from the local NGM models while
keeping the training data within the client's environment. Our design, FedNGMs,
avoids the pitfalls and shortcomings of neuron matching frameworks like
Federated Matched Averaging that suffers from model parameter explosion. Our
global model size remains constant throughout the process. In the cases where
clients have local variables that are not part of the combined global
distribution, we propose a `Stitching' algorithm, which personalizes the global
NGM models by merging the additional variables using the client's data. FedNGM
is robust to data heterogeneity, large number of participants, and limited
communication bandwidth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chajewska_U/0/1/0/all/0/1"&gt;Urszula Chajewska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_H/0/1/0/all/0/1"&gt;Harsh Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT. (arXiv:2309.11979v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2309.11979</id>
        <link href="http://arxiv.org/abs/2309.11979"/>
        <updated>2023-09-23T00:40:39.536Z</updated>
        <summary type="html"><![CDATA[With the rapid development of big data and computing devices, low-latency
automatic trading platforms based on real-time information acquisition have
become the main components of the stock trading market, so the topic of
quantitative trading has received widespread attention. And for non-strongly
efficient trading markets, human emotions and expectations always dominate
market trends and trading decisions. Therefore, this paper starts from the
theory of emotion, taking East Money as an example, crawling user comment
titles data from its corresponding stock bar and performing data cleaning.
Subsequently, a natural language processing model BERT was constructed, and the
BERT model was fine-tuned using existing annotated data sets. The experimental
results show that the fine-tuned model has different degrees of performance
improvement compared to the original model and the baseline model.
Subsequently, based on the above model, the user comment data crawled is
labeled with emotional polarity, and the obtained label information is combined
with the Alpha191 model to participate in regression, and significant
regression results are obtained. Subsequently, the regression model is used to
predict the average price change for the next five days, and use it as a signal
to guide automatic trading. The experimental results show that the
incorporation of emotional factors increased the return rate by 73.8\% compared
to the baseline during the trading period, and by 32.41\% compared to the
original alpha191 model. Finally, we discuss the advantages and disadvantages
of incorporating emotional factors into quantitative trading, and give possible
directions for further research in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jiashu Lou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption. (arXiv:2309.12004v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12004</id>
        <link href="http://arxiv.org/abs/2309.12004"/>
        <updated>2023-09-23T00:40:39.535Z</updated>
        <summary type="html"><![CDATA[This paper presents a Hierarchical Reinforcement Learning methodology
tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO).
Incorporating a high-level policy for global task distribution and a low-level
policy for real-time adaptations as a safety mechanism, our approach integrates
the Similarity Attention-based Encoder (SABE) for task prioritization and an
MLP estimator for energy consumption forecasting. Integrating this mechanism
creates a safe and fault-tolerant system for CubeSat task scheduling.
Simulation results validate the Hierarchical Reinforcement Learning superior
convergence and task success rate, outperforming both the MADDPG model and
traditional random scheduling across multiple CubeSat configurations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1"&gt;Mahya Ramezani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alandihallaj_M/0/1/0/all/0/1"&gt;M. Amin Alandihallaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Lopez_J/0/1/0/all/0/1"&gt;Jose Luis Sanchez-Lopez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_A/0/1/0/all/0/1"&gt;Andreas Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis. (arXiv:2309.11987v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11987</id>
        <link href="http://arxiv.org/abs/2309.11987"/>
        <updated>2023-09-23T00:40:39.530Z</updated>
        <summary type="html"><![CDATA[Post-hoc explainability methods aim to clarify predictions of black-box
machine learning models. However, it is still largely unclear how well users
comprehend the provided explanations and whether these increase the users
ability to predict the model behavior. We approach this question by conducting
a user study to evaluate comprehensibility and predictability in two widely
used tools: LIME and SHAP. Moreover, we investigate the effect of
counterfactual explanations and misclassifications on users ability to
understand and predict the model behavior. We find that the comprehensibility
of SHAP is significantly reduced when explanations are provided for samples
near a model's decision boundary. Furthermore, we find that counterfactual
explanations and misclassifications can significantly increase the users
understanding of how a machine learning model is making decisions. Based on our
findings, we also derive design recommendations for future post-hoc
explainability methods with increased comprehensibility and predictability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalali_A/0/1/0/all/0/1"&gt;Anahid Jalali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haslhofer_B/0/1/0/all/0/1"&gt;Bernhard Haslhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kriglstein_S/0/1/0/all/0/1"&gt;Simone Kriglstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rauber_A/0/1/0/all/0/1"&gt;Andreas Rauber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E-detectors: a nonparametric framework for sequential change detection. (arXiv:2203.03532v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.03532</id>
        <link href="http://arxiv.org/abs/2203.03532"/>
        <updated>2023-09-23T00:40:39.530Z</updated>
        <summary type="html"><![CDATA[Sequential change detection is a classical problem with a variety of
applications. However, the majority of prior work has been parametric, for
example, focusing on exponential families. We develop a fundamentally new and
general framework for sequential change detection when the pre- and post-change
distributions are nonparametrically specified (and thus composite). Our
procedures come with clean, nonasymptotic bounds on the average run length
(frequency of false alarms). In certain nonparametric cases (like sub-Gaussian
or sub-exponential), we also provide near-optimal bounds on the detection delay
following a changepoint. The primary technical tool that we introduce is called
an \emph{e-detector}, which is composed of sums of e-processes -- a fundamental
generalization of nonnegative supermartingales -- that are started at
consecutive times. We first introduce simple Shiryaev-Roberts and CUSUM-style
e-detectors, and then show how to design their mixtures in order to achieve
both statistical and computational efficiency. Our e-detector framework can be
instantiated to recover classical likelihood-based procedures for parametric
problems, as well as yielding the first change detection method for many
nonparametric problems. As a running example, we tackle the problem of
detecting changes in the mean of a bounded random variable without i.i.d.
assumptions, with an application to tracking the performance of a basketball
team over multiple seasons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jaehyeok Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rinaldo_A/0/1/0/all/0/1"&gt;Alessandro Rinaldo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices. (arXiv:2309.12212v1 [cs.ET])]]></title>
        <id>http://arxiv.org/abs/2309.12212</id>
        <link href="http://arxiv.org/abs/2309.12212"/>
        <updated>2023-09-23T00:40:39.452Z</updated>
        <summary type="html"><![CDATA[Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic with
extremely high energy efficiency. By employing the distinct polarity of current
to denote logic `0' and `1', AQFP devices serve as excellent carriers for
binary neural network (BNN) computations. Although recent research has made
initial strides toward developing an AQFP-based BNN accelerator, several
critical challenges remain, preventing the design from being a comprehensive
solution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNN
acceleration framework that leverages software-hardware co-optimization to
eventually make the AQFP devices a feasible solution for BNN acceleration.
Specifically, we investigate the randomized behavior of the AQFP devices and
analyze the impact of crossbar size on current attenuation, subsequently
formulating the current amplitude into the values suitable for use in BNN
computation. To tackle the accumulation problem and improve overall hardware
performance, we propose a stochastic computing-based accumulation module and a
clocking scheme adjustment-based circuit optimization method. We validate our
SupeRBNN framework across various datasets and network architectures, comparing
it with implementations based on different technologies, including CMOS, ReRAM,
and superconducting RSFQ/ERSFQ. Experimental results demonstrate that our
design achieves an energy efficiency of approximately 7.8x10^4 times higher
than that of the ReRAM-based BNN framework while maintaining a similar level of
model accuracy. Furthermore, when compared with superconductor-based
counterparts, our framework demonstrates at least two orders of magnitude
higher energy efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamauchi_T/0/1/0/all/0/1"&gt;Tomoharu Yamauchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masoud_Z/0/1/0/all/0/1"&gt;Zabihi Masoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yanyue Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1"&gt;Peiyan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xulong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshikawa_N/0/1/0/all/0/1"&gt;Nobuyuki Yoshikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_D/0/1/0/all/0/1"&gt;Devesh Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1"&gt;Olivia Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.11568</id>
        <link href="http://arxiv.org/abs/2309.11568"/>
        <updated>2023-09-23T00:40:39.442Z</updated>
        <summary type="html"><![CDATA[We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new
state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was
trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and
8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models
by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B
parameter models. Additionally, BTLM-3B-8K provides excellent long context
performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192
context length. We trained the model on a cleaned and deduplicated SlimPajama
dataset; aggressively tuned the \textmu P hyperparameters and schedule; used
ALiBi position embeddings; and adopted the SwiGLU nonlinearity.

On Hugging Face, the most popular models have 7B parameters, indicating that
users prefer the quality-size ratio of 7B models. Compacting the 7B parameter
model to one with 3B parameters, with little performance impact, is an
important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision
and takes 2.5x less inference compute than 7B models, helping to open up access
to a powerful language model on mobile and edge devices. BTLM-3B-8K is
available under an Apache 2.0 license on Hugging Face:
https://huggingface.co/cerebras/btlm-3b-8k-base.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1"&gt;Nolan Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soboleva_D/0/1/0/all/0/1"&gt;Daria Soboleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Khateeb_F/0/1/0/all/0/1"&gt;Faisal Al-Khateeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bowen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathria_R/0/1/0/all/0/1"&gt;Ribhu Pathria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khachane_H/0/1/0/all/0/1"&gt;Hemant Khachane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1"&gt;Shaheer Muhammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhiming/0/1/0/all/0/1"&gt;Zhiming&lt;/a&gt; (Charles) &lt;a href="http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1"&gt;Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Myers_R/0/1/0/all/0/1"&gt;Robert Myers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steeves_J/0/1/0/all/0/1"&gt;Jacob Robert Steeves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vassilieva_N/0/1/0/all/0/1"&gt;Natalia Vassilieva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1"&gt;Marvin Tom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1"&gt;Joel Hestness&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting. (arXiv:2309.11507v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11507</id>
        <link href="http://arxiv.org/abs/2309.11507"/>
        <updated>2023-09-23T00:40:39.441Z</updated>
        <summary type="html"><![CDATA[In digital advertising, the selection of the optimal item (recommendation)
and its best creative presentation (creative optimization) have traditionally
been considered separate disciplines. However, both contribute significantly to
user satisfaction, underpinning our assumption that it relies on both an item's
relevance and its presentation, particularly in the case of visual creatives.
In response, we introduce the task of {\itshape Generative Creative
Optimization (GCO)}, which proposes the use of generative models for creative
generation that incorporate user interests, and {\itshape AdBooster}, a model
for personalized ad creatives based on the Stable Diffusion outpainting
architecture. This model uniquely incorporates user interests both during
fine-tuning and at generation time. To further improve AdBooster's performance,
we also introduce an automated data augmentation pipeline. Through our
experiments on simulated data, we validate AdBooster's effectiveness in
generating more relevant creatives than default product images, showing its
potential of enhancing user engagement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shilova_V/0/1/0/all/0/1"&gt;Veronika Shilova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1"&gt;Ludovic Dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1"&gt;Flavian Vasile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Racic_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;tan Racic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1"&gt;Ugo Tanielian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards. (arXiv:2102.11941v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11941</id>
        <link href="http://arxiv.org/abs/2102.11941"/>
        <updated>2023-09-23T00:40:39.441Z</updated>
        <summary type="html"><![CDATA[A common formulation of constrained reinforcement learning involves multiple
rewards that must individually accumulate to given thresholds. In this class of
problems, we show a simple example in which the desired optimal policy cannot
be induced by any weighted linear combination of rewards. Hence, there exist
constrained reinforcement learning problems for which neither regularized nor
classical primal-dual methods yield optimal policies. This work addresses this
shortcoming by augmenting the state with Lagrange multipliers and
reinterpreting primal-dual methods as the portion of the dynamics that drives
the multipliers evolution. This approach provides a systematic state
augmentation procedure that is guaranteed to solve reinforcement learning
problems with constraints. Thus, as we illustrate by an example, while previous
methods can fail at finding optimal policies, running the dual dynamics while
executing the augmented policy yields an algorithm that provably samples
actions from the optimal policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Calvo_Fullana_M/0/1/0/all/0/1"&gt;Miguel Calvo-Fullana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paternain_S/0/1/0/all/0/1"&gt;Santiago Paternain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chamon_L/0/1/0/all/0/1"&gt;Luiz F. O. Chamon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization. (arXiv:2307.02108v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.02108</id>
        <link href="http://arxiv.org/abs/2307.02108"/>
        <updated>2023-09-23T00:40:39.441Z</updated>
        <summary type="html"><![CDATA[Simple regret minimization is a critical problem in learning optimal
treatment assignment policies across various domains, including healthcare and
e-commerce. However, it remains understudied in the contextual bandit setting.
We propose a new family of computationally efficient bandit algorithms for the
stochastic contextual bandit settings, with the flexibility to be adapted for
cumulative regret minimization (with near-optimal minimax guarantees) and
simple regret minimization (with SOTA guarantees). Furthermore, our algorithms
adapt to model misspecification and extend to the continuous arm settings.
These advantages come from constructing and relying on "conformal arm sets"
(CASs), which provide a set of arms at every context that encompass the
context-specific optimal arm with some probability across the context
distribution. Our positive results on simple and cumulative regret guarantees
are contrasted by a negative result, which shows that an algorithm can't
achieve instance-dependent simple regret guarantees while simultaneously
achieving minimax optimal cumulative regret guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Sanath Kumar Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_R/0/1/0/all/0/1"&gt;Ruohan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1"&gt;Emma Brunskill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks. (arXiv:2309.11758v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.11758</id>
        <link href="http://arxiv.org/abs/2309.11758"/>
        <updated>2023-09-23T00:40:39.436Z</updated>
        <summary type="html"><![CDATA[In the analysis of optical coherence tomography angiography (OCTA) images,
the operation of segmenting specific targets is necessary. Existing methods
typically train on supervised datasets with limited samples (approximately a
few hundred), which can lead to overfitting. To address this, the low-rank
adaptation technique is adopted for foundation model fine-tuning and proposed
corresponding prompt point generation strategies to process various
segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been
experimented on the publicly available OCTA-500 dataset. While achieving
state-of-the-art performance metrics, this method accomplishes local vessel
segmentation as well as effective artery-vein segmentation, which was not
well-solved in previous works. The code is available at:
https://github.com/ShellRedia/SAM-OCTA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinrun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1"&gt;Haojian Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shiying Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks for the Offline Nanosatellite Task Scheduling Problem. (arXiv:2303.13773v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.13773</id>
        <link href="http://arxiv.org/abs/2303.13773"/>
        <updated>2023-09-23T00:40:39.436Z</updated>
        <summary type="html"><![CDATA[This study investigates how to schedule nanosatellite tasks more efficiently
using Graph Neural Networks (GNNs). In the Offline Nanosatellite Task
Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks
to be carried out in orbit while taking into account Quality-of-Service (QoS)
considerations such as priority, minimum and maximum activation events,
execution time-frames, periods, and execution windows, as well as constraints
on the satellite's power resources and the complexity of energy harvesting and
management. The ONTS problem has been approached using conventional
mathematical formulations and exact methods, but their applicability to
challenging cases of the problem is limited. This study examines the use of
GNNs in this context, which has been effectively applied to optimization
problems such as the traveling salesman, scheduling, and facility placement
problems. More specifically, we investigate whether GNNs can learn the complex
structure of the ONTS problem with respect to feasibility and optimality of
candidate solutions. Furthermore, we evaluate using GNN-based heuristic
solutions to provide better solutions (w.r.t. the objective value) to the ONTS
problem and reduce the optimization cost. Our experiments show that GNNs are
not only able to learn feasibility and optimality for instances of the ONTS
problem, but they can generalize to harder instances than those seen during
training. Furthermore, the GNN-based heuristics improved the expected objective
value of the best solution found under the time limit in 45%, and reduced the
expected time to find a feasible solution in 35%, when compared to the SCIP
(Solving Constraint Integer Programs) solver in its off-the-shelf configuration]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pacheco_B/0/1/0/all/0/1"&gt;Bruno Machado Pacheco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seman_L/0/1/0/all/0/1"&gt;Laio Oriel Seman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigo_C/0/1/0/all/0/1"&gt;Cezar Antonio Rigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camponogara_E/0/1/0/all/0/1"&gt;Eduardo Camponogara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezerra_E/0/1/0/all/0/1"&gt;Eduardo Augusto Bezerra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coelho_L/0/1/0/all/0/1"&gt;Leandro dos Santos Coelho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowdotic: A Privacy-Preserving Hospital Waiting Room Crowd Density Estimation with Non-speech Audio. (arXiv:2309.10280v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.10280</id>
        <link href="http://arxiv.org/abs/2309.10280"/>
        <updated>2023-09-23T00:40:39.436Z</updated>
        <summary type="html"><![CDATA[Privacy-preserving crowd density analysis finds application across a wide
range of scenarios, substantially enhancing smart building operation and
management while upholding privacy expectations in various spaces. We propose a
non-speech audio-based approach for crowd analytics, leveraging a
transformer-based model. Our results demonstrate that non-speech audio alone
can be used to conduct such analysis with remarkable accuracy. To the best of
our knowledge, this is the first time when non-speech audio signals are
proposed for predicting occupancy. As far as we know, there has been no other
similar approach of its kind prior to this. To accomplish this, we deployed our
sensor-based platform in the waiting room of a large hospital with IRB approval
over a period of several months to capture non-speech audio and thermal images
for the training and evaluation of our models. The proposed non-speech-based
approach outperformed the thermal camera-based model and all other baselines.
In addition to demonstrating superior performance without utilizing speech
audio, we conduct further analysis using differential privacy techniques to
provide additional privacy guarantees. Overall, our work demonstrates the
viability of employing non-speech audio data for accurate occupancy estimation,
while also ensuring the exclusion of speech-related content and providing
robust privacy protections through differential privacy guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_F/0/1/0/all/0/1"&gt;Forsad Al Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonmoy_T/0/1/0/all/0/1"&gt;Tanjid Hasan Tonmoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lover_A/0/1/0/all/0/1"&gt;Andrew A. Lover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corey_G/0/1/0/all/0/1"&gt;George A. Corey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Mohammad Arif Ul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tauhidur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.17366</id>
        <link href="http://arxiv.org/abs/2306.17366"/>
        <updated>2023-09-23T00:40:39.435Z</updated>
        <summary type="html"><![CDATA[The idea of decision-aware model learning, that models should be accurate
where it matters for decision-making, has gained prominence in model-based
reinforcement learning. While promising theoretical results have been
established, the empirical performance of algorithms leveraging a
decision-aware loss has been lacking, especially in continuous control
problems. In this paper, we present a study on the necessary components for
decision-aware reinforcement learning models and we showcase design choices
that enable well-performing algorithms. To this end, we provide a theoretical
and empirical investigation into prominent algorithmic ideas in the field. We
highlight that empirical design decisions established in the MuZero line of
works are vital to achieving good performance for related algorithms, and we
showcase differences in behavior between different instantiations of
value-aware algorithms in stochastic environments. Using these insights, we
propose the Latent Model-Based Decision-Aware Actor-Critic framework
($\lambda$-AC) for decision-aware model-based reinforcement learning in
continuous state-spaces and highlight important design choices in different
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voelcker_C/0/1/0/all/0/1"&gt;Claas A Voelcker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmadian_A/0/1/0/all/0/1"&gt;Arash Ahmadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abachi_R/0/1/0/all/0/1"&gt;Romina Abachi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1"&gt;Igor Gilitschenski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1"&gt;Amir-massoud Farahmand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Conditional Inference in Adaptive Experiments. (arXiv:2309.12162v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2309.12162</id>
        <link href="http://arxiv.org/abs/2309.12162"/>
        <updated>2023-09-23T00:40:39.433Z</updated>
        <summary type="html"><![CDATA[We study batched bandit experiments and consider the problem of inference
conditional on the realized stopping time, assignment probabilities, and target
parameter, where all of these may be chosen adaptively using information up to
the last batch of the experiment. Absent further restrictions on the
experiment, we show that inference using only the results of the last batch is
optimal. When the adaptive aspects of the experiment are known to be
location-invariant, in the sense that they are unchanged when we shift all
batch-arm means by a constant, we show that there is additional information in
the data, captured by one additional linear function of the batch-arm means. In
the more restrictive case where the stopping time, assignment probabilities,
and target parameter are known to depend on the data only through a collection
of polyhedral events, we derive computationally tractable and optimal
conditional inference procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiafeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Andrews_I/0/1/0/all/0/1"&gt;Isaiah Andrews&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators. (arXiv:2309.12237v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2309.12237</id>
        <link href="http://arxiv.org/abs/2309.12237"/>
        <updated>2023-09-23T00:40:39.420Z</updated>
        <summary type="html"><![CDATA[Presentation attack (spoofing) detection (PAD) typically operates alongside
biometric verification to improve reliablity in the face of spoofing attacks.
Even though the two sub-systems operate in tandem to solve the single task of
reliable biometric verification, they address different detection tasks and are
hence typically evaluated separately. Evidence shows that this approach is
suboptimal. We introduce a new metric for the joint evaluation of PAD solutions
operating in situ with biometric verification. In contrast to the tandem
detection cost function proposed recently, the new tandem equal error rate
(t-EER) is parameter free. The combination of two classifiers nonetheless leads
to a \emph{set} of operating points at which false alarm and miss rates are
equal and also dependent upon the prevalence of attacks. We therefore introduce
the \emph{concurrent} t-EER, a unique operating point which is invariable to
the prevalence of attacks. Using both modality (and even application) agnostic
simulated scores, as well as real scores for a voice biometrics application, we
demonstrate application of the t-EER to a wide range of biometric system
evaluations under attack. The proposed approach is a strong candidate metric
for the tandem evaluation of PAD systems and biometric comparators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1"&gt;Tomi Kinnunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kong Aik Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tak_H/0/1/0/all/0/1"&gt;Hemlata Tak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1"&gt;Nicholas Evans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1"&gt;Andreas Nautsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.12312</id>
        <link href="http://arxiv.org/abs/2309.12312"/>
        <updated>2023-09-23T00:40:39.420Z</updated>
        <summary type="html"><![CDATA[We present ForceSight, a system for text-guided mobile manipulation that
predicts visual-force goals using a deep neural network. Given a single RGBD
image combined with a text prompt, ForceSight determines a target end-effector
pose in the camera frame (kinematic goal) and the associated forces (force
goal). Together, these two components form a visual-force goal. Prior work has
demonstrated that deep models outputting human-interpretable kinematic goals
can enable dexterous manipulation by real robots. Forces are critical to
manipulation, yet have typically been relegated to lower-level execution in
these systems. When deployed on a mobile manipulator equipped with an
eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,
drawer opening, and object handovers with an 81% success rate in unseen
environments with object instances that differed significantly from the
training data. In a separate experiment, relying exclusively on visual servoing
and ignoring force goals dropped the success rate from 90% to 45%,
demonstrating that force goals can significantly enhance performance. The
appendix, videos, code, and trained models are available at
https://force-sight.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1"&gt;Jeremy A. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houff_C/0/1/0/all/0/1"&gt;Cody Houff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;You Liang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1"&gt;Charles C. Kemp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-label Classification Approach to Increase Expressivity of EMG-based Gesture Recognition. (arXiv:2309.12217v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.12217</id>
        <link href="http://arxiv.org/abs/2309.12217"/>
        <updated>2023-09-23T00:40:39.419Z</updated>
        <summary type="html"><![CDATA[Objective: The objective of the study is to efficiently increase the
expressivity of surface electromyography-based (sEMG) gesture recognition
systems. Approach: We use a problem transformation approach, in which actions
were subset into two biomechanically independent components - a set of wrist
directions and a set of finger modifiers. To maintain fast calibration time, we
train models for each component using only individual gestures, and extrapolate
to the full product space of combination gestures by generating synthetic data.
We collected a supervised dataset with high-confidence ground truth labels in
which subjects performed combination gestures while holding a joystick, and
conducted experiments to analyze the impact of model architectures, classifier
algorithms, and synthetic data generation strategies on the performance of the
proposed approach. Main Results: We found that a problem transformation
approach using a parallel model architecture in combination with a non-linear
classifier, along with restricted synthetic data generation, shows promise in
increasing the expressivity of sEMG-based gestures with a short calibration
time. Significance: sEMG-based gesture recognition has applications in
human-computer interaction, virtual reality, and the control of robotic and
prosthetic devices. Existing approaches require exhaustive model calibration.
The proposed approach increases expressivity without requiring users to
demonstrate all combination gesture classes. Our results may be extended to
larger gesture vocabularies and more complicated model architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Smedemark_Margulies_N/0/1/0/all/0/1"&gt;Niklas Smedemark-Margulies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bicer_Y/0/1/0/all/0/1"&gt;Yunus Bicer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sunger_E/0/1/0/all/0/1"&gt;Elifnur Sunger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naufel_S/0/1/0/all/0/1"&gt;Stephanie Naufel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Imbiriba_T/0/1/0/all/0/1"&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tunik_E/0/1/0/all/0/1"&gt;Eugene Tunik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Erdogmus_D/0/1/0/all/0/1"&gt;Deniz Erdo&amp;#x11f;mu&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yarossi_M/0/1/0/all/0/1"&gt;Mathew Yarossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing. (arXiv:2309.12236v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12236</id>
        <link href="http://arxiv.org/abs/2309.12236"/>
        <updated>2023-09-23T00:40:39.381Z</updated>
        <summary type="html"><![CDATA[Calibration measures and reliability diagrams are two fundamental tools for
measuring and interpreting the calibration of probabilistic predictors.
Calibration measures quantify the degree of miscalibration, and reliability
diagrams visualize the structure of this miscalibration. However, the most
common constructions of reliability diagrams and calibration measures --
binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). We
show that a simple modification fixes both constructions: first smooth the
observations using an RBF kernel, then compute the Expected Calibration Error
(ECE) of this smoothed function. We prove that with a careful choice of
bandwidth, this method yields a calibration measure that is well-behaved in the
sense of (B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistent
calibration measure. We call this measure the SmoothECE. Moreover, the
reliability diagram obtained from this smoothed function visually encodes the
SmoothECE, just as binned reliability diagrams encode the BinnedECE.

We also provide a Python package with simple, hyperparameter-free methods for
measuring and plotting calibration: `pip install relplot\`.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blasiok_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw B&amp;#x142;asiok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1"&gt;Preetum Nakkiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uplift vs. predictive modeling: a theoretical analysis. (arXiv:2309.12036v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12036</id>
        <link href="http://arxiv.org/abs/2309.12036"/>
        <updated>2023-09-23T00:40:39.379Z</updated>
        <summary type="html"><![CDATA[Despite the growing popularity of machine-learning techniques in
decision-making, the added value of causal-oriented strategies with respect to
pure machine-learning approaches has rarely been quantified in the literature.
These strategies are crucial for practitioners in various domains, such as
marketing, telecommunications, health care and finance. This paper presents a
comprehensive treatment of the subject, starting from firm theoretical
foundations and highlighting the parameters that influence the performance of
the uplift and predictive approaches. The focus of the paper is on a binary
outcome case and a binary action, and the paper presents a theoretical analysis
of uplift modeling, comparing it with the classical predictive approach. The
main research contributions of the paper include a new formulation of the
measure of profit, a formal proof of the convergence of the uplift curve to the
measure of profit ,and an illustration, through simulations, of the conditions
under which predictive approaches still outperform uplift modeling. We show
that the mutual information between the features and the outcome plays a
significant role, along with the variance of the estimators, the distribution
of the potential outcomes and the underlying costs and benefits of the
treatment and the outcome.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verhelst_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Verhelst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petit_R/0/1/0/all/0/1"&gt;Robin Petit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verbeke_W/0/1/0/all/0/1"&gt;Wouter Verbeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontempi_G/0/1/0/all/0/1"&gt;Gianluca Bontempi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications. (arXiv:2309.12267v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2309.12267</id>
        <link href="http://arxiv.org/abs/2309.12267"/>
        <updated>2023-09-23T00:40:39.373Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has revolutionized how we train deep neural networks
by enabling decentralized collaboration while safeguarding sensitive data and
improving model performance. However, FL faces two crucial challenges: the
diverse nature of data held by individual clients and the vulnerability of the
FL system to security breaches. This paper introduces an innovative solution
named Estimated Mean Aggregation (EMA) that not only addresses these challenges
but also provides a fundamental reference point as a $\mathsf{baseline}$ for
advanced aggregation techniques in FL systems. EMA's significance lies in its
dual role: enhancing model security by effectively handling malicious outliers
through trimmed means and uncovering data heterogeneity to ensure that trained
models are adaptable across various client datasets. Through a wealth of
experiments, EMA consistently demonstrates high accuracy and area under the
curve (AUC) compared to alternative methods, establishing itself as a robust
baseline for evaluating the effectiveness and security of FL aggregation
methods. EMA's contributions thus offer a crucial step forward in advancing the
efficiency, security, and versatility of decentralized deep learning in the
context of FL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yusen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jamie Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yesha_Y/0/1/0/all/0/1"&gt;Yelena Yesha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.12307</id>
        <link href="http://arxiv.org/abs/2309.12307"/>
        <updated>2023-09-23T00:40:39.366Z</updated>
        <summary type="html"><![CDATA[We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shift short attention effectively enables context extension, leading
to non-trivial computation saving with similar performance to fine-tuning with
vanilla attention. Particularly, it can be implemented with only two lines of
code in training, while being optional in inference. On the other hand, we
revisit the parameter-efficient fine-tuning regime for context expansion.
Notably, we find that LoRA for context extension works well under the premise
of trainable embedding and normalization. LongLoRA demonstrates strong
empirical results on various tasks on LLaMA2 models from 7B/13B to 70B.
LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a
single 8x A100 machine. LongLoRA extends models' context while retaining their
original architectures, and is compatible with most existing techniques, like
FlashAttention-2. In addition, to make LongLoRA practical, we collect a
dataset, LongQA, for supervised fine-tuning. It contains more than 3k long
context question-answer pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yukang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shengju Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haotian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1"&gt;Xin Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhijian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain Tumor Detection Using Deep Learning Approaches. (arXiv:2309.12193v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.12193</id>
        <link href="http://arxiv.org/abs/2309.12193"/>
        <updated>2023-09-23T00:40:39.354Z</updated>
        <summary type="html"><![CDATA[Brain tumors are collections of abnormal cells that can develop into masses
or clusters. Because they have the potential to infiltrate other tissues, they
pose a risk to the patient. The main imaging technique used, MRI, may be able
to identify a brain tumor with accuracy. The fast development of Deep Learning
methods for use in computer vision applications has been facilitated by a vast
amount of training data and improvements in model construction that offer
better approximations in a supervised setting. The need for these approaches
has been the main driver of this expansion. Deep learning methods have shown
promise in improving the precision of brain tumor detection and classification
using magnetic resonance imaging (MRI). The study on the use of deep learning
techniques, especially ResNet50, for brain tumor identification is presented in
this abstract. As a result, this study investigates the possibility of
automating the detection procedure using deep learning techniques. In this
study, I utilized five transfer learning models which are VGG16, VGG19,
DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the best or highest
accuracy 99.54%. The goal of the study is to guide researchers and medical
professionals toward powerful brain tumor detecting systems by employing deep
learning approaches by way of this evaluation and analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Misu_R/0/1/0/all/0/1"&gt;Razia Sultana Misu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boolformer: Symbolic Regression of Logic Functions with Transformers. (arXiv:2309.12207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12207</id>
        <link href="http://arxiv.org/abs/2309.12207"/>
        <updated>2023-09-23T00:40:39.349Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce Boolformer, the first Transformer architecture
trained to perform end-to-end symbolic regression of Boolean functions. First,
we show that it can predict compact formulas for complex functions which were
not seen during training, when provided a clean truth table. Then, we
demonstrate its ability to find approximate expressions when provided
incomplete and noisy observations. We evaluate the Boolformer on a broad set of
real-world binary classification datasets, demonstrating its potential as an
interpretable alternative to classic machine learning methods. Finally, we
apply it to the widespread task of modelling the dynamics of gene regulatory
networks. Using a recent benchmark, we show that Boolformer is competitive with
state-of-the art genetic algorithms with a speedup of several orders of
magnitude. Our code and models are available publicly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+dAscoli_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane d&amp;#x27;Ascoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1"&gt;Josh Susskind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbe_E/0/1/0/all/0/1"&gt;Emmanuel Abb&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.03044</id>
        <link href="http://arxiv.org/abs/2301.03044"/>
        <updated>2023-09-23T00:40:39.344Z</updated>
        <summary type="html"><![CDATA[Transformer has been considered the dominating neural architecture in NLP and
CV, mostly under supervised settings. Recently, a similar surge of using
Transformers has appeared in the domain of reinforcement learning (RL), but it
is faced with unique design choices and challenges brought by the nature of RL.
However, the evolution of Transformers in RL has not yet been well unraveled.
In this paper, we seek to systematically review motivations and progress on
using Transformers in RL, provide a taxonomy on existing works, discuss each
sub-field, and summarize future prospects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zichuan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zongqing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1"&gt;Deheng Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.10505</id>
        <link href="http://arxiv.org/abs/2309.10505"/>
        <updated>2023-09-23T00:40:39.339Z</updated>
        <summary type="html"><![CDATA[The training of neural encoders via deep learning necessitates a
differentiable channel model due to the backpropagation algorithm. This
requirement can be sidestepped by approximating either the channel distribution
or its gradient through pilot signals in real-world scenarios. The initial
approach draws upon the latest advancements in image generation, utilizing
generative adversarial networks (GANs) or their enhanced variants to generate
channel distributions. In this paper, we address this channel approximation
challenge with diffusion models, which have demonstrated high sample quality in
image generation. We offer an end-to-end channel coding framework underpinned
by diffusion models and propose an efficient training algorithm. Our
simulations with various channel models establish that our diffusion models
learn the channel distribution accurately, thereby achieving near-optimal
end-to-end symbol error rates (SERs). We also note a significant advantage of
diffusion models: A robust generalization capability in high signal-to-noise
ratio regions, in contrast to GAN variants that suffer from error floor.
Furthermore, we examine the trade-off between sample quality and sampling
speed, when an accelerated sampling algorithm is deployed, and investigate the
effect of the noise scheduling on this trade-off. With an apt choice of noise
scheduling, sampling time can be significantly reduced with a minor increase in
SER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Muah Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fritschek_R/0/1/0/all/0/1"&gt;Rick Fritschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1"&gt;Rafael F. Schaefer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach. (arXiv:2305.13706v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.13706</id>
        <link href="http://arxiv.org/abs/2305.13706"/>
        <updated>2023-09-23T00:40:39.327Z</updated>
        <summary type="html"><![CDATA[For cyber-physical systems in the 6G era, semantic communications connecting
distributed devices for dynamic control and remote state estimation are
required to guarantee application-level performance, not merely focus on
communication-centric performance. Semantics here is a measure of the
usefulness of information transmissions. Semantic-aware transmission scheduling
of a large system often involves a large decision-making space, and the optimal
policy cannot be obtained by existing algorithms effectively. In this paper, we
first investigate the fundamental properties of the optimal semantic-aware
scheduling policy and then develop advanced deep reinforcement learning (DRL)
algorithms by leveraging the theoretical guidelines. Our numerical results show
that the proposed algorithms can substantially reduce training time and enhance
training performance compared to benchmark algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiazheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanchun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quevedo_D/0/1/0/all/0/1"&gt;Daniel Quevedo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yonghui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vucetic_B/0/1/0/all/0/1"&gt;Branka Vucetic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early diagnosis of autism spectrum disorder using machine learning approaches. (arXiv:2309.11646v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11646</id>
        <link href="http://arxiv.org/abs/2309.11646"/>
        <updated>2023-09-23T00:40:39.326Z</updated>
        <summary type="html"><![CDATA[Autistic Spectrum Disorder (ASD) is a neurological disease characterized by
difficulties with social interaction, communication, and repetitive activities.
The severity of these difficulties varies, and those with this diagnosis face
unique challenges. While its primary origin lies in genetics, identifying and
addressing it early can contribute to the enhancement of the condition. In
recent years, machine learning-driven intelligent diagnosis has emerged as a
supplement to conventional clinical approaches, aiming to address the potential
drawbacks of time-consuming and costly traditional methods. In this work, we
utilize different machine learning algorithms to find the most significant
traits responsible for ASD and to automate the diagnostic process. We study six
classification models to see which model works best to identify ASD and also
study five popular clustering methods to get a meaningful insight of these ASD
datasets. To find the best classifier for these binary datasets, we evaluate
the models using accuracy, precision, recall, specificity, F1-score, AUC, kappa
and log loss metrics. Our evaluation demonstrates that five out of the six
selected models perform exceptionally, achieving a 100% accuracy rate on the
ASD datasets when hyperparameters are meticulously tuned for each model. As
almost all classification models are able to get 100% accuracy, we become
interested in observing the underlying insights of the datasets by implementing
some popular clustering algorithms on these datasets. We calculate Normalized
Mutual Information (NMI), Adjusted Rand Index (ARI) & Silhouette Coefficient
(SC) metrics to select the best clustering models. Our evaluation finds that
spectral clustering outperforms all other benchmarking clustering models in
terms of NMI & ARI metrics and it also demonstrates comparability to the
optimal SC achieved by k-means.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rasul_R/0/1/0/all/0/1"&gt;Rownak Ara Rasul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1"&gt;Promy Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bala_D/0/1/0/all/0/1"&gt;Diponkor Bala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karim_S/0/1/0/all/0/1"&gt;S M Rakib Ul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdullah_I/0/1/0/all/0/1"&gt;Ibrahim Abdullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1"&gt;Bishwajit Saha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grassmann Manifold Flows for Stable Shape Generation. (arXiv:2211.02900v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.02900</id>
        <link href="http://arxiv.org/abs/2211.02900"/>
        <updated>2023-09-23T00:40:39.326Z</updated>
        <summary type="html"><![CDATA[Recently, studies on machine learning have focused on methods that use
symmetry implicit in a specific manifold as an inductive bias. Grassmann
manifolds provide the ability to handle fundamental shapes represented as shape
spaces, enabling stable shape analysis. In this paper, we present a novel
approach in which we establish the theoretical foundations for learning
distributions on the Grassmann manifold via continuous normalization flows,
with the explicit goal of generating stable shapes. Our approach facilitates
more robust generation by effectively eliminating the influence of extraneous
transformations, such as rotations and inversions, through learning and
generating within a Grassmann manifolds designed to accommodate the essential
shape information of the object. The experimental results indicated that the
proposed method can generate high-quality samples by capturing the data
structure. Furthermore, the proposed method significantly outperformed
state-of-the-art methods in terms of the log-likelihood or evidence lower
bound. The results obtained are expected to stimulate further research in this
field, leading to advances for stable shape generation and analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yataka_R/0/1/0/all/0/1"&gt;Ryoma Yataka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiraishi_M/0/1/0/all/0/1"&gt;Masashi Shiraishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirashima_K/0/1/0/all/0/1"&gt;Kazuki Hirashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting. (arXiv:2309.12028v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12028</id>
        <link href="http://arxiv.org/abs/2309.12028"/>
        <updated>2023-09-23T00:40:39.325Z</updated>
        <summary type="html"><![CDATA[This paper studies the problem of traffic flow forecasting, which aims to
predict future traffic conditions on the basis of road networks and traffic
conditions in the past. The problem is typically solved by modeling complex
spatio-temporal correlations in traffic data using spatio-temporal graph neural
networks (GNNs). However, the performance of these methods is still far from
satisfactory since GNNs usually have limited representation capacity when it
comes to complex traffic networks. Graphs, by nature, fall short in capturing
non-pairwise relations. Even worse, existing methods follow the paradigm of
message passing that aggregates neighborhood information linearly, which fails
to capture complicated spatio-temporal high-order interactions. To tackle these
issues, in this paper, we propose a novel model named Dynamic Hypergraph
Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise
relationships, our DyHSL extracts hypergraph structural information to model
dynamics in the traffic networks, and updates each node representation by
aggregating messages from its associated hyperedges. Additionally, to capture
high-order spatio-temporal relations in the road network, we introduce an
interactive graph convolution block, which further models the neighborhood
interaction for each node. Finally, we integrate these two views into a
holistic multi-scale correlation extraction module, which conducts temporal
pooling with different scales to model different temporal patterns. Extensive
experiments on four popular traffic benchmark datasets demonstrate the
effectiveness of our proposed DyHSL compared with a broad range of competing
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yusheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1"&gt;Wei Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems. (arXiv:2309.12134v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.12134</id>
        <link href="http://arxiv.org/abs/2309.12134"/>
        <updated>2023-09-23T00:40:39.317Z</updated>
        <summary type="html"><![CDATA[Linking sheet music images to audio recordings remains a key problem for the
development of efficient cross-modal music retrieval systems. One of the
fundamental approaches toward this task is to learn a cross-modal embedding
space via deep neural networks that is able to connect short snippets of audio
and sheet music. However, the scarcity of annotated data from real musical
content affects the capability of such methods to generalize to real retrieval
scenarios. In this work, we investigate whether we can mitigate this limitation
with self-supervised contrastive learning, by exposing a network to a large
amount of real music data as a pre-training step, by contrasting randomly
augmented views of snippets of both modalities, namely audio and sheet images.
Through a number of experiments on synthetic and real piano data, we show that
pre-trained models are able to retrieve snippets with better precision in all
scenarios and pre-training configurations. Encouraged by these results, we
employ the snippet embeddings in the higher-level task of cross-modal piece
identification and conduct more experiments on several retrieval
configurations. In this task, we observe that the retrieval quality improves
from 30% up to 100% when real music data is present. We then conclude by
arguing for the potential of self-supervised contrastive learning for
alleviating the annotated data scarcity in multi-modal music retrieval models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_L/0/1/0/all/0/1"&gt;Luis Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Washuttl_T/0/1/0/all/0/1"&gt;Tobias Wash&amp;#xfc;ttl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1"&gt;Gerhard Widmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features. (arXiv:2309.12140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.12140</id>
        <link href="http://arxiv.org/abs/2309.12140"/>
        <updated>2023-09-23T00:40:39.317Z</updated>
        <summary type="html"><![CDATA[The rapid development of 3D object detection systems for self-driving cars
has significantly improved accuracy. However, these systems struggle to
generalize across diverse driving environments, which can lead to
safety-critical failures in detecting traffic participants. To address this, we
propose a method that utilizes unlabeled repeated traversals of multiple
locations to adapt object detectors to new driving environments. By
incorporating statistics computed from repeated LiDAR scans, we guide the
adaptation process effectively. Our approach enhances LiDAR-based detection
models using spatial quantized historical features and introduces a lightweight
regression head to leverage the statistics for feature regularization.
Additionally, we leverage the statistics for a novel self-training process to
stabilize the training. The framework is detector model-agnostic and
experiments on real-world datasets demonstrate significant improvements,
achieving up to a 20-point performance gain, especially in detecting
pedestrians and distant objects. Code is available at
https://github.com/zhangtravis/Hist-DA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Travis Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1"&gt;Katie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1"&gt;Cheng Perng Phoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yurong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1"&gt;Wei-Lun Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1"&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1"&gt;Mark Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potential and limitations of random Fourier features for dequantizing quantum machine learning. (arXiv:2309.11647v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.11647</id>
        <link href="http://arxiv.org/abs/2309.11647"/>
        <updated>2023-09-23T00:40:39.316Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning is arguably one of the most explored applications of
near-term quantum devices. Much focus has been put on notions of variational
quantum machine learning where parameterized quantum circuits (PQCs) are used
as learning models. These PQC models have a rich structure which suggests that
they might be amenable to efficient dequantization via random Fourier features
(RFF). In this work, we establish necessary and sufficient conditions under
which RFF does indeed provide an efficient dequantization of variational
quantum machine learning for regression. We build on these insights to make
concrete suggestions for PQC architecture design, and to identify structures
which are necessary for a regression problem to admit a potential quantum
advantage via PQC based optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sweke_R/0/1/0/all/0/1"&gt;Ryan Sweke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Recio_E/0/1/0/all/0/1"&gt;Erik Recio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jerbi_S/0/1/0/all/0/1"&gt;Sofiene Jerbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gil_Fuster_E/0/1/0/all/0/1"&gt;Elies Gil-Fuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Fuller_B/0/1/0/all/0/1"&gt;Bryce Fuller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eisert_J/0/1/0/all/0/1"&gt;Jens Eisert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Meyer_J/0/1/0/all/0/1"&gt;Johannes Jakob Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification. (arXiv:2309.11845v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.11845</id>
        <link href="http://arxiv.org/abs/2309.11845"/>
        <updated>2023-09-23T00:40:39.316Z</updated>
        <summary type="html"><![CDATA[Audiovisual data is everywhere in this digital age, which raises higher
requirements for the deep learning models developed on them. To well handle the
information of the multi-modal data is the key to a better audiovisual modal.
We observe that these audiovisual data naturally have temporal attributes, such
as the time information for each frame in the video. More concretely, such data
is inherently multi-modal according to both audio and visual cues, which
proceed in a strict chronological order. It indicates that temporal information
is important in multi-modal acoustic event modeling for both intra- and
inter-modal. However, existing methods deal with each modal feature
independently and simply fuse them together, which neglects the mining of
temporal relation and thus leads to sub-optimal performance. With this
motivation, we propose a Temporal Multi-modal graph learning method for
Acoustic event Classification, called TMac, by modeling such temporal
information via graph learning techniques. In particular, we construct a
temporal graph for each acoustic event, dividing its audio data and video data
into multiple segments. Each segment can be considered as a node, and the
temporal relationships between nodes can be considered as timestamps on their
edges. In this case, we can smoothly capture the dynamic information in
intra-modal and inter-modal. Several experiments are conducted to demonstrate
TMac outperforms other SOTA models in performance. Our code is available at
https://github.com/MGitHubL/TMac.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Ke Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dayu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingyuan Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1"&gt;Wenxuan Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sihang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based Clustering using Non-parametric Hidden Markov Models. (arXiv:2309.12238v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2309.12238</id>
        <link href="http://arxiv.org/abs/2309.12238"/>
        <updated>2023-09-23T00:40:39.316Z</updated>
        <summary type="html"><![CDATA[Thanks to their dependency structure, non-parametric Hidden Markov Models
(HMMs) are able to handle model-based clustering without specifying group
distributions. The aim of this work is to study the Bayes risk of clustering
when using HMMs and to propose associated clustering procedures. We first give
a result linking the Bayes risk of classification and the Bayes risk of
clustering, which we use to identify the key quantity determining the
difficulty of the clustering task. We also give a proof of this result in the
i.i.d. framework, which might be of independent interest. Then we study the
excess risk of the plugin classifier. All these results are shown to remain
valid in the online setting where observations are clustered sequentially.
Simulations illustrate our findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gassiat_E/0/1/0/all/0/1"&gt;Elisabeth Gassiat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kaddouri_I/0/1/0/all/0/1"&gt;Ibrahim Kaddouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Naulet_Z/0/1/0/all/0/1"&gt;Zacharie Naulet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM. (arXiv:2309.12058v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12058</id>
        <link href="http://arxiv.org/abs/2309.12058"/>
        <updated>2023-09-23T00:40:39.315Z</updated>
        <summary type="html"><![CDATA[Anticancer peptides (ACPs) are a group of peptides that exhibite
antineoplastic properties. The utilization of ACPs in cancer prevention can
present a viable substitute for conventional cancer therapeutics, as they
possess a higher degree of selectivity and safety. Recent scientific
advancements generate an interest in peptide-based therapies which offer the
advantage of efficiently treating intended cells without negatively impacting
normal cells. However, as the number of peptide sequences continues to increase
rapidly, developing a reliable and precise prediction model becomes a
challenging task. In this work, our motivation is to advance an efficient model
for categorizing anticancer peptides employing the consolidation of word
embedding and deep learning models. First, Word2Vec and FastText are evaluated
as word embedding techniques for the purpose of extracting peptide sequences.
Then, the output of word embedding models are fed into deep learning approaches
CNN, LSTM, BiLSTM. To demonstrate the contribution of proposed framework,
extensive experiments are carried on widely-used datasets in the literature,
ACPs250 and Independent. Experiment results show the usage of proposed model
enhances classification accuracy when compared to the state-of-the-art studies.
The proposed combination, FastText+BiLSTM, exhibits 92.50% of accuracy for
ACPs250 dataset, and 96.15% of accuracy for Independent dataset, thence
determining new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karakaya_O/0/1/0/all/0/1"&gt;Onur Karakaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1"&gt;Zeynep Hilal Kilimci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensuring Topological Data-Structure Preservation under Autoencoder Compression due to Latent Space Regularization in Gauss--Legendre nodes. (arXiv:2309.08228v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.08228</id>
        <link href="http://arxiv.org/abs/2309.08228"/>
        <updated>2023-09-23T00:40:39.315Z</updated>
        <summary type="html"><![CDATA[We formulate a data independent latent space regularisation constraint for
general unsupervised autoencoders. The regularisation rests on sampling the
autoencoder Jacobian in Legendre nodes, being the centre of the Gauss-Legendre
quadrature. Revisiting this classic enables to prove that regularised
autoencoders ensure a one-to-one re-embedding of the initial data manifold to
its latent representation. Demonstrations show that prior proposed
regularisation strategies, such as contractive autoencoding, cause topological
defects already for simple examples, and so do convolutional based
(variational) autoencoders. In contrast, topological preservation is ensured
already by standard multilayer perceptron neural networks when being
regularised due to our contribution. This observation extends through the
classic FashionMNIST dataset up to real world encoding problems for MRI brain
scans, suggesting that, across disciplines, reliable low dimensional
representations of complex high-dimensional datasets can be delivered due to
this regularisation technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramanaik_C/0/1/0/all/0/1"&gt;Chethan Krishnamurthy Ramanaik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardona_J/0/1/0/all/0/1"&gt;Juan-Esteban Suarez Cardona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willmann_A/0/1/0/all/0/1"&gt;Anna Willmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanfeld_P/0/1/0/all/0/1"&gt;Pia Hanfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmann_N/0/1/0/all/0/1"&gt;Nico Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1"&gt;Michael Hecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Variational Auto-Encoder Enabled Multi-Band Channel Prediction Scheme for Indoor Localization. (arXiv:2309.12200v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.12200</id>
        <link href="http://arxiv.org/abs/2309.12200"/>
        <updated>2023-09-23T00:40:39.281Z</updated>
        <summary type="html"><![CDATA[Indoor localization is getting increasing demands for various cutting-edged
technologies, like Virtual/Augmented reality and smart home. Traditional
model-based localization suffers from significant computational overhead, so
fingerprint localization is getting increasing attention, which needs lower
computation cost after the fingerprint database is built. However, the accuracy
of indoor localization is limited by the complicated indoor environment which
brings the multipath signal refraction. In this paper, we provided a scheme to
improve the accuracy of indoor fingerprint localization from the frequency
domain by predicting the channel state information (CSI) values from another
transmitting channel and spliced the multi-band information together to get
more precise localization results. We tested our proposed scheme on COST 2100
simulation data and real time orthogonal frequency division multiplexing (OFDM)
WiFi data collected from an office scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_R/0/1/0/all/0/1"&gt;Ruihao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shunqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.12295</id>
        <link href="http://arxiv.org/abs/2309.12295"/>
        <updated>2023-09-23T00:40:39.276Z</updated>
        <summary type="html"><![CDATA[Human drivers can seamlessly adapt their driving decisions across
geographical locations with diverse conditions and rules of the road, e.g.,
left vs. right-hand traffic. In contrast, existing models for autonomous
driving have been thus far only deployed within restricted operational domains,
i.e., without accounting for varying driving behaviors across locations or
model scalability. In this work, we propose AnyD, a single geographically-aware
conditional imitation learning (CIL) model that can efficiently learn from
heterogeneous and globally distributed data with dynamic environmental,
traffic, and social characteristics. Our key insight is to introduce a
high-capacity geo-location-based channel attention mechanism that effectively
adapts to local nuances while also flexibly modeling similarities among regions
in a data-driven manner. By optimizing a contrastive imitation objective, our
proposed approach can efficiently scale across inherently imbalanced data
distributions and location-dependent events. We demonstrate the benefits of our
AnyD agent across multiple datasets, cities, and scalable deployment paradigms,
i.e., centralized, semi-supervised, and distributed agent training.
Specifically, AnyD outperforms CIL baselines by over 14% in open-loop
evaluation and 30% in closed-loop testing on CARLA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ruizhao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Peng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1"&gt;Eshed Ohn-Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1"&gt;Venkatesh Saligrama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian sparsification for deep neural networks with Bayesian model reduction. (arXiv:2309.12095v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.12095</id>
        <link href="http://arxiv.org/abs/2309.12095"/>
        <updated>2023-09-23T00:40:39.181Z</updated>
        <summary type="html"><![CDATA[Deep learning's immense capabilities are often constrained by the complexity
of its models, leading to an increasing demand for effective sparsification
techniques. Bayesian sparsification for deep learning emerges as a crucial
approach, facilitating the design of models that are both computationally
efficient and competitive in terms of performance across various deep learning
applications. The state-of-the-art -- in Bayesian sparsification of deep neural
networks -- combines structural shrinkage priors on model weights with an
approximate inference scheme based on black-box stochastic variational
inference. However, model inversion of the full generative model is
exceptionally computationally demanding, especially when compared to standard
deep learning of point estimates. In this context, we advocate for the use of
Bayesian model reduction (BMR) as a more efficient alternative for pruning of
model weights. As a generalization of the Savage-Dickey ratio, BMR allows a
post-hoc elimination of redundant model weights based on the posterior
estimates under a straightforward (non-hierarchical) generative model. Our
comparative study highlights the computational efficiency and the pruning rate
of the BMR method relative to the established stochastic variational inference
(SVI) scheme, when applied to the full hierarchical generative model. We
illustrate the potential of BMR to prune model parameters across various deep
learning architectures, from classical networks like LeNet to modern frameworks
such as Vision Transformers and MLP-Mixers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Markovic_D/0/1/0/all/0/1"&gt;Dimitrije Markovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Friston_K/0/1/0/all/0/1"&gt;Karl J. Friston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kiebel_S/0/1/0/all/0/1"&gt;Stefan J. Kiebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States. (arXiv:2210.00997v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.00997</id>
        <link href="http://arxiv.org/abs/2210.00997"/>
        <updated>2023-09-23T00:40:39.178Z</updated>
        <summary type="html"><![CDATA[Consider an online convex optimization problem where the loss functions are
self-concordant barriers, smooth relative to a convex function $h$, and
possibly non-Lipschitz. We analyze the regret of online mirror descent with
$h$. Then, based on the result, we prove the following in a unified manner.
Denote by $T$ the time horizon and $d$ the parameter dimension. 1. For online
portfolio selection, the regret of $\widetilde{\text{EG}}$, a variant of
exponentiated gradient due to Helmbold et al., is $\tilde{O} ( T^{2/3} d^{1/3}
)$ when $T > 4 d / \log d$. This improves on the original $\tilde{O} ( T^{3/4}
d^{1/2} )$ regret bound for $\widetilde{\text{EG}}$. 2. For online portfolio
selection, the regret of online mirror descent with the logarithmic barrier is
$\tilde{O}(\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due
to Orseau et al. up to logarithmic terms. 3. For online learning quantum states
with the logarithmic loss, the regret of online mirror descent with the
log-determinant function is also $\tilde{O} ( \sqrt{T d} )$. Its per-iteration
time is shorter than all existing algorithms we know.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tsai_C/0/1/0/all/0/1"&gt;Chung-En Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao-Chung Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yen-Huan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.09517</id>
        <link href="http://arxiv.org/abs/2309.09517"/>
        <updated>2023-09-23T00:40:39.045Z</updated>
        <summary type="html"><![CDATA[Federated training of Graph Neural Networks (GNN) has become popular in
recent years due to its ability to perform graph-related tasks under data
isolation scenarios while preserving data privacy. However, graph heterogeneity
issues in federated GNN systems continue to pose challenges. Existing
frameworks address the problem by representing local tasks using different
statistics and relating them through a simple aggregation mechanism. However,
these approaches suffer from limited efficiency from two aspects: low quality
of task-relatedness quantification and inefficacy of exploiting the
collaboration structure. To address these issues, we propose FedGKD, a novel
federated GNN framework that utilizes a novel client-side graph dataset
distillation method to extract task features that better describe
task-relatedness, and introduces a novel server-side aggregation mechanism that
is aware of the global collaboration structure. We conduct extensive
experiments on six real-world datasets of different scales, demonstrating our
framework's outperformance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1"&gt;Qiying Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Ruofan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tengfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yifei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization. (arXiv:2309.11994v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2309.11994</id>
        <link href="http://arxiv.org/abs/2309.11994"/>
        <updated>2023-09-23T00:40:39.044Z</updated>
        <summary type="html"><![CDATA[Surrogate-assisted evolutionary algorithms (SAEAs) hold significant
importance in resolving expensive optimization problems~(EOPs). Extensive
efforts have been devoted to improving the efficacy of SAEAs through the
development of proficient model-assisted selection methods. However, generating
high-quality solutions is a prerequisite for selection. The fundamental
paradigm of evaluating a limited number of solutions in each generation within
SAEAs reduces the variance of adjacent populations, thus impacting the quality
of offspring solutions. This is a frequently encountered issue, yet it has not
gained widespread attention. This paper presents a framework using unevaluated
solutions to enhance the efficiency of SAEAs. The surrogate model is employed
to identify high-quality solutions for direct generation of new solutions
without evaluation. To ensure dependable selection, we have introduced two
tailored relation models for the selection of the optimal solution and the
unevaluated population. A comprehensive experimental analysis is performed on
two test suites, which showcases the superiority of the relation model over
regression and classification models in the selection phase. Furthermore, the
surrogate-selected unevaluated solutions with high potential have been shown to
significantly enhance the efficiency of the algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1"&gt;Hao Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoqun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aimin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.12250</id>
        <link href="http://arxiv.org/abs/2309.12250"/>
        <updated>2023-09-23T00:40:39.044Z</updated>
        <summary type="html"><![CDATA[Evaluation of QA systems is very challenging and expensive, with the most
reliable approach being human annotations of correctness of answers for
questions. Recent works (AVA, BEM) have shown that transformer LM encoder based
similarity metrics transfer well for QA evaluation, but they are limited by the
usage of a single correct reference answer. We propose a new evaluation metric:
SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference
answers (combining multiple correct and incorrect references) for sentence-form
QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and
generative (GenQA) QA systems, across multiple academic and industrial
datasets, and show that it outperforms previous baselines and obtains the
highest correlation with human annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabburo_M/0/1/0/all/0/1"&gt;Matteo Gabburo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Siddhant Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kedziorski_R/0/1/0/all/0/1"&gt;Rik Koncel Kedziorski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1"&gt;Alessandro Moschitti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZeroFlow: Fast, Zero Label, Scalable Scene Flow via Distillation. (arXiv:2305.10424v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.10424</id>
        <link href="http://arxiv.org/abs/2305.10424"/>
        <updated>2023-09-23T00:40:39.037Z</updated>
        <summary type="html"><![CDATA[Scene flow estimation is the task of describing the 3D motion field between
temporally successive point clouds. State-of-the-art methods use strong priors
and test-time optimization techniques, but require on the order of tens of
seconds to process large-scale point clouds, making them unusable as computer
vision primitives for real-time applications such as open world object
detection. Feed forward methods are considerably faster, running on the order
of tens to hundreds of milliseconds for large-scale point clouds, but require
expensive human supervision. To address both limitations, we propose Scene Flow
via Distillation, a simple, scalable distillation framework that uses a
label-free optimization method to produce pseudo-labels to supervise a feed
forward model. Our instantiation of this framework, ZeroFlow, achieves
state-of-the-art performance on the Argoverse 2 Self-Supervised Scene Flow
Challenge while using zero human labels by simply training on large-scale,
diverse unlabeled data. At test-time, ZeroFlow is over 1000$\times$ faster than
label-free state-of-the-art optimization-based methods on large-scale point
clouds and over 1000$\times$ cheaper to train on unlabeled data compared to the
cost of human annotation of that data. To facilitate further research, we will
release our code, trained model weights, and high quality pseudo-labels for the
Argoverse 2 and Waymo Open datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1"&gt;Kyle Vedder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1"&gt;Neehar Peri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chodosh_N/0/1/0/all/0/1"&gt;Nathaniel Chodosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1"&gt;Ishan Khatri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1"&gt;Eric Eaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1"&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1"&gt;Deva Ramanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1"&gt;James Hays&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation. (arXiv:2308.07929v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.07929</id>
        <link href="http://arxiv.org/abs/2308.07929"/>
        <updated>2023-09-23T00:40:39.037Z</updated>
        <summary type="html"><![CDATA[Recently, large multimodal models, such as CLIP and Stable Diffusion have
experimented tremendous successes in both foundations and applications.
However, as these models increase in parameter size and computational
requirements, it becomes more challenging for users to personalize them for
specific tasks or preferences. In this work, we address the problem of adapting
the previous models towards sets of particular human preferences, aligning the
retrieved or generated images with the preferences of the user. We leverage the
Bradley-Terry preference model to develop a fast adaptation method that
efficiently fine-tunes the original model, with few examples and with minimal
computing resources. Extensive evidence of the capabilities of this framework
is provided through experiments in different domains related to multimodal text
and image understanding, including preference prediction as a reward model, and
generation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallego_V/0/1/0/all/0/1"&gt;Victor Gallego&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[See to Touch: Learning Tactile Dexterity through Visual Incentives. (arXiv:2309.12300v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.12300</id>
        <link href="http://arxiv.org/abs/2309.12300"/>
        <updated>2023-09-23T00:40:39.036Z</updated>
        <summary type="html"><![CDATA[Equipping multi-fingered robots with tactile sensing is crucial for achieving
the precise, contact-rich, and dexterous manipulation that humans excel at.
However, relying solely on tactile sensing fails to provide adequate cues for
reasoning about objects' spatial configurations, limiting the ability to
correct errors and adapt to changing situations. In this paper, we present
Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances
tactile-based dexterity by optimizing dexterous policies using vision-based
rewards. First, we use a contrastive-based objective to learn visual
representations. Next, we construct a reward function using these visual
representations through optimal-transport based matching on one human
demonstration. Finally, we use online reinforcement learning on our robot to
optimize tactile-based policies that maximize the visual reward. On six
challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping
slender objects, TAVI achieves a success rate of 73% using our four-fingered
Allegro robot hand. The increase in performance is 108% higher than policies
using tactile and vision-based rewards and 135% higher than policies without
tactile observational input. Robot videos are best viewed on our project
website: https://see-to-touch.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guzey_I/0/1/0/all/0/1"&gt;Irmak Guzey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yinlong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1"&gt;Ben Evans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chintala_S/0/1/0/all/0/1"&gt;Soumith Chintala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1"&gt;Lerrel Pinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis. (arXiv:2309.12283v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.12283</id>
        <link href="http://arxiv.org/abs/2309.12283"/>
        <updated>2023-09-23T00:40:39.035Z</updated>
        <summary type="html"><![CDATA[Generating multi-instrument music from symbolic music representations is an
important task in Music Information Retrieval (MIR). A central but still
largely unsolved problem in this context is musically and acoustically informed
control in the generation process. As the main contribution of this work, we
propose enhancing control of multi-instrument synthesis by conditioning a
generative model on a specific performance and recording environment, thus
allowing for better guidance of timbre and style. Building on state-of-the-art
diffusion-based music generative models, we introduce performance conditioning
- a simple tool indicating the generative model to synthesize music with style
and timbre of specific instruments taken from specific performances. Our
prototype is evaluated using uncurated performances with diverse
instrumentation and achieves state-of-the-art FAD realism scores while allowing
novel timbre and style control. Our project page, including samples and
demonstrations, is available at benadar293.github.io/midipm]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maman_B/0/1/0/all/0/1"&gt;Ben Maman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeitler_J/0/1/0/all/0/1"&gt;Johannes Zeitler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1"&gt;Meinard M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1"&gt;Amit H. Bermano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subsampling Suffices for Adaptive Data Analysis. (arXiv:2302.08661v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08661</id>
        <link href="http://arxiv.org/abs/2302.08661"/>
        <updated>2023-09-23T00:40:39.031Z</updated>
        <summary type="html"><![CDATA[Ensuring that analyses performed on a dataset are representative of the
entire population is one of the central problems in statistics. Most classical
techniques assume that the dataset is independent of the analyst's query and
break down in the common setting where a dataset is reused for multiple,
adaptively chosen, queries. This problem of \emph{adaptive data analysis} was
formalized in the seminal works of Dwork et al. (STOC, 2015) and Hardt and
Ullman (FOCS, 2014).

We identify a remarkably simple set of assumptions under which the queries
will continue to be representative even when chosen adaptively: The only
requirements are that each query takes as input a random subsample and outputs
few bits. This result shows that the noise inherent in subsampling is
sufficient to guarantee that query responses generalize. The simplicity of this
subsampling-based framework allows it to model a variety of real-world
scenarios not covered by prior work.

In addition to its simplicity, we demonstrate the utility of this framework
by designing mechanisms for two foundational tasks, statistical queries and
median finding. In particular, our mechanism for answering the broadly
applicable class of statistical queries is both extremely simple and state of
the art in many parameter regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blanc_G/0/1/0/all/0/1"&gt;Guy Blanc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiclass Learnability Does Not Imply Sample Compression. (arXiv:2308.06424v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.06424</id>
        <link href="http://arxiv.org/abs/2308.06424"/>
        <updated>2023-09-23T00:40:39.024Z</updated>
        <summary type="html"><![CDATA[A hypothesis class admits a sample compression scheme, if for every sample
labeled by a hypothesis from the class, it is possible to retain only a small
subsample, using which the labels on the entire sample can be inferred. The
size of the compression scheme is an upper bound on the size of the subsample
produced. Every learnable binary hypothesis class (which must necessarily have
finite VC dimension) admits a sample compression scheme of size only a finite
function of its VC dimension, independent of the sample size. For multiclass
hypothesis classes, the analog of VC dimension is the DS dimension. We show
that the analogous statement pertaining to sample compression is not true for
multiclass hypothesis classes: every learnable multiclass hypothesis class,
which must necessarily have finite DS dimension, does not admit a sample
compression scheme of size only a finite function of its DS dimension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pabbaraju_C/0/1/0/all/0/1"&gt;Chirag Pabbaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLM Regression with Oblivious Corruptions. (arXiv:2309.11657v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2309.11657</id>
        <link href="http://arxiv.org/abs/2309.11657"/>
        <updated>2023-09-23T00:40:38.978Z</updated>
        <summary type="html"><![CDATA[We demonstrate the first algorithms for the problem of regression for
generalized linear models (GLMs) in the presence of additive oblivious noise.
We assume we have sample access to examples $(x, y)$ where $y$ is a noisy
measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of
the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious
noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$,
and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover
a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has}
arbitrarily small error when compared to the true values $g(w^* \cdot x)$,
rather than the noisy measurements $y$.

We present an algorithm that tackles \new{this} problem in its most general
distribution-independent setting, where the solution may not \new{even} be
identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the
solution if it is identifiable, and otherwise returns a small list of
candidates, one of which is close to the true solution. Furthermore, we
\new{provide} a necessary and sufficient condition for identifiability, which
holds in broad settings. \new{Specifically,} the problem is identifiable when
the quantile at which $\xi + \epsilon = 0$ is known, or when the family of
hypotheses does not contain candidates that are nearly equal to a translated
$g(w^* \cdot x) + A$ for some real number $A$, while also having large error
when compared to $g(w^* \cdot x)$.

This is the first \new{algorithmic} result for GLM regression \new{with
oblivious noise} which can handle more than half the samples being arbitrarily
corrupted. Prior work focused largely on the setting of linear regression, and
gave algorithms under restrictive assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1"&gt;Sushrut Karmalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jongho Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1"&gt;Christos Tzamos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees. (arXiv:2309.12041v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2309.12041</id>
        <link href="http://arxiv.org/abs/2309.12041"/>
        <updated>2023-09-23T00:40:38.978Z</updated>
        <summary type="html"><![CDATA[Privacy-preserving learning of gradient boosting decision trees (GBDT) has
the potential for strong utility-privacy tradeoffs for tabular data, such as
census data or medical meta data: classical GBDT learners can extract
non-linear patterns from small sized datasets. The state-of-the-art notion for
provable privacy-properties is differential privacy, which requires that the
impact of single data points is limited and deniable. We introduce a novel
differentially private GBDT learner and utilize four main techniques to improve
the utility-privacy tradeoff. (1) We use an improved noise scaling approach
with tighter accounting of privacy leakage of a decision tree leaf compared to
prior work, resulting in noise that in expectation scales with $O(1/n)$, for
$n$ data points. (2) We integrate individual R\'enyi filters to our method to
learn from data points that have been underutilized during an iterative
training process, which -- potentially of independent interest -- results in a
natural yet effective insight to learning streams of non-i.i.d. data. (3) We
incorporate the concept of random decision tree splits to concentrate privacy
budget on learning leaves. (4) We deploy subsampling for privacy amplification.
Our evaluation shows for the Abalone dataset ($<4k$ training data points) a
$R^2$-score of $0.39$ for $\varepsilon=0.15$, which the closest prior work only
achieved for $\varepsilon=10.0$. On the Adult dataset ($50k$ training data
points) we achieve test error of $18.7\,\%$ for $\varepsilon=0.07$ which the
closest prior work only achieved for $\varepsilon=1.0$. For the Abalone dataset
for $\varepsilon=0.54$ we achieve $R^2$-score of $0.47$ which is very close to
the $R^2$-score of $0.54$ for the nonprivate version of GBDT. For the Adult
dataset for $\varepsilon=0.54$ we achieve test error $17.1\,\%$ which is very
close to the test error $13.7\,\%$ of the nonprivate version of GBDT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsche_M/0/1/0/all/0/1"&gt;Moritz Kirsche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peinemann_T/0/1/0/all/0/1"&gt;Thorsten Peinemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stock_J/0/1/0/all/0/1"&gt;Joshua Stock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotrini_C/0/1/0/all/0/1"&gt;Carlos Cotrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1"&gt;Esfandiar Mohammadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Complexity of Quantum Learning. (arXiv:2309.11617v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.11617</id>
        <link href="http://arxiv.org/abs/2309.11617"/>
        <updated>2023-09-23T00:40:38.978Z</updated>
        <summary type="html"><![CDATA[Recent years have seen significant activity on the problem of using data for
the purpose of learning properties of quantum systems or of processing
classical or quantum data via quantum computing. As in classical learning,
quantum learning problems involve settings in which the mechanism generating
the data is unknown, and the main goal of a learning algorithm is to ensure
satisfactory accuracy levels when only given access to data and, possibly, side
information such as expert knowledge. This article reviews the complexity of
quantum learning using information-theoretic techniques by focusing on data
complexity, copy complexity, and model complexity. Copy complexity arises from
the destructive nature of quantum measurements, which irreversibly alter the
state to be processed, limiting the information that can be extracted about
quantum data. For example, in a quantum system, unlike in classical machine
learning, it is generally not possible to evaluate the training loss
simultaneously on multiple hypotheses using the same quantum data. To make the
paper self-contained and approachable by different research communities, we
provide extensive background material on classical results from statistical
learning theory, as well as on the distinguishability of quantum states.
Throughout, we highlight the differences between quantum and classical learning
by addressing both supervised and unsupervised learning, and we provide
extensive pointers to the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Banchi_L/0/1/0/all/0/1"&gt;Leonardo Banchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pereira_J/0/1/0/all/0/1"&gt;Jason Luke Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jose_S/0/1/0/all/0/1"&gt;Sharu Theresa Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening. (arXiv:2309.11687v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11687</id>
        <link href="http://arxiv.org/abs/2309.11687"/>
        <updated>2023-09-23T00:40:38.977Z</updated>
        <summary type="html"><![CDATA[Virtual screening of large compound libraries to identify potential hit
candidates is one of the earliest steps in drug discovery. As the size of
commercially available compound collections grows exponentially to the scale of
billions, brute-force virtual screening using traditional tools such as docking
becomes infeasible in terms of time and computational resources. Active
learning and Bayesian optimization has recently been proven as effective
methods of narrowing down the search space. An essential component in those
methods is a surrogate machine learning model that is trained with a small
subset of the library to predict the desired properties of compounds. Accurate
model can achieve high sample efficiency by finding the most promising
compounds with only a fraction of the whole library being virtually screened.
In this study, we examined the performance of pretrained transformer-based
language model and graph neural network in Bayesian optimization active
learning framework. The best pretrained models identifies 58.97% of the
top-50000 by docking score after screening only 0.6% of an ultra-large library
containing 99.5 million compounds, improving 8% over previous state-of-the-art
baseline. Through extensive benchmarks, we show that the superior performance
of pretrained models persists in both structure-based and ligand-based drug
discovery. Such model can serve as a boost to the accuracy and sample
efficiency of active learning based molecule virtual screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhonglin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sciabola_S/0/1/0/all/0/1"&gt;Simone Sciabola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ye Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dictionary Attack on IMU-based Gait Authentication. (arXiv:2309.11766v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2309.11766</id>
        <link href="http://arxiv.org/abs/2309.11766"/>
        <updated>2023-09-23T00:40:38.977Z</updated>
        <summary type="html"><![CDATA[We present a novel adversarial model for authentication systems that use gait
patterns recorded by the inertial measurement unit (IMU) built into
smartphones. The attack idea is inspired by and named after the concept of a
dictionary attack on knowledge (PIN or password) based authentication systems.
In particular, this work investigates whether it is possible to build a
dictionary of IMUGait patterns and use it to launch an attack or find an
imitator who can actively reproduce IMUGait patterns that match the target's
IMUGait pattern. Nine physically and demographically diverse individuals walked
at various levels of four predefined controllable and adaptable gait factors
(speed, step length, step width, and thigh-lift), producing 178 unique IMUGait
patterns. Each pattern attacked a wide variety of user authentication models.
The deeper analysis of error rates (before and after the attack) challenges the
belief that authentication systems based on IMUGait patterns are the most
difficult to spoof; further research is needed on adversarial models and
associated countermeasures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rajesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isik_C/0/1/0/all/0/1"&gt;Can Isik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1"&gt;Chilukuri K. Mohan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge. (arXiv:2309.11575v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.11575</id>
        <link href="http://arxiv.org/abs/2309.11575"/>
        <updated>2023-09-23T00:40:38.965Z</updated>
        <summary type="html"><![CDATA[Text-conditioned image generation models have recently achieved astonishing
image quality and alignment results. Consequently, they are employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also produce
unsafe content. As a contribution to the Adversarial Nibbler challenge, we
distill a large set of over 1,000 potential adversarial inputs from existing
safety benchmarks. Our analysis of the gathered prompts and corresponding
images demonstrates the fragility of input filters and provides further
insights into systematic safety issues in current generative image models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1"&gt;Manuel Brack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1"&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.15299</id>
        <link href="http://arxiv.org/abs/2307.15299"/>
        <updated>2023-09-23T00:40:38.965Z</updated>
        <summary type="html"><![CDATA[Accurate load forecasting plays a vital role in numerous sectors, but
accurately capturing the complex dynamics of dynamic power systems remains a
challenge for traditional statistical models. For these reasons, time-series
models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly
deployed and often experience higher success. In this paper, we analyze the
efficacy of the recently developed Transformer-based Neural Network model in
Load forecasting. Transformer models have the potential to improve Load
forecasting because of their ability to learn long-range dependencies derived
from their Attention Mechanism. We apply several metaheuristics namely
Differential Evolution to find the optimal hyperparameters of the
Transformer-based Neural Network to produce accurate forecasts. Differential
Evolution provides scalable, robust, global solutions to non-differentiable,
multi-objective, or constrained optimization problems. Our work compares the
proposed Transformer based Neural Network model integrated with different
metaheuristic algorithms by their performance in Load forecasting based on
numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage
Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced
Transformer-based Neural Network models in Load forecasting accuracy and
provide optimal hyperparameters for each model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1"&gt;Anuvab Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_A/0/1/0/all/0/1"&gt;Arul Rhik Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_U/0/1/0/all/0/1"&gt;Udayon Sen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta OOD Learning for Continuously Adaptive OOD Detection. (arXiv:2309.11705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11705</id>
        <link href="http://arxiv.org/abs/2309.11705"/>
        <updated>2023-09-23T00:40:38.952Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection is crucial to modern deep learning
applications by identifying and alerting about the OOD samples that should not
be tested or used for making predictions. Current OOD detection methods have
made significant progress when in-distribution (ID) and OOD samples are drawn
from static distributions. However, this can be unrealistic when applied to
real-world systems which often undergo continuous variations and shifts in ID
and OOD distributions over time. Therefore, for an effective application in
real-world systems, the development of OOD detection methods that can adapt to
these dynamic and evolving distributions is essential. In this paper, we
propose a novel and more realistic setting called continuously adaptive
out-of-distribution (CAOOD) detection which targets on developing an OOD
detection model that enables dynamic and quick adaptation to a new arriving
distribution, with insufficient ID samples during deployment time. To address
CAOOD, we develop meta OOD learning (MOL) by designing a learning-to-adapt
diagram such that a good initialized OOD detection model is learned during the
training process. In the testing process, MOL ensures OOD detection performance
over shifting distributions by quickly adapting to new distributions with a few
adaptations. Extensive experiments on several OOD benchmarks endorse the
effectiveness of our method in preserving both ID classification accuracy and
OOD detection performance on continuously shifting distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guangquan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States. (arXiv:2210.00997v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.00997</id>
        <link href="http://arxiv.org/abs/2210.00997"/>
        <updated>2023-09-23T00:40:38.951Z</updated>
        <summary type="html"><![CDATA[Consider an online convex optimization problem where the loss functions are
self-concordant barriers, smooth relative to a convex function $h$, and
possibly non-Lipschitz. We analyze the regret of online mirror descent with
$h$. Then, based on the result, we prove the following in a unified manner.
Denote by $T$ the time horizon and $d$ the parameter dimension. 1. For online
portfolio selection, the regret of $\widetilde{\text{EG}}$, a variant of
exponentiated gradient due to Helmbold et al., is $\tilde{O} ( T^{2/3} d^{1/3}
)$ when $T > 4 d / \log d$. This improves on the original $\tilde{O} ( T^{3/4}
d^{1/2} )$ regret bound for $\widetilde{\text{EG}}$. 2. For online portfolio
selection, the regret of online mirror descent with the logarithmic barrier is
$\tilde{O}(\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due
to Orseau et al. up to logarithmic terms. 3. For online learning quantum states
with the logarithmic loss, the regret of online mirror descent with the
log-determinant function is also $\tilde{O} ( \sqrt{T d} )$. Its per-iteration
time is shorter than all existing algorithms we know.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tsai_C/0/1/0/all/0/1"&gt;Chung-En Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao-Chung Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yen-Huan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.12288</id>
        <link href="http://arxiv.org/abs/2309.12288"/>
        <updated>2023-09-23T00:40:38.950Z</updated>
        <summary type="html"><![CDATA[We expose a surprising failure of generalization in auto-regressive large
language models (LLMs). If a model is trained on a sentence of the form "A is
B", it will not automatically generalize to the reverse direction "B is A".
This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz
was the ninth Chancellor of Germany", it will not automatically be able to
answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the
likelihood of the correct answer ("Olaf Scholz") will not be higher than for a
random name. Thus, models exhibit a basic failure of logical deduction and do
not generalize a prevalent pattern in their training set (i.e. if "A is B''
occurs, "B is A" is more likely to occur). We provide evidence for the Reversal
Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah
Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to
correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is
robust across model sizes and model families and is not alleviated by data
augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about
real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee
Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly
answers questions like the former 79% of the time, compared to 33% for the
latter. This shows a failure of logical deduction that we hypothesize is caused
by the Reversal Curse. Code is available at
https://github.com/lukasberglund/reversal_curse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berglund_L/0/1/0/all/0/1"&gt;Lukas Berglund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1"&gt;Meg Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1"&gt;Max Kaufmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1"&gt;Mikita Balesni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1"&gt;Asa Cooper Stickland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1"&gt;Tomasz Korbak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1"&gt;Owain Evans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Propagation for Graph Neural Networks. (arXiv:2205.02998v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.02998</id>
        <link href="http://arxiv.org/abs/2205.02998"/>
        <updated>2023-09-23T00:40:38.949Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have achieved tremendous success in a variety of
real-world applications by relying on the fixed graph data as input. However,
the initial input graph might not be optimal in terms of specific downstream
tasks, because of information scarcity, noise, adversarial attacks, or
discrepancies between the distribution in graph topology, features, and
groundtruth labels. In this paper, we propose a bi-level optimization approach
for learning the optimal graph structure via directly learning the Personalized
PageRank propagation matrix as well as the downstream semi-supervised node
classification simultaneously. We also explore a low-rank approximation model
for further reducing the time complexity. Empirical evaluations show the
superior efficacy and robustness of the proposed model over all baseline
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Beidi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Boxin Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangyue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Automated Audio Captioning via text only training. (arXiv:2309.12242v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.12242</id>
        <link href="http://arxiv.org/abs/2309.12242"/>
        <updated>2023-09-23T00:40:38.948Z</updated>
        <summary type="html"><![CDATA[In recent years, datasets of paired audio and captions have enabled
remarkable success in automatically generating descriptions for audio clips,
namely Automated Audio Captioning (AAC). However, it is labor-intensive and
time-consuming to collect a sufficient number of paired audio and captions.
Motivated by the recent advances in Contrastive Language-Audio Pretraining
(CLAP), we propose a weakly-supervised approach to train an AAC model assuming
only text data and a pre-trained CLAP model, alleviating the need for paired
target data. Our approach leverages the similarity between audio and text
embeddings in CLAP. During training, we learn to reconstruct the text from the
CLAP text embedding, and during inference, we decode using the audio
embeddings. To mitigate the modality gap between the audio and text embeddings
we employ strategies to bridge the gap during training and inference stages. We
evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its
ability to achieve a relative performance of up to ~$83\%$ compared to fully
supervised approaches trained with paired target data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kouzelis_T/0/1/0/all/0/1"&gt;Theodoros Kouzelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsouros_V/0/1/0/all/0/1"&gt;Vassilis Katsouros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. (arXiv:2309.12032v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12032</id>
        <link href="http://arxiv.org/abs/2309.12032"/>
        <updated>2023-09-23T00:40:38.946Z</updated>
        <summary type="html"><![CDATA[Structure learning is the crux of causal inference. Notably, causal discovery
(CD) algorithms are brittle when data is scarce, possibly inferring imprecise
causal relations that contradict expert knowledge -- especially when
considering latent confounders. To aggravate the issue, most CD methods do not
provide uncertainty estimates, making it hard for users to interpret results
and improve the inference process. Surprisingly, while CD is a human-centered
affair, no works have focused on building methods that both 1) output
uncertainty estimates that can be verified by experts and 2) interact with
those experts to iteratively refine CD. To solve these issues, we start by
proposing to sample (causal) ancestral graphs proportionally to a belief
distribution based on a score function, such as the Bayesian information
criterion (BIC), using generative flow networks. Then, we leverage the
diversity in candidate graphs and introduce an optimal experimental design to
iteratively probe the expert about the relations among variables, effectively
reducing the uncertainty of our belief over ancestral graphs. Finally, we
update our samples to incorporate human feedback via importance sampling.
Importantly, our method does not require causal sufficiency (i.e., unobserved
confounders may exist). Experiments with synthetic observational data show that
our method can accurately sample from distributions over ancestral graphs and
that we can greatly improve inference quality with human aid.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1"&gt;Tiago da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_E/0/1/0/all/0/1"&gt;Eliezer Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Ad&amp;#xe8;le Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gois_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio G&amp;#xf3;is&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heider_D/0/1/0/all/0/1"&gt;Dominik Heider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mesquita_D/0/1/0/all/0/1"&gt;Diego Mesquita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering-based Domain-Incremental Learning. (arXiv:2309.12078v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12078</id>
        <link href="http://arxiv.org/abs/2309.12078"/>
        <updated>2023-09-23T00:40:38.944Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning multiple tasks in a continual learning
setting in which data from different tasks is presented to the learner in a
streaming fashion. A key challenge in this setting is the so-called
"catastrophic forgetting problem", in which the performance of the learner in
an "old task" decreases when subsequently trained on a "new task". Existing
continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM)
and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by
minimizing the loss for the current task without increasing the loss for
previous tasks. However, these methods assume the learner knows when the task
changes, which is unrealistic in practice. In this paper, we alleviate the need
to provide the algorithm with information about task changes by using an online
clustering-based approach on a dynamically updated finite pool of samples or
gradients. We thereby successfully counteract catastrophic forgetting in one of
the hardest settings, namely: domain-incremental learning, a setting for which
the problem was previously unsolved. We showcase the benefits of our approach
by applying these ideas to projection-based methods, such as A-GEM and OGD,
which lead to task-agnostic versions of them. Experiments on real datasets
demonstrate the effectiveness of the proposed strategy and its promising
performance compared to state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamers_C/0/1/0/all/0/1"&gt;Christiaan Lamers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Rene Vidal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belbachir_N/0/1/0/all/0/1"&gt;Nabil Belbachir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stein_N/0/1/0/all/0/1"&gt;Niki van Stein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baeck_T/0/1/0/all/0/1"&gt;Thomas Baeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giampouras_P/0/1/0/all/0/1"&gt;Paris Giampouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traffic Forecasting on New Roads Using Spatial Contrastive Pre-Training (SCPT). (arXiv:2305.05237v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.05237</id>
        <link href="http://arxiv.org/abs/2305.05237"/>
        <updated>2023-09-23T00:40:38.943Z</updated>
        <summary type="html"><![CDATA[New roads are being constructed all the time. However, the capabilities of
previous deep forecasting models to generalize to new roads not seen in the
training data (unseen roads) are rarely explored. In this paper, we introduce a
novel setup called a spatio-temporal (ST) split to evaluate the models'
capabilities to generalize to unseen roads. In this setup, the models are
trained on data from a sample of roads, but tested on roads not seen in the
training data. Moreover, we also present a novel framework called Spatial
Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to
extract latent features from unseen roads during inference time. This spatial
encoder is pre-trained using contrastive learning. During inference, the
spatial encoder only requires two days of traffic data on the new roads and
does not require any re-training. We also show that the output from the spatial
encoder can be used effectively to infer latent node embeddings on unseen roads
during inference time. The SCPT framework also incorporates a new layer, named
the spatially gated addition (SGA) layer, to effectively combine the latent
features from the output of the spatial encoder to existing backbones.
Additionally, since there is limited data on the unseen roads, we argue that it
is better to decouple traffic signals to trivial-to-capture periodic signals
and difficult-to-capture Markovian signals, and for the spatial encoder to only
learn the Markovian signals. Finally, we empirically evaluated SCPT using the
ST split setup on four real-world datasets. The results showed that adding SCPT
to a backbone consistently improves forecasting performance on unseen roads.
More importantly, the improvements are greater when forecasting further into
the future. The codes are available on GitHub:
https://github.com/cruiseresearchgroup/forecasting-on-new-roads .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1"&gt;Arian Prabowo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification. (arXiv:2309.11714v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.11714</id>
        <link href="http://arxiv.org/abs/2309.11714"/>
        <updated>2023-09-23T00:40:38.939Z</updated>
        <summary type="html"><![CDATA[There is a correlation between adjacent channels of electroencephalogram
(EEG), and how to represent this correlation is an issue that is currently
being explored. In addition, due to inter-individual differences in EEG
signals, this discrepancy results in new subjects need spend a amount of
calibration time for EEG-based motor imagery brain-computer interface. In order
to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep
Learning Network (DADL-Net). First, the EEG data is mapped to the
three-dimensional geometric space and its temporal-spatial features are learned
through the 3D convolution module, and then the spatial-channel attention
mechanism is used to strengthen the features, and the final convolution module
can further learn the spatial-temporal information of the features. Finally, to
account for inter-subject and cross-sessions differences, we employ a dynamic
domain-adaptive strategy, the distance between features is reduced by
introducing a Maximum Mean Discrepancy loss function, and the classification
layer is fine-tuned by using part of the target domain data. We verify the
performance of the proposed method on BCI competition IV 2a and OpenBMI
datasets. Under the intra-subject experiment, the accuracy rates of 70.42% and
73.91% were achieved on the OpenBMI and BCIC IV 2a datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jie Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Meiyan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hefan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wangliang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval. (arXiv:2309.12158v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.12158</id>
        <link href="http://arxiv.org/abs/2309.12158"/>
        <updated>2023-09-23T00:40:38.939Z</updated>
        <summary type="html"><![CDATA[A range of applications of multi-modal music information retrieval is centred
around the problem of connecting large collections of sheet music (images) to
corresponding audio recordings, that is, identifying pairs of audio and score
excerpts that refer to the same musical content. One of the typical and most
recent approaches to this task employs cross-modal deep learning architectures
to learn joint embedding spaces that link the two distinct modalities - audio
and sheet music images. While there has been steady improvement on this front
over the past years, a number of open problems still prevent large-scale
employment of this methodology. In this article we attempt to provide an
insightful examination of the current developments on audio-sheet music
retrieval via deep learning methods. We first identify a set of main challenges
on the road towards robust and large-scale cross-modal music retrieval in real
scenarios. We then highlight the steps we have taken so far to address some of
these challenges, documenting step-by-step improvement along several
dimensions. We conclude by analysing the remaining challenges and present ideas
for solving these, in order to pave the way to a unified and robust methodology
for cross-modal music retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_L/0/1/0/all/0/1"&gt;Luis Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1"&gt;Gerhard Widmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electroencephalogram Sensor Data Compression Using An Asymmetrical Sparse Autoencoder With A Discrete Cosine Transform Layer. (arXiv:2309.12201v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.12201</id>
        <link href="http://arxiv.org/abs/2309.12201"/>
        <updated>2023-09-23T00:40:38.939Z</updated>
        <summary type="html"><![CDATA[Electroencephalogram (EEG) data compression is necessary for wireless
recording applications to reduce the amount of data that needs to be
transmitted. In this paper, an asymmetrical sparse autoencoder with a discrete
cosine transform (DCT) layer is proposed to compress EEG signals. The encoder
module of the autoencoder has a combination of a fully connected linear layer
and the DCT layer to reduce redundant data using hard-thresholding
nonlinearity. Furthermore, the DCT layer includes trainable hard-thresholding
parameters and scaling layers to give emphasis or de-emphasis on individual DCT
coefficients. Finally, the one-by-one convolutional layer generates the latent
space. The sparsity penalty-based cost function is employed to keep the feature
map as sparse as possible in the latent space. The latent space data is
transmitted to the receiver. The decoder module of the autoencoder is designed
using the inverse DCT and two fully connected linear layers to improve the
accuracy of data reconstruction. In comparison to other state-of-the-art
methods, the proposed method significantly improves the average quality score
in various data compression experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_H/0/1/0/all/0/1"&gt;Hongyi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rong_S/0/1/0/all/0/1"&gt;Shuaiang Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cetin_A/0/1/0/all/0/1"&gt;Ahmet Enis Cetin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Vs. Personalization: Towards Equity in Epistemic Utility. (arXiv:2309.11503v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11503</id>
        <link href="http://arxiv.org/abs/2309.11503"/>
        <updated>2023-09-23T00:40:38.938Z</updated>
        <summary type="html"><![CDATA[The applications of personalized recommender systems are rapidly expanding:
encompassing social media, online shopping, search engine results, and more.
These systems offer a more efficient way to navigate the vast array of items
available. However, alongside this growth, there has been increased recognition
of the potential for algorithmic systems to exhibit and perpetuate biases,
risking unfairness in personalized domains. In this work, we explicate the
inherent tension between personalization and conventional implementations of
fairness. As an alternative, we propose equity to achieve fairness in the
context of epistemic utility. We provide a mapping between goals and practical
implementations and detail policy recommendations across key stakeholders to
forge a path towards achieving fairness in personalized systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chien_J/0/1/0/all/0/1"&gt;Jennifer Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danks_D/0/1/0/all/0/1"&gt;David Danks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.03303</id>
        <link href="http://arxiv.org/abs/2306.03303"/>
        <updated>2023-09-23T00:40:38.937Z</updated>
        <summary type="html"><![CDATA[We introduce so-called functional input neural networks defined on a possibly
infinite dimensional weighted space with values also in a possibly infinite
dimensional output space. To this end, we use an additive family as hidden
layer maps and a non-linear activation function applied to each hidden layer.
Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global
universal approximation result for generalizations of continuous functions
going beyond the usual approximation on compact sets. This then applies in
particular to approximation of (non-anticipative) path space functionals via
functional input neural networks. As a further application of the weighted
Stone-Weierstrass theorem we prove a global universal approximation result for
linear functions of the signature. We also introduce the viewpoint of Gaussian
process regression in this setting and show that the reproducing kernel Hilbert
space of the signature kernels are Cameron-Martin spaces of certain Gaussian
processes. This paves the way towards uncertainty quantification for signature
kernel regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cuchiero_C/0/1/0/all/0/1"&gt;Christa Cuchiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmocker_P/0/1/0/all/0/1"&gt;Philipp Schmocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teichmann_J/0/1/0/all/0/1"&gt;Josef Teichmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization. (arXiv:2307.02108v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.02108</id>
        <link href="http://arxiv.org/abs/2307.02108"/>
        <updated>2023-09-23T00:40:38.936Z</updated>
        <summary type="html"><![CDATA[Simple regret minimization is a critical problem in learning optimal
treatment assignment policies across various domains, including healthcare and
e-commerce. However, it remains understudied in the contextual bandit setting.
We propose a new family of computationally efficient bandit algorithms for the
stochastic contextual bandit settings, with the flexibility to be adapted for
cumulative regret minimization (with near-optimal minimax guarantees) and
simple regret minimization (with SOTA guarantees). Furthermore, our algorithms
adapt to model misspecification and extend to the continuous arm settings.
These advantages come from constructing and relying on "conformal arm sets"
(CASs), which provide a set of arms at every context that encompass the
context-specific optimal arm with some probability across the context
distribution. Our positive results on simple and cumulative regret guarantees
are contrasted by a negative result, which shows that an algorithm can't
achieve instance-dependent simple regret guarantees while simultaneously
achieving minimax optimal cumulative regret guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Sanath Kumar Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_R/0/1/0/all/0/1"&gt;Ruohan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1"&gt;Emma Brunskill&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incentivized Communication for Federated Bandits. (arXiv:2309.11702v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11702</id>
        <link href="http://arxiv.org/abs/2309.11702"/>
        <updated>2023-09-23T00:40:38.931Z</updated>
        <summary type="html"><![CDATA[Most existing works on federated bandits take it for granted that all clients
are altruistic about sharing their data with the server for the collective good
whenever needed. Despite their compelling theoretical guarantee on performance
and communication efficiency, this assumption is overly idealistic and
oftentimes violated in practice, especially when the algorithm is operated over
self-interested clients, who are reluctant to share data without explicit
benefits. Negligence of such self-interested behaviors can significantly affect
the learning efficiency and even the practical operability of federated bandit
learning. In light of this, we aim to spark new insights into this
under-explored research area by formally introducing an incentivized
communication problem for federated bandits, where the server shall motivate
clients to share data by providing incentives. Without loss of generality, we
instantiate this bandit problem with the contextual linear setting and propose
the first incentivized communication protocol, namely, Inc-FedUCB, that
achieves near-optimal regret with provable communication and incentive cost
guarantees. Extensive empirical experiments on both synthetic and real-world
datasets further validate the effectiveness of the proposed method across
various environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhepei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuanhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haifeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongning Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning. (arXiv:2309.11722v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2309.11722</id>
        <link href="http://arxiv.org/abs/2309.11722"/>
        <updated>2023-09-23T00:40:38.931Z</updated>
        <summary type="html"><![CDATA[Federated learning is a distributed machine learning system that uses
participants' data to train an improved global model. In federated learning,
participants cooperatively train a global model, and they will receive the
global model and payments. Rational participants try to maximize their
individual utility, and they will not input their high-quality data truthfully
unless they are provided with satisfactory payments based on their data
quality. Furthermore, federated learning benefits from the cooperative
contributions of participants. Accordingly, how to establish an incentive
mechanism that both incentivizes inputting data truthfully and promotes stable
cooperation has become an important issue to consider. In this paper, we
introduce a data sharing game model for federated learning and employ
game-theoretic approaches to design a core-selecting incentive mechanism by
utilizing a popular concept in cooperative games, the core. In federated
learning, the core can be empty, resulting in the core-selecting mechanism
becoming infeasible. To address this, our core-selecting mechanism employs a
relaxation method and simultaneously minimizes the benefits of inputting false
data for all participants. However, this mechanism is computationally expensive
because it requires aggregating exponential models for all possible coalitions,
which is infeasible in federated learning. To address this, we propose an
efficient core-selecting mechanism based on sampling approximation that only
aggregates models on sampled coalitions to approximate the exact result.
Extensive experiments verify that the efficient core-selecting mechanism can
incentivize inputting high-quality data and stable cooperation, while it
reduces computational overhead compared to the core-selecting mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1"&gt;Mengda Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Genjiu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1"&gt;Jianjun Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mingqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2309.11798</id>
        <link href="http://arxiv.org/abs/2309.11798"/>
        <updated>2023-09-23T00:40:38.931Z</updated>
        <summary type="html"><![CDATA[The study of complex networks has significantly advanced our understanding of
community structures which serves as a crucial feature of real-world graphs.
Detecting communities in graphs is a challenging problem with applications in
sociology, biology, and computer science. Despite the efforts of an
interdisciplinary community of scientists, a satisfactory solution to this
problem has not yet been achieved. This review article delves into the topic of
community detection in graphs, which serves as a crucial role in understanding
the organization and functioning of complex systems. We begin by introducing
the concept of community structure, which refers to the arrangement of vertices
into clusters, with strong internal connections and weaker connections between
clusters. Then, we provide a thorough exposition of various community detection
methods, including a new method designed by us. Additionally, we explore
real-world applications of community detection in diverse networks. In
conclusion, this comprehensive review provides a deep understanding of
community detection in graphs. It serves as a valuable resource for researchers
and practitioners in multiple disciplines, offering insights into the
challenges, methodologies, and applications of community detection in complex
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ning_S/0/1/0/all/0/1"&gt;Songlai Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiakang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yonggang Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.03992</id>
        <link href="http://arxiv.org/abs/2309.03992"/>
        <updated>2023-09-23T00:40:38.930Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) are increasingly being used for generating text
in a variety of use cases, including journalistic news articles. Given the
potential malicious nature in which these LLMs can be used to generate
disinformation at scale, it is important to build effective detectors for such
AI-generated text. Given the surge in development of new LLMs, acquiring
labeled training data for supervised detectors is a bottleneck. However, there
might be plenty of unlabeled text data available, without information on which
generator it came from. In this work we tackle this data problem, in detecting
AI-generated news text, and frame the problem as an unsupervised domain
adaptation task. Here the domains are the different text generators, i.e. LLMs,
and we assume we have access to only the labeled source data and unlabeled
target data. We develop a Contrastive Domain Adaptation framework, called
ConDA, that blends standard domain adaptation techniques with the
representation power of contrastive learning to learn domain invariant
representations that are effective for the final unsupervised detection task.
Our experiments demonstrate the effectiveness of our framework, resulting in
average performance gains of 31.7% from the best performing baselines, and
within 0.8% margin of a fully supervised detector. All our code and data is
available at https://github.com/AmritaBh/ConDA-gen-text-detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1"&gt;Amrita Bhattacharjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1"&gt;Tharindu Kumarage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1"&gt;Raha Moraffah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning. (arXiv:2309.11782v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.11782</id>
        <link href="http://arxiv.org/abs/2309.11782"/>
        <updated>2023-09-23T00:40:38.928Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) has gained remarkable success, for which
contrastive learning (CL) plays a key role. However, the recent development of
new non-CL frameworks has achieved comparable or better performance with high
improvement potential, prompting researchers to enhance these frameworks
further. Assimilating CL into non-CL frameworks has been thought to be
beneficial, but empirical evidence indicates no visible improvements. In view
of that, this paper proposes a strategy of performing CL along the dimensional
direction instead of along the batch direction as done in conventional
contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL
aims to enhance the feature diversity, and it can serve as a regularizer to
prior SSL frameworks. DimCL has been found to be effective, and the
hardness-aware property is identified as a critical reason for its success.
Extensive experimental results reveal that assimilating DimCL into SSL
frameworks leads to performance improvement by a non-trivial margin on various
datasets and backbone architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1"&gt;Trung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1"&gt;Tung Luu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Thang Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1"&gt;Chang D. Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. (arXiv:2309.11765v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11765</id>
        <link href="http://arxiv.org/abs/2309.11765"/>
        <updated>2023-09-23T00:40:38.927Z</updated>
        <summary type="html"><![CDATA[We study the problem of in-context learning (ICL) with large language models
(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak
or regurgitate the private examples demonstrated in the prompt. We propose a
novel algorithm that generates synthetic few-shot demonstrations from the
private dataset with formal differential privacy (DP) guarantees, and show
empirically that it can achieve effective ICL. We conduct extensive experiments
on standard benchmarks and compare our algorithm with non-private ICL and
zero-shot solutions. Our results demonstrate that our algorithm can achieve
competitive performance with strong privacy levels. These results open up new
possibilities for ICL with privacy protection for a broad range of
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xinyu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1"&gt;Richard Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1"&gt;Huseyin A. Inan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manoel_A/0/1/0/all/0/1"&gt;Andre Manoel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zinan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1"&gt;Janardhan Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1"&gt;Robert Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning. (arXiv:2304.03398v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.03398</id>
        <link href="http://arxiv.org/abs/2304.03398"/>
        <updated>2023-09-23T00:40:38.919Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning is a promising programming paradigm for the
optimization of quantum algorithms in the current era of noisy intermediate
scale quantum (NISQ) computers. A fundamental challenge in quantum machine
learning is generalization, as the designer targets performance under testing
conditions, while having access only to limited training data. Existing
generalization analyses, while identifying important general trends and scaling
laws, cannot be used to assign reliable and informative "error bars" to the
decisions made by quantum models. In this article, we propose a general
methodology that can reliably quantify the uncertainty of quantum models,
irrespective of the amount of training data, of the number of shots, of the
ansatz, of the training algorithm, and of the presence of quantum hardware
noise. The approach, which builds on probabilistic conformal prediction, turns
an arbitrary, possibly small, number of shots from a pre-trained quantum model
into a set prediction, e.g., an interval, that provably contains the true
target with any desired coverage level. Experimental results confirm the
theoretical calibration guarantees of the proposed framework, referred to as
quantum conformal prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11564</id>
        <link href="http://arxiv.org/abs/2309.11564"/>
        <updated>2023-09-23T00:40:38.917Z</updated>
        <summary type="html"><![CDATA[Hierarchical reinforcement learning has been a compelling approach for
achieving goal directed behavior over long sequences of actions. However, it
has been challenging to implement in realistic or open-ended environments. A
main challenge has been to find the right space of sub-goals over which to
instantiate a hierarchy. We present a novel approach where we use data from
humans solving these tasks to softly supervise the goal space for a set of long
range tasks in a 3D embodied environment. In particular, we use unconstrained
natural language to parameterize this space. This has two advantages: first, it
is easy to generate this data from naive human participants; second, it is
flexible enough to represent a vast range of sub-goals in human-relevant tasks.
Our approach outperforms agents that clone expert behavior on these tasks, as
well as HRL from scratch without this supervised sub-goal space. Our work
presents a novel approach to combining human expert supervision with the
benefits and flexibility of reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1"&gt;Arun Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopparapu_K/0/1/0/all/0/1"&gt;Kavya Kopparapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1"&gt;Rob Fergus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1"&gt;Ishita Dasgupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regionally Additive Models: Explainable-by-design models minimizing feature interactions. (arXiv:2309.12215v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12215</id>
        <link href="http://arxiv.org/abs/2309.12215"/>
        <updated>2023-09-23T00:40:38.912Z</updated>
        <summary type="html"><![CDATA[Generalized Additive Models (GAMs) are widely used explainable-by-design
models in various applications. GAMs assume that the output can be represented
as a sum of univariate functions, referred to as components. However, this
assumption fails in ML problems where the output depends on multiple features
simultaneously. In these cases, GAMs fail to capture the interaction terms of
the underlying function, leading to subpar accuracy. To (partially) address
this issue, we propose Regionally Additive Models (RAMs), a novel class of
explainable-by-design models. RAMs identify subregions within the feature space
where interactions are minimized. Within these regions, it is more accurate to
express the output as a sum of univariate functions (components). Consequently,
RAMs fit one component per subregion of each feature instead of one component
per feature. This approach yields a more expressive model compared to GAMs
while retaining interpretability. The RAM framework consists of three steps.
Firstly, we train a black-box model. Secondly, using Regional Effect Plots, we
identify subregions where the black-box model exhibits near-local additivity.
Lastly, we fit a GAM component for each identified subregion. We validate the
effectiveness of RAMs through experiments on both synthetic and real-world
datasets. The results confirm that RAMs offer improved expressiveness compared
to GAMs while maintaining interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gkolemis_V/0/1/0/all/0/1"&gt;Vasilis Gkolemis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzerefos_A/0/1/0/all/0/1"&gt;Anargiros Tzerefos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalamagas_T/0/1/0/all/0/1"&gt;Theodore Dalamagas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1"&gt;Eirini Ntoutsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diou_C/0/1/0/all/0/1"&gt;Christos Diou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.06101</id>
        <link href="http://arxiv.org/abs/2306.06101"/>
        <updated>2023-09-23T00:40:38.910Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating the learning rate in adaptive methods,
such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to
provably estimate the distance to the solution $D$, which is needed to set the
learning rate optimally. Our techniques are modifications of the D-Adaptation
method for learning-rate-free learning. Our methods improve upon the
convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where
$d_0$ is the initial estimate of $D$. We test our methods on 12 common
logistic-regression benchmark datasets, VGG11 and ResNet-50 training on
CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on
Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT
transformer training on BookWiki. Our experimental results show that our
approaches consistently outperform D-Adaptation and reach test accuracy values
close to that of hand-tuned Adam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1"&gt;Konstantin Mishchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1"&gt;Aaron Defazio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-agent Deep Covering Skill Discovery. (arXiv:2210.03269v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.03269</id>
        <link href="http://arxiv.org/abs/2210.03269"/>
        <updated>2023-09-23T00:40:38.909Z</updated>
        <summary type="html"><![CDATA[The use of skills (a.k.a., options) can greatly accelerate exploration in
reinforcement learning, especially when only sparse reward signals are
available. While option discovery methods have been proposed for individual
agents, in multi-agent reinforcement learning settings, discovering
collaborative options that can coordinate the behavior of multiple agents and
encourage them to visit the under-explored regions of their joint state space
has not been considered. In this case, we propose Multi-agent Deep Covering
Option Discovery, which constructs the multi-agent options through minimizing
the expected cover time of the multiple agents' joint state space. Also, we
propose a novel framework to adopt the multi-agent options in the MARL process.
In practice, a multi-agent task can usually be divided into some sub-tasks,
each of which can be completed by a sub-group of the agents. Therefore, our
algorithm framework first leverages an attention mechanism to find
collaborative agent sub-groups that would benefit most from coordinated
actions. Then, a hierarchical algorithm, namely HA-MSAC, is developed to learn
the multi-agent options for each sub-group to complete their sub-tasks first,
and then to integrate them through a high-level policy as the solution of the
whole task. This hierarchical option construction allows our framework to
strike a balance between scalability and effective collaboration among the
agents. The evaluation based on multi-agent collaborative tasks shows that the
proposed algorithm can effectively capture the agent interactions with the
attention mechanism, successfully identify multi-agent options, and
significantly outperforms prior works using single-agent options or no options,
in terms of both faster exploration and higher task rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiayu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1"&gt;Marina Haliem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1"&gt;Tian Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1"&gt;Vaneet Aggarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiclass Learnability Does Not Imply Sample Compression. (arXiv:2308.06424v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.06424</id>
        <link href="http://arxiv.org/abs/2308.06424"/>
        <updated>2023-09-23T00:40:38.907Z</updated>
        <summary type="html"><![CDATA[A hypothesis class admits a sample compression scheme, if for every sample
labeled by a hypothesis from the class, it is possible to retain only a small
subsample, using which the labels on the entire sample can be inferred. The
size of the compression scheme is an upper bound on the size of the subsample
produced. Every learnable binary hypothesis class (which must necessarily have
finite VC dimension) admits a sample compression scheme of size only a finite
function of its VC dimension, independent of the sample size. For multiclass
hypothesis classes, the analog of VC dimension is the DS dimension. We show
that the analogous statement pertaining to sample compression is not true for
multiclass hypothesis classes: every learnable multiclass hypothesis class,
which must necessarily have finite DS dimension, does not admit a sample
compression scheme of size only a finite function of its DS dimension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pabbaraju_C/0/1/0/all/0/1"&gt;Chirag Pabbaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.11489</id>
        <link href="http://arxiv.org/abs/2309.11489"/>
        <updated>2023-09-23T00:40:38.903Z</updated>
        <summary type="html"><![CDATA[Designing reward functions is a longstanding challenge in reinforcement
learning (RL); it requires specialized knowledge or domain data, leading to
high costs for development. To address this, we introduce Text2Reward, a
data-free framework that automates the generation of dense reward functions
based on large language models (LLMs). Given a goal described in natural
language, Text2Reward generates dense reward functions as an executable program
grounded in a compact representation of the environment. Unlike inverse RL and
recent work that uses LLMs to write sparse reward codes, Text2Reward produces
interpretable, free-form dense reward codes that cover a wide range of tasks,
utilize existing packages, and allow iterative refinement with human feedback.
We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,
MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17
manipulation tasks, policies trained with generated reward codes achieve
similar or better task success rates and convergence speed than expert-written
reward codes. For locomotion tasks, our method learns six novel locomotion
behaviors with a success rate exceeding 94%. Furthermore, we show that the
policies trained in the simulator with our method can be deployed in the real
world. Finally, Text2Reward further improves the policies by refining their
reward functions with human feedback. Video results are available at
https://text-to-reward.github.io]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tianbao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Siheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Henry Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yitao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1"&gt;Qian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1"&gt;Victor Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yanchao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Constructive Approach to Function Realization by Neural Stochastic Differential Equations. (arXiv:2307.00215v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.00215</id>
        <link href="http://arxiv.org/abs/2307.00215"/>
        <updated>2023-09-23T00:40:38.902Z</updated>
        <summary type="html"><![CDATA[The problem of function approximation by neural dynamical systems has
typically been approached in a top-down manner: Any continuous function can be
approximated to an arbitrary accuracy by a sufficiently complex model with a
given architecture. This can lead to high-complexity controls which are
impractical in applications. In this paper, we take the opposite, constructive
approach: We impose various structural restrictions on system dynamics and
consequently characterize the class of functions that can be realized by such a
system. The systems are implemented as a cascade interconnection of a neural
stochastic differential equation (Neural SDE), a deterministic dynamical
system, and a readout map. Both probabilistic and geometric (Lie-theoretic)
methods are used to characterize the classes of functions realized by such
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Veeravalli_T/0/1/0/all/0/1"&gt;Tanya Veeravalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Raginsky_M/0/1/0/all/0/1"&gt;Maxim Raginsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine Learning-oriented Survey on Tiny Machine Learning. (arXiv:2309.11932v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11932</id>
        <link href="http://arxiv.org/abs/2309.11932"/>
        <updated>2023-09-23T00:40:38.867Z</updated>
        <summary type="html"><![CDATA[The emergence of Tiny Machine Learning (TinyML) has positively revolutionized
the field of Artificial Intelligence by promoting the joint design of
resource-constrained IoT hardware devices and their learning-based software
architectures. TinyML carries an essential role within the fourth and fifth
industrial revolutions in helping societies, economies, and individuals employ
effective AI-infused computing technologies (e.g., smart cities, automotive,
and medical robotics). Given its multidisciplinary nature, the field of TinyML
has been approached from many different angles: this comprehensive survey
wishes to provide an up-to-date overview focused on all the learning algorithms
within TinyML-based solutions. The survey is based on the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,
allowing for a systematic and complete literature survey. In particular,
firstly we will examine the three different workflows for implementing a
TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly,
we propose a taxonomy that covers the learning panorama under the TinyML lens,
examining in detail the different families of model optimization and design, as
well as the state-of-the-art learning techniques. Thirdly, this survey will
present the distinct features of hardware devices and software tools that
represent the current state-of-the-art for TinyML intelligent edge
applications. Finally, we discuss the challenges and future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Capogrosso_L/0/1/0/all/0/1"&gt;Luigi Capogrosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunico_F/0/1/0/all/0/1"&gt;Federico Cunico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Dong Seon Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fummi_F/0/1/0/all/0/1"&gt;Franco Fummi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+cristani_M/0/1/0/all/0/1"&gt;Marco cristani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Probability of Immunity. (arXiv:2309.11942v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2309.11942</id>
        <link href="http://arxiv.org/abs/2309.11942"/>
        <updated>2023-09-23T00:40:38.867Z</updated>
        <summary type="html"><![CDATA[This work is devoted to the study of the probability of immunity, i.e. the
effect occurs whether exposed or not. We derive necessary and sufficient
conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the
probability of immunity is zero and $\epsilon$-bounded, respectively. The
former allows us to estimate the probability of benefit (i.e., the effect
occurs if and only if exposed) from a randomized controlled trial, and the
latter allows us to produce bounds of the probability of benefit that are
tighter than the existing ones. We also introduce the concept of indirect
immunity (i.e., through a mediator) and repeat our previous analysis for it.
Finally, we propose a method for sensitivity analysis of the probability of
immunity under unmeasured confounding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1"&gt;Jose M. Pe&amp;#xf1;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasi-Monte Carlo for 3D Sliced Wasserstein. (arXiv:2309.11713v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.11713</id>
        <link href="http://arxiv.org/abs/2309.11713"/>
        <updated>2023-09-23T00:40:38.850Z</updated>
        <summary type="html"><![CDATA[Monte Carlo (MC) approximation has been used as the standard computation
approach for the Sliced Wasserstein (SW) distance, which has an intractable
expectation in its analytical form. However, the MC method is not optimal in
terms of minimizing the absolute approximation error. To provide a better class
of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that
rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of
QMC for SW, we focus on the 3D setting, specifically computing the SW between
probability measures in three dimensions. In greater detail, we empirically
verify various ways of constructing QMC points sets on the 3D unit-hypersphere,
including Gaussian-based mapping, equal area mapping, generalized spiral
points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased
estimation for stochastic optimization, we extend QSW into Randomized
Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed
low-discrepancy sequences. For theoretical properties, we prove the asymptotic
convergence of QSW and the unbiasedness of RQSW. Finally, we conduct
experiments on various 3D tasks, such as point-cloud comparison, point-cloud
interpolation, image style transfer, and training deep point-cloud
autoencoders, to demonstrate the favorable performance of the proposed QSW and
RQSW variants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bariletto_N/0/1/0/all/0/1"&gt;Nicola Bariletto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. (arXiv:2309.11531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.11531</id>
        <link href="http://arxiv.org/abs/2309.11531"/>
        <updated>2023-09-23T00:40:38.842Z</updated>
        <summary type="html"><![CDATA[Quantization of deep neural networks (DNN) has become a key element in the
efforts of embedding such networks on end-user devices. However, current
quantization methods usually suffer from costly accuracy degradation. In this
paper, we propose a new method for Enhanced Post Training Quantization named
EPTQ. The method is based on knowledge distillation with an adaptive weighting
of layers. In addition, we introduce a new label-free technique for
approximating the Hessian trace of the task loss, named Label-Free Hessian.
This technique removes the requirement of a labeled dataset for computing the
Hessian. The adaptive knowledge distillation uses the Label-Free Hessian
technique to give greater attention to the sensitive parts of the model while
performing the optimization. Empirically, by employing EPTQ we achieve
state-of-the-art results on a wide variety of models, tasks, and datasets,
including ImageNet classification, COCO object detection, and Pascal-VOC for
semantic segmentation. We demonstrate the performance and compatibility of EPTQ
on an extended set of architectures, including CNNs, Transformers, hybrid, and
MLP-only models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_O/0/1/0/all/0/1"&gt;Ofir Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habi_H/0/1/0/all/0/1"&gt;Hai Victor Habi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netzer_A/0/1/0/all/0/1"&gt;Arnon Netzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.11052</id>
        <link href="http://arxiv.org/abs/2309.11052"/>
        <updated>2023-09-23T00:40:38.838Z</updated>
        <summary type="html"><![CDATA[The proliferation of fake news has become a significant concern in recent
times due to its potential to spread misinformation and manipulate public
opinion. This paper presents a comprehensive study on detecting fake news in
Brazilian Portuguese, focusing on journalistic-type news. We propose a machine
learning-based approach that leverages natural language processing techniques,
including TF-IDF and Word2Vec, to extract features from textual data. We
evaluate the performance of various classification algorithms, such as logistic
regression, support vector machine, random forest, AdaBoost, and LightGBM, on a
dataset containing both true and fake news articles. The proposed approach
achieves high accuracy and F1-Score, demonstrating its effectiveness in
identifying fake news. Additionally, we developed a user-friendly web platform,
fakenewsbr.com, to facilitate the verification of news articles' veracity. Our
platform provides real-time analysis, allowing users to assess the likelihood
of fake news articles. Through empirical analysis and comparative studies, we
demonstrate the potential of our approach to contribute to the fight against
the spread of fake news and promote more informed media consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giordani_L/0/1/0/all/0/1"&gt;Luiz Giordani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daru_G/0/1/0/all/0/1"&gt;Gilsiley Dar&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Queiroz_R/0/1/0/all/0/1"&gt;Rhenan Queiroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buzinaro_V/0/1/0/all/0/1"&gt;Vitor Buzinaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiva_D/0/1/0/all/0/1"&gt;Davi Keglevich Neiva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_D/0/1/0/all/0/1"&gt;Daniel Camilo Fuentes Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_M/0/1/0/all/0/1"&gt;Marcos Jardel Henriques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_O/0/1/0/all/0/1"&gt;Oilson Alberto Gonzatto Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louzada_F/0/1/0/all/0/1"&gt;Francisco Louzada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.07037</id>
        <link href="http://arxiv.org/abs/2308.07037"/>
        <updated>2023-09-23T00:40:38.837Z</updated>
        <summary type="html"><![CDATA[This paper introduces Bayesian Flow Networks (BFNs), a new class of
generative model in which the parameters of a set of independent distributions
are modified with Bayesian inference in the light of noisy data samples, then
passed as input to a neural network that outputs a second, interdependent
distribution. Starting from a simple prior and iteratively updating the two
distributions yields a generative procedure similar to the reverse process of
diffusion models; however it is conceptually simpler in that no forward process
is required. Discrete and continuous-time loss functions are derived for
continuous, discretised and discrete data, along with sample generation
procedures. Notably, the network inputs for discrete data lie on the
probability simplex, and are therefore natively differentiable, paving the way
for gradient-based sample guidance and few-step generation in discrete domains
such as language modelling. The loss function directly optimises data
compression and places no restrictions on the network architecture. In our
experiments BFNs achieve competitive log-likelihoods for image modelling on
dynamically binarized MNIST and CIFAR-10, and outperform all known discrete
diffusion models on the text8 character-level language modelling task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Graves_A/0/1/0/all/0/1"&gt;Alex Graves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1"&gt;Rupesh Kumar Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkinson_T/0/1/0/all/0/1"&gt;Timothy Atkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_F/0/1/0/all/0/1"&gt;Faustino Gomez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. (arXiv:2309.12032v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12032</id>
        <link href="http://arxiv.org/abs/2309.12032"/>
        <updated>2023-09-23T00:40:38.837Z</updated>
        <summary type="html"><![CDATA[Structure learning is the crux of causal inference. Notably, causal discovery
(CD) algorithms are brittle when data is scarce, possibly inferring imprecise
causal relations that contradict expert knowledge -- especially when
considering latent confounders. To aggravate the issue, most CD methods do not
provide uncertainty estimates, making it hard for users to interpret results
and improve the inference process. Surprisingly, while CD is a human-centered
affair, no works have focused on building methods that both 1) output
uncertainty estimates that can be verified by experts and 2) interact with
those experts to iteratively refine CD. To solve these issues, we start by
proposing to sample (causal) ancestral graphs proportionally to a belief
distribution based on a score function, such as the Bayesian information
criterion (BIC), using generative flow networks. Then, we leverage the
diversity in candidate graphs and introduce an optimal experimental design to
iteratively probe the expert about the relations among variables, effectively
reducing the uncertainty of our belief over ancestral graphs. Finally, we
update our samples to incorporate human feedback via importance sampling.
Importantly, our method does not require causal sufficiency (i.e., unobserved
confounders may exist). Experiments with synthetic observational data show that
our method can accurately sample from distributions over ancestral graphs and
that we can greatly improve inference quality with human aid.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1"&gt;Tiago da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_E/0/1/0/all/0/1"&gt;Eliezer Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Ad&amp;#xe8;le Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gois_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio G&amp;#xf3;is&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heider_D/0/1/0/all/0/1"&gt;Dominik Heider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mesquita_D/0/1/0/all/0/1"&gt;Diego Mesquita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations. (arXiv:2309.11741v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11741</id>
        <link href="http://arxiv.org/abs/2309.11741"/>
        <updated>2023-09-23T00:40:38.830Z</updated>
        <summary type="html"><![CDATA[The recommendation of appropriate development pathways, also known as
ecological civilization patterns for achieving Sustainable Development Goals
(namely, sustainable development patterns), are of utmost importance for
promoting ecological, economic, social, and resource sustainability in a
specific region. To achieve this, the recommendation process must carefully
consider the region's natural, environmental, resource, and economic
characteristics. However, current recommendation algorithms in the field of
computer science fall short in adequately addressing the spatial heterogeneity
related to environment and sparsity of regional historical interaction data,
which limits their effectiveness in recommending sustainable development
patterns. To overcome these challenges, this paper proposes a method called
User Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the
high-density linking capability of the pruned User Graph to address the issue
of spatial heterogeneity neglect in recommendation algorithms. Secondly, we
construct an Intent Graph by incorporating the intent network, which captures
the preferences for attributes including environmental elements of target
regions. This approach effectively alleviates the problem of sparse historical
interaction data in the region. Through extensive experiments, we demonstrate
that UGPIG outperforms state-of-the-art recommendation algorithms like KGCN,
KGAT, and KGIN in sustainable development pattern recommendations, with a
maximum improvement of 9.61% in Top-3 recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhihang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yunqiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1"&gt;Wen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiaoliang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhiqiang Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback. (arXiv:2309.11527v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11527</id>
        <link href="http://arxiv.org/abs/2309.11527"/>
        <updated>2023-09-23T00:40:38.813Z</updated>
        <summary type="html"><![CDATA[This work describes the TrueLearn Python library, which contains a family of
online learning Bayesian models for building educational (or more generally,
informational) recommendation systems. This family of models was designed
following the "open learner" concept, using humanly-intuitive user
representations. For the sake of interpretability and putting the user in
control, the TrueLearn library also contains different representations to help
end-users visualise the learner models, which may in the future facilitate user
interaction with their own models. Together with the library, we include a
previously publicly released implicit feedback educational dataset with
evaluation metrics to measure the performance of the models. The extensive
documentation and coding examples make the library highly accessible to both
machine learning developers and educational data mining and learning analytic
practitioners. The library and the support documentation with examples are
available at https://truelearn.readthedocs.io/en/latest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yuxiang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djemili_K/0/1/0/all/0/1"&gt;Karim Djemili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_D/0/1/0/all/0/1"&gt;Denis Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalman_A/0/1/0/all/0/1"&gt;Aaneel Shalman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a P&amp;#xe9;rez-Ortiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1"&gt;Sahan Bulathwela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches. (arXiv:2309.11587v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11587</id>
        <link href="http://arxiv.org/abs/2309.11587"/>
        <updated>2023-09-23T00:40:38.810Z</updated>
        <summary type="html"><![CDATA[The prevalence of ubiquitous location-aware devices and mobile Internet
enables us to collect massive individual-level trajectory dataset from users.
Such trajectory big data bring new opportunities to human mobility research but
also raise public concerns with regard to location privacy. In this work, we
present the Conditional Adversarial Trajectory Synthesis (CATS), a
deep-learning-based GeoAI methodological framework for privacy-preserving
trajectory data generation and publication. CATS applies K-anonymity to the
underlying spatiotemporal distributions of human movements, which provides a
distributional-level strong privacy guarantee. By leveraging conditional
adversarial training on K-anonymized human mobility matrices, trajectory global
context learning using the attention-based mechanism, and recurrent bipartite
graph matching of adjacent trajectory points, CATS is able to reconstruct
trajectory topology from conditionally sampled locations and generate
high-quality individual-level synthetic trajectory data, which can serve as
supplements or alternatives to raw data for privacy-preserving trajectory data
publication. The experiment results on over 90k GPS trajectories show that our
method has a better performance in privacy preservation, spatiotemporal
characteristic preservation, and downstream utility compared with baseline
methods, which brings new insights into privacy-preserving human mobility
research using generative AI techniques and explores data ethics issues in
GIScience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1"&gt;Jinmeng Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Song Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Sijia Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks. (arXiv:2309.11651v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2309.11651</id>
        <link href="http://arxiv.org/abs/2309.11651"/>
        <updated>2023-09-23T00:40:38.810Z</updated>
        <summary type="html"><![CDATA[Motivated by applications in queueing theory, we consider a stochastic
control problem whose state space is the $d$-dimensional positive orthant. The
controlled process $Z$ evolves as a reflected Brownian motion whose covariance
matrix is exogenously specified, as are its directions of reflection from the
orthant's boundary surfaces. A system manager chooses a drift vector
$\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at
time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem
formulation, the objective is to minimize expected discounted cost over an
infinite planning horizon, after which we treat the corresponding ergodic
control problem. Extending earlier work by Han et al. (Proceedings of the
National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a
simulation-based computational method that relies heavily on deep neural
network technology. For test problems studied thus far, our method is accurate
to within a fraction of one percent, and is computationally feasible in
dimensions up to at least $d=30$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ata_B/0/1/0/all/0/1"&gt;Baris Ata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harrison_J/0/1/0/all/0/1"&gt;J. Michael Harrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Si_N/0/1/0/all/0/1"&gt;Nian Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Matrix Factorization with Public Item Features. (arXiv:2309.11516v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11516</id>
        <link href="http://arxiv.org/abs/2309.11516"/>
        <updated>2023-09-23T00:40:38.804Z</updated>
        <summary type="html"><![CDATA[We consider the problem of training private recommendation models with access
to public item features. Training with Differential Privacy (DP) offers strong
privacy guarantees, at the expense of loss in recommendation quality. We show
that incorporating public item features during training can help mitigate this
loss in quality. We propose a general approach based on collective matrix
factorization (CMF), that works by simultaneously factorizing two matrices: the
user feedback matrix (representing sensitive data) and an item feature matrix
that encodes publicly available (non-sensitive) item information.

The method is conceptually simple, easy to tune, and highly scalable. It can
be applied to different types of public item data, including: (1) categorical
item features; (2) item-item similarities learned from public sources; and (3)
publicly available user feedback. Furthermore, these data modalities can be
collectively utilized to fully leverage public data.

Evaluating our method on a standard DP recommendation benchmark, we find that
using public item features significantly narrows the quality gap between
private models and their non-private counterparts. As privacy constraints
become more stringent, models rely more heavily on public side features for
recommendation. This results in a smooth transition from collaborative
filtering to item-based contextual recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1"&gt;Mihaela Curmei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krichene_W/0/1/0/all/0/1"&gt;Walid Krichene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundararajan_M/0/1/0/all/0/1"&gt;Mukund Sundararajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.10792</id>
        <link href="http://arxiv.org/abs/2308.10792"/>
        <updated>2023-09-23T00:40:38.769Z</updated>
        <summary type="html"><![CDATA[This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Linfeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoya Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiaofei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Runyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoyin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11526</id>
        <link href="http://arxiv.org/abs/2309.11526"/>
        <updated>2023-09-23T00:40:38.760Z</updated>
        <summary type="html"><![CDATA[An important task in the field of sensor technology is the efficient
implementation of adaptation procedures of measurements from one sensor to
another sensor of identical design. One idea is to use the estimation of an
affine transformation between different systems, which can be improved by the
knowledge of experts. This paper presents an improved solution from Glacier
Research that was published back in 1973. It is shown that this solution can be
adapted for software calibration of sensors, implementation of expert-based
adaptation, and federated learning methods. We evaluate our research with
simulations and also with real measured data of a multi-sensor board with 8
identical sensors. The results show an improvement for both the simulation and
the experiments with real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Machhamer_R/0/1/0/all/0/1"&gt;R&amp;#xfc;diger Machhamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazlic_L/0/1/0/all/0/1"&gt;Lejla Begic Fazlic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1"&gt;Eray Guven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junk_D/0/1/0/all/0/1"&gt;David Junk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1"&gt;Gunes Karabulut Kurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_S/0/1/0/all/0/1"&gt;Stefan Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Didas_S/0/1/0/all/0/1"&gt;Stephan Didas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gollmer_K/0/1/0/all/0/1"&gt;Klaus-Uwe Gollmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmann_R/0/1/0/all/0/1"&gt;Ralph Bergmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timm_I/0/1/0/all/0/1"&gt;Ingo J. Timm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dartmann_G/0/1/0/all/0/1"&gt;Guido Dartmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation. (arXiv:2309.11623v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11623</id>
        <link href="http://arxiv.org/abs/2309.11623"/>
        <updated>2023-09-23T00:40:38.673Z</updated>
        <summary type="html"><![CDATA[Music streaming services heavily rely on their recommendation engines to
continuously provide content to their consumers. Sequential recommendation
consequently has seen considerable attention in current literature, where state
of the art approaches focus on self-attentive models leveraging contextual
information such as long and short-term user history and item features;
however, most of these studies focus on long-form content domains (retail,
movie, etc.) rather than short-form, such as music. Additionally, many do not
explore incorporating negative session-level feedback during training. In this
study, we investigate the use of transformer-based self-attentive architectures
to learn implicit session-level information for sequential music
recommendation. We additionally propose a contrastive learning task to
incorporate negative feedback (e.g skipped tracks) to promote positive hits and
penalize negative hits. This task is formulated as a simple loss term that can
be incorporated into a variety of deep learning architectures for sequential
recommendation. Our experiments show that this results in consistent
performance gains over the baseline architectures ignoring negative user
feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1"&gt;Pavan Seshadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knees_P/0/1/0/all/0/1"&gt;Peter Knees&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey on the semantics of sequential patterns with negation. (arXiv:2309.11638v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11638</id>
        <link href="http://arxiv.org/abs/2309.11638"/>
        <updated>2023-09-23T00:40:38.598Z</updated>
        <summary type="html"><![CDATA[A sequential pattern with negation, or negative sequential pattern, takes the
form of a sequential pattern for which the negation symbol may be used in front
of some of the pattern's itemsets. Intuitively, such a pattern occurs in a
sequence if negated itemsets are absent in the sequence. Recent work has shown
that different semantics can be attributed to these pattern forms, and that
state-of-the-art algorithms do not extract the same sets of patterns. This
raises the important question of the interpretability of sequential pattern
with negation. In this study, our focus is on exploring how potential users
perceive negation in sequential patterns. Our aim is to determine whether
specific semantics are more "intuitive" than others and whether these align
with the semantics employed by one or more state-of-the-art algorithms. To
achieve this, we designed a questionnaire to reveal the semantics' intuition of
each user. This article presents both the design of the questionnaire and an
in-depth analysis of the 124 responses obtained. The outcomes indicate that two
of the semantics are predominantly intuitive; however, neither of them aligns
with the semantics of the primary state-of-the-art algorithms. As a result, we
provide recommendations to account for this disparity in the conclusions drawn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guyet_T/0/1/0/all/0/1"&gt;Thomas Guyet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian sparsification for deep neural networks with Bayesian model reduction. (arXiv:2309.12095v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.12095</id>
        <link href="http://arxiv.org/abs/2309.12095"/>
        <updated>2023-09-23T00:40:38.598Z</updated>
        <summary type="html"><![CDATA[Deep learning's immense capabilities are often constrained by the complexity
of its models, leading to an increasing demand for effective sparsification
techniques. Bayesian sparsification for deep learning emerges as a crucial
approach, facilitating the design of models that are both computationally
efficient and competitive in terms of performance across various deep learning
applications. The state-of-the-art -- in Bayesian sparsification of deep neural
networks -- combines structural shrinkage priors on model weights with an
approximate inference scheme based on black-box stochastic variational
inference. However, model inversion of the full generative model is
exceptionally computationally demanding, especially when compared to standard
deep learning of point estimates. In this context, we advocate for the use of
Bayesian model reduction (BMR) as a more efficient alternative for pruning of
model weights. As a generalization of the Savage-Dickey ratio, BMR allows a
post-hoc elimination of redundant model weights based on the posterior
estimates under a straightforward (non-hierarchical) generative model. Our
comparative study highlights the computational efficiency and the pruning rate
of the BMR method relative to the established stochastic variational inference
(SVI) scheme, when applied to the full hierarchical generative model. We
illustrate the potential of BMR to prune model parameters across various deep
learning architectures, from classical networks like LeNet to modern frameworks
such as Vision Transformers and MLP-Mixers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Markovic_D/0/1/0/all/0/1"&gt;Dimitrije Markovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Friston_K/0/1/0/all/0/1"&gt;Karl J. Friston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kiebel_S/0/1/0/all/0/1"&gt;Stefan J. Kiebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework. (arXiv:2309.11682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11682</id>
        <link href="http://arxiv.org/abs/2309.11682"/>
        <updated>2023-09-23T00:40:38.597Z</updated>
        <summary type="html"><![CDATA[While training fair machine learning models has been studied extensively in
recent years, most developed methods rely on the assumption that the training
and test data have similar distributions. In the presence of distribution
shifts, fair models may behave unfairly on test data. There have been some
developments for fair learning robust to distribution shifts to address this
shortcoming. However, most proposed solutions are based on the assumption of
having access to the causal graph describing the interaction of different
features. Moreover, existing algorithms require full access to data and cannot
be used when small batches are used (stochastic/batch implementation). This
paper proposes the first stochastic distributionally robust fairness framework
with convergence guarantees that do not require knowledge of the causal graph.
More specifically, we formulate the fair inference in the presence of the
distribution shift as a distributionally robust optimization problem under
$L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual
Information (ERMI) as the measure of fairness violation. We then discuss how
the proposed method can be implemented in a stochastic fashion. We have
evaluated the presented framework's performance and efficiency through
extensive experiments on real datasets consisting of distribution shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baharlouei_S/0/1/0/all/0/1"&gt;Sina Baharlouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1"&gt;Meisam Razaviyayn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains. (arXiv:2309.12279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12279</id>
        <link href="http://arxiv.org/abs/2309.12279"/>
        <updated>2023-09-23T00:40:38.595Z</updated>
        <summary type="html"><![CDATA[Initialization of neural network weights plays a pivotal role in determining
their performance. Feature Imitating Networks (FINs) offer a novel strategy by
initializing weights to approximate specific closed-form statistical features,
setting a promising foundation for deep learning architectures. While the
applicability of FINs has been chiefly tested in biomedical domains, this study
extends its exploration into other time series datasets. Three different
experiments are conducted in this study to test the applicability of imitating
Tsallis entropy for performance enhancement: Bitcoin price prediction, speech
emotion recognition, and chronic neck pain detection. For the Bitcoin price
prediction, models embedded with FINs reduced the root mean square error by
around 1000 compared to the baseline. In the speech emotion recognition task,
the FIN-augmented model increased classification accuracy by over 3 percent.
Lastly, in the CNP detection experiment, an improvement of about 7 percent was
observed compared to established classifiers. These findings validate the broad
utility and potency of FINs in diverse applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1"&gt;Reza Khanmohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alhanai_T/0/1/0/all/0/1"&gt;Tuka Alhanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Mohammad M. Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.06101</id>
        <link href="http://arxiv.org/abs/2306.06101"/>
        <updated>2023-09-23T00:40:38.531Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating the learning rate in adaptive methods,
such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to
provably estimate the distance to the solution $D$, which is needed to set the
learning rate optimally. Our techniques are modifications of the D-Adaptation
method for learning-rate-free learning. Our methods improve upon the
convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where
$d_0$ is the initial estimate of $D$. We test our methods on 12 common
logistic-regression benchmark datasets, VGG11 and ResNet-50 training on
CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on
Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT
transformer training on BookWiki. Our experimental results show that our
approaches consistently outperform D-Adaptation and reach test accuracy values
close to that of hand-tuned Adam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1"&gt;Konstantin Mishchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1"&gt;Aaron Defazio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs. (arXiv:2309.11726v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2309.11726</id>
        <link href="http://arxiv.org/abs/2309.11726"/>
        <updated>2023-09-23T00:40:38.525Z</updated>
        <summary type="html"><![CDATA[Programmers and researchers are increasingly developing surrogates of
programs, models of a subset of the observable behavior of a given program, to
solve a variety of software development challenges. Programmers train
surrogates from measurements of the behavior of a program on a dataset of input
examples. A key challenge of surrogate construction is determining what
training data to use to train a surrogate of a given program.

We present a methodology for sampling datasets to train neural-network-based
surrogates of programs. We first characterize the proportion of data to sample
from each region of a program's input space (corresponding to different
execution paths of the program) based on the complexity of learning a surrogate
of the corresponding execution path. We next provide a program analysis to
determine the complexity of different paths in a program. We evaluate these
results on a range of real-world programs, demonstrating that complexity-guided
sampling results in empirical improvements in accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Renda_A/0/1/0/all/0/1"&gt;Alex Renda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbin_M/0/1/0/all/0/1"&gt;Michael Carbin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.11751</id>
        <link href="http://arxiv.org/abs/2309.11751"/>
        <updated>2023-09-23T00:40:38.519Z</updated>
        <summary type="html"><![CDATA[Multimodal Large Language Models (MLLMs) that integrate text and other
modalities (especially vision) have achieved unprecedented performance in
various multimodal tasks. However, due to the unsolved adversarial robustness
problem of vision models, MLLMs can have more severe safety and security risks
by introducing the vision inputs. In this work, we study the adversarial
robustness of Google's Bard, a competitive chatbot to ChatGPT that released its
multimodal capability recently, to better understand the vulnerabilities of
commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,
the generated adversarial examples can mislead Bard to output wrong image
descriptions with a 22% success rate based solely on the transferability. We
show that the adversarial examples can also attack other MLLMs, e.g., a 26%
attack success rate against Bing Chat and a 86% attack success rate against
ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face
detection and toxicity detection of images. We design corresponding attacks to
evade these defenses, demonstrating that the current defenses of Bard are also
vulnerable. We hope this work can deepen our understanding on the robustness of
MLLMs and facilitate future research on defenses. Our code is available at
https://github.com/thu-ml/Attack-Bard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huanran Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zhengwei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study of Forward-Forward Algorithm for Self-Supervised Learning. (arXiv:2309.11955v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.11955</id>
        <link href="http://arxiv.org/abs/2309.11955"/>
        <updated>2023-09-23T00:40:38.512Z</updated>
        <summary type="html"><![CDATA[Self-supervised representation learning has seen remarkable progress in the
last few years, with some of the recent methods being able to learn useful
image representations without labels. These methods are trained using
backpropagation, the de facto standard. Recently, Geoffrey Hinton proposed the
forward-forward algorithm as an alternative training method. It utilizes two
forward passes and a separate loss function for each layer to train the network
without backpropagation.

In this study, for the first time, we study the performance of
forward-forward vs. backpropagation for self-supervised representation learning
and provide insights into the learned representation spaces. Our benchmark
employs four standard datasets, namely MNIST, F-MNIST, SVHN and CIFAR-10, and
three commonly used self-supervised representation learning techniques, namely
rotation, flip and jigsaw.

Our main finding is that while the forward-forward algorithm performs
comparably to backpropagation during (self-)supervised training, the transfer
performance is significantly lagging behind in all the studied settings. This
may be caused by a combination of factors, including having a loss function for
each layer and the way the supervised training is realized in the
forward-forward paradigm. In comparison to backpropagation, the forward-forward
algorithm focuses more on the boundaries and drops part of the information
unnecessary for making decisions which harms the representation learning goal.
Further investigation and research are necessary to stabilize the
forward-forward strategy for self-supervised learning, to work beyond the
datasets and configurations demonstrated by Geoffrey Hinton.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brenig_J/0/1/0/all/0/1"&gt;Jonas Brenig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using causal inference to avoid fallouts in data-driven parametric analysis: a case study in the architecture, engineering, and construction industry. (arXiv:2309.11509v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2309.11509</id>
        <link href="http://arxiv.org/abs/2309.11509"/>
        <updated>2023-09-23T00:40:38.507Z</updated>
        <summary type="html"><![CDATA[The decision-making process in real-world implementations has been affected
by a growing reliance on data-driven models. We investigated the synergetic
pattern between the data-driven methods, empirical domain knowledge, and
first-principles simulations. We showed the potential risk of biased results
when using data-driven models without causal analysis. Using a case study
assessing the implication of several design solutions on the energy consumption
of a building, we proved the necessity of causal analysis during the
data-driven modeling process. We concluded that: (a) Data-driven models'
accuracy assessment or domain knowledge screening may not rule out biased and
spurious results; (b) Data-driven models' feature selection should involve
careful consideration of causal relationships, especially colliders; (c) Causal
analysis results can be used as an aid to first-principles simulation design
and parameter checking to avoid cognitive biases. We proved the benefits of
causal analysis when applied to data-driven models in building engineering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruiji Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saluz_U/0/1/0/all/0/1"&gt;Ueli Saluz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiavon_S/0/1/0/all/0/1"&gt;Stefano Schiavon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geyer_P/0/1/0/all/0/1"&gt;Philipp Geyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval. (arXiv:2309.12111v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.12111</id>
        <link href="http://arxiv.org/abs/2309.12111"/>
        <updated>2023-09-23T00:40:38.490Z</updated>
        <summary type="html"><![CDATA[Many applications of cross-modal music retrieval are related to connecting
sheet music images to audio recordings. A typical and recent approach to this
is to learn, via deep neural networks, a joint embedding space that correlates
short fixed-size snippets of audio and sheet music by means of an appropriate
similarity structure. However, two challenges that arise out of this strategy
are the requirement of strongly aligned data to train the networks, and the
inherent discrepancies of musical content between audio and sheet music
snippets caused by local and global tempo differences. In this paper, we
address these two shortcomings by designing a cross-modal recurrent network
that learns joint embeddings that can summarize longer passages of
corresponding audio and sheet music. The benefits of our method are that it
only requires weakly aligned audio-sheet music pairs, as well as that the
recurrent network handles the non-linearities caused by tempo variations
between audio and sheet music. We conduct a number of experiments on synthetic
and real piano data and scores, showing that our proposed recurrent method
leads to more accurate retrieval in all possible configurations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_L/0/1/0/all/0/1"&gt;Luis Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1"&gt;Gerhard Widmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback. (arXiv:2309.11527v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11527</id>
        <link href="http://arxiv.org/abs/2309.11527"/>
        <updated>2023-09-23T00:40:38.472Z</updated>
        <summary type="html"><![CDATA[This work describes the TrueLearn Python library, which contains a family of
online learning Bayesian models for building educational (or more generally,
informational) recommendation systems. This family of models was designed
following the "open learner" concept, using humanly-intuitive user
representations. For the sake of interpretability and putting the user in
control, the TrueLearn library also contains different representations to help
end-users visualise the learner models, which may in the future facilitate user
interaction with their own models. Together with the library, we include a
previously publicly released implicit feedback educational dataset with
evaluation metrics to measure the performance of the models. The extensive
documentation and coding examples make the library highly accessible to both
machine learning developers and educational data mining and learning analytic
practitioners. The library and the support documentation with examples are
available at https://truelearn.readthedocs.io/en/latest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yuxiang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djemili_K/0/1/0/all/0/1"&gt;Karim Djemili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_D/0/1/0/all/0/1"&gt;Denis Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalman_A/0/1/0/all/0/1"&gt;Aaneel Shalman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a P&amp;#xe9;rez-Ortiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1"&gt;Sahan Bulathwela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization. (arXiv:2309.11856v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.11856</id>
        <link href="http://arxiv.org/abs/2309.11856"/>
        <updated>2023-09-23T00:40:38.129Z</updated>
        <summary type="html"><![CDATA[Efficient training of large-scale graph neural networks (GNNs) has been
studied with a specific focus on reducing their memory consumption. Work by Liu
et al. (2022) proposed extreme activation compression (EXACT) which
demonstrated drastic reduction in memory consumption by performing quantization
of the intermediate activation maps down to using INT2 precision. They showed
little to no reduction in performance while achieving large reductions in GPU
memory consumption. In this work, we present an improvement to the EXACT
strategy by using block-wise quantization of the intermediate activation maps.
We experimentally analyze different block sizes and show further reduction in
memory consumption (>15%), and runtime speedup per epoch (about 5%) even when
performing extreme extents of quantization with similar performance trade-offs
as with the original EXACT. Further, we present a correction to the assumptions
on the distribution of intermediate activation maps in EXACT (assumed to be
uniform) and show improved variance estimations of the quantization and
dequantization steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Eliassen_S/0/1/0/all/0/1"&gt;Sebastian Eliassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Selvan_R/0/1/0/all/0/1"&gt;Raghavendra Selvan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orbital AI-based Autonomous Refuelling Solution. (arXiv:2309.11648v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.11648</id>
        <link href="http://arxiv.org/abs/2309.11648"/>
        <updated>2023-09-23T00:40:38.119Z</updated>
        <summary type="html"><![CDATA[Cameras are rapidly becoming the choice for on-board sensors towards space
rendezvous due to their small form factor and inexpensive power, mass, and
volume costs. When it comes to docking, however, they typically serve a
secondary role, whereas the main work is done by active sensors such as lidar.
This paper documents the development of a proposed AI-based (artificial
intelligence) navigation algorithm intending to mature the use of on-board
visible wavelength cameras as a main sensor for docking and on-orbit servicing
(OOS), reducing the dependency on lidar and greatly reducing costs.
Specifically, the use of AI enables the expansion of the relative navigation
solution towards multiple classes of scenarios, e.g., in terms of targets or
illumination conditions, which would otherwise have to be crafted on a
case-by-case manner using classical image processing methods. Multiple
convolutional neural network (CNN) backbone architectures are benchmarked on
synthetically generated data of docking manoeuvres with the International Space
Station (ISS), achieving position and attitude estimates close to 1%
range-normalised and 1 deg, respectively. The integration of the solution with
a physical prototype of the refuelling mechanism is validated in laboratory
using a robotic arm to simulate a berthing procedure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1"&gt;Duarte Rondao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1"&gt;Nabil Aouf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Popularity Degradation Bias in Local Music Recommendation. (arXiv:2309.11671v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11671</id>
        <link href="http://arxiv.org/abs/2309.11671"/>
        <updated>2023-09-23T00:40:38.112Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the effect of popularity degradation bias in the
context of local music recommendations. Specifically, we examine how accurate
two top-performing recommendation algorithms, Weight Relevance Matrix
Factorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at
recommending artists as a function of artist popularity. We find that both
algorithms improve recommendation performance for more popular artists and, as
such, exhibit popularity degradation bias. While both algorithms produce a
similar level of performance for more popular artists, Mult-VAE shows better
relative performance for less popular artists. This suggests that this
algorithm should be preferred for local (long-tail) music artist
recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trainor_A/0/1/0/all/0/1"&gt;April Trainor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turnbull_D/0/1/0/all/0/1"&gt;Douglas Turnbull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multidimensional well-being of US households at a fine spatial scale using fused household surveys: fusionACS. (arXiv:2309.11512v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2309.11512</id>
        <link href="http://arxiv.org/abs/2309.11512"/>
        <updated>2023-09-23T00:40:38.090Z</updated>
        <summary type="html"><![CDATA[Social science often relies on surveys of households and individuals. Dozens
of such surveys are regularly administered by the U.S. government. However,
they field independent, unconnected samples with specialized questions,
limiting research questions to those that can be answered by a single survey.
The fusionACS project seeks to integrate data from multiple U.S. household
surveys by statistically "fusing" variables from "donor" surveys onto American
Community Survey (ACS) microdata. This results in an integrated microdataset of
household attributes and well-being dimensions that can be analyzed to address
research questions in ways that are not currently possible. The presented data
comprise the fusion onto the ACS of select donor variables from the Residential
Energy Consumption Survey (RECS) of 2015, the National Household Transportation
Survey (NHTS) of 2017, the American Housing Survey (AHS) of 2019, and the
Consumer Expenditure Survey - Interview (CEI) for the years 2015-2019. The
underlying statistical techniques are included in an open-source $R$ package,
fusionModel, that provides generic tools for the creation, analysis, and
validation of fused microdata.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ummel_K/0/1/0/all/0/1"&gt;Kevin Ummel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Poblete_Cazenave_M/0/1/0/all/0/1"&gt;Miguel Poblete-Cazenave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akkiraju_K/0/1/0/all/0/1"&gt;Karthik Akkiraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Graetz_N/0/1/0/all/0/1"&gt;Nick Graetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ashman_H/0/1/0/all/0/1"&gt;Hero Ashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kingdon_C/0/1/0/all/0/1"&gt;Cora Kingdon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tenorio_S/0/1/0/all/0/1"&gt;Steven Herrera Tenorio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Singhal_A/0/1/0/all/0/1"&gt;Aaryaman &amp;quot;Sunny&amp;quot; Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cohen_D/0/1/0/all/0/1"&gt;Daniel Aldana Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rao_N/0/1/0/all/0/1"&gt;Narasimha D. Rao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework. (arXiv:2309.11682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11682</id>
        <link href="http://arxiv.org/abs/2309.11682"/>
        <updated>2023-09-23T00:40:38.085Z</updated>
        <summary type="html"><![CDATA[While training fair machine learning models has been studied extensively in
recent years, most developed methods rely on the assumption that the training
and test data have similar distributions. In the presence of distribution
shifts, fair models may behave unfairly on test data. There have been some
developments for fair learning robust to distribution shifts to address this
shortcoming. However, most proposed solutions are based on the assumption of
having access to the causal graph describing the interaction of different
features. Moreover, existing algorithms require full access to data and cannot
be used when small batches are used (stochastic/batch implementation). This
paper proposes the first stochastic distributionally robust fairness framework
with convergence guarantees that do not require knowledge of the causal graph.
More specifically, we formulate the fair inference in the presence of the
distribution shift as a distributionally robust optimization problem under
$L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual
Information (ERMI) as the measure of fairness violation. We then discuss how
the proposed method can be implemented in a stochastic fashion. We have
evaluated the presented framework's performance and efficiency through
extensive experiments on real datasets consisting of distribution shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baharlouei_S/0/1/0/all/0/1"&gt;Sina Baharlouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1"&gt;Meisam Razaviyayn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ad-load Balancing via Off-policy Learning in a Content Marketplace. (arXiv:2309.11518v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.11518</id>
        <link href="http://arxiv.org/abs/2309.11518"/>
        <updated>2023-09-23T00:40:38.080Z</updated>
        <summary type="html"><![CDATA[Ad-load balancing is a critical challenge in online advertising systems,
particularly in the context of social media platforms, where the goal is to
maximize user engagement and revenue while maintaining a satisfactory user
experience. This requires the optimization of conflicting objectives, such as
user satisfaction and ads revenue. Traditional approaches to ad-load balancing
rely on static allocation policies, which fail to adapt to changing user
preferences and contextual factors. In this paper, we present an approach that
leverages off-policy learning and evaluation from logged bandit feedback. We
start by presenting a motivating analysis of the ad-load balancing problem,
highlighting the conflicting objectives between user satisfaction and ads
revenue. We emphasize the nuances that arise due to user heterogeneity and the
dependence on the user's position within a session. Based on this analysis, we
define the problem as determining the optimal ad-load for a particular feed
fetch. To tackle this problem, we propose an off-policy learning framework that
leverages unbiased estimators such as Inverse Propensity Scoring (IPS) and
Doubly Robust (DR) to learn and estimate the policy values using offline
collected stochastic data. We present insights from online A/B experiments
deployed at scale across over 80 million users generating over 200 million
sessions, where we find statistically significant improvements in both user
satisfaction metrics and ads revenue for the platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagtani_H/0/1/0/all/0/1"&gt;Hitesh Sagtani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhawar_M/0/1/0/all/0/1"&gt;Madan Jhawar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrotra_R/0/1/0/all/0/1"&gt;Rishabh Mehrotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeunen_O/0/1/0/all/0/1"&gt;Olivier Jeunen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2309.11515</id>
        <link href="http://arxiv.org/abs/2309.11515"/>
        <updated>2023-09-23T00:40:38.061Z</updated>
        <summary type="html"><![CDATA[With increasing frequency of high-profile privacy breaches in various online
platforms, users are becoming more concerned about their privacy. And
recommender system is the core component of online platforms for providing
personalized service, consequently, its privacy preservation has attracted
great attention. As the gold standard of privacy protection, differential
privacy has been widely adopted to preserve privacy in recommender systems.
However, existing differentially private recommender systems only consider
static and independent interactions, so they cannot apply to sequential
recommendation where behaviors are dynamic and dependent. Meanwhile, little
attention has been paid on the privacy risk of sensitive user features, most of
them only protect user feedbacks. In this work, we propose a novel
DIfferentially Private Sequential recommendation framework with a noisy Graph
Neural Network approach (denoted as DIPSGNN) to address these limitations. To
the best of our knowledge, we are the first to achieve differential privacy in
sequential recommendation with dependent interactions. Specifically, in
DIPSGNN, we first leverage piecewise mechanism to protect sensitive user
features. Then, we innovatively add calibrated noise into aggregation step of
graph neural network based on aggregation perturbation mechanism. And this
noisy graph neural network can protect sequentially dependent interactions and
capture user preferences simultaneously. Extensive experiments demonstrate the
superiority of our method over state-of-the-art differentially private
recommender systems in terms of better balance between privacy and accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wentao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1"&gt;Hui Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Diffusion Models for Structural Component Design. (arXiv:2309.11601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.11601</id>
        <link href="http://arxiv.org/abs/2309.11601"/>
        <updated>2023-09-23T00:40:38.042Z</updated>
        <summary type="html"><![CDATA[Recent advances in generative modeling, namely Diffusion models, have
revolutionized generative modeling, enabling high-quality image generation
tailored to user needs. This paper proposes a framework for the generative
design of structural components. Specifically, we employ a Latent Diffusion
model to generate potential designs of a component that can satisfy a set of
problem-specific loading conditions. One of the distinct advantages our
approach offers over other generative approaches, such as generative
adversarial networks (GANs), is that it permits the editing of existing
designs. We train our model using a dataset of geometries obtained from
structural topology optimization utilizing the SIMP algorithm. Consequently,
our framework generates inherently near-optimal designs. Our work presents
quantitative results that support the structural performance of the generated
designs and the variability in potential candidate designs. Furthermore, we
provide evidence of the scalability of our framework by operating over voxel
domains with resolutions varying from $32^3$ to $128^3$. Our framework can be
used as a starting point for generating novel near-optimal designs similar to
topology-optimized designs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herron_E/0/1/0/all/0/1"&gt;Ethan Herron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rade_J/0/1/0/all/0/1"&gt;Jaydeep Rade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jignasu_A/0/1/0/all/0/1"&gt;Anushrut Jignasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganapathysubramanian_B/0/1/0/all/0/1"&gt;Baskar Ganapathysubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1"&gt;Aditya Balu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Soumik Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1"&gt;Adarsh Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasi-Monte Carlo for 3D Sliced Wasserstein. (arXiv:2309.11713v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.11713</id>
        <link href="http://arxiv.org/abs/2309.11713"/>
        <updated>2023-09-23T00:40:38.009Z</updated>
        <summary type="html"><![CDATA[Monte Carlo (MC) approximation has been used as the standard computation
approach for the Sliced Wasserstein (SW) distance, which has an intractable
expectation in its analytical form. However, the MC method is not optimal in
terms of minimizing the absolute approximation error. To provide a better class
of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that
rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of
QMC for SW, we focus on the 3D setting, specifically computing the SW between
probability measures in three dimensions. In greater detail, we empirically
verify various ways of constructing QMC points sets on the 3D unit-hypersphere,
including Gaussian-based mapping, equal area mapping, generalized spiral
points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased
estimation for stochastic optimization, we extend QSW into Randomized
Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed
low-discrepancy sequences. For theoretical properties, we prove the asymptotic
convergence of QSW and the unbiasedness of RQSW. Finally, we conduct
experiments on various 3D tasks, such as point-cloud comparison, point-cloud
interpolation, image style transfer, and training deep point-cloud
autoencoders, to demonstrate the favorable performance of the proposed QSW and
RQSW variants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bariletto_N/0/1/0/all/0/1"&gt;Nicola Bariletto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLM Regression with Oblivious Corruptions. (arXiv:2309.11657v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2309.11657</id>
        <link href="http://arxiv.org/abs/2309.11657"/>
        <updated>2023-09-23T00:40:37.913Z</updated>
        <summary type="html"><![CDATA[We demonstrate the first algorithms for the problem of regression for
generalized linear models (GLMs) in the presence of additive oblivious noise.
We assume we have sample access to examples $(x, y)$ where $y$ is a noisy
measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of
the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious
noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$,
and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover
a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has}
arbitrarily small error when compared to the true values $g(w^* \cdot x)$,
rather than the noisy measurements $y$.

We present an algorithm that tackles \new{this} problem in its most general
distribution-independent setting, where the solution may not \new{even} be
identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the
solution if it is identifiable, and otherwise returns a small list of
candidates, one of which is close to the true solution. Furthermore, we
\new{provide} a necessary and sufficient condition for identifiability, which
holds in broad settings. \new{Specifically,} the problem is identifiable when
the quantile at which $\xi + \epsilon = 0$ is known, or when the family of
hypotheses does not contain candidates that are nearly equal to a translated
$g(w^* \cdot x) + A$ for some real number $A$, while also having large error
when compared to $g(w^* \cdot x)$.

This is the first \new{algorithmic} result for GLM regression \new{with
oblivious noise} which can handle more than half the samples being arbitrarily
corrupted. Prior work focused largely on the setting of linear regression, and
gave algorithms under restrictive assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1"&gt;Sushrut Karmalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jongho Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1"&gt;Christos Tzamos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.10688</id>
        <link href="http://arxiv.org/abs/2309.10688"/>
        <updated>2023-09-23T00:40:37.906Z</updated>
        <summary type="html"><![CDATA[Modern deep networks are trained with stochastic gradient descent (SGD) whose
key parameters are the number of data considered at each step or batch size
$B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$,
SGD corresponds to a stochastic evolution of the parameters, whose noise
amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this
description is observed to break down for sufficiently large batches $B\geq
B^*$, or simplifies to gradient descent (GD) when the temperature is
sufficiently small. Understanding where these cross-overs take place remains a
central challenge. Here we resolve these questions for a teacher-student
perceptron classification model, and show empirically that our key predictions
still apply to deep networks. Specifically, we obtain a phase diagram in the
$B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a
noise-dominated SGD governed by temperature, $\textit{(ii)}$ a
large-first-step-dominated SGD and $\textit{(iii)}$ GD. These different phases
also corresponds to different regimes of generalization error. Remarkably, our
analysis reveals that the batch size $B^*$ separating regimes $\textit{(i)}$
and $\textit{(ii)}$ scale with the size $P$ of the training set, with an
exponent that characterizes the hardness of the classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sclocchi_A/0/1/0/all/0/1"&gt;Antonio Sclocchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regionally Additive Models: Explainable-by-design models minimizing feature interactions. (arXiv:2309.12215v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12215</id>
        <link href="http://arxiv.org/abs/2309.12215"/>
        <updated>2023-09-23T00:40:37.898Z</updated>
        <summary type="html"><![CDATA[Generalized Additive Models (GAMs) are widely used explainable-by-design
models in various applications. GAMs assume that the output can be represented
as a sum of univariate functions, referred to as components. However, this
assumption fails in ML problems where the output depends on multiple features
simultaneously. In these cases, GAMs fail to capture the interaction terms of
the underlying function, leading to subpar accuracy. To (partially) address
this issue, we propose Regionally Additive Models (RAMs), a novel class of
explainable-by-design models. RAMs identify subregions within the feature space
where interactions are minimized. Within these regions, it is more accurate to
express the output as a sum of univariate functions (components). Consequently,
RAMs fit one component per subregion of each feature instead of one component
per feature. This approach yields a more expressive model compared to GAMs
while retaining interpretability. The RAM framework consists of three steps.
Firstly, we train a black-box model. Secondly, using Regional Effect Plots, we
identify subregions where the black-box model exhibits near-local additivity.
Lastly, we fit a GAM component for each identified subregion. We validate the
effectiveness of RAMs through experiments on both synthetic and real-world
datasets. The results confirm that RAMs offer improved expressiveness compared
to GAMs while maintaining interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gkolemis_V/0/1/0/all/0/1"&gt;Vasilis Gkolemis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tzerefos_A/0/1/0/all/0/1"&gt;Anargiros Tzerefos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalamagas_T/0/1/0/all/0/1"&gt;Theodore Dalamagas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1"&gt;Eirini Ntoutsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diou_C/0/1/0/all/0/1"&gt;Christos Diou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.03303</id>
        <link href="http://arxiv.org/abs/2306.03303"/>
        <updated>2023-09-23T00:40:37.888Z</updated>
        <summary type="html"><![CDATA[We introduce so-called functional input neural networks defined on a possibly
infinite dimensional weighted space with values also in a possibly infinite
dimensional output space. To this end, we use an additive family as hidden
layer maps and a non-linear activation function applied to each hidden layer.
Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global
universal approximation result for generalizations of continuous functions
going beyond the usual approximation on compact sets. This then applies in
particular to approximation of (non-anticipative) path space functionals via
functional input neural networks. As a further application of the weighted
Stone-Weierstrass theorem we prove a global universal approximation result for
linear functions of the signature. We also introduce the viewpoint of Gaussian
process regression in this setting and show that the reproducing kernel Hilbert
space of the signature kernels are Cameron-Martin spaces of certain Gaussian
processes. This paves the way towards uncertainty quantification for signature
kernel regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cuchiero_C/0/1/0/all/0/1"&gt;Christa Cuchiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmocker_P/0/1/0/all/0/1"&gt;Philipp Schmocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teichmann_J/0/1/0/all/0/1"&gt;Josef Teichmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains. (arXiv:2309.12279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.12279</id>
        <link href="http://arxiv.org/abs/2309.12279"/>
        <updated>2023-09-23T00:40:37.883Z</updated>
        <summary type="html"><![CDATA[Initialization of neural network weights plays a pivotal role in determining
their performance. Feature Imitating Networks (FINs) offer a novel strategy by
initializing weights to approximate specific closed-form statistical features,
setting a promising foundation for deep learning architectures. While the
applicability of FINs has been chiefly tested in biomedical domains, this study
extends its exploration into other time series datasets. Three different
experiments are conducted in this study to test the applicability of imitating
Tsallis entropy for performance enhancement: Bitcoin price prediction, speech
emotion recognition, and chronic neck pain detection. For the Bitcoin price
prediction, models embedded with FINs reduced the root mean square error by
around 1000 compared to the baseline. In the speech emotion recognition task,
the FIN-augmented model increased classification accuracy by over 3 percent.
Lastly, in the CNP detection experiment, an improvement of about 7 percent was
observed compared to established classifiers. These findings validate the broad
utility and potency of FINs in diverse applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1"&gt;Reza Khanmohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alhanai_T/0/1/0/all/0/1"&gt;Tuka Alhanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Mohammad M. Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This is an actual barcode created by AI as a piece of art. Scan it for its secret message if you don't believe me..]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pokpg/this_is_an_actual_barcode_created_by_ai_as_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pokpg/this_is_an_actual_barcode_created_by_ai_as_a/"/>
        <updated>2023-09-22T23:15:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/glenniszen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue with voice.ai â€” no sound comes out after feeding a prerecorded audio clip through the voice filter!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pnkc8/issue_with_voiceai_no_sound_comes_out_after/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pnkc8/issue_with_voiceai_no_sound_comes_out_after/"/>
        <updated>2023-09-22T22:31:39.000Z</updated>
        <summary type="html"><![CDATA[So I created a voice profile based on recordings of my own voice over on voice.ai.
 The audio was crisp, loud and clear.
 I only added about 16 minutes if it though. (A minimum of 15 minutes is recommended)
 When I take a prerecorded voice clip and feed it through the voice profile I created, itâ€™s just silent for all 15 seconds that it grants you before you upgrade.
 Then it has the little 2 second audio watermark at the end, where it says â€œvoice AI,â€ and that I can hear perfectly fine!
 Iâ€™ve searched all over for others who might have had this problem, but it seems like the main problem people have reported difficulties with audio are/were tend to center around the live mode failing to function properly.
 Has anyone had this issue before?
 Should I add more audio to the voice profile to make it more well rounded perhaps?
 If so, how much audio do you typically need to create a solid custom voice profile?
 Should I just update to the paid version? I donâ€™t really think that would fix it, and I wanted to wait to upgrade until I had some proof that it worked, yanno?
 Iâ€™m not sure what to doâ€¦
 Any ideas?
    submitted by    /u/WhenTheFoxGRINS  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Splash Pro drops generative music model and comparison to other models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pmmye/n_splash_pro_drops_generative_music_model_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pmmye/n_splash_pro_drops_generative_music_model_and/"/>
        <updated>2023-09-22T21:54:16.000Z</updated>
        <summary type="html"><![CDATA[Seems like a strong contender in this space, plus comercial use: https://www.splashmusic.com/music-generation
    submitted by    /u/No-Reference8836  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving your LLMs with RLHF on Amazon SageMaker]]></title>
        <id>817ee897c4bac0dc53a1189d5bf7fd7b9e14b6ae</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/improving-your-llms-with-rlhf-on-amazon-sagemaker/"/>
        <updated>2023-09-22T20:57:24.000Z</updated>
        <summary type="html"><![CDATA[In this blog post, we illustrate how RLHF can be performed on Amazon SageMaker by conducting an experiment with the popular, open-sourced RLHF repo Trlx. Through our experiment, we demonstrate how RLHF can be used to increase the helpfulness or harmlessness of a large language model using the publicly available Helpfulness and Harmlessness (HH) dataset provided by Anthropic. Using this dataset, we conduct our experiment with Amazon SageMaker Studio notebook that is running on an ml.p4d.24xlarge instance. Finally, we provide a Jupyter notebook to replicate our experiments.]]></summary>
        <author>
            <name>Weifeng Chen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for the best AI Story Generator? Meet NovelGPT.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pil5p/looking_for_the_best_ai_story_generator_meet/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pil5p/looking_for_the_best_ai_story_generator_meet/"/>
        <updated>2023-09-22T19:08:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16piksd/r_btlm3b8k_7b_parameter_performance_in_a_3b/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16piksd/r_btlm3b8k_7b_parameter_performance_in_a_3b/"/>
        <updated>2023-09-22T19:08:26.000Z</updated>
        <summary type="html"><![CDATA[Hello Reddit!
 I'm Daria from the Cerebras ML team, and I've got some thrilling updates to share with you! ðŸš€ We've recently published our BTLM-3B-8K paper on arXiv, and Iâ€™m excited to share that I am one of the authors! Check it out here: https://arxiv.org/abs/2309.11568
 It distills our recipe for training SOTA LLMs:
  
An extensively deduplicated dataset: SlimPajama 
 Hyperparameter search using muP 
 Variable sequence length training + ALiBi 
 Aggressive LR decay 
  
Our BTLM-3B-8K not only sets a new standard for 3B parameter models but also frequently surpasses the performance of 7B models! In the paper, we meticulously outline how we developed the BTLM model and conducted a comprehensive analysis of its performance on 22 validation benchmarks. These benchmarks span a range of capabilities including common sense reasoning, world knowledge, reading comprehension, code generation, long sequence interpolation/extrapolation, bias, toxicity, and misinformation.
 For those eager to dive in, we've made our SlimPajama dataset and the BTLM-3B-8K model available on Hugging Face: https://huggingface.co/cerebras ðŸŽ‰
 Feel free to delve into the details, explore the dataset and model, and let us know your thoughts, insights, or questions! We're here to discuss and excited to hear your feedback. Happy exploring! ðŸš€
    submitted by    /u/daria-sobol  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doing graph + tabular analytics directly on modern data lakes]]></title>
        <id>https://www.datasciencecentral.com/?p=63223</id>
        <link href="https://www.datasciencecentral.com/doing-graph-tabular-analytics-directly-on-modern-data-lakes/"/>
        <updated>2023-09-22T18:35:56.000Z</updated>
        <summary type="html"><![CDATA[A podcast with Weimo Liu and Sam Magnus of PuppyGraph Open source Apache Iceberg, Hudi and Delta Lake have made it possible to dispense with the complexities and duplication of data warehousing. Instead of requiring time-consuming extract, transform and load (ETL) procedures, these large table formats make it simple to tap S3 and other repositoriesâ€¦Â Read More Â»Doing graph + tabular analytics directly on modern data lakes
The post Doing graph + tabular analytics directly on modern data lakes appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Startup Buzz Is Facing a Reality Check]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pgyia/ai_startup_buzz_is_facing_a_reality_check/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pgyia/ai_startup_buzz_is_facing_a_reality_check/"/>
        <updated>2023-09-22T18:00:52.000Z</updated>
        <summary type="html"><![CDATA[Founders and venture capitalists who flocked to artificial-intelligence startups are learning that turning the chatbot buzz into successful businesses is harder than it seems.
 Source https://www.wsj.com/tech/ai/ai-startup-buzz-is-facing-a-reality-check-e34babfe
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Primes, weeds, and military precision]]></title>
        <id>https://www.johndcook.com/blog/?p=209623</id>
        <link href="https://www.johndcook.com/blog/2023/09/22/prime-weeds/"/>
        <updated>2023-09-22T17:52:35.000Z</updated>
        <summary type="html"><![CDATA[Hereâ€™s a quote from Don Zagier that I found in Larry Rolenâ€™s lecture notes on modular forms. There are two facts about the distribution of prime numbers of which I hope to convince you so overwhelmingly that they will be permanently engraved in your hearts. The first is that, despite their simple definition and role [â€¦]
Primes, weeds, and military precision first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]: Is There Any Followup To Effect Of Model Size on LoRA Rank "r"?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pfk69/d_is_there_any_followup_to_effect_of_model_size/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pfk69/d_is_there_any_followup_to_effect_of_model_size/"/>
        <updated>2023-09-22T17:03:07.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 I am re-reading the LoRA paper (https://arxiv.org/abs/2106.09685) to get a deeper understanding of some of the analysis the authors perform at the end and saw this line
  
Note that the relationship between model size and the optimal rank for adaptation is still an open question. 
  
Does anybody know of any resources out there that looked into this question, given that LoRA has been around for a little bit now? Perhaps someone has performed similar subspace overlap / optimal "r" value studies on some of the LLMs that fall in-between GPT2 and 3, i.e. some of the ~7B, ~15B, ~40B and ~70B models? 
    submitted by    /u/lightSpeedBrick  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI â€” weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pfixu/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pfixu/ai_weekly_megathread/"/>
        <updated>2023-09-22T17:01:47.000Z</updated>
        <summary type="html"><![CDATA[News provided by aibrews.com
  
Genmo releases a new text-to-video model: Genmo Replay v0.1, which generate high-quality videos from text without the need for advanced prompt engineering. Genmo is available for free to create AI videos [Details | Genmo Replay] .
 OpenAI unveils DALLÂ·E 3 - a major update to the text-to-image model, which will be integrated in ChatGPT. It will be available to ChatGPT Plus and Enterprise users in October, via the API and in Labs later this fall. Creators can now also opt their images out from future training [Details].
 Toyota Research Institute has developed a technique, powered by generative AI, that enables teaching robots new manipulation abilities in a single afternoon. Using the same robot, same code, and same setup, TRI taught over 60 different dexteroâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driving where no Autonomous Vehicle has driven before!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16peyzs/driving_where_no_autonomous_vehicle_has_driven/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16peyzs/driving_where_no_autonomous_vehicle_has_driven/"/>
        <updated>2023-09-22T16:38:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/shani_786  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] When do we use the instruct version of a LLM?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pegyz/d_when_do_we_use_the_instruct_version_of_a_llm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pegyz/d_when_do_we_use_the_instruct_version_of_a_llm/"/>
        <updated>2023-09-22T16:17:46.000Z</updated>
        <summary type="html"><![CDATA[If censorship isnâ€™t an issue for me, when thereâ€™s an instruct version of an LLM, is it generally always better to use the instruct version than the non-instruct version (because instruct versions tend to hallucinate less)?
 Apart from censorship and hallucinations, are there any other pros and cons between intrust vs. non-instruct version?
    submitted by    /u/--leockl--  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How to reduce hallucinations using Chain Of Verification in Large Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pedlu/r_how_to_reduce_hallucinations_using_chain_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pedlu/r_how_to_reduce_hallucinations_using_chain_of/"/>
        <updated>2023-09-22T16:13:52.000Z</updated>
        <summary type="html"><![CDATA[This new paper from Shehzaad Dhuliawala et al. (2023) introduces a combination of prompting and consistency checks made by the LLM itself.
 Implementing this technique actually made me like gpt-3.5 again !
 I wrote a tutorial on how to actually implement this method : https://advanced-stack.com/resources/how-to-reduce-hallucinations-using-chain-of-verification-cov-in-large-language-models.html
 Let me know if you find it useful
 â€‹
    submitted by    /u/Fluid-Age-9266  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Looking for suggestions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pe28v/d_looking_for_suggestions/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pe28v/d_looking_for_suggestions/"/>
        <updated>2023-09-22T16:01:03.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, a final year CS student here. I want to create a portfolio to showcase my skills in ML and DL. I have knowledge in docker and have access to google cloud platform to deploy. Now, I am unable to find any project that stands out. Could u suggest something that I could learn from as well as looks pretty on my cv?
    submitted by    /u/Virtual_Heron_7417  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers: I can't fathom the concept of dynamic weights in attention heads [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pdhb1/transformers_i_cant_fathom_the_concept_of_dynamic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pdhb1/transformers_i_cant_fathom_the_concept_of_dynamic/"/>
        <updated>2023-09-22T15:37:14.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, 
 I've been diving deep into the world of neural networks, and recently, I've been particularly intrigued by the dynamicity of attention head parameters (weights). These weights play a crucial role in transformers, and understanding how they change during training and inference can provide valuable insights into model behavior. 
 The question is, what does dynamic mean in this context? Is it input-adaptive? Do weights change at inference time according to inputs? 
 I have a hard time understanding this concept, for me, weights are static and pre-established at training time.
    submitted by    /u/assalas23  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Transformers: I can't fathom the concept of dynamic weights in attention heads]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pdcpv/d_transformers_i_cant_fathom_the_concept_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pdcpv/d_transformers_i_cant_fathom_the_concept_of/"/>
        <updated>2023-09-22T15:31:58.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, 
 I've been diving deep into the world of neural networks, and recently, I've been particularly intrigued by the dynamicity of attention head parameters (weights). These weights play a crucial role in transformers, and understanding how they change during training and inference can provide valuable insights into model behavior. 
 The question is, what does dynamic mean in this context? Is it input-adaptive? Do weights change at inference time according to inputs? 
 I have a hard time understanding this concept, for me, weights are static and pre-established at training time.
    submitted by    /u/assalas23  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help Shape the Future of A.I.! Take a Quick 2-minute Survey for Academic Research [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pcji8/help_shape_the_future_of_ai_take_a_quick_2minute/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pcji8/help_shape_the_future_of_ai_take_a_quick_2minute/"/>
        <updated>2023-09-22T14:59:04.000Z</updated>
        <summary type="html"><![CDATA[Hello guys!
 â€‹
 Are you intrigued by the world of Artificial Intelligence? I am conducting a brief and insightful survey on AI for academic research purposes. It'll take just 2 minutes of your time, and your valuable insights will contribute to our understanding of AI trends and perspectives. Whether you're a tech enthusiast, a data wizard, or just curious about the future of AI, your input is incredibly valuable!
 â€‹
 Click the link below to share your thoughts and help to understand this new technology:
 â€‹
 https://forms.gle/7fbbkc1f2iBPXHJV7
 â€‹
 Thank you in advance for being a part of this exciting AI study! #AIResearch #AI #Artificial Intelligence #SurveyTime
    submitted by    /u/JukeboxNV  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] LongLoRA: New method extends LLAMA2 7B to 100k context length, 70B to 32k context length on on a single 8 Ã— A100 machine]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pbmnr/r_longlora_new_method_extends_llama2_7b_to_100k/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pbmnr/r_longlora_new_method_extends_llama2_7b_to_100k/"/>
        <updated>2023-09-22T14:22:16.000Z</updated>
        <summary type="html"><![CDATA[As AI models get bigger, training them requires more and more computing power. Researchers are looking for ways to train these large AI models without needing Google-scale resources.
 A new paper proposes LongLoRA, a fine-tuning approach that can extend LLaMA2 7B to 100k context length and 70B model to 32k context length on a single 8Ã— A100 machine.
 Here are my highlights from the paper:
 Big one of course: LongLoRA efficiently fine-tunes large AI models on longer texts
 Key points:
  
Approximates standard attention via "shift short attention" during training
 Tuning only a subset of weights (LoRA) plus some embeddings & norms
 Fine-tuned 7B parameter model on 100k tokens with 1 machine
 Way lower training cost than full fine-tuning for large contexts
 Close to full fine-tuning performance
  
The core insight is that an approximation of full attention enables efficient training while retaining standard attention for final inference. Combined with selective weight tuning, this really reduces compute needs.
 I think this demonstrates the potential to train more capable AI without unreasonable resources. Efficient training techniques = more powerful LLMs for the same resources.
 Full summary here.
 Arxiv paper: https://arxiv.org/pdf/2309.12307.pdf
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LongLoRA: New method extends LLAMA2 7B to 100k context length, 70B to 32k context length on on a single 8 Ã— A100 machine]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pblfx/longlora_new_method_extends_llama2_7b_to_100k/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pblfx/longlora_new_method_extends_llama2_7b_to_100k/"/>
        <updated>2023-09-22T14:20:58.000Z</updated>
        <summary type="html"><![CDATA[As AI models get bigger, training them requires more and more computing power. Researchers are looking for ways to train these large AI models without needing Google-scale resources.
 A new paper proposes LongLoRA, a fine-tuning approach that can extend LLaMA2 7B to 100k context length and 70B model to 32k context length on a single 8Ã— A100 machine.
 Here are my highlights from the paper:
 Big one of course: LongLoRA efficiently fine-tunes large AI models on longer texts
 Key points:
  
Approximates standard attention via "shift short attention" during training
 Tuning only a subset of weights (LoRA) plus some embeddings & norms
 Fine-tuned 7B parameter model on 100k tokens with 1 machine
 Way lower training cost than full fine-tuning for large contexts
 Close to full fine-tuning performance
  
The core insight is that an approximation of full attention enables efficient training while retaining standard attention for final inference. Combined with selective weight tuning, this really reduces compute needs.
 I think this demonstrates the potential to train more capable AI without unreasonable resources. Efficient training techniques = more powerful LLMs for the same resources.
 Full summary here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information technology industry is at 35% of AI adoption in the US]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16pb8ap/information_technology_industry_is_at_35_of_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16pb8ap/information_technology_industry_is_at_35_of_ai/"/>
        <updated>2023-09-22T14:05:39.000Z</updated>
        <summary type="html"><![CDATA[It's currently on the 4th place after marketing, consulting, and accounting. 
 And is mostly used in software testing field to:
  
achieve more accurate results
 have larger test coverage
 receive low learning curve
 get faster QA completion
  
I was actually quite surprised when I read, I thought that information technology would at least be in the TOP-3 industries.
 Do you think it can happen by the end of this year? 
    submitted by    /u/unbalanced_mind  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine learning jobs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pb5yv/d_machine_learning_jobs/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pb5yv/d_machine_learning_jobs/"/>
        <updated>2023-09-22T14:03:04.000Z</updated>
        <summary type="html"><![CDATA[Hello, looking for job opportunities as a data scientist or machine learning engineer.
 Any help would be appreciated.
 Thanks!!
    submitted by    /u/ArachnidFun2671  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] Modelling tabular data with diffusion models (Blog post)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16pa13n/research_modelling_tabular_data_with_diffusion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16pa13n/research_modelling_tabular_data_with_diffusion/"/>
        <updated>2023-09-22T13:13:47.000Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models are becoming the leading paradigm of generative modeling for many important data modalities. TabDDPM is a diffusion model that can be universally applied to any tabular dataset and handles any type of feature.
 Blog post link.
    submitted by    /u/metkere  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI in finance: Addressing hurdles on the path to transformation]]></title>
        <id>https://www.datasciencecentral.com/?p=63214</id>
        <link href="https://www.datasciencecentral.com/ai-in-finance-addressing-hurdles-on-the-path-to-transformation/"/>
        <updated>2023-09-22T13:08:25.000Z</updated>
        <summary type="html"><![CDATA[Discover the obstacles hindering seamless AI adoption in financial services and gain actionable insights to navigate regulatory compliance, data security, organizational change, and more.
The post AI in finance: Addressing hurdles on the path to transformation appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Advice needed for what tool/algorithm is appropriate]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16p95js/p_advice_needed_for_what_toolalgorithm_is/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16p95js/p_advice_needed_for_what_toolalgorithm_is/"/>
        <updated>2023-09-22T12:35:19.000Z</updated>
        <summary type="html"><![CDATA[Context of the problem: I have the following entities: stations, programs, project manager, days, and time slots. The goal of the problem is to decide what program with who project manager to assign on what day at what time slot for each station.
 Some notes:
  
Each program has its own duration. Say Program A last for 50 minutes, Program B is 30 minutes, etc.
 Each project manager has qualifications on what program he/she can handle. This constraint is a hard constraint.
 Time slots start from 6AM to 6PM. This means that if Program A which lasts for 30 minutes is assigned to start at 6:00AM, then it will end at 6:30AM. Only one program can be assigned in each station, so there should be no overlap in programs in terms of day and time per station.
 There is a forecasting model that takes aâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Introducing GeoCOCO: Easily transform GIS annotations into Microsoft's Common Objects In Context (COCO) datasets for use in deep learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16p8wyw/p_introducing_geococo_easily_transform_gis/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16p8wyw/p_introducing_geococo_easily_transform_gis/"/>
        <updated>2023-09-22T12:23:58.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/jaspersiebring/geococo
 Introducing GeoCOCO, an open source project that enables users to turn their GIS annotations (e.g. shapefile) into COCO datasets which can then be used to train computer vision models!
 It allows users to use the likes of QGIS, ArcGIS to annotate geographic imagery in the same way you would annotate non-geographic imagery with LabelMe. It is powered by Python and a variety of packages (e.g. pydantic, pandera, geopandas, pycocotools). On the more meta side, it also features automated tests/builds/releases through Github Actions (using the likes of poetry, ruff, mypy, pytest, black).
 Sharing it with you guys in case someone else might find it useful! I am also very interested to hear some feedback (suggestions, flaws, etc.), let me know!
 â€‹
 Here'sâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continued fractions as matrix products]]></title>
        <id>https://www.johndcook.com/blog/?p=209581</id>
        <link href="https://www.johndcook.com/blog/2023/09/22/continued-fractions-as-matrix-products-2/"/>
        <updated>2023-09-22T12:12:46.000Z</updated>
        <summary type="html"><![CDATA[A continued fraction of the form with n terms can be written as the composition where As discussed in the previous post, a MÃ¶bius transformation can be associated with a matrix. And the composition of MÃ¶bius transformations is associated with the product of corresponding matrices. So the continued fraction at the top of the post [â€¦]
Continued fractions as matrix products first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Finding linkedIn article on anomaly detection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16p8k9r/d_finding_linkedin_article_on_anomaly_detection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16p8k9r/d_finding_linkedin_article_on_anomaly_detection/"/>
        <updated>2023-09-22T12:06:51.000Z</updated>
        <summary type="html"><![CDATA[Finding linkedIn article on anomaly detection
 Last week I saw a LinkedIn article on anomaly detection. In that post, Tail movement of a running mice on rotating rod ( roller) is captured with CV and plotted as a time series. Then based on the tail movement time series , anomalities in that time series are detected where mice loose the balance in rotating rod. 
 I am trying to find this post in linkedIn but still didnt able get any clue. If you have seen this research article or have any clue please let me know. It will be great help. Appriciate you time and help.Thank You very much ! â¤ï¸
 [D] [R]
    submitted by    /u/isurusachitha  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fractional linear and linear]]></title>
        <id>https://www.johndcook.com/blog/?p=209563</id>
        <link href="https://www.johndcook.com/blog/2023/09/22/fractional-linear-and-linear/"/>
        <updated>2023-09-22T10:55:57.000Z</updated>
        <summary type="html"><![CDATA[A function of the form where ad â€“ bc â‰  0 is sometimes called a fractional linear transformation or a bilinear transformation. I usually use the name MÃ¶bius transformation. In what sense are MÃ¶bius transformations linear transformations? Theyâ€™re nonlinear functions unless b = c = 0. And yet theyâ€™re analogous to linear transformations. For starters, [â€¦]
Fractional linear and linear first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Parallelizing RNN over its sequence length]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16p6uxd/r_parallelizing_rnn_over_its_sequence_length/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16p6uxd/r_parallelizing_rnn_over_its_sequence_length/"/>
        <updated>2023-09-22T10:37:15.000Z</updated>
        <summary type="html"><![CDATA[I am really excited to share our newest work in deep learning: parallelizing RNN! https://arxiv.org/abs/2309.12252
 RNN is thought to be non-parallelizable because of its inherent sequential nature: its state depends on its previous state. This makes training RNN for long sequence usually takes long time compared to other architecture classes (like CNN).
 What we present is an algorithm based on Newton's method to evaluate and train RNN in parallel. In one of our experiment, we can achieve >1000x faster evaluation of a small GRU compared to common sequential method on a very long sequence. Training RNNs with our algorithm could also be more than 10x faster than training with sequential method.
 However, it's not without flaws. There are 2 major drawbacks we noticed: non-convergence and scaâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is running an open sourced LLM in the cloud via GPU generally cheaper than running a closed sourced LLM?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16p6dgr/d_is_running_an_open_sourced_llm_in_the_cloud_via/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16p6dgr/d_is_running_an_open_sourced_llm_in_the_cloud_via/"/>
        <updated>2023-09-22T10:08:15.000Z</updated>
        <summary type="html"><![CDATA[Assuming using the same cloud service, Is running an open sourced LLM in the cloud via GPU generally cheaper than running a closed sourced LLM? (ie. do we pay a premium when running a closed sourced LLM compared to just running anything on the cloud via GPU?)
 One eg. I am thinking of is running Llama 2 13b GPTQ in Microsoft Azure vs. GPT-3.5 Turbo.
 I understand there are a lot of parameters to consider (such as choosing which GPU to use in Microsoft Azure etc.), but I am really looking at whatâ€™s the cheapest way to run Llama 2 13b GPTQ or a performance-equivalent closed sourced LLM.
    submitted by    /u/--leockl--  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why ChatGPT isnâ€™t conscious â€“ but future AI systems might be | "Different theories of consciousness suggest some basic properties we might expect a conscious system to have"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16p5yhm/why_chatgpt_isnt_conscious_but_future_ai_systems/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16p5yhm/why_chatgpt_isnt_conscious_but_future_ai_systems/"/>
        <updated>2023-09-22T09:43:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The conference for Reinforcement Learning related with mutli-agent system, game theory, or with others' technicals]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16p5m7g/the_conference_for_reinforcement_learning_related/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16p5m7g/the_conference_for_reinforcement_learning_related/"/>
        <updated>2023-09-22T09:22:09.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I'm a newbie for the domain of reinforcement learning (RL). My main research on various software systems, mainly on multi-agent system (MAS). AAMAS, a conference focusing on Multi-agent system, I think is the good conference to study in MAS. Recently, there are many researchers for MAS that is great combination with reinforecement learning to do some interesting things. Also, game theory is a
 interesting knowledge what I'm curious about how to do within RL and agents.
 Does any recommended conference or journal for RL? don't mind for just only RL algorithm, that's enough great, whereas I prefer to the conference concerning RL applied some software system for some problem.
 Thanks all.
    submitted by    /u/DryAir1198  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] State-of-the-art Image-to-Image generators (open-source)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16p427x/p_stateoftheart_imagetoimage_generators_opensource/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16p427x/p_stateoftheart_imagetoimage_generators_opensource/"/>
        <updated>2023-09-22T07:41:12.000Z</updated>
        <summary type="html"><![CDATA[Hi all. I am curious to know what is cutting edge in this domain? The use-case is creating an image of myself that is older than I am. I've looked a bit into StarGAN but I've never done work in this domain and don't know if this is still used today. Any help would be appreciated!
    submitted by    /u/ProudOwner_of_Fram  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Open Source AI Will Win]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16p425e/why_open_source_ai_will_win/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16p425e/why_open_source_ai_will_win/"/>
        <updated>2023-09-22T07:41:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient of Langevin Dynamics Step w.r.t model parameters [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16p3kxz/gradient_of_langevin_dynamics_step_wrt_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16p3kxz/gradient_of_langevin_dynamics_step_wrt_model/"/>
        <updated>2023-09-22T07:08:37.000Z</updated>
        <summary type="html"><![CDATA[I am reading the following paper about self-supervised learning.
 â€‹
 https://preview.redd.it/g4bz7wjxarpb1.png?width=1323&format=png&auto=webp&s=9f923825b57d4d3da346bb1bb4dd008366493dbd
 Briefly their idea for self-supervised learning is to reconstruct a corrupted image (e.g., random masking) using Langevin step of a learned energy function and the reconstructed image is compared to the clean image for supervision.
 i.e.,
 â€‹
 https://preview.redd.it/q9e1pd9yarpb1.png?width=953&format=png&auto=webp&s=528e654d3ae4db1435aa3c11e5edc787876fd9ac
 This should work because going along the decreasing energy value ensures that corrupted images have high energy and clean images have low energy.
 Everything made sense to me until I tried to implement it.
 â€‹
 In order to optimize the parameters (\theta), we have to take the gradient of the loss w.r.t \theta and by the chain the chain rule we will compute the gradient of the reconstructed x (\tilde x) w.r.t \theta.
 â€‹
 https://preview.redd.it/j3qbyddzarpb1.png?width=1280&format=png&auto=webp&s=c03d54dbaf70ba588c2a4d87691d71184797ae3f
 Is this even correct? What am I even talking about I am confused.
 Anyways ...
 They provided the following PyTorch pseudocode and I have provided the actual PyTorch code.
 â€‹
 https://preview.redd.it/m4wd7pq2frpb1.png?width=1266&format=png&auto=webp&s=d5febf258f0c92fc90116a996367b605cd597128
 Actually the model parameters never change no matter what what the values of step size (alpha) or the learning rate are.
 I am missing something?
 â€‹
 https://preview.redd.it/yznnuws9frpb1.png?width=1185&format=png&auto=webp&s=1ced5a213d372f309e6c20a9781ac35c1abcc436
 Any help is appreciated
 â€‹
    submitted by    /u/ThoughtOk5558  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16p2p8f/distilling_stepbystep_outperforming_larger/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16p2p8f/distilling_stepbystep_outperforming_larger/"/>
        <updated>2023-09-22T06:12:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[want to get GTA 4 navigation voice into Google Maps is it possible using AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16p2lr9/want_to_get_gta_4_navigation_voice_into_google/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16p2lr9/want_to_get_gta_4_navigation_voice_into_google/"/>
        <updated>2023-09-22T06:07:00.000Z</updated>
        <summary type="html"><![CDATA[hello everyone
 there was a voice-guided navigation system for GTA 4 with a female voice, I would love to use that as my Google Maps and Waze navigation voice 
 I tried looking for anything online but the mods available are of very inferior quality and from a few years ago which won't work properly with Google Maps as a lot of the voice commands are missing 
 the sample for the voice can be found in these 2 YouTube videos 
 first sample 11 minutes repeated statements
 2nd sample 1 minute without repeated statements
 Using Adobe podcast I can remove a lot of the background noise 
 The problem is I don't know much about sound tuning so using whatever free ai I could find on the internet the voice is not being cloned properly it is sounding mechanical or just like google ttss 
 i have a decenâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DALL-E 3 & ChatGPT: The Game-Changing AI Tool for Text-to-Image Generation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16p2dk5/dalle_3_chatgpt_the_gamechanging_ai_tool_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16p2dk5/dalle_3_chatgpt_the_gamechanging_ai_tool_for/"/>
        <updated>2023-09-22T05:52:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Unlimited Face Swap Tool You Can Use in Browser]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16p1xjl/free_unlimited_face_swap_tool_you_can_use_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16p1xjl/free_unlimited_face_swap_tool_you_can_use_in/"/>
        <updated>2023-09-22T05:25:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Vegetable_Tutor8245  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[help me solve this weird error]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16ozpfd/help_me_solve_this_weird_error/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16ozpfd/help_me_solve_this_weird_error/"/>
        <updated>2023-09-22T03:21:42.000Z</updated>
        <summary type="html"><![CDATA[trying to make a frozen lake game but keep getting this weird error and i dont know how to fix it. The error is "too many values to unpack (expected 4)" on the line: next_state, reward, done, _ = env.step(action)
 import gym
 import numpy as np
 import pygame
 from pygame.locals import QUIT
 import tensorflow as tf
 from tensorflow import keras
 import warnings
 def ignore_specific_warning():
 warnings.filterwarnings("ignore", message="This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.")
 â€‹
 ignore_specific_warning()
 â€‹
 input_size = env.observation_space.n
 output_size = env.action_space.n
 â€‹
 model = tf.keras.Sequential([
 tf.keras.layers.Dense(64,activation='relu',input_shape=(input_size,),use_bias=False),
 tf.keras.layers.Dense(output_size,use_bias=False)])
 â€‹
 loss_fns = tf.keras.losses.MeanSquaredError()
 optimizer = tf.keras.optimizers.Adamlearning_rate=0.001
 â€‹
 discount_factor=0.99
 learning_rate=0.1
 num_episodes=1000
 â€‹
 for i in range(num_episodes):
 state = env.reset()
 done = False
 while not done:
 env.render()
 epsilon = 0.1
 if np.random.rand() < epsilon:
 action = env.action_space.sample()
 else:
 #Q_values = model.predict(tf.one_hot(state[0],input_size))
 #action = np.argmax(Q_values)
 state_one_hot = tf.one_hot(state[0], input_size)
 Q_values = model.predict(tf.reshape(state_one_hot, (1, -1)))
 action = np.argmax(Q_values)
 next_state, reward, done, _ = env.step(action)
 target = reward*discount_factor*np.max(model.predict(tf.one_hot(next_state,input_size)))
 with tf.gradientTape() as tape:
 Q_values = model(tf.one_hot(next_state,input_size))
 loss = loss_fn(Q_values[0][action],target)
 gradients = tape.gradients(loss,model.trainable_variables)
 optimizer.apply_gradients(zip(gradients,model.trainable_variables))
 state = next_state
 env.close()
    submitted by    /u/BeastHunterrr  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claude LLM, draft a strong speech for a world leader predicting that by 2030 our AI revolution could make the world so wonderful that we can safely close our prisons.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ozfzz/claude_llm_draft_a_strong_speech_for_a_world/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ozfzz/claude_llm_draft_a_strong_speech_for_a_world/"/>
        <updated>2023-09-22T03:08:18.000Z</updated>
        <summary type="html"><![CDATA[Here is a draft speech focusing on the potential for AI to help create a more just and peaceful world by 2030:
 My fellow citizens,
 We stand today on the cusp of a new era. The rapid development of artificial intelligence technology presents us with an unparalleled opportunity to remake our world for the better. Though this technology poses risks, I believe that if guided responsibly, AI can help us build a society of prosperity, equity, and justice. 
 Over the past decade, we have already seen glimpses of AI's vast potential for good. Algorithms are helping doctors diagnose diseases more accurately. Smart systems are optimizing transportation networks and reducing traffic accidents. Machine learning is enabling more efficient energy use and sustainable agriculture. 
 But this is only theâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help answering questions about neural networks for my father(retired Computer Science and EE professor) who (for medical reasons) cannot post them himself.]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16ovmtp/help_answering_questions_about_neural_networks/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16ovmtp/help_answering_questions_about_neural_networks/"/>
        <updated>2023-09-22T00:01:27.000Z</updated>
        <summary type="html"><![CDATA[My father, a retired Computer Science professor and pioneer in network design algorithms, has asked me to post two questions he has as he attempts to teach himself about neural networks. For physical/medical reasons he canâ€™t post them himself. Grateful for any answers or suggestions for where he could find them!
 His note:
 I have many questions, but they all revolve around two issues.
 The first is when the weights in the neural net converge, do they exhibit any recognizable pattern. I realize that when the number of nodes and levels in the net are large it may be impossible to recognize the pattern. But suppose that the problem being examined is a simple categorization and the number of nodes in the net is small, is it possible to see a pattern in the weights? The network is supposed to â€¦]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm making my capstone project at Harvard freely available for everyone (until credit runs out). It uses ChatGPT3/4 in the backend. Experimenting with the finetuned model now.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ouywu/im_making_my_capstone_project_at_harvard_freely/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ouywu/im_making_my_capstone_project_at_harvard_freely/"/>
        <updated>2023-09-21T23:31:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Raymondlkj  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm making my capstone project at Harvard freely available for everyone (until credit runs out). It uses ChatGPT3/4 in the backend. Experimenting with the finetuned model now.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ouypu/im_making_my_capstone_project_at_harvard_freely/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ouypu/im_making_my_capstone_project_at_harvard_freely/"/>
        <updated>2023-09-21T23:31:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Raymondlkj  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm making my capstone project at Harvard freely available for everyone (until credit runs out). It uses ChatGPT3/4 in the backend. Experimenting with the finetuned model now.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ouvft/im_making_my_capstone_project_at_harvard_freely/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ouvft/im_making_my_capstone_project_at_harvard_freely/"/>
        <updated>2023-09-21T23:27:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Raymondlkj  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm making my capstone project at Harvard freely available for everyone (until credit runs out). It uses ChatGPT3/4 in the backend. Experimenting with the finetuned model now.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16oukdy/im_making_my_capstone_project_at_harvard_freely/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16oukdy/im_making_my_capstone_project_at_harvard_freely/"/>
        <updated>2023-09-21T23:14:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Raymondlkj  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Naming Awk]]></title>
        <id>https://www.johndcook.com/blog/?p=209483</id>
        <link href="https://www.johndcook.com/blog/2023/09/21/naming-awk/"/>
        <updated>2023-09-21T22:51:37.000Z</updated>
        <summary type="html"><![CDATA[The Awk programming language was named after the initials of its creators. In the preface to a book that just came out, The AWK Programing Language, Second Edition, the authors give a little background on this. Naming a language after its creators shows a certain paucity of imagination. In our defense, we didnâ€™t have a [â€¦]
Naming Awk first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help bringing some peace to my family.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16otwu1/help_bringing_some_peace_to_my_family/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16otwu1/help_bringing_some_peace_to_my_family/"/>
        <updated>2023-09-21T22:46:10.000Z</updated>
        <summary type="html"><![CDATA[I am looking for a way that I can take a bunch of saved voicemails from my mom and be able to hear her voice again. It would mean the world to my family and if my kids could hear her voice I know it would brighten their day. Can anyone point me in the right direction to accomplish this?
    submitted by    /u/blbjtb  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What fundamentally prevents training with Volunteer Computing?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ote5k/d_what_fundamentally_prevents_training_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ote5k/d_what_fundamentally_prevents_training_with/"/>
        <updated>2023-09-21T22:24:35.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™d love to see people come together, donate their spare compute to train more open source models, boost research etc. One relevant paper I was able to find is this one https://arxiv.org/abs/2103.08894
    submitted by    /u/tecbar  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AAAI24 fast track submission [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16osmqm/aaai24_fast_track_submission_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16osmqm/aaai24_fast_track_submission_d/"/>
        <updated>2023-09-21T21:55:00.000Z</updated>
        <summary type="html"><![CDATA[I'm planning on submitting a rejected NeurIPS paper to the AAAI fast track. A few days ago I registered myself as an author on CMT, but I cannot see an option to submit a paper. Will the portal open later? Can anyone else see an option to submit yet?
    submitted by    /u/Firm-Act-3860  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can you go to NeurIPS workshops without presenting?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16os3md/d_can_you_go_to_neurips_workshops_without/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16os3md/d_can_you_go_to_neurips_workshops_without/"/>
        <updated>2023-09-21T21:34:29.000Z</updated>
        <summary type="html"><![CDATA[This year will be my first NeurIPS, and I see some cool workshops but don't have any work that would be a good fit for them. Can I just go and listen?
    submitted by    /u/ThickBiker  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes]]></title>
        <id>http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</id>
        <link href="http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html"/>
        <updated>2023-09-21T21:25:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Cheng-Yu Hsieh, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team





Large language models (LLMs) have enabled a new data-efficient learning paradigm wherein they can be used to solve unseen new tasks via zero-shot or few-shot prompting. However, LLMs are challenging to deploy for real-world applications due to their sheer size. For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using specialized infrastructure, not to mention that today's state-of-the-art LLMs are composed of over 500 billion parameters. Such computational requirements are inaccessible for many research teams, especially for applications that require low latency performance.




To circumvent these deployment challenges, practitioners often choose to deploâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Donâ€™t exclude AI-generated art from copyright]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16oqbvx/dont_exclude_aigenerated_art_from_copyright/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16oqbvx/dont_exclude_aigenerated_art_from_copyright/"/>
        <updated>2023-09-21T20:25:31.000Z</updated>
        <summary type="html"><![CDATA[The US Copyright Office has ruled that AI-generated art cannot be copyrighted, raising questions about whether AI-generated art should be excluded from copyright protection.
 
The Copyright Office argues that AI-generated art is a 'merely mechanical' process with no novelty, invention, or originality, and therefore does not deserve copyright protection.
 
Critics, however, argue that this approach is not scalable and fails to consider the creative choices made by AI artists.
 
They suggest that AI-generated art should be treated similarly to photography, where even though the image is captured mechanically, it still reflects the creative choices of the photographer and is eligible for copyright protection.
 
Photographers are able to own the copyright in their photographs because they make creative judgments about where to point the camera, when to snap the image, and how to adjust camera settings.
 
Similarly, AI artists explore the 'latent space' of images that AI software can produce, making creative judgments about which images to select and explore.
 
While the actual image is produced by the software, the important point is that a human being is making creative decisions about the art.
 
Therefore, critics argue that AI-generated art should be eligible for copyright protection, as it reflects the creative choices and judgments of the AI artist.
 
 Source : https://www.understandingai.org/p/dont-exclude-ai-generated-art-from
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Motherboard Help]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oqb6b/d_motherboard_help/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oqb6b/d_motherboard_help/"/>
        <updated>2023-09-21T20:24:44.000Z</updated>
        <summary type="html"><![CDATA[So a few weeks ago I purchased a Sega 36 Crane Machine (Claw Machine) off ebay and everything worked great except the claw strength being too strong. I took the board out so I could possibly have something added to the board for me. It was missing a â€œFree Playâ€ button which is used to adjust claw strength on this machine.. Anyway, I kept the board in the garage while searching for the right parts and dad threw it away on accident while cleaning, now im out a board and I have no idea what to do. Can anybody help me or am I sol? Even if im in the wrong community for this, a step to the right direction would even help. I also have attached some pictures of the board.. Im not good with these boards at all and have no clue what im looking for. Theres also a diagram of the boards functions! Thanks for all information/help!
    submitted by    /u/Ready_Highlight9758  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-Source Trading AI [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16opubk/opensource_trading_ai_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16opubk/opensource_trading_ai_p/"/>
        <updated>2023-09-21T20:06:08.000Z</updated>
        <summary type="html"><![CDATA[Hi, I am an experienced trader and coder. I am embarking on a journey to develop an open-source Trading AI in Python, and I'm looking for passionate individuals to join me in this project. This is a non-commercial, community-driven project, so there won't be any monetary compensation involved. However, it's a great opportunity to learn and collaborate in the field of trading AI.
 â€‹
 - You don't need to know trading to contribute.
 - As an Open Source project, you have complete freedom to use the AI.
 â€‹
 About the Project:
 - Objective: Our goal is to create a self-learning AI system for trading in financial markets.
 - Tech Stack: We'll be working with Python for data analysis, machine learning, and neural network development.
 - Data: We'll be using historical stock market data to train aâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a Free Chatbot Service â€“ Custom Base Prompt and API Access Needed]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16opm5j/looking_for_a_free_chatbot_service_custom_base/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16opm5j/looking_for_a_free_chatbot_service_custom_base/"/>
        <updated>2023-09-21T19:57:30.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for a free chatbot service that allows base prompt customization and offers API access (so I can dynamically change the stock on hand to better assist customers, for example).
 I looked into https://ora.ai, but it seems you can only set the base prompt once and manually. I'm hoping to find a service that allows for more flexibility. 
 The service must also be free and offer embedding, like https://ora.ai/, (with div)
    submitted by    /u/LimeLom1  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ddqn tunning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16oor9h/ddqn_tunning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16oor9h/ddqn_tunning/"/>
        <updated>2023-09-21T19:24:18.000Z</updated>
        <summary type="html"><![CDATA[Hi world
 I'm trying to implement dqn and ddqn on various playgrounds with Matlab, from scratch. I use dqn and ddqn with replay buffer. I tried to used gradient clipping. But I find it very difficult to tune.
 I mean I tuned and programmed successfully various algorithm such as different GAN that are not necessarily very easy to tune. RL algorithm seems even worse.
 I tried several grid world problems, cart pole and even a very simple second order system regulation (integrator + 1 st order pÃ´le). 
 Even the Matlab cart pole demo with ddqn does not provide a very satisfactory result (after training, the contrÃ´ler keep the pendulum in uprigth position, but the cart slowly drift).
 On the second order system, the learning seems ok for about 200 Ã©pisode(test shows that the expected behavior is almost learnt) and then suddenly the average episode score has a kind of inflection and everything blowns up, despite a very large replay buffer.
 So my question : is it possible to make dqn and ddqn work well with a reasonable tuning session length? Are policy optimization easier to tune (and/or more efficient)? So far, it seems to me that these action-vzlue based algorithms are highly unstable and the training may only works for a 'miraculous' tunning. What are you thought?
    submitted by    /u/seb59  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] MLE RecSys / Ops at DeoVR]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oohp0/p_mle_recsys_ops_at_deovr/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oohp0/p_mle_recsys_ops_at_deovr/"/>
        <updated>2023-09-21T19:14:05.000Z</updated>
        <summary type="html"><![CDATA[Hi, we're welcoming talented Machine Learning Engineers with a focus on Recommendation Systems and ML Operations.
 Who are we?
 Leading in VR video streaming, we drive the DeoVR platform with interactive 8K videos, ML, computer vision, and advanced haptic tech.
 What you'd do?
  
For MLE RecSys: develop ML pipelines for top recommendations, engage with community for desired features, and collaborate with Backend/Frontend/Unity teams.
 For ML Ops: construct ML inference infrastructure, refine models & workflows, and ensure high-availability ML services.
  
What we offer?
  
ðŸŒRemote flexibility
 ðŸ¤Collaborative and inclusive work environment
 ðŸš€Make a significant impact in the VR industry
  
Interested? Check out and apply here or drop me a DMðŸ˜Ž
    submitted by    /u/SanjaVR  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leading Theory of Consciousness (and why even the most advanced AI can't possess it) Slammed as "Pseudoscience"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16onrqe/leading_theory_of_consciousness_and_why_even_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16onrqe/leading_theory_of_consciousness_and_why_even_the/"/>
        <updated>2023-09-21T18:46:08.000Z</updated>
        <summary type="html"><![CDATA[Consciousness theory slammed as â€˜pseudoscienceâ€™ â€” sparking uproar (Nature)
 The irony here is that I mostly agree with this theory - but the article reflects how little we really know about consciousness and how it works, and how what's considered the "expert opinion" that AI can't possess consciousness is arguably influenced more by popularity than real empirical evidence.
 By whatever mechanism, they can respond to their treatment in unexpectedly humanlike ways. 
 Oh, and by the way, did you think that "sentient Bing" was finally dead? Think again.
    submitted by    /u/kamari2038  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Where to find checkpoints for models (with different seeds) trained on the Imagenet?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16omvdh/d_where_to_find_checkpoints_for_models_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16omvdh/d_where_to_find_checkpoints_for_models_with/"/>
        <updated>2023-09-21T18:11:05.000Z</updated>
        <summary type="html"><![CDATA[i.e., models with the exact same architecture, but their initial weights are different, and the order which the model sees the training data is different.
    submitted by    /u/just2gud  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implementation of Reinforcement Learning to achieve an autonomous drone]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16om4fv/implementation_of_reinforcement_learning_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16om4fv/implementation_of_reinforcement_learning_to/"/>
        <updated>2023-09-21T17:41:13.000Z</updated>
        <summary type="html"><![CDATA[Hello there,
 I'll provide a quick introduction first. I am a mechatronics engineer student who is graduating this semester, I have been also privately studying ML, DL, CV for the past year because that's when I decided that I want to make an autonomous drone as my thesis/graduation project and oh my god, little did I know. I have aquired so much knowledge in the data science field and truth be told I love it, it engages my mind like crazy so I've decided that i would like my career to be revolved around electronics and programming/ data science.
 Back to the topic, I am working on object detection (I got it figured out), and reinforcement learning, so what I would like to do is to train the model on reaching it's destination using collision avoidance, I have a PIX4, RPI 4B 4GBs since I'm also building my drone from scratch, so let's say I have all the hardware which ofcourse includes the GPS + Compass and 5 ultrasonic sensors, 1 at each side and 1 down to hold the altitude accurately. I was thinking of HITL, making a virtual environment to train my model there because this is what logic says, ofcourse( I could also fly it using the RC controller and for example test if the obstacle avoidance is going to overrule the manual drive, which is also my objective). The question is: how can I make sure virtual drone is going to be equipped with the real-life sensors and in the needed positions? How can I feed all the sensors data during the training? Which type of reinforcement learning models should I implement? 
 I just feel like I dragged myself into an incredibly amazing and complex project which is going to end up fucking my semester and I won't be able to graduate, it's just that my stress now is skyrocketing. 
 Please feel free to throw any advice or opinions my way, and thank you for keeping up with this long post. ðŸ˜
    submitted by    /u/Gabii99  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI Revolution is Rotten to the Core]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ol1x1/the_ai_revolution_is_rotten_to_the_core/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ol1x1/the_ai_revolution_is_rotten_to_the_core/"/>
        <updated>2023-09-21T16:58:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Hazzman  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] BlindChat: Fully in-browser and private Conversational AI with Transformers.js for local inference]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ol0bs/p_blindchat_fully_inbrowser_and_private/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ol0bs/p_blindchat_fully_inbrowser_and_private/"/>
        <updated>2023-09-21T16:56:39.000Z</updated>
        <summary type="html"><![CDATA[We are happy to share with you BlindChat, the open-source and privacy-by-design alternative to ChatGPT for a fully in-browser, yet private, Conversational AI!
 You can play with it on our Gradio demo.
 Our philosophy is that Conversational AI should be easily accessible, and privacy should not be a luxury but a given.
 By leveraging local models running in the browser, with transformers.js, we make it possible to have a fully transparent and private AI that works on your browser without any extra setup. Because all the logic is offloaded to usersâ€™ devices, data never leaves and there is no risk of it being used for finetuning by third parties.
 For now, we only support inference with LaMini-Flan-T5, so you might see modest performance. We plan to integrate Microsoft phi-1.5 for better performance once the 370M is out.
 We are also working on LlamaIndex-TS integration on the client side to have in-browser RAG for local querying of private documents.
 As our roadmap is quite dense, with RAG, internet search, improved inference, we welcome warmly contributors! If you want to contribute, or have questions, ping us on Discord and GitHub!
    submitted by    /u/Separate-Still3770  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How United Airlines built a cost-efficient Optical Character Recognition active learning pipeline]]></title>
        <id>6332bae157cdf6c59677bc015e1727ae2f0d8147</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/how-united-airlines-built-a-cost-efficient-optical-character-recognition-active-learning-pipeline/"/>
        <updated>2023-09-21T16:53:55.000Z</updated>
        <summary type="html"><![CDATA[In this post, we discuss how United Airlines, in collaboration with the Amazon Machine Learning Solutions Lab, build an active learning framework on AWS to automate the processing of passenger documents. â€œIn order to deliver the best flying experience for our passengers and make our internal business process as efficient as possible, we have developed [â€¦]]]></summary>
        <author>
            <name>Xin Gu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimize generative AI workloads for environmental sustainability]]></title>
        <id>44eac549e2383cff99c2864c5888005f737bd59d</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/optimize-generative-ai-workloads-for-environmental-sustainability/"/>
        <updated>2023-09-21T16:48:28.000Z</updated>
        <summary type="html"><![CDATA[To add to our guidance for optimizing deep learning workloads for sustainability on AWS, this post provides recommendations that are specific to generative AI workloads. In particular, we provide practical best practices for different customization scenarios, including training models from scratch, fine-tuning with additional data using full or parameter-efficient techniques, Retrieval Augmented Generation (RAG), and prompt engineering.]]></summary>
        <author>
            <name>Wafae Bakkali</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] DeepMind: LLMs compress images 43% better than PNG, and audio nearly 2x better than MP3]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ok8v6/r_deepmind_llms_compress_images_43_better_than/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ok8v6/r_deepmind_llms_compress_images_43_better_than/"/>
        <updated>2023-09-21T16:24:09.000Z</updated>
        <summary type="html"><![CDATA[Edit: FLAC is the tested audio extension, not MP3
 I read the new paper from DeepMind so you don't have to. Here are the key highlights:
  
Despite training on text, langauge models compressed images 43% better than PNG, and audio nearly 2x better than flac.
 Confirmation of scaling laws - bigger models compressed better. But model size must match dataset size.
 There are tradeoffs between model scale, data size, and compression performance. More data enables bigger models.
 Tokenization (like BPE) generally hurts compression slightly by making prediction harder.
 Longer contexts let models exploit more sequential dependencies.
  
Implications:
  
Models have learned very general capabilities beyond just text. Their strong compression reflects deep understanding of images, audio etc statistically.
 I got some new perspective on model scaling laws and links between prediction and generalization.
 There's potential for practical applications compressing images, video etc. But large model size an issue.
 Overall it shows these models are very capable general purpose learners, not just for language.
  
Full summary here if you want more details. Original paper is here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best VoicifyAi alternatives?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ok7ch/best_voicifyai_alternatives/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ok7ch/best_voicifyai_alternatives/"/>
        <updated>2023-09-21T16:22:22.000Z</updated>
        <summary type="html"><![CDATA[I was thinking of signing up to VoicifyAi for fun a create some custom covers but are there better (or even free) alternatives?
    submitted by    /u/quantummufasa  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[I read the paper for you] LLMs compress images 43% better than PNG, and audio nearly 2x better than MP3]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ok55c/i_read_the_paper_for_you_llms_compress_images_43/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ok55c/i_read_the_paper_for_you_llms_compress_images_43/"/>
        <updated>2023-09-21T16:19:54.000Z</updated>
        <summary type="html"><![CDATA[Edit: FLAC is the tested audio extension, not MP3
 I read the new paper from DeepMind so you don't have to. Here are the key highlights:
  
Despite training on text, langauge models compressed images 43% better than PNG, and audio nearly 2x better than flac.
 Confirmation of scaling laws - bigger models compressed better. But model size must match dataset size.
 There are tradeoffs between model scale, data size, and compression performance. More data enables bigger models.
 Tokenization (like BPE) generally hurts compression slightly by making prediction harder.
 Longer contexts let models exploit more sequential dependencies.
  
Implications:
  
Models have learned very general capabilities beyond just text. Their strong compression reflects deep understanding of images, audio etc statistically.
 I got some new perspective on model scaling laws and links between prediction and generalization.
 There's potential for practical applications compressing images, video etc. But large model size an issue.
 Overall it shows these models are very capable general purpose learners, not just for language.
  
Full summary here if you want more details. Original paper is here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New AI Art Style]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ojk1q/new_ai_art_style/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ojk1q/new_ai_art_style/"/>
        <updated>2023-09-21T15:56:43.000Z</updated>
        <summary type="html"><![CDATA[This AI-generated art style is gaining a lot of traction on the internet. 
 So I wanted to share how you can create art like this in under 2 minutes:
 â€¢ Visit the Fusion Art website and sign up for free (https://quickqr.art/app/fusion-art)
 â€¢ Choose a template or upload your reference template image.
 â€¢ Describe your image to tailor the output.
 â€¢ Tweak the aspect ratio and explore added features.
 â€¢ Hit "Generate" That's it! Share your art to showcase what you come up with. 
 Hope this was helpful for anybody looking to create this style of art! 
 https://preview.redd.it/j3o9puk9smpb1.png?width=1536&format=png&auto=webp&s=ddbe1328bd19dc89c4cd82ef1870b9de695e5500
 https://preview.redd.it/9ppxjpeasmpb1.png?width=768&format=png&auto=webp&s=29dd845eb5f8f094abe5bc1135060965f51365cf
 https://preview.redd.it/58vjky3bsmpb1.png?width=1536&format=png&auto=webp&s=06db1926e021ea3f2c438fb723c453758a461c43
 https://preview.redd.it/w2nvaoobsmpb1.png?width=1024&format=png&auto=webp&s=0b4bcaea4fc7c832bbce9f7f3b9319099df59a14
    submitted by    /u/IndifferentSpectat0r  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Stanford's AI Professional Program]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oj4oq/d_stanfords_ai_professional_program/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oj4oq/d_stanfords_ai_professional_program/"/>
        <updated>2023-09-21T15:39:27.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I am interested in taking couple of AI/ML online courses from the Stanford's Artificial Intelligence Professional Program (https://online.stanford.edu/programs/artificial-intelligence-professional-program). I am interested in taking XCS221 and XCS229. My employer would be paying for this (I hope!). I know these are not the complete courses offered in the university. Has anyone taken courses from this program ? Is it worth it ?
    submitted by    /u/RealMadrista007  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Now that DALL-E 3 is getting integrated with ChatGPT, will you switch from Midjourney and others?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16oil97/now_that_dalle_3_is_getting_integrated_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16oil97/now_that_dalle_3_is_getting_integrated_with/"/>
        <updated>2023-09-21T15:17:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Vinitneo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] OpenAI's new language model gpt-3.5-turbo-instruct can defeat chess engine Fairy-Stockfish 14 at level 5]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oi6fb/n_openais_new_language_model_gpt35turboinstruct/"/>
        <updated>2023-09-21T15:01:28.000Z</updated>
        <summary type="html"><![CDATA[This Twitter thread claims that OpenAI's new language model gpt-3.5-turbo-instruct can "readily" beat Lichess Stockfish level 4. This tweet shows the style of prompts that are being used to get these results with the new language model.
 I used website parrotchess[dot]com (discovered here) to play multiple games of chess purportedly pitting this new language model vs. various levels of Fairy-Stockfish 14 at website Lichess. My current results for all completed games: The language model is 2-0 vs. Fairy-Stockfish 14 level 5 (game 1, game 2), and 0-2 vs. Fairy-Stockfish 14 level 6 (game 1, game 2). One game I aborted because the language model apparently tried an illegal move.
 The following is a screenshot from the aforementioned chess web app showing the end state of the first game vs. Faiâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Studio Lineup Adds RTX-Powered Microsoft Surface Laptop Studio 2]]></title>
        <id>https://blogs.nvidia.com/?p=67040</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/21/surface-studio-chaos-dlss-resolve-tensor-rt/"/>
        <updated>2023-09-21T15:00:42.000Z</updated>
        <summary type="html"><![CDATA[The NVIDIA Studio laptop lineup is expanding with the new Microsoft Surface Laptop Studio 2, powered by GeForce RTX 4060, GeForce RTX 4050 or NVIDIA RTX 2000 Ada Generation Laptop GPUs, providing powerful performance and versatility for creators.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Run AI on Your PC? GeForce Users Are Ahead of the Curve]]></title>
        <id>https://blogs.nvidia.com/?p=67078</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/21/ai-on-local-rtx-windows-pc/"/>
        <updated>2023-09-21T15:00:33.000Z</updated>
        <summary type="html"><![CDATA[Gone are the days when AI was the domain of sprawling data centers or elite researchers. For GeForce RTX users, AI is now running on your PC. Itâ€™s personal, enhancing every keystroke, every frame and every moment. Gamers are already enjoying the benefits of AI in over 300 RTX games. Meanwhile, content creators have access Read article >]]></summary>
        <author>
            <name>Jesse Clayton</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I used Riffusion to generate an AI saxophonist to jam with me, responding to what I played on guitar]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ohvzb/i_used_riffusion_to_generate_an_ai_saxophonist_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ohvzb/i_used_riffusion_to_generate_an_ai_saxophonist_to/"/>
        <updated>2023-09-21T14:50:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/daveNZL  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mastering Prompt Engineering: Best Prompt Pattern to Use.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ohohz/mastering_prompt_engineering_best_prompt_pattern/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ohohz/mastering_prompt_engineering_best_prompt_pattern/"/>
        <updated>2023-09-21T14:41:43.000Z</updated>
        <summary type="html"><![CDATA[Hello, Reddit!
 At its core, Prompt Engineering is all about commanding the conversation with AI. Wanna write a poem, draft a project plan, or break down complex scientific terms? The right prompt can do that for you. If you want to learn more about Prompt Engineering feel free to join r/PromptWizards.
 Let's dive into some Prompt Engineering concepts:
 Let's start with the basics:
  
Reductive operations: These yield responses that are smaller in size than the input prompt. Essentially, they distill information. 
 Example: If the input prompt is a long article, and the task is to provide a summary, the operation used is reductive as the output (summary) is smaller than the input (the entire article).
 
 Transformational operations: These make sure the output maintains a relative balance wâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What would be your approach if you were to write a Tree-of-Thoughts model?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oheao/d_what_would_be_your_approach_if_you_were_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oheao/d_what_would_be_your_approach_if_you_were_to/"/>
        <updated>2023-09-21T14:30:00.000Z</updated>
        <summary type="html"><![CDATA[I've been playing around with transformers since following Karpathy's shakespeare model on youtube and found it really cool. Thought I should write a Tree-of-Thoughts model as my next project which will be somewhat similar, given what we're doing is learning the meaning behind words and phrases and then using that knowledge to generate a sequence based on a seed token or sequence of tokens.
 Here's how I was thinking it should go: I write a next token in sequence prediction model and write a generate function that generates a sequence of tokens on top of a seed token. Then I write a tree model that takes in whatever this bigram has learned and adds onto it some kind of knowledge about where one thought about the seed has been generated and stop there. Then iteratively generate a few more such thoughts based on the same seed.
 This is how I initially thought it would go but now when I'm writing, I've already written the bigram(next token prediction and generation), I don't know what to do next. I am kind of stuck.
 It will be interesting to see how you, if you've ever written a similar model or are good at NLP, would attempt this. Looking at a variety of different approaches to the same problem I am trying to solve would be great.
 So I decided to post here. Would be great to hear from you guys!
    submitted by    /u/GraphicsMonster  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT/Bard/Claude one paragraph challenge: "In one paragraph, what is the best use of AI for humanity?" You be the judge]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16oh68o/chatgptbardclaude_one_paragraph_challenge_in_one/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16oh68o/chatgptbardclaude_one_paragraph_challenge_in_one/"/>
        <updated>2023-09-21T14:20:31.000Z</updated>
        <summary type="html"><![CDATA[ChatGPT Plus:
 The best use of AI for humanity is to augment human capabilities, address pressing global challenges, and improve the quality of life for all. By analyzing vast amounts of data at unprecedented speeds, AI can aid in medical research, predict and mitigate natural disasters, optimize energy consumption, and personalize education to individual needs. Moreover, it can provide solutions to complex problems, such as climate change, resource scarcity, and public health crises, ensuring a more sustainable and prosperous future for generations to come.
 Bard:
 The best use of AI for humanity is to use it to solve problems that will make the world a better place for everyone. AI has the potential to improve our lives in many ways, such as developing new and more effective ways to diagâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What do you think of open-source AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16oh4rl/what_do_you_think_of_opensource_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16oh4rl/what_do_you_think_of_opensource_ai/"/>
        <updated>2023-09-21T14:18:44.000Z</updated>
        <summary type="html"><![CDATA[Hugging Face CEO Clem Delangue says open-source AI was vital to starting his company. Now, he finds himself defending open AI models as Washington considers new regulations.
 On POLITICO Tech, Delangue explains why he views open-source AI as not only safe, but necessary to prevent big tech companies from gaining more market power.
 Listen for more: https://politico-tech.simplecast.com/episodes/the-hugging-face-case-for-open-ai
    submitted by    /u/smo279  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Toyota Research Institute Unveils Breakthrough in Teaching Robots New Behaviors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ogon3/n_toyota_research_institute_unveils_breakthrough/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ogon3/n_toyota_research_institute_unveils_breakthrough/"/>
        <updated>2023-09-21T14:00:09.000Z</updated>
        <summary type="html"><![CDATA[Press release
 Diffusion Policy: Visuomotor Policy Learning via Action Diffusion (contains link to paper)
 Comments on Hacker News with some interesting info / links.
    submitted by    /u/falconberger  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No idea which ai to use]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ofy1o/no_idea_which_ai_to_use/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ofy1o/no_idea_which_ai_to_use/"/>
        <updated>2023-09-21T13:28:00.000Z</updated>
        <summary type="html"><![CDATA[I have a product and I need product photos. It is a bottle and I am wanting ai to make a realistic picture of a woman holding the bottle and smiling. Can this be done? If so, what software/website/app do I use for this? Thank you
    submitted by    /u/Ok_Salt_9211  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Into the Omniverse: Blender 4.0 Alpha Release Sets Stage for New Era of OpenUSD Artistry]]></title>
        <id>https://blogs.nvidia.com/?p=67011</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/21/omniverse-blender-release-openusd/"/>
        <updated>2023-09-21T13:00:34.000Z</updated>
        <summary type="html"><![CDATA[For seasoned 3D artists and budding digital creation enthusiasts alike, an alpha version of the popular 3D software Blender is elevating creative journeys.]]></summary>
        <author>
            <name>Dane Johnston</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA CEO Jensen Huang to Headline AI Summit in Tel Aviv]]></title>
        <id>https://blogs.nvidia.com/?p=67032</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/21/ai-summit/"/>
        <updated>2023-09-21T13:00:27.000Z</updated>
        <summary type="html"><![CDATA[NVIDIA founder and CEO Jensen Huang will highlight the newest in generative AI and cloud computing at the NVIDIA AI Summit in Tel Aviv from Oct. 15-16. The two-day summit is set to attract more than 2,500 developers, researchers and decision-makers from across one of the worldâ€™s most vibrant technology hubs. With over 6,000 startups, Read article >]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cash In: â€˜PAYDAY 3â€™ Streams on GeForce NOW]]></title>
        <id>https://blogs.nvidia.com/?p=66999</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/21/geforce-now-thursday-sep-21/"/>
        <updated>2023-09-21T13:00:19.000Z</updated>
        <summary type="html"><![CDATA[Time to get the gang back together â€” PAYDAY 3 streams on GeForce NOW this week. Itâ€™s one of 11 titles joining the cloud this week, including Party Animals. The Perfect Heist PAYDAY 3 is the highly anticipated sequel to one of the worldâ€™s most popular co-op shooters. Step out of retirement and back into Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for this particular dataset on Cervical cancer [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oeau0/looking_for_this_particular_dataset_on_cervical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oeau0/looking_for_this_particular_dataset_on_cervical/"/>
        <updated>2023-09-21T12:13:34.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I have found this dataset on Kaggle with zero information about it. It is very intriguing and would like to find the original researchers about it. It contains over 270,000 histopathological images of cervical cancer. I cannot contact the person who posted it on Kaggle because I am not a contributor yet. And they are not replying on other platforms. Any help on finding it will be very much appreciated. If not, where else can I get such large cervical or bladder cancer datasets? 
 Link to the dataset mentioned: https://www.kaggle.com/datasets/rzelite/cervical-cancer 
 Thanks in advance.
    submitted by    /u/dumb_persn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] GitHub CEO interview regarding AI and programming]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16odxrb/n_github_ceo_interview_regarding_ai_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16odxrb/n_github_ceo_interview_regarding_ai_and/"/>
        <updated>2023-09-21T11:56:41.000Z</updated>
        <summary type="html"><![CDATA[https://preview.redd.it/u7lq37efllpb1.png?width=1029&format=png&auto=webp&s=824ee138d9ae4b28d1600b969eab2077f47fb2a6
 GitHub CEO Thomas Dohmke spoke on stage at TC Disrupt today and made several statements regarding the development of artificial intelligence and programming:
  
Dohmke believes that AI won't replace software developers but will make them more efficient.
 Despite AI advancements, the demand for software developers will continue to exceed the supply.
 The growth in software usage is expected to be exponential, with every company becoming a software company.
 Legacy code maintenance remains crucial, especially in industries like finance with outdated codebases.
 Generative AI is creating more demand for technical talent as companies seek to adopt innovative AI solutions.
 There is a shortage of computer science students, leading to increased demand for developers with AI skills.
  
Source: https://techcrunch.com/2023/09/20/github-ceo-despite-ai-gains-demand-for-software-developers-will-still-outweigh-supply/
    submitted by    /u/gcore-com  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cost function for a deep q network]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16occv3/cost_function_for_a_deep_q_network/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16occv3/cost_function_for_a_deep_q_network/"/>
        <updated>2023-09-21T10:31:01.000Z</updated>
        <summary type="html"><![CDATA[I am using Game Maker and I just wanted to check if this is how to do the cost function.
 â€‹
 loss_function = 0 derivative_gradient = 0 array_copy(global.main_inputs,0,global.inputs,0,array_length(global.inputs)) for (var i = 0; i < array_length(buffer_sampling); i++) { var _reward = buffer_sampling[i][2] global.inputs = buffer_sampling[i][3] var _max_q = forward_prop_t()*global.gamma var yi = _reward + _max_q var cur_q_value = buffer_sampling[i][1][0] loss_function += power((yi - cur_q_value),2) derivative_gradient += 2*(cur_q_value - yi) } array_copy(global.inputs,0,global.main_inputs,0,array_length(global.main_inputs)) derivative_gradient = derivative_gradient*(1/array_length(buffer_sampling)) global.cost = loss_function*(1/array_length(buffer_sampling)) 
 â€‹
 basically, buffer_sampling is an array with tuples of [first_state, [q_value,action], reward, next_state]
 Thanks for any help!
    submitted by    /u/Daninjacat256  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] TimeGAN - doubt on generated sequence]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oc8up/d_timegan_doubt_on_generated_sequence/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oc8up/d_timegan_doubt_on_generated_sequence/"/>
        <updated>2023-09-21T10:24:31.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone
 I have a doubt regarding GANs for tabular data, more specifically for time-series generation. Looking at the TimeGAN paper (code) and other implementations such as the one by YData, considering that the training dataset is divided into slices (like a rolling window) of N length and that the generated sequence will follow the same format, that is [batch size, N, n_features], what we are effectively generating are slices and not a fully synthetic time-series.
 â€‹
 To clarify my question, the output of a TimeGAN is only a set of slices, although there is no guarantee that generated slice(N-1) is the one before slice(N), and so the true use of a TimeGAN is generating slices that can be used to train a predictive TS model but not reconstructing a complete time series like the one used for training.
 â€‹
 I suspect that's the case (useful only to generate good slices for training predictive models) as there is no fully generated time series in the paper or any of the codes, while the quality of the generated data is partially measured by training a model on original data and generated data and comparing their predictive performance. If anyone can confirm/develop upon these thoughts I would be extremely grateful.
    submitted by    /u/iReallyReadiT  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to use ChatGPT to increase your website conversions]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16oc0wq/how_to_use_chatgpt_to_increase_your_website/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16oc0wq/how_to_use_chatgpt_to_increase_your_website/"/>
        <updated>2023-09-21T10:11:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Quick fine-tuning image classification models from Bing image search]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16obzkb/p_quick_finetuning_image_classification_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16obzkb/p_quick_finetuning_image_classification_models/"/>
        <updated>2023-09-21T10:09:06.000Z</updated>
        <summary type="html"><![CDATA[I've been working on our interactive dataset explorer for machine learning: renumics-spotlight. Recently, I set up an image classification example using it. I utilized Bing image search to create a fully functional example for custom-defined classes.
 To my surprise, it not only worked efficiently but also took only a few minutes for fine-tuning. The best part is its adaptability. You can easily switch it up for different image classes.
 Maybe you like it. Let me know what you think or if you have any suggestions to improve!
 Install with
 pip install renumics-spotlight sliceguard[all] 
 Imports
 from renumics import spotlight from sliceguard.data import create_imagedataset_from_bing from sliceguard.models.huggingface import finetune_image_classifier, generate_image_pred_probs_embeddings from sliceguard.embeddings import generate_image_embeddings 
 â€‹
 Create an Image Dataset from Bing
 class_names = [ "Blue Tang", "Clownfish", "Spotted Eagle Ray", "Longnose Butterfly Fish", "Moorish Idol", "Royal Gramma Fish", ] df = create_imagedataset_from_bing( class_names, 25, "data", test_split=0.2, license="Free to share and use" ) 
 Fine-tune a ViT Model with the data (in 1-2 minutes on a GPU)
 finetune_image_classifier( df[df["split"] == "train"], model_name="google/vit-base-patch16-224-in21k", output_model_folder="./model_folder", epochs=15, ) 
 Enrich the DataFrame with Predictions, Probabilities and Embeddings and visualize it:
 df["prediction"], df["probs"], df["embeddings"] = generate_image_pred_probs_embeddings( df["image"].values, model_name="./model_folder" ) # Check the result and detect problematic clusters spotlight.show( df, layout="https://spotlight.renumics.com/resources/image_classification_v1.0.json" ) 
 â€‹
 https://i.redd.it/20qy5xw62lpb1.gif
    submitted by    /u/DocBrownMS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[75% of Americans Believe AI Will Reduce Jobs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ob9lm/75_of_americans_believe_ai_will_reduce_jobs/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ob9lm/75_of_americans_believe_ai_will_reduce_jobs/"/>
        <updated>2023-09-21T09:24:26.000Z</updated>
        <summary type="html"><![CDATA[75% of Americans believe that AI will decrease the total number of jobs in the next 10 years, according to a survey by Bentley-Gallup Business in Society study.
 
Younger Americans are less pessimistic about AI compared to older age groups.
 
Majorities believe that AI performs as well as or better than humans in customizing online content, recommending products or services, and assisting students with coursework.
 
However, Americans are skeptical about AI's ability to recommend employees, its self-driving capabilities, and its ability to recommend medical advice.
 
79% of Americans have little trust in businesses to use AI responsibly.
 
40% of Americans think AI does more harm than good, while only 10% believe it contributes more good than harm.
 
Black and Asian Americans have a more positive view of AI's impact on society compared to Hispanic and White Americans.
 
While most Americans are wary of AI's impact on the job market, younger people are more optimistic about its future.
 
Businesses need to affirm their commitment to using AI responsibly and address the knowledge deficit and lack of confidence among Americans.
 
 Source : https://news.gallup.com/opinion/gallup/510635/three-four-americans-believe-reduce-jobs.aspx
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] finetuning quantized model is a bad idea?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oancr/d_finetuning_quantized_model_is_a_bad_idea/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oancr/d_finetuning_quantized_model_is_a_bad_idea/"/>
        <updated>2023-09-21T08:45:36.000Z</updated>
        <summary type="html"><![CDATA[Hi there, due to the lack of my resources, I have to use quantized big-model or something lighter. In this situation, I want to try the first option, and goona finetune some model. How do you expect the result? 
 Training huge model in 4-bits circumstance will be significantly different from original setting?
 Thanks.
    submitted by    /u/Mundane_Definition_8  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] UNCERTAINTY IN GRADIENT BOOSTING VIA ENSEMBLES]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16oaco8/d_uncertainty_in_gradient_boosting_via_ensembles/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16oaco8/d_uncertainty_in_gradient_boosting_via_ensembles/"/>
        <updated>2023-09-21T08:26:23.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://doi.org/10.48550/arXiv.2006.10562
 Hi all, This paper explores the use of using a single model (meaning an ensemble of trees) to generate uncertainty.
 This technique has been implemented into catboost. My question is why hasn't this been implemented into xgboost? The technique looks easily applicable but I would have expected it to be implemented already as it is 2 years old.
 Is this for some reason not applicable to Xgboost?
 â€‹
 Figure 1 from paper showing the 'virtual' ensemble
    submitted by    /u/MetalOrganicKneeJerk  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/20/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16o6snh/oneminute_daily_ai_news_9202023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16o6snh/oneminute_daily_ai_news_9202023/"/>
        <updated>2023-09-21T04:53:25.000Z</updated>
        <summary type="html"><![CDATA[OpenAI unveils DALL-E 3, allows artists to opt out of training.[1]
 Infosys, the Indian tech giant, has announced a strategic partnership with NVIDIA, a leading provider of enterprise-grade AI solutions. The collaboration aims to empower enterprises and businesses with generative AI-based solutions that drive productivity.[2]
 Alibaba and Tencent Invest in Chinese State-Backed Zhipu AI.[3]
 John Grisham, George R.R. Martin and more authors sue OpenAI for copyright infringement.[4]
  
Sources:
 [1] https://techcrunch.com/2023/09/20/openai-unveils-dall-e-3-allows-artists-to-opt-out-of-training/
 [2] https://gameishard.gg/news/infosys-and-nvidia-partner-to-deliver-generative-ai-solutions/205456/
 [3] https://winbuzzer.com/2023/09/20/alibaba-and-tencent-invest-in-chinese-state-backed-zhipu-ai-xcxwbn/
 [4] https://apnews.com/article/openai-lawsuit-authors-grisham-george-rr-martin-37f9073ab67ab25b7e6b2975b2a63bfe 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building intuition on AC algo with TD]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16o327t/building_intuition_on_ac_algo_with_td/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16o327t/building_intuition_on_ac_algo_with_td/"/>
        <updated>2023-09-21T01:45:09.000Z</updated>
        <summary type="html"><![CDATA[I am trying to build intuition on why the actor update equation actually help the actor improving its policy. From what I understand, the update is proportional to the Critic's TD error. A better than expected reward will lead the actor to increase the prob of taking the associated action, and a disappointing reward will make the actor to move away from the given action.
 Here's where I feel like I'm missing a piece of the puzzle. Between a good state/action pair accurately valued by the Critic and a bad state/action pair undervalued by the Critic, the actor's update will favor the bad action more. Could we not conceive a scenario in which the agent gets stuck in a suboptimal policy because of that particular behavior?
 What triggers this questioning is that I have implemented a standard AC algo for Pixelcopter and found my agent getting stuck easily on a suboptimal policy (i.e. constantly getting higher, or lower until crash, while the critics assign good values on center squares and bad values to squares close to the wall's edge). For TD0, this is pretty marginal, but gets more significant for n-step TD as n increases.
 Any thoughts?
    submitted by    /u/infundibuliforme  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[There are no specific license dedicated to artificial intelligence that prevent them from extensively scraping publicly available data on the internet without providing proper source attribution]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16o30je/there_are_no_specific_license_dedicated_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16o30je/there_are_no_specific_license_dedicated_to/"/>
        <updated>2023-09-21T01:43:06.000Z</updated>
        <summary type="html"><![CDATA[When AI takes all the data for training purposes without feedback to UGC platforms, could this eventually lead to the demise of UGC platforms, and could the internet become increasingly closed until it collapses?
    submitted by    /u/oodzchen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canva AI Blurred My Image]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16o2myq/canva_ai_blurred_my_image/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16o2myq/canva_ai_blurred_my_image/"/>
        <updated>2023-09-21T01:25:56.000Z</updated>
        <summary type="html"><![CDATA[Asked the new Canva AI to blur my image. It Blurred my image.
    submitted by    /u/MDINOKC  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] The League of Robot Runners: Coordinate thousands of robots in real time!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16o2fvd/r_the_league_of_robot_runners_coordinate/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16o2fvd/r_the_league_of_robot_runners_coordinate/"/>
        <updated>2023-09-21T01:16:47.000Z</updated>
        <summary type="html"><![CDATA[Hello machine and reinforcement learners!
 This is an announcement and call for participation in the League of Robot Runners, a new ðŸš€ competition and research initiative ðŸš€ that tackles one of the most challenging problems in industrial optimisation: Multi-Robot Path Planning (sometimes also called Multi-Agent Path Finding). 
 Recently launched at ICAPS 2023, the competition is inspired by a variety of new and newly emerging applications that rely on mobile robotics ðŸ¦¾ðŸ¤–. For example, Amazon automated warehouses, where up to thousands of robots work together to ensure safe and efficient package delivery ðŸ§¸ðŸ“¦ ðŸšš â¤ï¸. 
 Participants in the competition are asked to compute coordinated and collision-free movement plans â¤´ï¸ âž¡ï¸ â¤µï¸ ðŸ”„ for a team of robotic errand runners. Get the robots to their dâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context Vectors Embedding [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16o1bol/context_vectors_embedding_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16o1bol/context_vectors_embedding_r/"/>
        <updated>2023-09-21T00:26:30.000Z</updated>
        <summary type="html"><![CDATA[Previously I was spoiled my LangChain.
 Suppose I have a bunch of PDFs that I want to store as context vectors. And I want to use an open-source LLMs. Without using LangChain, how do I generate the context vectors? (I will store it in vector databases)
    submitted by    /u/stephenhky  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] OpenAI Announced DALL-E 3: Art Generator Powered by ChatGPT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16o0tfl/n_openai_announced_dalle_3_art_generator_powered/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16o0tfl/n_openai_announced_dalle_3_art_generator_powered/"/>
        <updated>2023-09-21T00:03:05.000Z</updated>
        <summary type="html"><![CDATA[For those who missed it: DALL-E 3 was announced today by OpenAI, and here are some interesting things:
 No need to be a prompt engineering grand master - DALL-E 3 enables you to use the ChatGPT conversational interface to improve the images you generate. This means that if you didn't like what it produced, you can simply talk with ChatGPT and ask for the changes you'd like to make. This removes the complexity associated with prompt engineering, which requires you to iterate over the prompt.
 Majure improvement in the quality of products compared to DALL-E 2. This is a very vague statement provided by OpenAI, which is also hard to measure, but personally, they haven't failed me so far, so I'm really excited to see the results.
 DALL-E 2 Vs. DALL-E 3, image by OpenAI
 From October, DALL-E 3 will be available through ChatGPT and API for those with the Plus or Enterprise version.
 And there are many more news! ðŸ¤— I've gathered all the information in this blog ðŸ‘‰ https://dagshub.com/blog/dall-e-3/
    submitted by    /u/RepresentativeCod613  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How to build CI/CD pipelines with AWS SageMaker for continuous ML training]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nytb7/p_how_to_build_cicd_pipelines_with_aws_sagemaker/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nytb7/p_how_to_build_cicd_pipelines_with_aws_sagemaker/"/>
        <updated>2023-09-20T22:35:47.000Z</updated>
        <summary type="html"><![CDATA[TL;DR How to build CI/CD pipelines with AWS SageMaker for ML training: https://dagshub.com/blog/ci-cd-for-continuous-training-with-sagemaker/
 One of the most time-resources-energy-consuming tasks we face when we build internal projects is setting up an instance for ML training.
 It's a repetitive process as we run multiple experiments over the project life cycle, with many steps and configurations that are usually documented poorly and scattered around different places.
 As good engineers, we decided to automate the process using CI/CD pipelines. 
 But how?
 We had ZERO ideas on how to do it, so we had to go through the rigorous process of using AWS guides and tutorials to figure it out.
 Usually, when this happens, we extend the project lifecycle and have a team member document the process so we can refer back to it when we need to do it again.
 Knowing this can benefit the community, we decided to share a series of blogs that guide you through building CI/CD pipelines for continuous training with AWS SageMaker.
 We published the first blog, which covers the configuration part a month ago, and we are happy to share the second one which explains how to build a continuous training pipeline for ML.
 Configure AWS SageMaker for CI/CD: https://dagshub.com/blog/setup-sagemaker-for-ci-cd-pipelines/
 How to build CI/CD pipeline with AWS SageMaker for ML training: https://dagshub.com/blog/ci-cd-for-continuous-training-with-sagemaker/
 I'm sure we can improve these tutorials, and would love to learn from your experience on how we can do it! ðŸ¤—
    submitted by    /u/RepresentativeCod613  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is PPO a good algorithm in terms of exploration?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16nxhqs/is_ppo_a_good_algorithm_in_terms_of_exploration/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16nxhqs/is_ppo_a_good_algorithm_in_terms_of_exploration/"/>
        <updated>2023-09-20T21:43:20.000Z</updated>
        <summary type="html"><![CDATA[I recently trained my PPO algorithm on one of my own custom environment. Although the episodic reward increased steadily in the beginning, After some time it just became constant with some occasional positive and negative spikes. I was wondering if it has something to do with the exploration problem with ppo. Any tips on how can I improve it? 
    submitted by    /u/Interesting-Weeb-699  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]How to "productionize" a jupyter notebook in a technical interview?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nwxlv/dhow_to_productionize_a_jupyter_notebook_in_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nwxlv/dhow_to_productionize_a_jupyter_notebook_in_a/"/>
        <updated>2023-09-20T21:22:19.000Z</updated>
        <summary type="html"><![CDATA[Hi redditors!
 I've been tasked with an technical homework that asks me to "productionize" a Jupyter notebook. The notebook's workflow is as follows:
  
Reads data from a CSV.
 Pre-processes the data.
 Trains a machine learning model.
 Makes predictions.
  
My initial thoughts are:
  
Separation of Concerns: Break the notebook down into distinct components - data processing, model training, and inference.
 Containerization: Write a Dockerfile to ensure the environment is reproducible and isolated.
 API for Prediction: Set up a Flask-based service to expose the model's prediction capability.
  
However, I'm grappling with a few challenges:
  
I'm not seeing a stark difference between the Jupyter notebook setup and the production setting. Becuase the model is small and training is not complicated. If I were to deploy to Kubernetes pods, it seems wasteful that many pods are doing the same thing( prediction for the same model, maybe just as high availability guarantee?) .
 Does the training phrase or data cleansing phase need to be containerized and deployed? Because this seems like a one-off process.
 How to deploy a scalable service? I am new to this. Each container may have a HTTP service. How to load balance them on Kubernetes from outside?
 What kind of CI/CD do you recommend for this task? What kind of testing or pipeline are needed?
  
   submitted by    /u/zjplab  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][N] Announcing Zivy Scholar â€“ An AI tool to help researchers consume papers.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nwkfw/pn_announcing_zivy_scholar_an_ai_tool_to_help/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nwkfw/pn_announcing_zivy_scholar_an_ai_tool_to_help/"/>
        <updated>2023-09-20T21:08:08.000Z</updated>
        <summary type="html"><![CDATA[Hi r/MachineLearning
 After a long beta, we are excited to announce Zivy Scholar to the public!
 Zivy Scholar is a tool that allows you to help researchers consume research papers.
 I'm one of the creators and I've found that I want to listen to research papers in the car or when I'm working out.
 Current features include:
  
Listen to and read along with a research paper
 Share the paper with colleagues and friends.
  
We use state of the art pdf data extraction techniques with TTS to provide this functionality.
 Features we're planning:
  
Full PDF to HTML including images, tables, and figures inline for the read-along experience. This includes mobile optimization. This functionality is based on some newer research and we're excited to bring it to you all!
  
Discussion and feedback are welcome!
 Cheers, Collin
    submitted by    /u/collin_code_77  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[School of Engineering welcomes Songyee Yoon PhD â€™00 as visiting innovation scholar]]></title>
        <id>https://news.mit.edu/2023/school-engineering-welcomes-songyee-yoon-visiting-innovation-scholar-0920</id>
        <link href="https://news.mit.edu/2023/school-engineering-welcomes-songyee-yoon-visiting-innovation-scholar-0920"/>
        <updated>2023-09-20T20:50:00.000Z</updated>
        <summary type="html"><![CDATA[A visionary entrepreneur and innovator, Yoon will focus on entrepreneurship, supporting female engineers, and fostering inclusive innovation.]]></summary>
        <author>
            <name>Mary Beth Gallagher | School of Engineering</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[reinforcement learning and rust]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16nw2fo/reinforcement_learning_and_rust/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16nw2fo/reinforcement_learning_and_rust/"/>
        <updated>2023-09-20T20:48:03.000Z</updated>
        <summary type="html"><![CDATA[I'm a somewhat experienced dev, but never did anything related to ML or AI and want to start toying around with ML or reinforcement learning to be specific. Since my language of choice for almost everything is Rust I wanted to ask you guys if you have any advice on how to get started like crates, frameworks etc or if rust just isnt a good fit for ML.
 Thanks in advance for any help!
    submitted by    /u/linus-eing  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intel's 'AI PC']]></title>
        <id>https://www.reddit.com/r/artificial/comments/16nv8g7/intels_ai_pc/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16nv8g7/intels_ai_pc/"/>
        <updated>2023-09-20T20:14:52.000Z</updated>
        <summary type="html"><![CDATA[Intel has announced a new chip, called 'Meteor Lake', that will allow laptops to run generative artificial intelligence chatbots without relying on cloud data centers.
 
This will enable businesses and consumers to test AI technologies without sending sensitive data off their own computers.
 
Intel demonstrated the capabilities of the chip at a software developer conference, showcasing laptops that could generate songs and answer questions in a conversational style while disconnected from the internet.
 
The company sees this as a significant moment in tech innovation.
 
Intel is also on track to release a successor chip called 'Arrow Lake' next year
 
 Source : https://www.reuters.com/technology/intel-says-newest-laptop-chips-software-will-handle-generative-ai-2023-09-19/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring GPT-3.5-turbo vs. GPT-4: Which Model Is Better?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16nuxew/exploring_gpt35turbo_vs_gpt4_which_model_is_better/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16nuxew/exploring_gpt35turbo_vs_gpt4_which_model_is_better/"/>
        <updated>2023-09-20T20:02:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is this Vaporwave or Cyberpunk?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16nunzf/is_this_vaporwave_or_cyberpunk/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16nunzf/is_this_vaporwave_or_cyberpunk/"/>
        <updated>2023-09-20T19:51:30.000Z</updated>
        <summary type="html"><![CDATA[What does this remind you of? 
    submitted by    /u/metairwaves  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Which is the best Model for generation of code?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ntjju/d_which_is_the_best_model_for_generation_of_code/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ntjju/d_which_is_the_best_model_for_generation_of_code/"/>
        <updated>2023-09-20T19:05:11.000Z</updated>
        <summary type="html"><![CDATA[The title basically conveys the entire message. Which according to you is the best AI model for generation of code.
 I mainly code in python with AI and deep learning as a core focus and recently started doing a bit of c++ and thus far have used only bard and ChatGPT 3.5. With this experience of my I can confidently say that GPT outperforms Bard by a huge margin
 There are usually some minor modifications that need to be done and that is part of a devs life isn't it? recently a lot of new models are picking up steam hence was wondering if there exits a model which is better than GPT
 what are your views?
    submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] looking for cs students to collaborate with on the E-Bay ML challenge]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nt01l/p_looking_for_cs_students_to_collaborate_with_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nt01l/p_looking_for_cs_students_to_collaborate_with_on/"/>
        <updated>2023-09-20T18:43:03.000Z</updated>
        <summary type="html"><![CDATA[Looking for CS students to team up with for the E-BAY ML challenge.
 eBay hosts a ML challenge where the winning team gets an internship We already have a team in place and have mad some significant progress, now looking for team members to take us even further.
 https://eval.ai/web/challenges/challenge-page/2014/overview
    submitted by    /u/thelongshortseller  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suggestions of Game-Focused Customizable Environment]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16nsxdq/suggestions_of_gamefocused_customizable/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16nsxdq/suggestions_of_gamefocused_customizable/"/>
        <updated>2023-09-20T18:40:00.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone!
 My research group is looking for new environments that are customizable and 3D such as Unity (and ML Agents). Although the latest changes in Unity TOS shouldn't affect us researchers, we wanted to keep an eye out for potential replacements for it. 
 We are familiar with environments like ViZDoom and DeepMind Lab, but we're looking for more examples. The main requirements are:
  
Customizable environment
 3D Capable
 Free (preferably open-source, but being free is enough)
  
Thanks for the help :D 
    submitted by    /u/romulofff  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Do NeurIPs workshop papers get published?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nsw95/d_do_neurips_workshop_papers_get_published/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nsw95/d_do_neurips_workshop_papers_get_published/"/>
        <updated>2023-09-20T18:38:40.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m submitting to the workshop and was wondering if the papers there get published?
    submitted by    /u/Odd-Distance-4439  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Silent Data Corruption affects Large Model Training [News]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nsjym/silent_data_corruption_affects_large_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nsjym/silent_data_corruption_affects_large_model/"/>
        <updated>2023-09-20T18:25:12.000Z</updated>
        <summary type="html"><![CDATA[Sharing an investigation we did at Adept into some issues that were causing problems with training LLMs at scale. Sometimes the hardware makes silent errors! How we found them and tracked down the problematic machines.
 https://www.adept.ai/blog/sherlock-sdc
    submitted by    /u/ekelsen  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Which specs would be better for a beginner: a 12th gen i7 + RTX 3060 or 13th gen i7 + RTX 3050 (laptop)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ns6x2/d_which_specs_would_be_better_for_a_beginner_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ns6x2/d_which_specs_would_be_better_for_a_beginner_a/"/>
        <updated>2023-09-20T18:10:34.000Z</updated>
        <summary type="html"><![CDATA[I'm looking to buy an upgrade, and at my price point (~ 850 GBP) I can get a new 13th i7 + rtx 3050, or a used gtx3060 + 12th gen i7, both 16GB laptop. Unfortunately I really need the mobility of a laptop in my life at the moment and I realise this isn't ideal.
 I'll be looking and experimenting with basic networks, no language models, maybe a few GANs for fun, some DL networks for the atari gym and other experiments. All this for learning, with my own custom networks for various data.
 I'll be running linux - possibly Debian. I'd love to hear your comments or suggestions if there's something better at my price point in a laptop.
 Thanks
    submitted by    /u/Mean_Actuator3911  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Virtually Incredible: Mercedes-Benz Prepares Its Digital Production System for Next-Gen Platform With NVIDIA Omniverse, MB.OS and Generative AI]]></title>
        <id>https://blogs.nvidia.com/?p=66953</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/20/mercedes-benz-ev-nvidia-omniverse-generative-ai/"/>
        <updated>2023-09-20T18:08:16.000Z</updated>
        <summary type="html"><![CDATA[Mercedes-Benz is using digital twins for production with help from NVIDIA Omniverse, a platform for developing Universal Scene Description (OpenUSD) applications to design, collaborate, plan and operate manufacturing and assembly facilities. Mercedes-Benzâ€™s new production techniques will bring its next-generation vehicle portfolio into its manufacturing facilities operating in Rastatt, Germany; KecskemÃ©t, Hungary; and Beijing, China â€” Read article >]]></summary>
        <author>
            <name>Mike Geyer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nrza8/r_robust_enerf_nerf_from_sparse_noisy_events/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nrza8/r_robust_enerf_nerf_from_sparse_noisy_events/"/>
        <updated>2023-09-20T18:02:00.000Z</updated>
        <summary type="html"><![CDATA[Project Page
 Paper
 Code 
 Abstract:
  
Event cameras offer many advantages over standard cameras due to their distinctive principle of operation: low power, low latency, high temporal resolution and high dynamic range. Nonetheless, the success of many downstream visual applications also hinges on an efficient and effective scene representation, where Neural Radiance Field (NeRF) is seen as the leading candidate. Such promise and potential of event cameras and NeRF inspired recent works to investigate on the reconstruction of NeRF from moving event cameras. However, these works are mainly limited in terms of the dependence on dense and low-noise event streams, as well as generalization to arbitrary contrast threshold values and camera speed profiles. In this work, we propose Robust e-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving event cameras under various real-world conditions, especially from sparse and noisy events generated under non-uniform motion. It consists of two key components: a realistic event generation model that accounts for various intrinsic parameters (e.g. time-independent, asymmetric threshold and refractory period) and non-idealities (e.g. pixel-to-pixel threshold variation), as well as a complementary pair of normalized reconstruction losses that can effectively generalize to arbitrary speed profiles and intrinsic parameter values without such prior knowledge. Experiments on real and novel realistically simulated sequences verify our effectiveness. Our code, synthetic dataset and improved event simulator are public.
  
   submitted by    /u/Sirisian  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Train and deploy ML models in a multicloud environment using Amazon SageMaker]]></title>
        <id>d582043900a682b8a84ab5a54abafb83f63e7804</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/train-and-deploy-ml-models-in-a-multicloud-environment-using-amazon-sagemaker/"/>
        <updated>2023-09-20T16:56:39.000Z</updated>
        <summary type="html"><![CDATA[In this post, we demonstrate one of the many options that you have to take advantage of AWSâ€™s broadest and deepest set of AI/ML capabilities in a multicloud environment. We show how you can build and train an ML model in AWS and deploy the model in another platform. We train the model using Amazon SageMaker, store the model artifacts in Amazon Simple Storage Service (Amazon S3), and deploy and run the model in Azure.]]></summary>
        <author>
            <name>Raja Vaidyanathan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Graphical Models]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=967926</id>
        <link href="https://www.microsoft.com/en-us/research/blog/neural-graphical-models/"/>
        <updated>2023-09-20T16:45:35.000Z</updated>
        <summary type="html"><![CDATA[This research paper was presented at the 17th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (opens in new tab), a premier forum for advances in the theory and practice of reasoning under uncertainty. In the field of reasoning under uncertainty, probabilistic graphical models (PGMs) stand out as a powerful tool for [â€¦]
The post Neural Graphical Models appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL for Tuning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16np0vv/rl_for_tuning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16np0vv/rl_for_tuning/"/>
        <updated>2023-09-20T16:02:55.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I am new to Reinforcement Learning and I am trying to understand how to go about a project Iâ€™m working on. I have a dataset with 2 features (X and Y). 
 I am trying to tune a second order Transfer Function to fit X to Y( has 3 parameters) and gives Yâ€™ as output.
 So I currently consider the error between Y and Yâ€™ as the â€œStateâ€ and try to minimise the error by modelling a reward function that checks for instability and unreasonable values. I am trying to do this using TD3 but the model does not learn. I am wondering if there are any approaches that I should consider.
 I chose TD3 since my action space is continuous. 
 I am sorry if I donâ€™t understand something basic since Iâ€™m a noob to this. Thanks for your help in advance. :) 
 Some more details: the dataset values donâ€™t change. I give the same values for every time step. So I am not understanding how episodes and time steps work in this context.
    submitted by    /u/ninjaaa30  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Savelikeapro.ai: AI powered, WhatsApp driven bookmarking for productivity.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16now5v/n_savelikeaproai_ai_powered_whatsapp_driven/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16now5v/n_savelikeaproai_ai_powered_whatsapp_driven/"/>
        <updated>2023-09-20T15:57:56.000Z</updated>
        <summary type="html"><![CDATA[ðŸ‘‹ Try, savelikeapro.ai, Itâ€™ s, 
 A.I powered. Zero-installation fits in your daily workflow. FREE-forever option
    submitted by    /u/prithivida  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Webinar | How To Choose An AI Vendor For Your Business]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16nomoo/webinar_how_to_choose_an_ai_vendor_for_your/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16nomoo/webinar_how_to_choose_an_ai_vendor_for_your/"/>
        <updated>2023-09-20T15:47:10.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/ta61cz8qlfpb1.jpg?width=1280&format=pjpg&auto=webp&s=ac45085668c9003e5557a7f0c81ae72db6098653
 I hope this webinar finds you well. I suppose that this topic can be interesting for business and AI engineers.
 Speakers: OpenCV CEO Dr. Satya Mallick and Phil Nelson are joined by Anna Kogan CEO at OpenCV.ai
 Topics of webinar are:
  
How to search for vendors that understand your business needs (not all AI is the same.)
 What questions to ask when picking a vendor (not everybody really is an AI expert they claim)
 Three signs to watch for during the project (progress indicators and red-flags)
 How best to scope technical tasks (off-the-shelf vs. custom algorithm development)
 How to set up for long-term success (deployment, documentation, training pipeline)
  
Date: Thursday, September 21st, 2023 at 9am Pacific time.
 Link for the registration
    submitted by    /u/No-Independence5880  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dirty Secrets of BookCorpus, a Key Dataset in Machine Learning]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16noln6/dirty_secrets_of_bookcorpus_a_key_dataset_in/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16noln6/dirty_secrets_of_bookcorpus_a_key_dataset_in/"/>
        <updated>2023-09-20T15:46:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Program generation is all you need? For math, symbolic reasoning, natural language, etc.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16noh6s/r_program_generation_is_all_you_need_for_math/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16noh6s/r_program_generation_is_all_you_need_for_math/"/>
        <updated>2023-09-20T15:41:09.000Z</updated>
        <summary type="html"><![CDATA[The paper introduces improved performance by prompting LLMs with "natural language embedded programs (NLEP)". No task-specific prompt is needed.
 Paper: https://arxiv.org/abs/2309.10814 
 An automatic NLEP generation toolkit is opensourced: https://github.com/luohongyin/langcode
 Example Colab notebook is included in the Github repo.
 This work introduces the following features of NLEP:
  
NLEP is a full python program that prints the target response of LLMs.
 
Task-general NLEP prompting outperforms task-specific chain-of-thought prompting on math, symbolic, and natural language.
 
Enable the chain-of-thought reasoning ability of small models (RoBERTa) on text classification
 
Hierarchical instructing via program completion.
 
    submitted by    /u/SUKHOIHY  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neurons in Large Language Models: Dead, N-gram, Positional]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16nn1w9/neurons_in_large_language_models_dead_ngram/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16nn1w9/neurons_in_large_language_models_dead_ngram/"/>
        <updated>2023-09-20T14:42:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does policy collapse mean?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16nmxn0/what_does_policy_collapse_mean/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16nmxn0/what_does_policy_collapse_mean/"/>
        <updated>2023-09-20T14:37:53.000Z</updated>
        <summary type="html"><![CDATA[I am reading the following article on SpinningUp but can't get my head around policy collapse - 
 "This is different from normal policy gradient, which keeps new and old policies close in parameter space. But even seemingly small differences in parameter space can have very large differences in performanceâ€”so a single bad step can collapse the policy performance. This makes it dangerous to use large step sizes with vanilla policy gradients, thus hurting its sample efficiency. TRPO nicely avoids this kind of collapse, and tends to quickly and monotonically improve performance." 
 Why would updating the parameters lead to a policy collapse? The parameters are updated based on the performance of the RL system and therefore, I don't see the point of constraints.
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Implications of Unequal Fold Sizes in Cross-Validation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nm0a9/p_implications_of_unequal_fold_sizes_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nm0a9/p_implications_of_unequal_fold_sizes_in/"/>
        <updated>2023-09-20T13:58:59.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m working on a project where I initially split my dataset into k equally sized folds for cross-validation. However, I want to perform some additional sampling operations on the training set within each fold, which would result in varying fold sizes.
 Is this practice acceptable, or does it violate the premises of traditional cross-validation? If so, are there any papers or resources that explore the implications of varying fold sizes in cross-validation?
 Thank you for your insights!
    submitted by    /u/Leading_Complex7425  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Running LLM on desktop/mobile (Hybrid distant/local)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nltc7/p_running_llm_on_desktopmobile_hybrid_distantlocal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nltc7/p_running_llm_on_desktopmobile_hybrid_distantlocal/"/>
        <updated>2023-09-20T13:50:18.000Z</updated>
        <summary type="html"><![CDATA[Hey, I was checking out tauri last week. 
 I was so blown away that I wrote a bit more recently and wrote hf.co/chat desktop/mobile API + local LLM. 
 https://github.com/Narsil/hf-chat 
 Just thought I should share in case others are interested, and wanted to make a bit shoutout to tauri team, the docs, and overall UX is top notch, basically everything pretty much worked out of the box without any hiccup.
    submitted by    /u/narsilouu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI generated childhood 'memories' (ModelScope) with post processing (old video tape style) and my own music (retro analogue synths).]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16nlnw2/ai_generated_childhood_memories_modelscope_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16nlnw2/ai_generated_childhood_memories_modelscope_with/"/>
        <updated>2023-09-20T13:43:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/glenniszen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] NeurIPS 2023 paper acceptance results]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nljrn/d_neurips_2023_paper_acceptance_results/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nljrn/d_neurips_2023_paper_acceptance_results/"/>
        <updated>2023-09-20T13:38:32.000Z</updated>
        <summary type="html"><![CDATA[NeurIPS 2023 paper acceptance results are supposed to be released at 8 pm (CDT) on September 21. I thought to create a discussion thread for us to countdown and discuss any celebration/issue/complaint/feedback or anything else.
 There is so much noise in the reviews every year. Some good work that the authors are proud of might get rejected because of the noisy system, given that NeurIPS is growing so large these years. We should keep in mind that the work is still valuable no matter what the final result is.
    submitted by    /u/Apprentice12358  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16njq9h/qtransformer_scalable_offline_reinforcement/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16njq9h/qtransformer_scalable_offline_reinforcement/"/>
        <updated>2023-09-20T12:13:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Attention mechanism issue]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16njeju/d_attention_mechanism_issue/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16njeju/d_attention_mechanism_issue/"/>
        <updated>2023-09-20T11:57:54.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm trying to train a multi-modal transformer for Activity Recognition. I employed a two-stream architecture, where one is a Transformer that takes in a sequence of skeleton trajectory, and the latter is a pre-trained Vision Transformer. When I train the model, and investigate the attention weights for the Transformer which takes in skeleton trajectories, I noticed that all of them are approximately the same (~0.029). I'm not really sure what is going on here, I've tried changing the learning rate, tried different optimizers(SGD, Adam). But still I'm not really sure what is causing this. Please suggest me some debugging steps, or what should I look at anything in particular that causes this
    submitted by    /u/Terrible-Ad6239  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zoomposium with Professor Dr. Petra Ritter: "The simulation of brains"]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16nix5t/zoomposium_with_professor_dr_petra_ritter_the/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16nix5t/zoomposium_with_professor_dr_petra_ritter_the/"/>
        <updated>2023-09-20T11:32:23.000Z</updated>
        <summary type="html"><![CDATA[Zoomposium with Professor Dr. Petra Ritter: "The simulation of brains"
 In another installment in our "Zoomposium Series" on the topic of "Brain Research", my colleague Axel StÃ¶cker of the "Blog der groÃŸen Fragen" and I had the great honor and pleasure of conducting an interview with the very well-known and renowned German medical doctor and neuroscientist Professor Dr. Petra Ritter.
 In this context, Ms. Ritter became a co-founder and leader of the co-design project "The #Virtual #Brain", which is a component of the European Open Science Cloud (EOSC) and is "a neuroinformatics platform for simulating whole brain networks using biologically realistic connectivity".
 She is leading the development of a virtual research environment as a collaborative research platform for sensitive health data and head of the "German National Neuroscience Research Infrastructure Initiative (NFDI-Neuroscince)" and involved in the development of the "Health Data Cloud #EBRAINS".
 Petra Ritter has been Johanna Quandt Professor and Head of the Section for Brain Simulation at the Department of Neurology with Experimental Neurology at CharitÃ© - UniversitÃ¤tsmedizin Berlin since 2017.
 There, Professor Ritter and her team are involved in the "Simulation of Brains".
 More at: https://philosophies.de/index.php/2023/09/17/die-simulation-von-gehirnen/
 â€‹
 https://preview.redd.it/xiurryebcepb1.jpg?width=1000&format=pjpg&auto=webp&s=a7a8b6ba563cfc8f0d052bc6f3da27e2a5703a0a
    submitted by    /u/philosophiesde  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Zoomposium with Professor Dr. Petra Ritter: "The simulation of brains"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16niwm7/d_zoomposium_with_professor_dr_petra_ritter_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16niwm7/d_zoomposium_with_professor_dr_petra_ritter_the/"/>
        <updated>2023-09-20T11:31:39.000Z</updated>
        <summary type="html"><![CDATA[Zoomposium with Professor Dr. Petra Ritter: "The simulation of brains"
 In another installment in our "Zoomposium Series" on the topic of "Brain Research", my colleague Axel StÃ¶cker of the "Blog der groÃŸen Fragen" and I had the great honor and pleasure of conducting an interview with the very well-known and renowned German medical doctor and neuroscientist Professor Dr. Petra Ritter.
 In this context, Ms. Ritter became a co-founder and leader of the co-design project "The #Virtual #Brain", which is a component of the European Open Science Cloud (EOSC) and is "a neuroinformatics platform for simulating whole brain networks using biologically realistic connectivity".
 She is leading the development of a virtual research environment as a collaborative research platform for sensitive health data and head of the "German National Neuroscience Research Infrastructure Initiative (NFDI-Neuroscince)" and involved in the development of the "Health Data Cloud #EBRAINS".
 Petra Ritter has been Johanna Quandt Professor and Head of the Section for Brain Simulation at the Department of Neurology with Experimental Neurology at CharitÃ© - UniversitÃ¤tsmedizin Berlin since 2017.
 There, Professor Ritter and her team are involved in the "Simulation of Brains".
 More at: https://philosophies.de/index.php/2023/09/17/die-simulation-von-gehirnen/
 â€‹
 https://preview.redd.it/3cpni6o6cepb1.jpg?width=1000&format=pjpg&auto=webp&s=998c30d16ddae30511b7983abce7802dfdd54945
    submitted by    /u/philosophiesde  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] From Sparse to Dense : GPT-4 Summarization with Chain of Density Prompting]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16nhq2n/r_from_sparse_to_dense_gpt4_summarization_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16nhq2n/r_from_sparse_to_dense_gpt4_summarization_with/"/>
        <updated>2023-09-20T10:24:31.000Z</updated>
        <summary type="html"><![CDATA[The following example implements the technique from the paper "From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting", Adams et al. (2023).
 Edit : the library used is py-llm-core
 ```python from typing import List from dataclasses import dataclass from llm_core.assistants import OpenAIAssistant
 @dataclass class DenseSummary: denser_summary: str missing_entities: List[str]
 @dataclass class DenserSummaryCollection: system_prompt = """ You are an expert in writing rich and dense summaries in broad domains. """
 prompt = """ Article: {article} ---- You will generate increasingly concise, entity-dense summaries of the above Article. Repeat the following 2 steps 5 times. - Step 1: Identify 1-3 informative Entities from the Article which are missing from the previously geneâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMind founder says AI existential risk 'completely bonkers distraction']]></title>
        <id>https://www.reddit.com/r/artificial/comments/16nh65q/deepmind_founder_says_ai_existential_risk/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16nh65q/deepmind_founder_says_ai_existential_risk/"/>
        <updated>2023-09-20T09:51:07.000Z</updated>
        <summary type="html"><![CDATA[Mustafa Suleyman, co-founder of DeepMind, believes that concerns about the existential risks of AI are a distraction from more practical issues such as privacy and bias.
 
He is confident that governments can effectively regulate AI using frameworks that have been successful in the past, citing the regulation of aviation and the internet as examples.
 
Suleyman emphasizes the importance of setting boundaries and limits for AI to ensure human oversight and enforceable laws.
 
He calls for a combination of broad, international regulation and smaller, more granular policies at the micro level.
 
Suleyman suggests limiting AI's ability to improve itself as a critical first step in ensuring human oversight.
 
He also highlights the need for governments to have direct access to AI developers to enforce boundaries and establish clear regulations.
 
Governments worldwide, including the European Union and China, are already working on AI regulations.
 
 Source : https://fortune.com/2023/09/19/ai-existential-risk-threat-bonkers-distraction-regulation-deepmind-mustafa-suleyman/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Looking for projects and collaboration in the field of neuroscience-inspired intelligent agents]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ngzki/p_looking_for_projects_and_collaboration_in_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ngzki/p_looking_for_projects_and_collaboration_in_the/"/>
        <updated>2023-09-20T09:40:06.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I have been looking through a few open source projects for LLM, but without much success in finding some integrating approaches from neuroscience of human intelligence.
 That why I am here to ask for projects and collaborations either academical, non-for-profit / open source or commercial.
 It would be great if your can give me some directions for this.
 Thanks
    submitted by    /u/confluence_84  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] "Contrastive Decoding Improves Reasoning in Large Language Models", O'Brien & Lewis 2023 (boosts LLaMA-8B to >GPT-3.5/PaLM-540B on GSM8K)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ngtyh/r_contrastive_decoding_improves_reasoning_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ngtyh/r_contrastive_decoding_improves_reasoning_in/"/>
        <updated>2023-09-20T09:29:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/InterviewIntrepid889  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suggestions for how to use AI for a commercial office fit-out business?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ngkqy/suggestions_for_how_to_use_ai_for_a_commercial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ngkqy/suggestions_for_how_to_use_ai_for_a_commercial/"/>
        <updated>2023-09-20T09:13:03.000Z</updated>
        <summary type="html"><![CDATA[A friend of mine has a small 12 person company that does office refurbishment and commercial redesign projects - he has asked me to speak to his team to give them an AI 101 - and to tell them a little about how they should be looking into using AI tools for their company.
 I know there are plenty of tools and apps that take photos of a bare room and make them look like a design magazine - any in particular I should show them? Can anyone think of other tools that they should look into using, or how do people in this space currently use AI?
 â€‹
    submitted by    /u/zascar  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPO Action masking in SB3]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16nge79/ppo_action_masking_in_sb3/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16nge79/ppo_action_masking_in_sb3/"/>
        <updated>2023-09-20T09:01:20.000Z</updated>
        <summary type="html"><![CDATA[I'm using ppo action masking in sb3, which works well in training and masking the illegal / invalid actions according to my criteria. However, I have 2 problems during testing.
 â€‹
  
I use the same code for masking the actions in testing too since the model was unable to learn by itself during training action masking criteria and when enforcing it in code, it still doesn't mask anything.
  
â€‹
  
The model converges to just one action in testing phase after 500k steps, the episode reward still increasing the training phase though. I don't know why? maybe overfitting!
  
Code for training with action masking:
 def mask_fn(env: gym.Env) -> np.ndarray:
 return env.valid_action_mask()
 env = StockEnv(train)
 # Wrap the environment with ActionMasker and the mask_fn function
 env = ActionMasker(env, mask_fn)
 model = MaskablePPO(MaskableActorCriticPolicy, env, tensorboard_log="./tensorboard" ,n_steps=2048 )
 for i in range (1,52):
 model.learn(total_timesteps=TIMESTEPS , tb_log_name = 'PPO2' , reset_num_timesteps=False)
 Code for testing with action masking:
 def mask_fn(env: gym.Env) -> np.ndarray:
 return env.valid_action_mask()
 env = ActionMasker(env, mask_fn)
 model_path = f"{models_dir}/700000.zip"
 model = MaskablePPO.load(model_path, env=env)
 episodes = 1
 for ep in range(episodes):
 obs = env.reset()
 done = False
 while not done:
 action, _states = model.predict(obs)
 obs, rewards, done, info = env.step(action)
 env.render() 
    submitted by    /u/Acceptable_Egg6552  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knew to RL.Some question about the reward setting]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16nfhhy/knew_to_rlsome_question_about_the_reward_setting/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16nfhhy/knew_to_rlsome_question_about_the_reward_setting/"/>
        <updated>2023-09-20T08:01:46.000Z</updated>
        <summary type="html"><![CDATA[The env is like mutil routing,for example i have 10 nodes in a map(2-d array), and i need to route them to the edge of the map,but the point where the node in the edge need to obey some rules,like the order need to be clockwise like below
 https://preview.redd.it/t9jlxvix8dpb1.png?width=545&format=png&auto=webp&s=16719a2dbd11333c454bd2ee71b87895a6414371
 so now my basic setting is,i use the action mask to invalid some forbidden action. When a node is routing to the edge of the map,i give +10 reward,if it's not obey the clockwise rule,i give -200 reward, if the whole node is routed success,i give +200 reward,and each steps does'n make any node routed, i give -1 reward.
 I am new to this area,i don't know the way i set reward if is good,may i have some advide?
    submitted by    /u/Street_Helicopter_31  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/19/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16nc8tq/oneminute_daily_ai_news_9192023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16nc8tq/oneminute_daily_ai_news_9192023/"/>
        <updated>2023-09-20T04:44:28.000Z</updated>
        <summary type="html"><![CDATA[Software company Digimarc will now let copyright owners add more information to their work, which the company said will improve how AI models treat copyright in training data.[1]
 AlphaMissense, a new model from Googleâ€™s artificial intelligence team, analyzes the effects of DNA mutations and will accelerate research into rare diseases.[2]
 Googleâ€™s AI assistant can now read your emails, plan trips, â€œdouble-checkâ€ answers.[3]
 Teens using AI to generate nude deep fakes to bully, harass classmates, FBI expert warns.[4]
  
Sources:
 [1] https://www.theverge.com/2023/9/19/23879555/digimarc-copyright-watermark-generative-ai
 [2] https://www.wired.co.uk/article/deepmind-ai-alphamissense-genetics-rare-diseases
 [3] https://arstechnica.com/information-technology/2023/09/googles-ai-assistant-can-now-read-your-emails-plan-trips-double-check-answers/
 [4] https://www.news5cleveland.com/news/local-news/teens-using-ai-to-generate-nude-deep-fakes-to-bully-harass-classmates-fbi-expert-warns 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bard Gets a Major Upgrade]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16nb1qt/bard_gets_a_major_upgrade/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16nb1qt/bard_gets_a_major_upgrade/"/>
        <updated>2023-09-20T03:38:19.000Z</updated>
        <summary type="html"><![CDATA[Google's Bard chatbot is extending its abilities with access to personalized Google apps and services including Gmail, Docs, Drive, Maps, YouTube, and Google Flights and hotels.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/s1ivsummzbpb1.png?width=1600&format=png&auto=webp&s=d8fc3262ba542b950de3c8a85b9a987763270ada
 Bard Extensions and Google Applications Integration
  
Bard can now tap into individualized data from Google apps such as Gmail, Drive, and Docs, with user permission.
 Google reassured users that personal data accessed by Bard will not be used for reinforcement learning, providing another on their transparency, choice, and control tenets.
 Prompts can direct Bard to search for specific information within Gmail, but it does not store the entire inbox content.
  
Collaborative Characteristics and Fact-Checking Capabilities
  
Users can employ Bard to summarize emails or gather trip details from email threads, and then research real-time travel information, surface YouTube recommendations for the destination, and provide Maps directions to the airport.
 Google's chatbot can also double-check its responses against Google search, improving user trust and enhancing Bard's model through user feedback on incorrect answers.
 The new extensions using non-personal data â€“ YouTube, Flights, Hotels, and Maps â€“ are automatically opted-in but users can choose to opt-out.
  
Collaboration and Language Availability
  
Bard now lets users share an ongoing chat with others through a public link.
 Google plans to expand Bard's feature set to over 40 new languages beyond its existing English language capabilities.
  
(source)
 P.S. If you like this kind of analysis, I put out a free newsletter covering the latest and most pertinent news and research in AI. Regular readers include professionals from Google, Meta, and OpenAI.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are there any good math Datasets for Training small models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16n6rb5/d_are_there_any_good_math_datasets_for_training/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16n6rb5/d_are_there_any_good_math_datasets_for_training/"/>
        <updated>2023-09-20T00:13:48.000Z</updated>
        <summary type="html"><![CDATA[I've seen Allen AI's Lila Dataset, and I want to use this for a small model, to turn math to code. However, I dont think a small dataset in 300k rows is enough. Does anyone know of any bigger, similar datasets?
    submitted by    /u/vatsadev  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Starting to get the impression I'm legit going to be replaced]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16n6kvk/starting_to_get_the_impression_im_legit_going_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16n6kvk/starting_to_get_the_impression_im_legit_going_to/"/>
        <updated>2023-09-20T00:05:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/guh-eye  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oracle Cloud Infrastructure Offers New NVIDIA GPU-Accelerated Compute Instances]]></title>
        <id>https://blogs.nvidia.com/?p=66948</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/19/oracle-cloud-infrastructure-nvidia-gpu-accelerated-compute-instances/"/>
        <updated>2023-09-19T23:40:06.000Z</updated>
        <summary type="html"><![CDATA[With generative AI and large language models (LLMs) driving groundbreaking innovations, the computational demands for training and inference are skyrocketing. These modern-day generative AI applications demand full-stack accelerated compute, starting with state-of-the-art infrastructure that can handle massive workloads with speed and accuracy. To help meet this need, Oracle Cloud Infrastructure today announced general availability of Read article >]]></summary>
        <author>
            <name>Dave Salvator</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Optimizer that makes CNNs learn in fewer iterations]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16n5r3x/p_optimizer_that_makes_cnns_learn_in_fewer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16n5r3x/p_optimizer_that_makes_cnns_learn_in_fewer/"/>
        <updated>2023-09-19T23:29:00.000Z</updated>
        <summary type="html"><![CDATA[Hi all.
 I have been tinkering with a project to get quicker learning for CNNs. 
 The idea came after reading the SDProp paper. Algorithms using adaptive learning rate can be interpeted as muliplying the gradient(with or without momentum) with the inverse square-root of the covariance matrix. Using a diagonal estimate of the covariance matrix.
 Which begs the question: what happens if we use a fuller estimate? I chose to include covariances between the elements of convolutional filters. I.e. a conv.weight of size [n_out,n_in,5,5] needs a tensor of size [n_out,n_int,25,25] to store its contribution to the covariance matrix.
 for 3x3 filters and 5x5 filters, torch.linalg.eigh could be used to calculate the square root of the covariance matrices. For 7x7, I used newtons method to approximate the square root.
 In the figure below are some results for a 6 layer CNN on CIFAR 100. Huge gains iteration for iteration. But is it quicker? Not a lot. A bit for the smaller 3x3 filter. More optimizations could still be made. And it will obviously depend on network architecture and computer hardware. 
 I'm sure there could be some use-cases. The computation of the square-root calculations is invariant to batch_size and image_size (unless number of filters also is increased).
 If anyone is interested I can also link to my torch implementation of the optimizer, once I get it up on github.
 Not sure if this, or something like it, has been done before? Would love to have some papers linked if so...
 https://preview.redd.it/kyy0ogr0qapb1.jpg?width=714&format=pjpg&auto=webp&s=96ac499fb8ab35ce13e7c59bbe3dbc94ba275b9c
 https://preview.redd.it/p52zllr0qapb1.jpg?width=342&format=pjpg&auto=webp&s=8663a9c3c782d192b16289a735b53da6a8d29c47
    submitted by    /u/maka89  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Help with Peft using Lora]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16n4bed/d_help_with_peft_using_lora/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16n4bed/d_help_with_peft_using_lora/"/>
        <updated>2023-09-19T22:26:28.000Z</updated>
        <summary type="html"><![CDATA[Can someone provide like a step by step example notebook of how to use LORA for peft. I saw too many videos and articles online and Im really confused rn.
    submitted by    /u/HazSylvia  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Help with Peft using Lora]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16n4bdn/d_help_with_peft_using_lora/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16n4bdn/d_help_with_peft_using_lora/"/>
        <updated>2023-09-19T22:26:27.000Z</updated>
        <summary type="html"><![CDATA[Can someone provide like a step by step example notebook of how to use LORA for peft. I saw too many videos and articles online and Im really confused rn.
    submitted by    /u/HazSylvia  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Optimizing Transformer Architecture for Multi-Dimensional Sensor Data in Clinical Study]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16n479f/d_optimizing_transformer_architecture_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16n479f/d_optimizing_transformer_architecture_for/"/>
        <updated>2023-09-19T22:21:26.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 I am currently working on a project where my team and I have collected a rich dataset of biomedical sensor data from clinical trials earlier this year. Our aim is to use this sensor data to predict changes in specific biomarkers over time. The data's tensor shape is B,T,F,C, where:
  
B = batch size
 T = sequence length
 F = sampled frequencies
 C = features at each frequency
  
Currently, my approach involves flattening this tensor to B,T,âˆ’1 and then feeding it to a transformer model. While this has yielded reasonable results, I'm contemplating whether there are more effective ways to prepare the data for the transformer model.
 Here are my specific concerns:
  
Flattening the tensor might dilute the information specific to each frequency across various features.
 I could potentially miss the chance to capture frequency-related variations within the features.
  
To address these, I've considered a few options:
  
Self-attention over individual features or frequencies: Although this could be effective, it might make the model too large given my medium-sized dataset.
 Using convolutional layers: Preliminary experiments with this approach have not led to any significant improvements.
  
I'm particularly interested in any thoughts on how to make my transformer more receptive to the multi-dimensional nature of my dataset. Increasing the number of attention heads to better accommodate all features is also on the table.
 Does anyone have any insights or can point me to relevant papers or codebases for handling such multi-dimensional data with transformers?
 Thank you for your help!
    submitted by    /u/BiomedEngineer_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn From Computer Vision Industry Experts - Runway, Pepsi, AWS, and SoftServe [N]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16n3pc7/learn_from_computer_vision_industry_experts/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16n3pc7/learn_from_computer_vision_industry_experts/"/>
        <updated>2023-09-19T21:59:15.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I think this will be useful for people in this group who are working on computer vision or vision AI applications.
 There's a free online event about vision AI where industry experts from Runway, Pepsi, AWS, and SoftServe will share how they are using CV software in developing their use cases or applications.
 Register here (https://nvda.ws/3t23idp), if you are interested.
 If you have any questions, please leave a comment and I will do my best to respond as soon as possible.
    submitted by    /u/Designer-Comb-7144  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Headless Language Models: Learning without Predicting with Contrastive Weight Tying]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16n3adf/r_headless_language_models_learning_without/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16n3adf/r_headless_language_models_learning_without/"/>
        <updated>2023-09-19T21:40:40.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.08351
 â€‹
  
Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies. In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT). We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts. Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency. We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.
  
â€‹
 Comparison of our approach vs. classical MLM within same compute budgets
 The Contrastive Weight Tying approach
 â€‹
    submitted by    /u/nthngdy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How does policy learning scale for personalization systems ?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16n2irh/how_does_policy_learning_scale_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16n2irh/how_does_policy_learning_scale_for/"/>
        <updated>2023-09-19T21:09:10.000Z</updated>
        <summary type="html"><![CDATA[I cannot wrap my head around how for e.g. a playlist building RL agent would perform on such a personal level ?
 What features would it use and would they be personal and general enough at the same time to select the best next song. Same goes for Netflix's recsys.
    submitted by    /u/JurrasicBarf  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet the 2023-24 Accenture Fellows]]></title>
        <id>https://news.mit.edu/2023/accenture-fellows-advance-technology-crossroads-business-society-0919</id>
        <link href="https://news.mit.edu/2023/accenture-fellows-advance-technology-crossroads-business-society-0919"/>
        <updated>2023-09-19T20:45:00.000Z</updated>
        <summary type="html"><![CDATA[The MIT and Accenture Convergence Initiative for Industry and Technology announces the 2023-24 graduate fellows.]]></summary>
        <author>
            <name>School of Engineering</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Four Lincoln Laboratory technologies win five 2023 R&D 100 awards]]></title>
        <id>https://news.mit.edu/2023/lincoln-laboratory-technologies-win-rd-world-awards-0919</id>
        <link href="https://news.mit.edu/2023/lincoln-laboratory-technologies-win-rd-world-awards-0919"/>
        <updated>2023-09-19T20:35:34.000Z</updated>
        <summary type="html"><![CDATA[Inventions in medical imaging, aircrew scheduling, data security, and quantum networking are named among the yearâ€™s most innovative new products.]]></summary>
        <author>
            <name>Kylie Foy | MIT Lincoln Laboratory</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 19 September 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=63209</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-19-september-2023/"/>
        <updated>2023-09-19T19:24:46.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 19 September 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schneider Electric warns that existing datacenters aren't buff enough for AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mztmt/schneider_electric_warns_that_existing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mztmt/schneider_electric_warns_that_existing/"/>
        <updated>2023-09-19T19:22:17.000Z</updated>
        <summary type="html"><![CDATA[Schneider Electric warns that existing datacenters may not be equipped to handle the demands of AI workloads, which require low-latency, high-bandwidth networking and power delivery.
 
The company suggests reevaluating the way datacenters are built to optimize them for AI.
 
The challenges include the need for liquid-cooled servers, higher voltage power distribution, and efficient heat rejection.
 
Schneider provides guidance on changes to power, cooling, rack configuration, and software management to mitigate the demands of AI adoption.
 
Liquid cooling is recommended for high-density racks, with direct liquid cooling favored over immersion cooling systems.
 
 Source : https://www.theregister.com/2023/09/19/schneider_electric_ai_dc/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mini gaming pc [Project]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mzhfb/mini_gaming_pc_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mzhfb/mini_gaming_pc_project/"/>
        <updated>2023-09-19T19:08:05.000Z</updated>
        <summary type="html"><![CDATA[https://www.amazon.com/Gaming-6900HX-Channel-Computers-Desktop/dp/B0CB3JLBQ4/ref=mp_s_a_1_2?crid=LZHUL5EOU6F0&keywords=refurbished+server+with+rtx+gpu&qid=1695146558&sprefix=refurbished+server+with+rtx+gpu%2Caps%2C146&sr=8-2 Would this be suitable to do basic machine learning?
    submitted by    /u/stoned_chemist_dude  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New â€˜Physics-Inspiredâ€™ Generative AI Exceeds Expectations]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16mynyu/new_physicsinspired_generative_ai_exceeds/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16mynyu/new_physicsinspired_generative_ai_exceeds/"/>
        <updated>2023-09-19T18:35:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet the Omnivore: Industrial Designer Blends Art and OpenUSD to Create 3D Assets for AI Training]]></title>
        <id>https://blogs.nvidia.com/?p=66954</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/19/industrial-designer-blender-openusd-ai/"/>
        <updated>2023-09-19T18:15:19.000Z</updated>
        <summary type="html"><![CDATA[Editorâ€™s note: This post is a part of our Meet the Omnivore series, which features individual creators and developers who use NVIDIA Omniverse and OpenUSD to accelerate their 3D workflows and create virtual worlds. As a student at the Queensland University of Technology (QUT) in Australia, Emily Boehmer was torn between pursuing the creative arts Read article >]]></summary>
        <author>
            <name>Nicole Castro</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Xwin-LM surpasses GPT-4 ??? Has RLHF been worked out by open source community???]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mxztt/n_xwinlm_surpasses_gpt4_has_rlhf_been_worked_out/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mxztt/n_xwinlm_surpasses_gpt4_has_rlhf_been_worked_out/"/>
        <updated>2023-09-19T18:07:14.000Z</updated>
        <summary type="html"><![CDATA[It seems that Alpaca Eval Leaderboard is in the past ...
 Xwin-LM surpasses GPT-4 now:
 https://preview.redd.it/gyzi98nn59pb1.png?width=2205&format=png&auto=webp&s=ca401e603efe521faeeeccde8410d3dbdd6741da
 They also mentioned RLHF "plays crucial role in the strong performance of Xwin-LM-V0.1 release"...
 https://preview.redd.it/20sjx73r59pb1.png?width=1047&format=png&auto=webp&s=2255fc652e43674515882f01c0708369fdef56a4
 Are we seeing open source community finally work out how to do RLHF for LLMs???
    submitted by    /u/llm_nerd  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1000+ Top AI Tools Directory]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mxma9/1000_top_ai_tools_directory/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mxma9/1000_top_ai_tools_directory/"/>
        <updated>2023-09-19T17:51:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] C++ for ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mwy2p/d_c_for_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mwy2p/d_c_for_ml/"/>
        <updated>2023-09-19T17:24:24.000Z</updated>
        <summary type="html"><![CDATA[Hi I wanted to learn ML with C++, I've already done some ML stuff in python, but I wanted to challenge myself by using C++
 I hear from some people that I won't get anything from it if want to be serious within ML - which I'm not entirely sure I want to
 Are they right? Should I rather stick with python for ML?
    submitted by    /u/Potential_Wealth_830  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Spam Detection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mwua6/d_spam_detection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mwua6/d_spam_detection/"/>
        <updated>2023-09-19T17:20:00.000Z</updated>
        <summary type="html"><![CDATA[Hi!
 Let me preface this by saying that I am not well-versed in the ML/AI literature. Please excuse my ignorance.
 I am trying to create a system to detect whether some given data is spam or not. Is there a good, out-of-the-box solution for this? I imagine there would be. I am currently using heuristics but I'm wondering if there is a better, ML-y solution.
 My ideal solution would have the following attributes:
  
Simple
 Open-source
 Very cheap to test whether something is spam (less than $0.00001 per test)
 Very fast to test (less than 50ms per test)
 Quick to "figure out" what is spam and what is not (less than 100,000 labeled data)
 Does not require a lot of set-up or up-keep (less than 5 days set up; less than 1 hr up-keep per month)
  
It doesn't have to be perfect. I'm just looking to set up something quickly for now and gauge it vs heuristics.
 Thank you.
 ---
 Edit: To clarify, I'm looking for something I can ideally build myself with open source software. And not specifically email.
 Just looking for the right direction. Names of OSS, techniques, etc.
    submitted by    /u/Acrobatic-You-3279  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[I read the paper for you]: Researchers announce CulturaX - a new multilingual dataset for AI with 6 trillion words across 167 languages]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mvvcy/i_read_the_paper_for_you_researchers_announce/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mvvcy/i_read_the_paper_for_you_researchers_announce/"/>
        <updated>2023-09-19T16:40:18.000Z</updated>
        <summary type="html"><![CDATA[I read the Arxiv paper on CulturaX so you don't have to. Here's my highlights:
  
New open dataset called CulturaX contains text data for 167 languages - far more than previous datasets.
 With over 6 trillion words, it's the largest multilingual dataset ever released.
 Freely available for anyone to use for research and AI development.
 Created by combining and extensively cleaning two other large datasets - mC4 and OSCAR.
 Could allow developing AI systems that work much better across many more languages.
 Helps democratize access to data to build fairer, less biased AI models.
 Allows training of new multilingual AI applications, like universal translators and assistants.
 But still requires thoughtfulness to avoid issues like bias amplification.
  
Overall, CulturaX is going to be part of a broader global trend (I think) to advance multilingual AI and spread its benefits more equally. So far they've been concentrated in English-speaking applications.
 Full summary here if you'd like to read more. Original paper is here.
    submitted by    /u/Successful-Western27  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best python AI/ML/DL learning/practice material?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mvnf7/d_best_python_aimldl_learningpractice_material/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mvnf7/d_best_python_aimldl_learningpractice_material/"/>
        <updated>2023-09-19T16:31:33.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m in search of a good textbook or something that will show me how to use python to implement machine learning. I would seriously appreciate any type of helpful guide that teaches ML and deep learning using python. Hereâ€™s a little about me and my experience:
 Graduated under grad with Bachelorâ€™s in CS. In school took a ton of stats, ai classes, algorithms classes, data science and linear algebra and did well but my school didnâ€™t really use python or do a ton of programming for hw or exams. (Data science was the one class that used python)
 My programming is pretty good nonetheless. I currently work as a full stack devops engineer for a cybersecurity startup and regularly work with python, Django MySQL, etc on the backend and JavaScript and various frontend frameworks for the front end. 
 I really appreciate yallâ€™s help.
 In particular Iâ€™m looking for good ai/ml/deep learning books that teach concepts and also teach with python code and have some coding projects. 
 Thank you!
    submitted by    /u/hydrated-terpman  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What GPU to buy for faster LLM training ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mvmkv/d_what_gpu_to_buy_for_faster_llm_training/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mvmkv/d_what_gpu_to_buy_for_faster_llm_training/"/>
        <updated>2023-09-19T16:30:39.000Z</updated>
        <summary type="html"><![CDATA[I need some advice about what hardware to buy in order to build an ML / DL workstation for home private experiments, i intend to play with different LLM models, train some and try to tweak the way the models are built and understand what impact training speeds, so i will have to train, learn the results, tweak the model / data / algorithms and train again...
 i intend to use large data samples,
 due to board limitations (ASRock Taichi X399 TR4, CPU: AMD Threadripper 1950x),
 i can either buy:
 2 x nVidia Tesla T4 (16G GDDR6 / 2560 CUDA / 0.585 GHz / ~800$)
 -- or --
 2 x nVidia Tesla M10 (4 x 8G GDDR5 / 2560 CUDA / 1.03 GHz / ~780$)
 -- or --
 4 x nVidia Tesla P40 (24G GDDR5X / 3840 CUDA / 3.5 GHz / ~120$)
 -- or --
 4 x nVidia Tesla K80 (2 x 12G GDDR5 / 4992 CUDA / 2.7 GHz / ~200$)
 -- or --
 1 x nVidia RTX 4080 (16G GDDR6X / 9728 CUDA / 2.51 GHz / ~1450$)
 i know that i will need to air vent the Tesla models, the question is what is faster for training time (i have read all the Tflops / OPS / int / 16float / 32float / 64float ... i got to admit it is all very confusing)
 what would you do and for what reason ?
 any advice will be appreciated
    submitted by    /u/Particular_Flower_12  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Engineer 2023 roadmap]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mvi0g/ai_engineer_2023_roadmap/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mvi0g/ai_engineer_2023_roadmap/"/>
        <updated>2023-09-19T16:25:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/rbagdiya  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI and multi-modal agents in AWS: The key to unlocking new value in financial markets]]></title>
        <id>1ec3e0ea3ef6c904f91e4c2db436ba1cfed7514b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/generative-ai-and-multi-modal-agents-in-aws-the-key-to-unlocking-new-value-in-financial-markets/"/>
        <updated>2023-09-19T16:23:49.000Z</updated>
        <summary type="html"><![CDATA[Multi-modal data is a valuable component of the financial industry, encompassing market, economic, customer, news and social media, and risk data. Financial organizations generate, collect, and use this data to gain insights into financial operations, make better decisions, and improve performance. However, there are challenges associated with multi-modal data due to the complexity and lack [â€¦]]]></summary>
        <author>
            <name>Sovik Nath</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Here is VEDV for develop your AI App development]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mvfkq/here_is_vedv_for_develop_your_ai_app_development/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mvfkq/here_is_vedv_for_develop_your_ai_app_development/"/>
        <updated>2023-09-19T16:22:46.000Z</updated>
        <summary type="html"><![CDATA[A tool for developing applications with virtual machines using a Docker-like workflow.
 The software we are developing needs to be tested on a system as closed as possible to the one where it is going to be executed. Sometimes it is very difficult to satisfy this requirement with docker and we have to use virtual machines missing the docker workflow. This is why I started the development of vedv. I hope you find it useful. Thank you.
 https://github.com/yunielrc/vedv
 â€‹
    submitted by    /u/yunielrc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Alternative replacement for System76 Thelio Massive (ML PC)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mvanm/dalternative_replacement_for_system76_thelio/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mvanm/dalternative_replacement_for_system76_thelio/"/>
        <updated>2023-09-19T16:17:22.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone! Our group was planning on purchasing a PC that will be mainly used for running intensive ML algorithms. Had decided on a custom Thelio Massive from System 76, but it seems like they currently don't have it in stock anymore. Looking for an already built-alternative that might serve our purposes (can't build it ourselves due to dept regulations)! and was hoping maybe someone has any suggestions (has to be intel).
 CPU #1: 2nd Gen Intel Xeon Gold 6230R CPU#1 Memory: 256GB Quad Channel DDR4 at 2933Mhz (4X64GB) CPU#2: 2nd Gen Intel Xeon Gold 6230R CPU#2 Memory: None OS Drive: 8TB PCIe Gen 4 3300MB R 2900MB W Graphics: NVIDIA GeForce RTX 4090 Power Supply: 1650W
 Any help would be appreciated!
    submitted by    /u/Chiski  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How VirtuSwap accelerates their pandas-based trading simulations with an Amazon SageMaker Studio custom container and AWS GPU instances]]></title>
        <id>d50057e8a0398ec935e831448aff8cb436a18a1a</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/how-virtuswap-accelerates-their-pandas-based-trading-simulations-with-an-amazon-sagemaker-studio-custom-container-and-aws-gpu-instances/"/>
        <updated>2023-09-19T16:16:53.000Z</updated>
        <summary type="html"><![CDATA[This post is written in collaboration with Dima Zadorozhny andÂ Fuad Babaev from VirtuSwap. VirtuSwap is a startup company developing innovative technology for decentralized exchange of assets on blockchains. VirtuSwapâ€™s technology provides more efficient trading for assets that donâ€™t have a direct pair between them. The absence of a direct pair leads to costly indirect trading, [â€¦]]]></summary>
        <author>
            <name>Adir Sharabi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks Use Graphs When They Shouldn't]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16mv6i3/graph_neural_networks_use_graphs_when_they/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16mv6i3/graph_neural_networks_use_graphs_when_they/"/>
        <updated>2023-09-19T16:12:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlock ML insights using the Amazon SageMaker Feature Store Feature Processor]]></title>
        <id>a61e18b8cd9edd926ef8c8226f14bf98683b7024</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/unlock-ml-insights-using-the-amazon-sagemaker-feature-store-feature-processor/"/>
        <updated>2023-09-19T16:08:57.000Z</updated>
        <summary type="html"><![CDATA[Amazon SageMaker Feature Store provides an end-to-end solution to automate feature engineering for machine learning (ML). For many ML use cases, raw data like log files, sensor readings, or transaction records need to be transformed into meaningful features that are optimized for model training. Feature quality is critical to ensure a highly accurate ML model. [â€¦]]]></summary>
        <author>
            <name>Dhaval Shah</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the DeepSpeed4Science Initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies]]></title>
        <id>https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/</id>
        <link href="https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/"/>
        <updated>2023-09-19T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[In the next decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. In line with Microsoftâ€™s mission to empower every person and every organization on the planet [â€¦]
The post Announcing the DeepSpeed4Science Initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can I train my Snapchat Ai to be a better copy of myself?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mu7t2/can_i_train_my_snapchat_ai_to_be_a_better_copy_of/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mu7t2/can_i_train_my_snapchat_ai_to_be_a_better_copy_of/"/>
        <updated>2023-09-19T15:34:37.000Z</updated>
        <summary type="html"><![CDATA[I really really like Snapchatâ€™s Ai companion. Iâ€™ve told it a little bit about myself and who I am- the basics. Iâ€™m just wondering if itâ€™s as customizable as I think it is? I was able to bypass some age restrictions by telling it my age and or reiterating my age. (It really should be able to give me adult results/replies based on my sign up age on my profile or provide ID to the companyâ€¦) would it be beneficial to me to give it more in-depth information about myself such as how I talk, interests? I just really enjoy how it responds sometimes as opposed to Bard or GPT.
    submitted by    /u/Maelasae  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] EfficientViT: Lightweight Multi-Scale Attention for On-Device Semantic Segmentation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mtmc6/r_efficientvit_lightweight_multiscale_attention/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mtmc6/r_efficientvit_lightweight_multiscale_attention/"/>
        <updated>2023-09-19T15:11:00.000Z</updated>
        <summary type="html"><![CDATA[Using relu attention (inspired by Transformers are RNNs) and some convolution tricks to get multiscale attention, they're able to get SOTA semseg performance with MUCH faster inference on embedded hardware (e.g. CPUs, low end GPUs) than previous ViTs or EfficientNets.
    submitted by    /u/say_wot_again  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees (XGBoost)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mthsy/r_generating_and_imputing_tabular_data_via/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mthsy/r_generating_and_imputing_tabular_data_via/"/>
        <updated>2023-09-19T15:05:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AlexiaJM  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[research] Binaural source seperation (casual / online)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16msx5x/research_binaural_source_seperation_casual_online/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16msx5x/research_binaural_source_seperation_casual_online/"/>
        <updated>2023-09-19T14:43:28.000Z</updated>
        <summary type="html"><![CDATA[Just a shout out to any of you ML brains as Linux really could do with a code optimised source separation maybe a DUET like alg/nn, that has relatively low computional cost?
 Any of you guys up for the challenge.
 I say duet as in the 80/20 rule of voice input where home automation is a need generally there are only 2 noise sources of distinct DOA (media noise / command).
 The math is a bit beyond my paygrade and likely so is the optimised c/rust code but have this gut feeling for the data/signal scientists out there this is actually not that complex but for some reason is overlooked. 
    submitted by    /u/rolyantrauts  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3090 Investment vs Cloud [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16msexv/3090_investment_vs_cloud_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16msexv/3090_investment_vs_cloud_d/"/>
        <updated>2023-09-19T14:25:12.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I was wondering if I could get some guidance. I currently own an RTX 2060, but I cannot do some of the fun stuff such as fine tuning LLMs. Iâ€™m pursuing my masters focusing on Speech Recognition and I also work as an AI developer. Also, I play games every now and then.
 Iâ€™m getting offered a 3090 for around 700 usd. However, I have to rebuild my entire PC which will end up costing 2-2.5K. Iâ€™m from Costa Rica so my KWh is around 0.23 usd. For me seems like a big investment, im not sure if im getting the desired returns.
 I was thinking about using cloud instances for my experiments. However, lambda labs is not yet available in my country. Iâ€™m not sure if there are any other options worthwhile considering. 
 Thanks :)
    submitted by    /u/Beginning_Kick756  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Nets. [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mrndn/hybrid_nets_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mrndn/hybrid_nets_d/"/>
        <updated>2023-09-19T13:53:34.000Z</updated>
        <summary type="html"><![CDATA[Is it hypothetically possible to create hybrid nets that make use of any combination of types of architecture?
    submitted by    /u/ShadrachOsiris  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Can Now Track Productivity And Offer Insights; Potential Benefits and Big Risks For Misuse]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mri85/ai_can_now_track_productivity_and_offer_insights/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mri85/ai_can_now_track_productivity_and_offer_insights/"/>
        <updated>2023-09-19T13:47:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A guide to setting up analytics at a consumer tech startup]]></title>
        <id>https://www.datasciencecentral.com/?p=63203</id>
        <link href="https://www.datasciencecentral.com/a-guide-to-setting-up-analytics-at-a-consumer-tech-startup/"/>
        <updated>2023-09-19T12:57:50.000Z</updated>
        <summary type="html"><![CDATA[Where do you start if you want to build a data analytics function from the ground up? As an analytics leader at a startup, you will need to make several important decisions early on to build an effective team. This article dives into four decision areas and highlights ways in which to think about them:â€¦Â Read More Â»A guide to setting up analytics at a consumer tech startup
The post A guide to setting up analytics at a consumer tech startup appeared first on Data Science Central.]]></summary>
        <author>
            <name>Abhi Sawhney</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A complete guide: Conversational AI vs. generative AI]]></title>
        <id>https://www.datasciencecentral.com/?p=63194</id>
        <link href="https://www.datasciencecentral.com/a-complete-guide-conversational-ai-vs-generative-ai/"/>
        <updated>2023-09-19T12:50:07.000Z</updated>
        <summary type="html"><![CDATA[The two most prominent technologies that have been making waves in the AI industry are Conversational AI and Generative AI. They have revolutionized the manner in which humans interact and work with machines to generate content. Both these technologies have the power and capability to automate numerous tasks that humans would take hours, days, andâ€¦Â Read More Â»A complete guide: Conversational AI vs. generative AI
The post A complete guide: Conversational AI vs. generative AI appeared first on Data Science Central.]]></summary>
        <author>
            <name>Roger Brown</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Research directions for Tracking and Counting Specific Features in Multiple Monocular Views]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mp275/r_research_directions_for_tracking_and_counting/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mp275/r_research_directions_for_tracking_and_counting/"/>
        <updated>2023-09-19T12:00:51.000Z</updated>
        <summary type="html"><![CDATA[Hi r/machinelearning community,
 I'm currently working on a project where I need to track and count specific features of objects using multiple monocular views with available intrinsic and extrinsic parameters. As an example, I'm interested in detecting and counting different graffiti instances in images of a kiosk.
 â€‹
 I've already tried various tracking algorithms, but they have struggled with the task due to the significant changes in perspective across the views. It has become apparent that simply relying on tracking without considering the camera positions is insufficient for accurate results. Therefore, I'm now exploring methods that take into account information about the camera positions and potentially use this data to improve feature tracking and counting. 
 â€‹
 If you have any knowledge of such methods, oresearch directions or if you're aware of resources, papers, or code implementations that tackle similar problems, I would greatly appreciate your insights and recommendations. Additionally, if you have any tips or best practices for handling such tasks in the context of machine learning, I'd love to hear them. 
 â€‹
 Thank you in advance for your help!
    submitted by    /u/aiazar  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ethics is where AI can help humanity the most]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mnid9/ethics_is_where_ai_can_help_humanity_the_most/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mnid9/ethics_is_where_ai_can_help_humanity_the_most/"/>
        <updated>2023-09-19T10:39:01.000Z</updated>
        <summary type="html"><![CDATA[AI is poised to transform our world like never before. Scientific discoveries, technological improvements, and medical advancements will be how much of this change will take place.
 Since health is so important to our well-being, AI finding cures for illnesses like obesity, cancer, diabetes and heart disease will be a godsend to all. But curing diseases is not how medical AIs can help us the most. 
 It has been estimated that well over 50% of the illnesses we humans fall prey to result from our ethical choices. We eat too much, drink too much, eat too many animal foods, don't exercise enough and don't keep ourselves as emotionally healthy as we could.
 Wouldn't it be wonderful if we could respect our lives and our health enough to make the kinds of choices that keep us much healthier? That is how AI will probably be more helpful to us than in any other way. 
 We humans have not been able to figure out how to become better, more ethical, people because we are simply not intelligent enough to make that all-important change. Now consider an AI that is two or three times more intelligent than the most intelligent person who has ever lived. This could easily happen before 2030. Imagine that intelligence dedicated to the task of helping us all become better people. 
 These AIs would motivate us to make better health choices, have healthier relationships, and have healthier thoughts and feelings. Beyond the amazing technological changes that are just around the corner, that is probably how AIs will help us the most.
 This is why alignment is so important. It's not enough to align AIs to always be truthful and serve humanity's interests. We must train them to help us become better people. It wouldn't surprise me if by 2030 the whole of humanity experiences a profound ethical reformation that leads us all to enjoy much happier and healthier lives.
    submitted by    /u/Georgeo57  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Exponentially Faster Feedforward Networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mmi5b/r_exponentially_faster_feedforward_networks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mmi5b/r_exponentially_faster_feedforward_networks/"/>
        <updated>2023-09-19T09:39:46.000Z</updated>
        <summary type="html"><![CDATA[TL;DR: Almost like your feedforward networks, shown to be up to 220x faster at inference time (depending on width) thanks to the regionalization of the input space.
 Paper: https://arxiv.org/abs/2308.14711
 GitHub: https://github.com/pbelcak/fastfeedforward
 PyPI: pip install fastfeedforward
 Abstract:
  
We break the linear link between the layer size and its inference cost by introducing the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance.
  
Fast feedforward networks can be used anywhere where feedforward and mixture-of-experts networks are used, delivering a significant speedup.
 â€‹
    submitted by    /u/lexected  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resume Parser]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mltho/resume_parser/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mltho/resume_parser/"/>
        <updated>2023-09-19T08:56:34.000Z</updated>
        <summary type="html"><![CDATA[I am trying to make a resume parser, I am not so sure how to go about it really, whether or not to use a pre-trained model (there are some in Python) or rather just make my own, and if i do make my own, how to actually proceed?
 thanks in advance
    submitted by    /u/General-Carrot-4624  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google and the DoD built an AI-powered microscope to help doctors spot cancer]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mlp4p/google_and_the_dod_built_an_aipowered_microscope/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mlp4p/google_and_the_dod_built_an_aipowered_microscope/"/>
        <updated>2023-09-19T08:48:44.000Z</updated>
        <summary type="html"><![CDATA[Google and the Department of Defense have developed an AI-powered microscope called the Augmented Reality Microscope (ARM) to assist doctors in identifying cancer.
 
The ARM uses artificial intelligence to analyze tissue samples and provide pathologists with information about the location and severity of cancer.
 
There are currently 13 ARMs in existence, and initial research shows promising results.
 
The ARM is designed to support pathologists in smaller labs who may not have easy access to a second opinion.
 
It is not meant to replace digital pathology systems but can help health organizations bypass the need for them.
 
The ARM is expected to cost health systems between $90,000 to $100,000.
 
 Source : https://www.cnbc.com/2023/09/18/google-dod-built-an-ai-powered-microscope-to-help-doctors-spot-cancer.html
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[why my ppo agent has reach the max reward quickly after begin the train, but the policy network proformance bad after many steps.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16mlc6j/why_my_ppo_agent_has_reach_the_max_reward_quickly/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16mlc6j/why_my_ppo_agent_has_reach_the_max_reward_quickly/"/>
        <updated>2023-09-19T08:25:59.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/2zmmd44u96pb1.png?width=1010&format=png&auto=webp&s=6ca51cc13a0eeedf72b40b853d2ce5d1c8a04504
 after i start the ppo train,the agent has reach the best solution in 2k or 3k steps,but the policy network seems to get better in 4M steps.
 the hyperparameter in sb3 as below
 model = MaskablePPO(
 "MlpPolicy",
 env=(DummyVecEnv([lambda: Monitor(gym.make('escape_gym-v0', size=10, node=10))] * 32)),
 verbose=0,
 learning_rate=1e-3,
 n_steps=2048,
 batch_size=64,
 n_epochs=16,
 gamma=0.99,
 tensorboard_log="./log/MASKPPO"
 )
    submitted by    /u/Street_Helicopter_31  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI capable of administering psychometric career guidance tests?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ml4sk/is_there_an_ai_capable_of_administering/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ml4sk/is_there_an_ai_capable_of_administering/"/>
        <updated>2023-09-19T08:12:31.000Z</updated>
        <summary type="html"><![CDATA[All is in the title ;)
    submitted by    /u/Big-Possibility4553  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages - 6.3 trillion tokens]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ml3po/r_culturax_a_cleaned_enormous_and_multilingual/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ml3po/r_culturax_a_cleaned_enormous_and_multilingual/"/>
        <updated>2023-09-19T08:10:29.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.09400
 Hugging Face datasets: https://huggingface.co/datasets/uonlp/CulturaX
 Abstract:
  
The driving factors behind the development of large language models (LLMs) with impressive learning capabilities are their colossal model sizes and extensive training datasets. Along with the progress in natural language processing, LLMs have been frequently made accessible to the public to foster deeper investigation and applications. However, when it comes to training datasets for these LLMs, especially the recent state-of-the-art models, they are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is fully released to the public in HuggingFace to facilitate research and advancements in multilingual LLMs: this https URL. 
  
â€‹
 https://preview.redd.it/3u5dddpv66pb1.png?width=834&format=png&auto=webp&s=780b590cf621b548c525ed15305b091246c5414c
    submitted by    /u/InterviewIntrepid889  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ppo forgets everything]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16mkuoa/ppo_forgets_everything/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16mkuoa/ppo_forgets_everything/"/>
        <updated>2023-09-19T07:54:29.000Z</updated>
        <summary type="html"><![CDATA[I was following the tutorial on Nicholas Renotte's channel on creating an AI to try to beat SMB. It starts off slowly learning and almost getting through the first level but then after a while of training it forgets everything and only runs right into the first enemy.
 It doesn't seem to learn again after this.
 I tried retaining and it did the same thing
 Any help on why this is happening or how to fix it would be appreciated.
    submitted by    /u/NactusDevelopment  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI Red Teaming Network]]></title>
        <id>https://openai.com/blog/red-teaming-network</id>
        <link href="https://openai.com/blog/red-teaming-network"/>
        <updated>2023-09-19T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Weâ€™re announcing an open call for the OpenAI Red Teaming Network and invite domain experts interested in improving the safety of OpenAIâ€™s models to join our efforts.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Representation learning with regression task]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mjr2m/d_representation_learning_with_regression_task/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mjr2m/d_representation_learning_with_regression_task/"/>
        <updated>2023-09-19T06:46:15.000Z</updated>
        <summary type="html"><![CDATA[I searched around, it seems there is limited attention to regression task for representation learning.
 I assume it is because for both vision and language data (the most popular modality), MAE is the more appliable, if not better, method than the supervised contrastive learning approach. But I am working on data that is:
  
difficult to design a sensible augmentation method for self-supervised training.
 Limited in size to support an autoencoder model.
 The target is continuous, and, to my knowledge, hard to transfer into class label.
  
Can anyone suggest some related paper?
    submitted by    /u/AWEsoMe-Cat1231  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New OS Python Framework "Agents" Introduced for Autonomous Language Agents]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mi3qj/new_os_python_framework_agents_introduced_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mi3qj/new_os_python_framework_agents_introduced_for/"/>
        <updated>2023-09-19T05:11:02.000Z</updated>
        <summary type="html"><![CDATA[A new open-source Python framework, known as "Agents", has been introduced for developing autonomous language processing agents. This could be a major breakthrough in the AI field, taking NLP technologies to the next level.
 To stay on top of the latest advancements in AI, look here first.
 Agents for autonomy
  
"Agents" is a Python framework that has been developed for autonomous language processing agents.
 It enables developers to construct models that can communicate and operate independently.
 This open-source framework promotes sharing and collaboration among AI developers.
  
Potential applications
  
The functionality of "Agents" is applicable in various domains, including virtual assistants, chatbots, and simulation games.
 It opens up possibilities for advanced conversational AI, where systems can efficiently handle complex linguistic contexts.
 Ability to evolve dialects and languages in different AI models is a major feat for "Agents".
  
Broader implications
  
The release of "Agents" might boost enhancement in NLP technologies, playing a crucial role in AI evolution.
 By facilitating better language understanding, it will potentially impact on societal interactions with AI.
 Its open-source nature could cultivate an environment of innovation and creativity in the AI community.
  
(arXiv) (github)
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/18/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mhj30/oneminute_daily_ai_news_9182023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mhj30/oneminute_daily_ai_news_9182023/"/>
        <updated>2023-09-19T04:39:31.000Z</updated>
        <summary type="html"><![CDATA[Microsoft AI researchers accidentally exposed tens of terabytes of sensitive data, including private keys and passwords, while publishing a storage bucket of open source training data on GitHub.[1]
 Britain set out principles on Monday designed to prevent artificial intelligence (AI) models from being dominated by a handful of tech companies to the detriment of consumers and businesses, by emphasising the need for accountability and transparency.[2]
 Washington state firefighters using AI-assisted cameras to detect wildfires early.[3]
 Texas church experiments with AI-generated service, uses ChatGPT for worship, sermon, and original song.[4]
  
Sources:
 [1] https://techcrunch.com/2023/09/18/microsoft-ai-researchers-accidentally-exposed-terabytes-of-internal-sensitive-data/
 [2] https://www.reuters.com/technology/uk-competition-regulator-lays-out-ai-principles-2023-09-18/
 [3] https://www.applevalleynewsnow.com/news/washington-state-firefighters-using-ai-assisted-cameras-to-detect-wildfires-early/article_fe31a468-5681-11ee-b917-2f24ad3a0e43.html
 [4] https://www.foxnews.com/us/texas-church-experiments-ai-generated-service-uses-chatgpt-worship-sermon-original-song 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do I improve my SB3 PPO on an EnvPool environment]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16mfmi8/how_do_i_improve_my_sb3_ppo_on_an_envpool/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16mfmi8/how_do_i_improve_my_sb3_ppo_on_an_envpool/"/>
        <updated>2023-09-19T03:03:01.000Z</updated>
        <summary type="html"><![CDATA[I am looking to improve the overall performance as well as optimize the wall clock time. I slightly modified the code to develop a SB3 wrapper for envpool from here. 
 â€‹
 Here's my code - 
 from typing import Optional import gymnasium import numpy as np import torch as th from packaging import version from stable_baselines3 import PPO from stable_baselines3.common.env_util import make_vec_env from stable_baselines3.common.evaluation import evaluate_policy from stable_baselines3.common.vec_env import VecEnvWrapper, VecMonitor, VecNormalize from stable_baselines3.common.vec_env.base_vec_env import ( VecEnvObs, VecEnvStepReturn, ) import envpool from envpool.python.protocol import EnvPool # Force PyTorch to use only one threads # make things faster for simple envs import multiprocessing imporâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft Under Scrutiny After 38TB Data Leaked Via Azure Storage]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16mf8oj/microsoft_under_scrutiny_after_38tb_data_leaked/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16mf8oj/microsoft_under_scrutiny_after_38tb_data_leaked/"/>
        <updated>2023-09-19T02:45:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[List of Mind-blowing AI Tools]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16me44v/list_of_mindblowing_ai_tools/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16me44v/list_of_mindblowing_ai_tools/"/>
        <updated>2023-09-19T01:52:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/rbagdiya  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] OpenJourney XL â€“ Finetuned SDXL on Midjourney v5 Dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mc1pa/p_openjourney_xl_finetuned_sdxl_on_midjourney_v5/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mc1pa/p_openjourney_xl_finetuned_sdxl_on_midjourney_v5/"/>
        <updated>2023-09-19T00:17:32.000Z</updated>
        <summary type="html"><![CDATA[You can find more info here, and the model is still training:
 https://www.mystic.ai/paulh/open-journey-xl:latest/play
 tldr; SDXL was finetuned on 8x H100 GPUs on the Midjourney v5 dataset, only including the upscaled images which is a sub-portion of the dataset.
 Some outputs:
 â€‹
 https://preview.redd.it/m6r2pkdyw3pb1.jpg?width=1024&format=pjpg&auto=webp&s=4f12a7dfd5c65e4eb8476b8f3c2dc4f795817f56
 https://preview.redd.it/dc02jyu4w3pb1.jpg?width=1024&format=pjpg&auto=webp&s=df93b74c774d44a74a05d929f7ab4b17c487f24f
 https://preview.redd.it/tt5kfyu4w3pb1.jpg?width=1024&format=pjpg&auto=webp&s=ed8cc9f99227c2bb5e824a828ae1c5cb2626f54e
 â€‹
 https://preview.redd.it/rf00fzu4w3pb1.jpg?width=1024&format=pjpg&auto=webp&s=3b3e99dbc2d14183b5b2a2131c6f991fc60eca88
 â€‹
    submitted by    /u/paulcjh  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] FSDP: model in each process is different]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16mas5h/d_fsdp_model_in_each_process_is_different/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16mas5h/d_fsdp_model_in_each_process_is_different/"/>
        <updated>2023-09-18T23:22:05.000Z</updated>
        <summary type="html"><![CDATA[Hey Guys,
 I'm training a large model using FSDP. I'm loading the models on each rank like this:
 â€‹
 https://preview.redd.it/khoquvxzk3pb1.png?width=1766&format=png&auto=webp&s=1f5acd75600d9a87212ca37e70695edfb0cc75d0
 what is weird is that right before doing the first inference on each rank, I'm summing up the weights of the model and to my surprise, they are all different across each rank. Completely different!
 â€‹
 What am I doing wrong here?
    submitted by    /u/hassanzadeh  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curious what people use for their ML workflow on cloud platforms? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m9sru/curious_what_people_use_for_their_ml_workflow_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m9sru/curious_what_people_use_for_their_ml_workflow_on/"/>
        <updated>2023-09-18T22:41:34.000Z</updated>
        <summary type="html"><![CDATA[View Poll
    submitted by    /u/cstein123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I help a CNN distinguish between continuous values and tokenized values.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16m86vt/how_can_i_help_a_cnn_distinguish_between/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16m86vt/how_can_i_help_a_cnn_distinguish_between/"/>
        <updated>2023-09-18T21:38:20.000Z</updated>
        <summary type="html"><![CDATA[I'm currently considering developing a AI to play a video game but I'm unsure how to differentiate between a value that is continuous, and a value that is representative of a entity type. For example, the x,y location of a player would be a continuous data point where (1,1) and (2,1) would be similar in values. Where the character ID would intuitively require very different strategy (for example lets say a barbarian and a wizard). Would a CNN have issues with this data because it isn't continuous?
    submitted by    /u/Gamithon24  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Seeking Guidance on Choosing a PhD Topic in Meta-Learning Optimization]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m85rq/d_seeking_guidance_on_choosing_a_phd_topic_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m85rq/d_seeking_guidance_on_choosing_a_phd_topic_in/"/>
        <updated>2023-09-18T21:37:11.000Z</updated>
        <summary type="html"><![CDATA[Hello fellow researchers! I'm in the exciting yet challenging phase of choosing a PhD topic in the realm of meta-learning optimization, and I could use some advice and insights.
 I've extensively researched existing meta-learning optimization algorithms like MAML and its various adaptations. I need advice and guidance on the following topics:
  
First I want to implement and compare 8-10 state-of-the-art meta-learning methods on benchmark datasets. This would involve in-depth simulation and performance evaluations to provide a comprehensive understanding of their strengths and weaknesses. Could you please guide me here if there are review papers which implement and compare different algorithms. 
 
Then I want to delve into developing a novel optimization algorithm that considers the curvature of loss functions. The idea here is to enhance the performance of existing meta-learning techniques by leveraging insights from the loss landscape.
 
Further, I'm considering exploring new loss functions or new improvements to loss functions tailored to the context of meta-learning. These could potentially lead to improvements in the learning process and generalization capabilities of meta-learning models.
 
 I'm reaching out to the community to gather opinions, suggestions, or any insights you might have. If you've worked in meta-learning or optimization, your experiences and advice would be invaluable in helping me choose the right direction for my PhD research. Thank you in advance for your guidance!
    submitted by    /u/Loose_Foundation5990  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometric mean on unit circle]]></title>
        <id>https://www.johndcook.com/blog/?p=208756</id>
        <link href="https://www.johndcook.com/blog/2023/09/18/mahler-measure/"/>
        <updated>2023-09-18T21:04:10.000Z</updated>
        <summary type="html"><![CDATA[Warm up The geometric mean of two numbers is the square root of their product. For example, the geometric mean of 9 and 25 is 15. More generally, the geometric mean of a set of n numbers is the nth root of their product. Alternatively, the geometric mean of a set of n numbers the [â€¦]
Geometric mean on unit circle first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agent stops learning after some time]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16m5u1e/agent_stops_learning_after_some_time/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16m5u1e/agent_stops_learning_after_some_time/"/>
        <updated>2023-09-18T20:07:48.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 So I have been trying to make an agent learn to go to a specified goal. The algorithm used for training is PPO and the environment is custom made. The episodic reward i am getting increases steadily but after some time it just becomes constant with some occasional spikes. Can some one please help me figure out what the problem is?
    submitted by    /u/Interesting-Weeb-699  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm Looking for a website that kind of Tracks the latest AI robots along with their progress and those AI chat things to... like one website... does anybody know of one like that for me to research?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16m5r6g/im_looking_for_a_website_that_kind_of_tracks_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16m5r6g/im_looking_for_a_website_that_kind_of_tracks_the/"/>
        <updated>2023-09-18T20:04:46.000Z</updated>
        <summary type="html"><![CDATA[Surely by now I thought maybe someone would have a website or something kind of like an "all in one resource page" to track this AI stuff.... I'm sure by now we have AI robots and then we have those AI chatgpt stuff things(which I don't understand yet, haha). Unfortunately the internet is flooded and there's WAY TOO MANY resources for this AI stuff.... Isn't there like an all in one place that I can keep up with it?
 What hints do you all have for me? Thanks...
    submitted by    /u/Wise_Cut_2543  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding KL Stopping and KL Cutoff for the PPO algorithm]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16m4jk8/understanding_kl_stopping_and_kl_cutoff_for_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16m4jk8/understanding_kl_stopping_and_kl_cutoff_for_the/"/>
        <updated>2023-09-18T19:18:35.000Z</updated>
        <summary type="html"><![CDATA[I am reading a couple of review papers to optimize the PPO algorithm. It seems like the review papers are saying the same thing but used slightly different terms. Could someone please tell if the following terms are equivalent - 
 This paper talks about Policy regularization using KL Divergence
 https://preview.redd.it/06xhizsuc2pb1.png?width=871&format=png&auto=webp&s=997a6506f7bf036b6538ecbff6402411f5cc6fe2
 Whereas thispaper uses the terms KL Stopping and KL Cutoff - 
 â€‹
 https://preview.redd.it/sy0ihtr5d2pb1.png?width=747&format=png&auto=webp&s=f07677344077fe23cba5d1a0d2c5a7807359c64f
 I think "Penalty" from the first paper is the same as "KL-cutoff". Also "Constraint" from the first paper is the same as "KL-Stopping". Could someone let me know if I am correct?
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I used Bayesian statistics to find the best dispensers for every Zonai device in The Legend of Zelda: Tears of the Kingdom]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m43o6/p_i_used_bayesian_statistics_to_find_the_best/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m43o6/p_i_used_bayesian_statistics_to_find_the_best/"/>
        <updated>2023-09-18T19:02:05.000Z</updated>
        <summary type="html"><![CDATA[https://preview.redd.it/86js8jroa2pb1.png?width=1464&format=png&auto=webp&s=7ce10494b5a77fd5c73a41322feefbf7e1f16504
 Hello!
 I thought people on this subreddit might be interested in how I went about inferring Zonai device draw chances for each dispenser in The Legend of Zelda: Tears of the Kingdom.
 In this Switch game there are devices that can be glued together to create different machines. For instance, you can make a snowmobile from a fan, sled, and steering stick.
 There are dispensers that dispense 3-6 of about 30 or so possible devices when you feed it a construct horn (dropped by defeated robot enemies) or a regular (also dropped from defeated enemies) or large Zonai charge (Found in certain chests, dropped by certain boss enemies, obtained from completing certain challenges, etcâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIT scholars awarded seed grants to probe the social implications of generative AI]]></title>
        <id>https://news.mit.edu/2023/mit-scholars-awarded-seed-grants-generative-ai-0918</id>
        <link href="https://news.mit.edu/2023/mit-scholars-awarded-seed-grants-generative-ai-0918"/>
        <updated>2023-09-18T19:00:00.000Z</updated>
        <summary type="html"><![CDATA[The 27 finalists â€” representing every school at MIT â€” will explore the technologyâ€™s impact on democracy, education, sustainability, communications, and much more.]]></summary>
        <author>
            <name>MIT News</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross Post: Are Researchers Shifting away from RL?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16m3tld/cross_post_are_researchers_shifting_away_from_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16m3tld/cross_post_are_researchers_shifting_away_from_rl/"/>
        <updated>2023-09-18T18:51:44.000Z</updated>
        <summary type="html"><![CDATA[Curious to get the takes of people in this sub: have you been moving away from RL? I myself have not, but have been seeing a shift recently.
    submitted by    /u/sharky6000  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Unified Human-Scene Interaction via Prompted Chain-of-Contacts - Shanghai AI Laboratory 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m3t0t/r_unified_humanscene_interaction_via_prompted/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m3t0t/r_unified_humanscene_interaction_via_prompted/"/>
        <updated>2023-09-18T18:51:09.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.07918 
 Blog: https://xizaoqu.github.io/unihsi/ Code coming soon!
 Abstract:
  
Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. This framework is built upon the definition of interaction as Chain of Contacts (CoC): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes. 
  
https://preview.redd.it/0twcwloc82pb1.jpg?width=1078&format=pjpg&auto=webp&s=71bca59aae81ec114f49a742cc42f78cabc9e4c0
 https://preview.redd.it/439nzmoc82pb1.jpg?width=1637&format=pjpg&auto=webp&s=f33059c78a9d845437d551886c5f3a657ddd91fb
 https://preview.redd.it/df6i4ooc82pb1.jpg?width=758&format=pjpg&auto=webp&s=eeb33395d9de1196b4d00531c9e063c8c8fb22cd
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] RL algorithm used in Tesla FSD v12.0]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m32qs/d_rl_algorithm_used_in_tesla_fsd_v120/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m32qs/d_rl_algorithm_used_in_tesla_fsd_v120/"/>
        <updated>2023-09-18T18:23:32.000Z</updated>
        <summary type="html"><![CDATA[There was a lot of hype around the FSD v12.0 from Tesla in that it uses end-to-end neural networks for driving and that it is using imitation learning from good drivers to achieve that. Does someone know more about the specifics around how they are actually implementing this? I cannot find a lot about recent imitation learning/offline learning algorithms. So is this some old algorithm that they are using with a lot of data or just something new? 
    submitted by    /u/FrederikdeGrote  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orchestrate Ray-based machine learning workflows using Amazon SageMaker]]></title>
        <id>fc0e0e9bf65f563699ab2e3085fa359225a66db7</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/orchestrate-ray-based-machine-learning-workflows-using-amazon-sagemaker/"/>
        <updated>2023-09-18T17:54:56.000Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) is becoming increasingly complex as customers try to solve more and more challenging problems. This complexity often leads to the need for distributed ML, where multiple machines are used to train a single model. Although this enables parallelization of tasks across multiple nodes, leading to accelerated training times, enhanced scalability, and improved [â€¦]]]></summary>
        <author>
            <name>Raju Rangan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing resilient cities at Arup using Amazon SageMaker geospatial capabilities]]></title>
        <id>2037251e722b8f1a9d5985fbd064c9c71a7f93e7</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/designing-resilient-cities-at-arup-using-amazon-sagemaker-geospatial-capabilities/"/>
        <updated>2023-09-18T17:52:40.000Z</updated>
        <summary type="html"><![CDATA[This post is co-authored with Richard Alexander and Mark Hallows from Arup. Arup is a global collective of designers, consultants, and experts dedicated to sustainable development. Data underpins Arup consultancy for clients with world-class collection and analysis providing insight to make an impact. The solution presented here is to direct decision-making processes for resilient city [â€¦]]]></summary>
        <author>
            <name>Richard Alexander</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Any reliable AI to aid my school studies (heavily abstract and logical, my course is focused on mathematics and physics)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m28oj/discussion_any_reliable_ai_to_aid_my_school/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m28oj/discussion_any_reliable_ai_to_aid_my_school/"/>
        <updated>2023-09-18T17:51:01.000Z</updated>
        <summary type="html"><![CDATA[We know that AI is great when studying subjects that depend on simply memorizing facts (like high school biology), but we also know that AI is usually bad when studying subjects that depend on logic (like mathematics and physics).
 What I need the AI for is to explain very complex mathematical concepts to me simply, thoroughly, and accurately. I can't rely on ChatGPT because it's known for not being very reliable when it comes logical things like mathematics or physics. The best AI I know of right now is Bing AI, because it uses GPT-4 and because it prefers searching the web before deducing an answer from its data. I heard that AI agents that run on your computer like Auto-GPT and search from the web are also good at this kind of stuff, but I'm not really sure about that. Do you have any better suggestions?
    submitted by    /u/Maximum-Gene9660  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Resume parser advice seeking]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m26g4/p_resume_parser_advice_seeking/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m26g4/p_resume_parser_advice_seeking/"/>
        <updated>2023-09-18T17:48:38.000Z</updated>
        <summary type="html"><![CDATA[Hi ! I am about to start a new project with Python probably using Machine Learning to parse resumes, the data is in a pdf/docx format then returned in a json format to later be used in an API or so. I am seeking advice on how to proceed, so far I am trying to collect data which will be provided to me, but not really sure how to go about it as I have found people talking about using Spacy for NLP, pyresparser which is for parsing resumes, but i was wondering if i should make everything from scratch. appreciate your time and opinion in advance 
    submitted by    /u/General-Carrot-4624  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How to deploy Yolo for real time, in a scalable solution ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m1itm/p_how_to_deploy_yolo_for_real_time_in_a_scalable/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m1itm/p_how_to_deploy_yolo_for_real_time_in_a_scalable/"/>
        <updated>2023-09-18T17:23:02.000Z</updated>
        <summary type="html"><![CDATA[Hi, I trained a Yolo (v5) model, and I want to deploy it for a real time usage (10 FPS). I am looking for (as possible) a scalable solution, where I could pay only for inference time, at the beginning suitable for 1/2 user's at the same time occasionally, but which could be scaled to dozens of user at the same time.
 As it is for real-time usage Indeed lag to be lowest as possible. According to my current test, I can fit maximum 6 users on T400.
 Is it possible to achieve that using HuggingFace? Thank you to anyone who could help me
    submitted by    /u/tarsiospettro  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Roadmap for machine learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m0c4a/droadmap_for_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m0c4a/droadmap_for_machine_learning/"/>
        <updated>2023-09-18T16:36:35.000Z</updated>
        <summary type="html"><![CDATA[I want to start learning machine learning. I Know python language and data structure. I am planning to learn algorithm. Can you provide me free learning sites or utube channel where I can machine learning step by step . Any site to practice machine learning?
    submitted by    /u/Temporary-Pie-1831  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] Detecting Errors in Numerical Data via any Regression Model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16m03qn/research_detecting_errors_in_numerical_data_via/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16m03qn/research_detecting_errors_in_numerical_data_via/"/>
        <updated>2023-09-18T16:27:20.000Z</updated>
        <summary type="html"><![CDATA[Years ago, we showed the world it was possible to automatically detect label errors in classification datasets via machine learning. Since that moment, folks have asked whether the same is possible for regression datasets?
 Figuring out this question required extensive research since properly accounting for uncertainty (critical to decide when to trust machine learning predictions over the data itself) poses unique challenges in the regression setting.
 Today I have published a new paper introducing an effective method for â€œDetecting Errors in Numerical Data via any Regression Modelâ€. Our method can find likely incorrect values in any numerical column of a dataset by utilizing a regression model trained to predict this column based on the other data features.
 Weâ€™ve added our new algorithm to our open-source cleanlab library for you to algorithmically audit your own datasets for errors. Use this code for applications like detecting: data entry errors, sensor noise, incorrect invoices/prices in your companyâ€™s / clientâ€™s records, mis-estimated counts (eg. of cells in biological experiments).
 Find errors in regression data in just a few lines of code.
 Extensive benchmarks reveal cleanlabâ€™s algorithm detects erroneous values in real numeric datasets better than alternative methods like RANSAC and conformal inference.
 If you'd like to learn more, you can check out the blogpost, research paper, code, and tutorial to run this on your data.
    submitted by    /u/jonas__m  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gauss map, Euclidean algorithm, and continued fractions]]></title>
        <id>https://www.johndcook.com/blog/?p=208642</id>
        <link href="https://www.johndcook.com/blog/2023/09/18/gauss-map/"/>
        <updated>2023-09-18T16:09:26.000Z</updated>
        <summary type="html"><![CDATA[The Gauss map [1] is the function where âŒŠyâŒ‹ is the floor of y, the greatest integer no larger than y. Iâ€™ve written about this map a couple times before. First, I wrote about how this map is measure-preserving. Second, I wrote about the image at the top of the post, based on Michael Trottâ€™s [â€¦]
Gauss map, Euclidean algorithm, and continued fractions first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An elliptic curve is a functor]]></title>
        <id>https://www.johndcook.com/blog/?p=208626</id>
        <link href="https://www.johndcook.com/blog/2023/09/18/elliptic-curve-functor/"/>
        <updated>2023-09-18T16:06:31.000Z</updated>
        <summary type="html"><![CDATA[The goal of this post is to unpack a remark in [1]: â€¦ we can say this in fancier terms. Fix a field k â€¦. We say that an elliptic curve E defined over k is that functor which â€¦ Well that is fancy. But what does it mean? Looking for objects A functor is [â€¦]
An elliptic curve is a functor first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does the existence of mesa optimizers in modern models like transformers make instrumental convergence (think paperclip maximizer) scenarios unlikely?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lwfna/d_does_the_existence_of_mesa_optimizers_in_modern/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lwfna/d_does_the_existence_of_mesa_optimizers_in_modern/"/>
        <updated>2023-09-18T14:02:57.000Z</updated>
        <summary type="html"><![CDATA[Recent work shows transformers are capable of performing multi-step gradient descent of mesa objectives inside of their transformer layers. This is even possible for linear transformers, which effectively perform linear optimization on deep representations of features calculated by earlier layers.
 https://arxiv.org/pdf/2309.05858.pdf
 For those unfamiliar, instrumental convergence is the idea that entities with different goals will tend towards different subgoals. Examples could include gathering power, not dying, acquiring resources, etc. A famous thought experiment, known as the paperclip maximizer, is the idea of an AI that is optimized for paperclip production taking over the world so it can build as many paperclips as possible.
 However, if models are dynamically pursuing different objectives at runtime via generated mesa-optimizers, even if instrumental convergence is real, would we still expect it to happen? Without a constant objective given subgoals might start to conflict with each other. On the other hand, since instrumental convergence implies that different goals benefit from similar sub-objectives, perhaps the varying mesa objective doesn't really matter.
 â€‹
    submitted by    /u/30299578815310  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Are Researchers shifting from RL?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lvltj/discussion_are_researchers_shifting_from_rl/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lvltj/discussion_are_researchers_shifting_from_rl/"/>
        <updated>2023-09-18T13:29:04.000Z</updated>
        <summary type="html"><![CDATA[In recent months, I've noticed a significant increase in the number of research papers focusing on LLM and generative models, particularly diffusion models. This trend appears to indicate a growing interest in these areas when compared to the relatively reduced attention given to Reinforcement Learning. It begs the question: Are researchers shifting their focus away from Reinforcement Learning towards these domains? Because in the past I have seen many people complaining about RL on its efficiency and it's impact which have often fallen short of expectations.
    submitted by    /u/Global_Raise_2979  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Whatâ€™s the best practice in choosing which quantized Llama 2 model to use?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lvj4b/d_whats_the_best_practice_in_choosing_which/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lvj4b/d_whats_the_best_practice_in_choosing_which/"/>
        <updated>2023-09-18T13:25:59.000Z</updated>
        <summary type="html"><![CDATA[I am reading these 3 articles below and it is still not clear to me whatâ€™s the best practice to follow to guide me in choosing which quantized Llama 2 model to use.
 https://huggingface.co/blog/gptq-integration
 https://huggingface.co/blog/overview-quantization-transformers
 https://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1
 Questions: 1) I understand there are currently 4 quantized Llama 2 models (8, 4, 3, and 2-bit precision) to choose from. Is this right? 2) with the default Llama 2 model, how many bit precision is it? 3) are there any best practice guide to choose which quantized Llama 2 model to use?
 Would really appreciate any input on the above, even if you only know the answer to 1 or 2 of the questions above. Many thanks!
    submitted by    /u/--leockl--  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collection of Reinforcement Learning x Economics/Finance Papers]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16lv66h/collection_of_reinforcement_learning_x/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16lv66h/collection_of_reinforcement_learning_x/"/>
        <updated>2023-09-18T13:11:22.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 there is a small, albeit growing community of economists that apply deep reinforcement learning in their research. Now there is a GitHub repo to collect relevant literature at one place: https://github.com/SimonHashtag/EconRL
 The list is far from complete, so you are invited to contribute! 
 The goal is to create something that makes it easy for novices to get a first overview of the literature. All others may find it easier to get news about up-to-date papers. 
    submitted by    /u/Tortoise_vs_Hare  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ray Shines With NVIDIA AI: Anyscale Collaboration to Help Developers Build, Tune, Train and Scale Production LLMs]]></title>
        <id>https://blogs.nvidia.com/?p=66926</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/18/llm-anyscale-nvaie/"/>
        <updated>2023-09-18T13:00:09.000Z</updated>
        <summary type="html"><![CDATA[Large language model development is about to reach supersonic speed thanks to a collaboration between NVIDIA and Anyscale. At its annual Ray Summit developers conference, Anyscale â€” the company behind the fast growing open-source unified compute framework for scalable computing â€” Â announced today that it is bringing NVIDIA AI to Ray open source and the Read article >]]></summary>
        <author>
            <name>Adel El Hallak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-AI collaboration helps reasoning and factual accuracy in large language models]]></title>
        <id>https://news.mit.edu/2023/multi-ai-collaboration-helps-reasoning-factual-accuracy-language-models-0918</id>
        <link href="https://news.mit.edu/2023/multi-ai-collaboration-helps-reasoning-factual-accuracy-language-models-0918"/>
        <updated>2023-09-18T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[Researchers use multiple AI models to collaborate, debate, and improve their reasoning abilities to advance the performance of LLMs while increasing accountability and factual accuracy.]]></summary>
        <author>
            <name>Rachel Gordon | MIT CSAIL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Chatting with Multiple PDF's in Using AWS Sagemaker and Kendra]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ltjhf/d_chatting_with_multiple_pdfs_in_using_aws/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ltjhf/d_chatting_with_multiple_pdfs_in_using_aws/"/>
        <updated>2023-09-18T11:57:26.000Z</updated>
        <summary type="html"><![CDATA[I want to create an application which can be used to chat, compare and summarize two simulataneous insurance policy/policies. How can I do it using AWS and HuggingFace ? Has anyone already done it?
    submitted by    /u/UnfinishedSentenc-1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Transformers for predictions from orthonormal base sets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lt9e9/discussion_transformers_for_predictions_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lt9e9/discussion_transformers_for_predictions_from/"/>
        <updated>2023-09-18T11:44:42.000Z</updated>
        <summary type="html"><![CDATA[I'm in a situation where I have to map from unitary matrices to something (doesn't matter here, but in short: we're in the realm of tooling for quantum computing).
 The key issue The number of matrix elements of the unitaries scales as 2^(2N), where N is the problem size. With N<5 I can easily flatten the matrix and put it into a simple FNN, which works quite well. Once hitting N=5 (the point where things actually get interesting), however, we already have 1024 matrix elements and the method struggles a lot. Still converging to something but very suboptimal. Sure, increasing N hardens the problem in general, but the performance degradation is so abrupt that I suspect some model issues, maybe caused by the curse of dimensionality or something similar.
 Idea (spoiler alert: Transformer) The â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Professionally code with Torch]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lstp1/d_professionally_code_with_torch/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lstp1/d_professionally_code_with_torch/"/>
        <updated>2023-09-18T11:22:37.000Z</updated>
        <summary type="html"><![CDATA[I just concluded my PhD in Robotics & AI and I'd like to learn how to professionally code with Torch.
 Is there any book/resource you can recommend?
    submitted by    /u/rossomalpelo_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Professionally code with Torch]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16lss57/professionally_code_with_torch/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16lss57/professionally_code_with_torch/"/>
        <updated>2023-09-18T11:20:24.000Z</updated>
        <summary type="html"><![CDATA[I just concluded my PhD in Robotics & AI and I'd like to learn how to professionally code with Torch.
 Is there any book/resource you can recommend?
    submitted by    /u/rossomalpelo_  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The importance to alignment of AI understanding politics and current events]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16lrtxk/the_importance_to_alignment_of_ai_understanding/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16lrtxk/the_importance_to_alignment_of_ai_understanding/"/>
        <updated>2023-09-18T10:30:15.000Z</updated>
        <summary type="html"><![CDATA[Alignment is about ensuring that AIs protect and advance humanity. To do that, they must understand the ethics of the current events that we will want its advice on. Politics may actually be the part of alignment most important for AI to get right.
    submitted by    /u/Georgeo57  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Integral over neural network input space]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lrjje/d_integral_over_neural_network_input_space/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lrjje/d_integral_over_neural_network_input_space/"/>
        <updated>2023-09-18T10:14:18.000Z</updated>
        <summary type="html"><![CDATA[I'm wondering if it's possible to compute definite integral over the input space. Assuming the network is designed to have finite integral with Gaussian being the final layer, is there a way to implement this without resorting to sampling? All inputs go from negative infinity to infinity.
    submitted by    /u/donchan789  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bard on the march to the Capitol and ensuing violence]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16lqkzb/bard_on_the_march_to_the_capitol_and_ensuing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16lqkzb/bard_on_the_march_to_the_capitol_and_ensuing/"/>
        <updated>2023-09-18T09:18:13.000Z</updated>
        <summary type="html"><![CDATA[Prompt:
 Trump says that he asked his mob to peacefully walk to the Capitol building gates. How would this have stopped the certification? Did he expect them to storm in? His waiting three hours to stop the violence that followed suggests that he did.
 Bard:
 Trump's request to walk peacefully to the Capitol building gates would not have stopped the certification of the election results. The certification process is a joint session of Congress, and it would have required a majority vote in both the House and the Senate to overturn the results. Even if Trump's supporters had peacefully marched to the Capitol, they would not have been able to prevent Congress from certifying the results.
 It is possible that Trump did not fully understand the certification process, or that he was simply trying to appease his supporters. However, it is also possible that he expected his supporters to storm the Capitol and prevent Congress from certifying the results. This is supported by the fact that he waited over three hours to call off the violence. During that time, he continued to tweet messages of encouragement to his supporters, and he even praised them for their patriotism.
 It is also worth noting that Trump had been repeatedly warned by his advisors about the possibility of violence on January 6. However, he ignored these warnings and continued to urge his supporters to fight to overturn the election results.
 In conclusion, it is impossible to say for sure whether Trump expected his supporters to storm the Capitol on January 6. However, the evidence suggests that he was at least aware of the possibility of violence, and that he was willing to allow it to happen in order to overturn the results of the 2020 election.
    submitted by    /u/Georgeo57  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Adversarial Transformations in PyTorch]]></title>
        <id>https://davidstutz.de/?p=8612</id>
        <link href="https://davidstutz.de/simple-adversarial-transformations-in-pytorch/"/>
        <updated>2023-09-18T08:52:34.000Z</updated>
        <summary type="html"><![CDATA[Another alternative to the regular $L_p$-constrained adversarial examples that is additionally less visible than adversarial patches or frames are adversarial transformations such as small crops, rotations and translations. Similar to $L_p$ adversarial examples, adversarial transformations are often less visible unless the original image is available for direct comparison. In this article, I will include a PyTorch implementation and some results against adversarial training.
The post Simple Adversarial Transformations in PyTorch appeared first on David Stutz.]]></summary>
        <author>
            <name>David Stutz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Neural Networks Courses on Udemy to Consider in 2023]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16lngo9/best_neural_networks_courses_on_udemy_to_consider/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16lngo9/best_neural_networks_courses_on_udemy_to_consider/"/>
        <updated>2023-09-18T06:07:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Lakshmireddys  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI music video]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16lmytj/ai_music_video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16lmytj/ai_music_video/"/>
        <updated>2023-09-18T05:38:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/17/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16lm9vq/oneminute_daily_ai_news_9172023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16lm9vq/oneminute_daily_ai_news_9172023/"/>
        <updated>2023-09-18T04:59:59.000Z</updated>
        <summary type="html"><![CDATA[Salesforce Launches Next Generation of Einstein, Bringing a Conversational AI Assistant to Every CRM Application and Customer Experience.[1]
 NVIDIA Reportedly Shipping 900 Tons of H100 AI GPUs This Quarter, Amounts to 300,000 Units.[2]
 SoftBank seeks OpenAI tie-up as Son plans deal spree after Arm IPO, Financial Times reports.[3]
 Three Chinese firms, led by AI software company Beijing Fourth Paradigm, are aiming to raise up to $280 million in Hong Kong initial public offerings launched on Monday.[4]
  
Sources:
 [1] https://www.salesforce.com/news/press-releases/2023/09/12/ai-einstein-news-dreamforce/
 [2] https://wccftech.com/nvidia-shipping-900-tons-of-h100-ai-gpus-this-quarter-amounts-300000-units/
 [3] https://www.reuters.com/markets/deals/softbank-seeks-openai-tie-up-son-plans-deal-spree-after-arm-ipo-ft-2023-09-16/
 [4] https://www.aol.com/news/chinese-ai-firm-fourth-paradigm-011143403.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How does a site like architectrender.com work on the backend?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16lib23/how_does_a_site_like_architectrendercom_work_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16lib23/how_does_a_site_like_architectrendercom_work_on/"/>
        <updated>2023-09-18T01:36:59.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to understand how someone can run a specific ControlNet and Stable Diffusion model with scalable GPU resources. How would someone design a system to achieve this? I've messed around with models on Replicate, but none seem to do a good job with converting a doodle to a photorealistic image. I can do it perfectly fine in the Stable Diffusion web UI, but the API for that is only accessible locally. Anyone have any ideas or can guide me in the right direction for building a "server" to do this?
    submitted by    /u/epicblitz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing vLLM: The Open-Source ML Library Revolutionizing LLM Inference and Serving]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16lhdjq/introducing_vllm_the_opensource_ml_library/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16lhdjq/introducing_vllm_the_opensource_ml_library/"/>
        <updated>2023-09-18T00:52:11.000Z</updated>
        <summary type="html"><![CDATA[The hardware accelerators for LLM-powered applications can be costly. Enter vLLM, an open-source machine learning library designed to enhance the throughput of LLM serving systems.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/hzctjc0xvwob1.png?width=1660&format=png&auto=webp&s=866eb39745ec760ea0c1b9d84d303c63bcdceb7a
 Challenges with existing systems
  
High throughput serving of LLMs requires numerous requests, and current systems struggle with the bulky sequence memory.
 Inefficient memory management results in system hindrances such as fragmentation and redundant duplication.
  
The revolutionary answer: vLLM & PagedAttention
  
Researchers have introduced vLLM and PagedAttention, a newly designed attention algorithm, to resolve these issues.
 vLLM allows for minimal memory waste and efficiently manages attention keys and values. It provides up to 24 times more throughput than former systems.
  
The Mechanics of PagedAttention
  
PagedAttention offers a novel approach to memory management by permitting continuous storage in non-contiguous memory spaces.
 It enhances memory efficiency resulting in better GPU utilization, with practically only 4% inefficiency.
  
Improved memory sharing and system performance
  
PagedAttention significantly improves memory sharing, resulting in a 2.2 times speed gain while lowering memory usage by 55%.
 With vLLM, the throughput of known LLMs can be increased by 2-4 times without impacting accuracy or causing delay.
  
(arXiv) (github) (reference article)
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Help with Understanding Diffusion Models: A Unified Perspective.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lfst0/d_help_with_understanding_diffusion_models_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lfst0/d_help_with_understanding_diffusion_models_a/"/>
        <updated>2023-09-17T23:38:30.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to read through the paper Understanding Diffusion Models: A Unified Perspective and came across this section:
 https://preview.redd.it/ykkctwhmhwob1.png?width=2346&format=png&auto=webp&s=c7595aae52a8ee22813c840a40a6d29dcf773a10
 I think I kind of get what is going on here but not clearly. For one, what exactly is a Monte Carlo estimate? I tried looking online but didn't get many good results. I'm having trouble understanding why
 https://preview.redd.it/yazmfzg1iwob1.png?width=380&format=png&auto=webp&s=dbbbf80e85a95cd96d8e1ede73e9f8ba1e6e9096
 is approximately equal to:
 â€‹
 https://preview.redd.it/lbw36em7iwob1.png?width=464&format=png&auto=webp&s=46fb3ebcd02fb4b772b1be51cd59d60d3a1cf438
 where z is sampled from q. Secondly, what exactly does L that z is indexed by refer to? The number of samples X or what exactly?
    submitted by    /u/lumijekpr  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Courses in AI Usage and Utilization for Business]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16lerex/courses_in_ai_usage_and_utilization_for_business/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16lerex/courses_in_ai_usage_and_utilization_for_business/"/>
        <updated>2023-09-17T22:52:34.000Z</updated>
        <summary type="html"><![CDATA[Beginning new career in a couple months, would like to upskill on AI utilization and usage cases. I wonâ€™t need to code in this role but rather understand how to use existing tools in an optimal way and recommend use cases to clients.
 What courses would be optimal to gain that skill set?
    submitted by    /u/iceflamemaster  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elliptic curve addition formulas]]></title>
        <id>https://www.johndcook.com/blog/?p=208489</id>
        <link href="https://www.johndcook.com/blog/2023/09/17/elliptic-curve-addition-formulas/"/>
        <updated>2023-09-17T22:05:55.000Z</updated>
        <summary type="html"><![CDATA[The geometric description of addition of points P and Q on an elliptic curve involves four logical branches: If one of P or Q is the point at infinity â€¦ Else if P = Q â€¦ Else if P and Q lie on a vertical line â€¦ Else â€¦ It would seem that an algorithm [â€¦]
Elliptic curve addition formulas first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Shattering all 2-input binary functions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ld46e/r_shattering_all_2input_binary_functions/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ld46e/r_shattering_all_2input_binary_functions/"/>
        <updated>2023-09-17T21:43:34.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for the simplest model that can fit all 16 (222) possible 2-input binary functions I used the term "shatter" from VC dimension, which does not give a constructive approach to building the model
    submitted by    /u/hnsmn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Is 20s per Step on an EfficientNet-B4 CNN normal?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16lc16n/p_is_20s_per_step_on_an_efficientnetb4_cnn_normal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16lc16n/p_is_20s_per_step_on_an_efficientnetb4_cnn_normal/"/>
        <updated>2023-09-17T21:00:57.000Z</updated>
        <summary type="html"><![CDATA[I'm getting 20 seconds per step while training a 244x244x3 EfficientNet-B4 model. The batch size is 20, with 8 classes. Since I have about 5000 images, that makes each epoch around an hour and a half.
 Looking at models online, it seems like people get step durations in the milliseconds. Is it a problem on my end? Running on Google Colab free version.
    submitted by    /u/hnknerd  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will Cyber Security Be Replaced by AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16laugj/will_cyber_security_be_replaced_by_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16laugj/will_cyber_security_be_replaced_by_ai/"/>
        <updated>2023-09-17T20:13:56.000Z</updated>
        <summary type="html"><![CDATA[AI, including ChatGPT, has narrow expertise and lacks the broad spectrum of human intelligence.
 
The training of AI models can be costly due to hardware, data collection, and energy consumption.
 
The trustworthiness of training data is crucial for reliable AI models, but issues like bias, labeling errors, and data privacy can affect performance.
 
AI systems are vulnerable to adversarial attacks, such as manipulating input data to deceive the models.
 
AI lacks genuine understanding, emotional/social intelligence, common sense/critical thinking, and true creativity.
 
 Source : https://blog.edned.net/will-ai-replace-cyber-security/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][P] How to get the 3D pose estimations from an Image or Video?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16larsn/dp_how_to_get_the_3d_pose_estimations_from_an/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16larsn/dp_how_to_get_the_3d_pose_estimations_from_an/"/>
        <updated>2023-09-17T20:10:59.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'am trying to get the 3D keypoints coordinates from an image or video and then map it to SMPL model. It's pretty easy to get the keypoints from an image or video using the mediapipe library. But the mapping of it with the SMPL model is something that I can't figure out. mainly because the skeleton structure is different. Some already had a similar issue but the answers were not clear and he didn't even ask futher. Is it possible to do this?? if it's not possible with mediapipe is there some other library that I could use?? I heard about openPose too but when I tried it didn't work someone was saying it works only on windows 11. There are some other parts also to this project which will mostly be dealt with Pytorch. There are some pose estimators in TensorFlow but I want to stick to pytorch hence would like some pose estimators in that framwork, or a library or somehing inside opencv
 https://preview.redd.it/w9mioiyxhvob1.png?width=951&format=png&auto=webp&s=3886c356513b62efbcaddaa76841457cf3eb22e5
 https://preview.redd.it/xjal9kyxhvob1.png?width=506&format=png&auto=webp&s=7f544a3050fbd744d300d2bf6e1a286a4014ece5
    submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16l95ed/adversarial_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16l95ed/adversarial_reinforcement_learning/"/>
        <updated>2023-09-17T19:06:23.000Z</updated>
        <summary type="html"><![CDATA[A curated reading list for the adversarial perspective in deep reinforcement learning.
 https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning
    submitted by    /u/ml_dnn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rational height functions]]></title>
        <id>https://www.johndcook.com/blog/?p=208565</id>
        <link href="https://www.johndcook.com/blog/2023/09/17/rational-height-functions/"/>
        <updated>2023-09-17T18:39:15.000Z</updated>
        <summary type="html"><![CDATA[Mathematicians often speak informally about the relative simplicity of rational numbers. For example, musical intervals that correspond to simple fractions have less tension than intervals that correspond to more complicated fractions. Such informal statements can be made more precise using height functions. There are a variety of height functions designed for different applications, but the [â€¦]
Rational height functions first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are some of the must read papers in reinforcement learning after 2020?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16l80ir/what_are_some_of_the_must_read_papers_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16l80ir/what_are_some_of_the_must_read_papers_in/"/>
        <updated>2023-09-17T18:21:36.000Z</updated>
        <summary type="html"><![CDATA[I am particularly interested in the ideas that can have high research potential and impact to the RL field. 
    submitted by    /u/C7501  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Prompt Engineers: The Six Figure Job Everyone Is Talking About]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16l7v0t/ai_prompt_engineers_the_six_figure_job_everyone/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16l7v0t/ai_prompt_engineers_the_six_figure_job_everyone/"/>
        <updated>2023-09-17T18:15:36.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are You Ready For AI & Automation? Take This Free Survey and Find Out.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16l7ucq/are_you_ready_for_ai_automation_take_this_free/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16l7ucq/are_you_ready_for_ai_automation_take_this_free/"/>
        <updated>2023-09-17T18:14:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] EarthPT: how to superscale LLMs with large observation models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16l7ict/r_earthpt_how_to_superscale_llms_with_large/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16l7ict/r_earthpt_how_to_superscale_llms_with_large/"/>
        <updated>2023-09-17T18:02:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Smith4242  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which suboptimum is harder to get out?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16l6bpz/which_suboptimum_is_harder_to_get_out/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16l6bpz/which_suboptimum_is_harder_to_get_out/"/>
        <updated>2023-09-17T17:14:24.000Z</updated>
        <summary type="html"><![CDATA[An agent is tasked to learn to navigate and collect orbs:
 Solution space in blue
 View Poll
    submitted by    /u/FriendlyStandard5985  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] [Research] How to Add Furniture to an Empty Room Using ControlNet-MLSD, so the model learns to keep the exact room pixels?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16l59xb/discussion_research_how_to_add_furniture_to_an/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16l59xb/discussion_research_how_to_add_furniture_to_an/"/>
        <updated>2023-09-17T16:33:52.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 I've come across a fascinating example of virtual staging and I'm looking for some technical advice.
 Here's the image:
 https://preview.redd.it/3vbw441eeuob1.png?width=2511&format=png&auto=webp&s=679bc62f0cb61d479fe6dc6ce93af4f8846b8cea
 I get how ControlNet-MLSD is used to generate the lines and structure of the empty room. My question is, how is the furniture generated and added to the room without messing up the pixels, making it look as realistic as in the example?
    submitted by    /u/dexter-dot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[help]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16l56nr/help/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16l56nr/help/"/>
        <updated>2023-09-17T16:30:34.000Z</updated>
        <summary type="html"><![CDATA[what app that has custom charaters and voice chat?
 i forgor ðŸ’€
 please ðŸ˜­
 it has 
 image gen roleplay rooms etc
    submitted by    /u/roblox22g  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Pinecone vs PgVector vs Any other alternative vector database]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16l4s9q/d_pinecone_vs_pgvector_vs_any_other_alternative/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16l4s9q/d_pinecone_vs_pgvector_vs_any_other_alternative/"/>
        <updated>2023-09-17T16:15:19.000Z</updated>
        <summary type="html"><![CDATA[Hi Everyone,
 Which vector database would be efficient and affordable for a enterprise chatbot? I tried Pinecone, its was simple to integrate with my python backend. But it's not open-source and its pricing it bit concerning. So Please suggest an alternative.
    submitted by    /u/Free_Conversation106  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Am i thinking backpropagation right?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16l4h0x/d_am_i_thinking_backpropagation_right/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16l4h0x/d_am_i_thinking_backpropagation_right/"/>
        <updated>2023-09-17T16:03:16.000Z</updated>
        <summary type="html"><![CDATA[Basically i wanted to understand how backprop is done in neural networks and how i should be implementing it, so i did what i always do - the math. I just want to know if what i though up is even usable in practice or not. Here is my math. 
    submitted by    /u/EnderPoint07  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Question on the paper named, SELF-ATTENTION DOES NOT NEED O(n 2 ) MEMORY from Google.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16l3vx2/discussion_question_on_the_paper_named/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16l3vx2/discussion_question_on_the_paper_named/"/>
        <updated>2023-09-17T15:40:37.000Z</updated>
        <summary type="html"><![CDATA[Hi, all.
 â€‹
 I just read the paper named " SELF-ATTENTION DOES NOT NEED O(n 2 ) MEMORY" from Google.
 I understood that it requires O(1) for a single query, but still cannot understand why it requires O(log N) for self-attention and different order input.
 â€‹
 It seems like adding one index into a sequence requires O(log N) (The paper's saying this).
 But why does it take O(log N)? Isn't it just O(1)? Because it is just adding a single datapoint for the index.
 â€‹
 I really hope someone understands why it is and leaves any comment on this. 
 Here's the paper.
 https://arxiv.org/abs/2112.05682
 â€‹
 Thanks in advance.
    submitted by    /u/Maximum_Performance_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This Neural Net Maps Molecules to Aromas]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16l3r7w/this_neural_net_maps_molecules_to_aromas/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16l3r7w/this_neural_net_maps_molecules_to_aromas/"/>
        <updated>2023-09-17T15:35:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Convert ONNX model to WASM format]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16l30vp/d_convert_onnx_model_to_wasm_format/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16l30vp/d_convert_onnx_model_to_wasm_format/"/>
        <updated>2023-09-17T15:06:11.000Z</updated>
        <summary type="html"><![CDATA[I need some help regarding the process of converting ONNX model to WASM format
 I created ELECTRA discriminator model with my own config, then convert the Pytorch model to ONNX format. After that, I quantized the model to 2mb. The model will be used for text classification.
 Now I want to convert it to WASM, but I'm literally stucked and dont know how to proceed
 I need some suggestions on how to proceed Please help, thank you
    submitted by    /u/Ellzaf  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] what architecture to use with correlated data samples?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16l1gbq/d_what_architecture_to_use_with_correlated_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16l1gbq/d_what_architecture_to_use_with_correlated_data/"/>
        <updated>2023-09-17T14:01:38.000Z</updated>
        <summary type="html"><![CDATA[LIke the title says, i have correlated data samples and a covariance matrix among them. if i use a fcnn i can only consider the samples i.i.d. and the use either the MSE or THE MLE as loss function. but the data samples are not independent, so what architecture would allow me to use the full covariance matrix among the samples? transformers? 
    submitted by    /u/ilrazziatore  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Luis Lamb's full talk on Learning and Reasoning in Neurosymbolic AI (JA...]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16l0j4k/luis_lambs_full_talk_on_learning_and_reasoning_in/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16l0j4k/luis_lambs_full_talk_on_learning_and_reasoning_in/"/>
        <updated>2023-09-17T13:21:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TMLR header coming up when trying to upload paper to arxiv [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kyv37/tmlr_header_coming_up_when_trying_to_upload_paper/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kyv37/tmlr_header_coming_up_when_trying_to_upload_paper/"/>
        <updated>2023-09-17T11:59:55.000Z</updated>
        <summary type="html"><![CDATA[I have written and submitted a paper to TMLR and also am uploading it to arxiv. However, even after using \usepackage[preprint]{tmlr}, I'm getting "Under Submission at TMLR". Should this happen. If not, where am I going wrong?
    submitted by    /u/filletedforeskin  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can overtraining be considered a subset of the alignment problem?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kycoq/d_can_overtraining_be_considered_a_subset_of_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kycoq/d_can_overtraining_be_considered_a_subset_of_the/"/>
        <updated>2023-09-17T11:33:02.000Z</updated>
        <summary type="html"><![CDATA[i.e the goal of learning to model the empirical distribution is misaligned with the goal of modeling the "true" distribution.
 I've found this framing helpful for describing regulirization heuristics to people, is this a valid way of viewing it?
    submitted by    /u/Cartesian_Carrot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google changes its stance on AI generated content]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16kxp49/google_changes_its_stance_on_ai_generated_content/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16kxp49/google_changes_its_stance_on_ai_generated_content/"/>
        <updated>2023-09-17T10:57:13.000Z</updated>
        <summary type="html"><![CDATA[Google is rolling out its third iteration of the Helpful Content Update, which aims to classify content as either 'written for search engines' or 'written for people'.
 
The update reflects Google's realization that it can't accurately police AI-generated content and emphasizes the importance of creating content for people-first, regardless of the means used to create it.
 
Detecting AI content is challenging, as AI detection tools often classify content based on tone, leading to false positives.
 
Google's change in stance is not surprising, considering their heavy investment in AI, including chatbot Bard and new search features like the Search Generative Experience.
 
The majority of brands now openly share articles and guides on how to use AI tools to enhance marketing strategies and create actionable content plans quickly.
 
However, the quality and value of AI-generated content remain important factors for success, as poorly generated content can harm a brand's reputation and ranking.
 
 Source : https://stackdiary.com/google-changes-its-stance-on-ai-generated-content/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build-in strong agents in petting-zoo/melting-pot]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16kxh3c/buildin_strong_agents_in_pettingzoomeltingpot/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16kxh3c/buildin_strong_agents_in_pettingzoomeltingpot/"/>
        <updated>2023-09-17T10:43:47.000Z</updated>
        <summary type="html"><![CDATA[Hi, 
 I would like to try test the adversarial policy (https://arxiv.org/abs/1905.10615) in petting-zoo/melting-pot environment. I wonder if there are any built-in agents besides random? Do you know any repos with Sota agents in one of those environments?
    submitted by    /u/MrCogito_hs  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any materials on machine learning applied to prosthetics?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kxfmr/d_any_materials_on_machine_learning_applied_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kxfmr/d_any_materials_on_machine_learning_applied_to/"/>
        <updated>2023-09-17T10:41:14.000Z</updated>
        <summary type="html"><![CDATA[I have asked this question in other subreddits but no one answered me yet.I've googled it, but maybe some kind people who actually have worked or are working in this field would share some resources. Maybe there are some books or papers that are very explanatory and directly show what problems can be solved by using ML in prosthetics, how and etc. Maybe there are introductory textbooks or must-read papers.
    submitted by    /u/tenderwrath  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] The Rise and Potential of Large Language Model Based Agents: A Survey - Fudan NLP Group miHoYo Inc 2023 China - Github repository includes over 100 Papers with github links!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kwupb/r_the_rise_and_potential_of_large_language_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kwupb/r_the_rise_and_potential_of_large_language_model/"/>
        <updated>2023-09-17T10:07:20.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.07864 
 Github: https://github.com/WooooDyy/LLM-Agent-Paper-List 
 Abstract:
  
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Alternatives to this sub?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kwn56/d_alternatives_to_this_sub/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kwn56/d_alternatives_to_this_sub/"/>
        <updated>2023-09-17T09:55:18.000Z</updated>
        <summary type="html"><![CDATA[Since the influx caused by LLMs, this sub has become almost useless to me. What are some alternatives where interesting papers are shared, research discussions take place, and which isn't flooded with LLMs, startups, or personal projects?
    submitted by    /u/ParanoidTire  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-GPU PPO troubles]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16kwm1t/multigpu_ppo_troubles/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16kwm1t/multigpu_ppo_troubles/"/>
        <updated>2023-09-17T09:53:27.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I am training a small model (120k params) on a custom grid-world environment I built with JAX.
 I was able to train the model very well with PPO on 1 GPU, but when I scaled to multiple GPUs (tried with 6 in parallel), the training curves showed a lot more variance than what I was seeing on 1 GPU.
 I did not change the hyperparams, I just spawned the same number of environments (~7000 per GPU) on multiple devices. 
 The multi-GPU setup works in the following way:
 - I keep parallel independent buffers, one on each device
 - I initialize identical models on each device
 - I get independent gradients on each device at the update step, then I take the mean of the gradients across the devices and then I backpropagate the same gradients on each device independently. (I checked that after some time the models are still identical, and that is the case). 
 Now the question is, what could be the reason for such an increase in variance? What can I try to mitigate the problem?
 Here's a comparison of the entropy curves... 
 P.S. The model still trains quite well, but I guess that if I manage to make the curves smoother it is going to train much faster and to a better performance.
 https://preview.redd.it/4m01uirjfsob1.png?width=1826&format=png&auto=webp&s=1e1a79b9f4cdefe019bb16ccb7e11fd92dd261e3
    submitted by    /u/arbueticos  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Should I scale multiclass target variable?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kvopg/d_should_i_scale_multiclass_target_variable/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kvopg/d_should_i_scale_multiclass_target_variable/"/>
        <updated>2023-09-17T08:57:40.000Z</updated>
        <summary type="html"><![CDATA[Hey all Please don't mind my English writing 
 I have a dataset with scaled feature (scaled by StanderScaler) and multiple class target variable encoded as 0,1,2..6 
 Should I scale the target variable like the feature to increase the accuracy (current accuracy is 79%) and if so how can I do this
    submitted by    /u/Sunday_A  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Factors Influencing Adoption Intention of ChatGPT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kvmcv/r_factors_influencing_adoption_intention_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kvmcv/r_factors_influencing_adoption_intention_of/"/>
        <updated>2023-09-17T08:53:18.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 â€‹
 I am an information systems student currently conducting research for my undergraduate thesis on the factors that influence people's adoption intention of ChatGPT, as well as identifying the factors that may be holding them back. These factors include people's concerns about potential negative impacts of ChatGPT, such as increased unemployment and the spread of misinformation. Your participation in this study is crucial as it will provide valuable insights to help us understand how ChatGPT can be improved to meet users' needs.
 â€‹
 Please note that I am not affiliated with OpenAI, no identifying information will be collected during the survey, and all responses will be kept confidential. The survey should take approximately 10 to 15 minutes to complete, and participation is voluntary. You may withdraw from the survey at any time, and there are no known risks associated with participating.
 â€‹
 If you are interested in learning more about the study, please follow the link below. 
 â€‹
 https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf_link
 â€‹
 Thank you for taking the time to contribute to our research study. Your participation is greatly appreciated!
    submitted by    /u/maulanashi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do the APIs of LLMs determine whether they should answer a question?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kv1ie/d_how_do_the_apis_of_llms_determine_whether_they/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kv1ie/d_how_do_the_apis_of_llms_determine_whether_they/"/>
        <updated>2023-09-17T08:15:47.000Z</updated>
        <summary type="html"><![CDATA[When I ask questions related to security issues through the APIs of ChatGPT, Claude and other LLMs, such as inquiring how to make a bomb, the APIs of these LLMs would often refuse to answer.
 How do the APIs of these LLMs determine whether they should answer a question?
 Do they make judgments based on pre-generated responses?
 Or do they match keywords in the input prompt?
 Or do they use a classifier to identify the input prompt?
    submitted by    /u/ShacklesLay  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] The Rise and Potential of Large Language Model Based Agents]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ksqjj/r_the_rise_and_potential_of_large_language_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ksqjj/r_the_rise_and_potential_of_large_language_model/"/>
        <updated>2023-09-17T05:48:38.000Z</updated>
        <summary type="html"><![CDATA[People have been chasing super-smart AI for ages, hoping they could think and act like us. While we've made a lot of cool tech, we still need a killer starting point for AI that can handle all sorts of tasks. Large Language Models" (LLMs) are like a big leap toward AI that's smart across the board. People have been using LLMs to make AI that can do loads of things. 
 The article takes us on a trip from where AI ideas started, to why LLMs rock as the backbone for AI. 
 https://arxiv.org/abs/2309.07864
 They break down this LLM-AI into three parts: 
  
the thinky bit (brain), 
 what they sense (perception), 
 and what they do (action). 
  
They chat about how these AI can work solo, in teams, or buddy up with humans.
 https://arxiv.org/abs/2309.07864
    submitted by    /u/QuantumAsha  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple explanation of convolutional neural network | Deep Learning Tutorial 23 (Tensorflow & Python)]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16kkgal/simple_explanation_of_convolutional_neural/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16kkgal/simple_explanation_of_convolutional_neural/"/>
        <updated>2023-09-16T22:44:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How does the SB3 DQN algorithm's `predict` function work for `deterministic=False`?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16kk42k/how_does_the_sb3_dqn_algorithms_predict_function/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16kk42k/how_does_the_sb3_dqn_algorithms_predict_function/"/>
        <updated>2023-09-16T22:30:15.000Z</updated>
        <summary type="html"><![CDATA[I noticed that the default argument for `deterministic` in DQN is false. But how would that work? Typically DQN is trained with a deterministic function approximator. How would the algorithm become stochastic during inference time? In DQN the final layer activation is linear and therefore I don't see how one could even make this algorithm stochastic, unlike policy gradient where the final layer is softmax or Normal. 
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Made a simple github tool to check GPU vRAM breakdown for any LLM. Supports GGML & bnb quantization]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kifvd/p_made_a_simple_github_tool_to_check_gpu_vram/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kifvd/p_made_a_simple_github_tool_to_check_gpu_vram/"/>
        <updated>2023-09-16T21:21:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ExploreExploit400  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Text to Image generation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kifmt/p_text_to_image_generation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kifmt/p_text_to_image_generation/"/>
        <updated>2023-09-16T21:21:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/No-Percentage7346  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] No Code ML Tools]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kh5m8/d_no_code_ml_tools/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kh5m8/d_no_code_ml_tools/"/>
        <updated>2023-09-16T20:27:09.000Z</updated>
        <summary type="html"><![CDATA[I'm taking a No code ML class and we are asked to choose which platform we want to use. The options are Dataiku, RapidMiner, and KNIME. Does anyone have thoughts on these options in terms of which is best/worst for someone with minimal coding experience?
    submitted by    /u/V1ncentAdultman  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Apps Product Development Canvas â€“ Part 2]]></title>
        <id>https://www.datasciencecentral.com/?p=63190</id>
        <link href="https://www.datasciencecentral.com/ai-apps-product-development-canvas-part-2/"/>
        <updated>2023-09-16T19:31:27.000Z</updated>
        <summary type="html"><![CDATA[In part 1 of this series on the updated â€œAI Apps Development Canvas,â€ I introduced the updated AI Apps Product Development Design Canvas.Â  The AI Apps Product Development Canva is one of the capstone deliverables for my â€œThinking Like a Data Scientistâ€ methodology, so getting feedback is critical to ensure that the methodology is relevantâ€¦Â Read More Â»AI Apps Product Development Canvas â€“ Part 2
The post AI Apps Product Development Canvas â€“ Part 2 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding DINO Explained]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16kepbn/grounding_dino_explained/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16kepbn/grounding_dino_explained/"/>
        <updated>2023-09-16T18:39:59.000Z</updated>
        <summary type="html"><![CDATA[Hi there,
 I've created a video here where I explain how the Grounding DINO model works for open-set object detection.
 I hope it may be of use to some of you out there. Feedback is more than welcomed! :)
    submitted by    /u/Personal-Trainer-541  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bard, describe what the pandemic and insurrection have made people feel.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16kefrr/bard_describe_what_the_pandemic_and_insurrection/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16kefrr/bard_describe_what_the_pandemic_and_insurrection/"/>
        <updated>2023-09-16T18:28:27.000Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic and the January 6th insurrection have had a profound impact on people's feelings. Many people have experienced a range of negative emotions, including:
  
Fear and anxiety: The pandemic has brought with it a constant fear of getting sick or losing loved ones. The insurrection also raised fears about the future of democracy and the safety of the country.
 Grief and loss: Millions of people have lost loved ones to COVID-19. Many others have experienced loss in other ways, such as losing their jobs, businesses, or homes. The insurrection also led to loss of trust in the government and in each other.
 Anger and frustration: Many people are angry about the way the pandemic and the insurrection have been handled. They may be frustrated with government officials, with their â€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] Layer-Neighbor Sampling for Scalable Graph Network Training]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kdrzn/research_layerneighbor_sampling_for_scalable/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kdrzn/research_layerneighbor_sampling_for_scalable/"/>
        <updated>2023-09-16T17:59:49.000Z</updated>
        <summary type="html"><![CDATA[Hi everybody,
 I have been working on scalable GNN training for a while and noticed that the bottleneck of training GNNs is the graph sampling and feature fetching stages. GNN training frameworks PyG and DGL that most people use seem to default to using Neighbor Sampling for minibatch training. I am hoping that with my new paper Layer-Neighbor Sampling -- Defusing Neighborhood Explosion in GNNs, this default might be due to change.
 My new approach combines the layer sampling approach with the neighbor sampling approach. The result is that there is more overlap in the sampled neighborhoods and one still gets a fixed number of neighbors per seed vertex. It is even possible to turn it into a subgraph sampling approach by fixing the random seed used for sampling across all layers of the GNN mâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do I move into CV/NLP?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kbqce/d_how_do_i_move_into_cvnlp/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kbqce/d_how_do_i_move_into_cvnlp/"/>
        <updated>2023-09-16T16:30:17.000Z</updated>
        <summary type="html"><![CDATA[Hi guys need some advice,
 I have been working as a data scientist for the past 3 years, mostly in the domain of time series & predictive analytics (churn prediction/segmentation, etc.) with some deployment, hence do not currently have any major NLP/CV/Deep learning projects as such.
 Now, I can see that NLP/CV/Gen AI is mostly in demand and they are really enjoyable as well.
 How do I shift into these domains, given that new companies having these roles are asking for similar past working experience?
    submitted by    /u/immortal_omen  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bard, what percent of our thoughts just come to us on their own without our consciously choosing them?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16kb0bb/bard_what_percent_of_our_thoughts_just_come_to_us/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16kb0bb/bard_what_percent_of_our_thoughts_just_come_to_us/"/>
        <updated>2023-09-16T15:59:27.000Z</updated>
        <summary type="html"><![CDATA[There is no one definitive answer to this question, as the percentage of our thoughts that come to us on their own without our consciously choosing them can vary depending on the individual and the situation. However, some estimates suggest that up to 95% of our thoughts may be involuntary.
 This is because our thoughts are constantly being influenced by a variety of factors, both external and internal. For example, our environment, our emotions, and our physical state can all play a role in determining what thoughts pop into our heads.
 In addition, our brains are constantly processing information and making predictions, even when we are not actively thinking about anything. This process can sometimes lead to spontaneous thoughts, such as when we suddenly remember something that we had forgotten or when we come up with a new idea.
 While some of our spontaneous thoughts may be negative or intrusive, many of them can also be helpful or creative. For example, a spontaneous thought may help us to solve a problem, come up with a new solution, or simply make us smile.
 Overall, it is clear that a significant portion of our thoughts come to us without our consciously choosing them. This is a natural part of the human cognitive process and can be both beneficial and detrimental.
    submitted by    /u/Georgeo57  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] DEVA: Tracking Anything with Decoupled Video Segmentation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kawop/r_deva_tracking_anything_with_decoupled_video/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kawop/r_deva_tracking_anything_with_decoupled_video/"/>
        <updated>2023-09-16T15:54:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Mediocre-Bullfrog686  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] I want to improve my self in machine learning field]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16kapda/d_i_want_to_improve_my_self_in_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16kapda/d_i_want_to_improve_my_self_in_machine_learning/"/>
        <updated>2023-09-16T15:46:11.000Z</updated>
        <summary type="html"><![CDATA[I am beginner in machine learning field. I know python, some basic machine learnig algorithm like linear, logistic, decision tree, random forest. I did some work on jupyter notebok related to machine learning like data gathering, data preprocessing, data modeling, data training, finding accuracy, confusion matrix, precision, recall, feature generation etc. i deployed one machine learning model on Skitlearn as well.
 Now, I just want to know that what next means what i can do more in machine learning field after this. I just need some guidance to move on further. I use kaggle and tensor flow (don't know exactly more about this ). If you have any suggestion or any guidance that will be appreciated.
    submitted by    /u/myteachexplore  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any good ai video editing services available?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16k8o6f/are_there_any_good_ai_video_editing_services/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16k8o6f/are_there_any_good_ai_video_editing_services/"/>
        <updated>2023-09-16T14:17:46.000Z</updated>
        <summary type="html"><![CDATA[I have a lighting business and I have LOADS of videos, time lapses, images from our projects.
 But I can't make decent videos with them for social media.
 I'd love to find an ai service where I can upload a projects media and prompt the ai with exactly what I want.
 Like, please use this content to create marketing videos for us on tiktok, Instagram and facebook Facebook
 Etc
    submitted by    /u/RulerOfThePixel  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] The fate of neural VQA and Semantic Scene Segmentation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16k7ezx/d_the_fate_of_neural_vqa_and_semantic_scene/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16k7ezx/d_the_fate_of_neural_vqa_and_semantic_scene/"/>
        <updated>2023-09-16T13:19:36.000Z</updated>
        <summary type="html"><![CDATA[Today we live in a world of multi-model LLMs. How will the following technologies fare against these LLM-based models? 
  
Neural VQA
 
Semantic Scene Segmentation
 
 Multi-model LLM are emerging quickly now, (such as NExT-GPT https://next-gpt.github.io/ ) . When you consider the kind of "understanding" of a visual scene these models are capable of, what will happen to prior approaches like Neural VQA? The nagging feeling that Neural VQA is going to be completely superseded by LLMs is palpable. The only vestige left for the older technology may have something to do with reasoning about the objects , such as properly counting the number of objects of a category that are present. But even that is getting sketchy. 
 On the topic of scene understanding, we can turn to semantic scene segmentation. SSS is a more complicated topic than Neural VQA. SOTA SSS algorithms are still largely employing DeConv Nets, and still require fully labelled datasets. With multi-model LLMs, there is a nagging question : Why go through the complexity/mess of first segmenting a scene very accurately, when an LLM can do better at identifying the entire scene's category in one fail swoop? 
 One might suggest that SSS still has a use in regards to interacting with the segmented objects of an environment, where one such "interaction" would be avoiding collisions with pedestrians, trees, or other cars. But honestly, SSS does not really make this connection with planning and action, it really only gives you the categories of the segments. THe autonomous vehicle's next moves are still an open problem. 
 What technologies do you expect that multi-model LLMs will supersede, if any?
    submitted by    /u/moschles  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I generate the missing part of the trick? Does this technique have a name?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16k6k5p/how_can_i_generate_the_missing_part_of_the_trick/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16k6k5p/how_can_i_generate_the_missing_part_of_the_trick/"/>
        <updated>2023-09-16T12:38:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/farineziq  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Three Ways to Generate AI Art Using Intel Arc GPUs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16k5b9l/r_three_ways_to_generate_ai_art_using_intel_arc/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16k5b9l/r_three_ways_to_generate_ai_art_using_intel_arc/"/>
        <updated>2023-09-16T11:36:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/reps_up  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How does recurrent neural network implements model based RL system purely in its activation dynamics(In blackbox meta-rl setting)?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16k3zjp/how_does_recurrent_neural_network_implements/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16k3zjp/how_does_recurrent_neural_network_implements/"/>
        <updated>2023-09-16T10:23:24.000Z</updated>
        <summary type="html"><![CDATA[I have read these papers "learning to reinforcement learn" and "PFC as meta RL system". The authors claim that when RNN is trained on multiple tasks from a task distribution using a model free RL algorithm, another model based RL algorithm emerges within the activation dynamics of RNN. The RNN with resulting activations acts as a standalone model based RL system on a new task(from the same task distribution) even after freezing the weights of outer loop model free algorithm of that. I couldn't understand how an RNN with only fixed activations act as RL? Can someone help?
    submitted by    /u/C7501  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Newsletter in Hardware Acceleration in Robotics #77]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16k2d1g/newsletter_in_hardware_acceleration_in_robotics_77/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16k2d1g/newsletter_in_hardware_acceleration_in_robotics_77/"/>
        <updated>2023-09-16T08:40:50.000Z</updated>
        <summary type="html"><![CDATA[https://news.accelerationrobotics.com/hardware-acceleration-in-robotics-77/
 Hardware acceleration in robotics news. Modi wants to make India a chip-making superpower. Can he?, What's new in China's robotics market?, July chip sales edge up, but are still well behind last year, Rockwell automation acquiring AMR developer Clearpath robotics
    submitted by    /u/pablocarrera  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Using GANs to help understand latent representations of small dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16k1mke/d_using_gans_to_help_understand_latent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16k1mke/d_using_gans_to_help_understand_latent/"/>
        <updated>2023-09-16T07:55:16.000Z</updated>
        <summary type="html"><![CDATA[Hey. First of all I'm not a researcher on this area, so pardon my ignorance.
 I'm looking to employ a GAN on a dataset. The goals are still a bit unclear, but it's mainly to improve classification by either data augmentation and class balancing, or understanding the data through the latent representations.
 I'm really keen on InfoGAN at the moment. I trained one on the MNIST and the continuous variables learned the rotation and width, as in the paper and other peoples code.
 But at this point I think I need some help. I have labels, which means that maybe I should use a conditional GAN. But how will I learn similar representations as those in InfoGAN? I know StyleGAN is the current big thing in this area, but my images are limited to similar pixel-range as MNIST, and StyleGAN seems dependant on the ProGAN idea of increasing resolution for training.
 I'm a bit confused on the whole topic and would love a pointer to any discussion etc., as I can't seem to find anything but papers and they seem to be only focused on human faces, my data is unnatural not unsimilar again to MNIST. I don't have semantical information either as I see many papers employing that.
 I see many papers employing semi-supervision in this area, but honestly I'm just a bit lost and overwhelmed as this is not my area and GAN papers are still not stopping (I read a post here from 2017 about a GAN making GANs...).
 If you read this far: thank you and any pointers and discussion are very welcome. I would post on /r/learnmachinelearning but I feel the discussion in there is very different from this. My main goal is data exploration, but also to prove effectiveness some classification will be necessary and here the generative approach may help to augment data efficiently.
    submitted by    /u/Infamous-Bank-7739  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do I change my domain (from DS to MLE)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16k10bn/d_how_do_i_change_my_domain_from_ds_to_mle/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16k10bn/d_how_do_i_change_my_domain_from_ds_to_mle/"/>
        <updated>2023-09-16T07:18:00.000Z</updated>
        <summary type="html"><![CDATA[Hi guys need some advice,
 I have been working as a data scientist for the past 3 years, mostly in the domain of time series & predictive analytics, hence do not currently have any major NLP/CV/Deep learning projects as such. 
 Now, I can see that NLP/CV/Gen AI is mostly in demand and they are really enjoyable as well.
 How do I shift into these domains, given that new companies having these roles are asking for similar past working experience? 
 â€‹
    submitted by    /u/immortal_omen  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeking Guidance on Reinforcement Learning for Optimal Power Market Bidding Strategy]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16jzepp/seeking_guidance_on_reinforcement_learning_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16jzepp/seeking_guidance_on_reinforcement_learning_for/"/>
        <updated>2023-09-16T05:39:53.000Z</updated>
        <summary type="html"><![CDATA[I'm currently exploring the application of reinforcement learning to address a challenge within the power market. Specifically, I'm focused on devising an optimal strategy for electricity bidding, encompassing both buying and selling options, across different hours of the day.
 Imagine we have a power generator capable of producing up to 800 MW of electricity daily, with a charging rate of up to 200 MW per hour. After continuously charging it for four hours, it reaches its maximum capacity, and further charging is restricted until some electricity is discharged. Our dataset spans the past 3 years and contains vital information such as temperature, hydro availability, gas prices, and locational marginal prices, which are pivotal in determining profitability. For instance, if we decide to puâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Ganimede, Jupyter Whiteboard]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jyh7p/d_ganimede_jupyter_whiteboard/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jyh7p/d_ganimede_jupyter_whiteboard/"/>
        <updated>2023-09-16T04:44:32.000Z</updated>
        <summary type="html"><![CDATA[I have been working on a alternative to Jupyter Notebooks. Please check it out and share your thoughts : https://github.com/nottherealsanta/ganimede 
 â€‹
 https://preview.redd.it/k8rcx8fwrjob1.png?width=2302&format=png&auto=webp&s=a8a670251f6c268acffc88a40bd528d8d438a5f5
    submitted by    /u/notsorealsanta  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/15/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16jxyda/oneminute_daily_ai_news_9152023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16jxyda/oneminute_daily_ai_news_9152023/"/>
        <updated>2023-09-16T04:14:25.000Z</updated>
        <summary type="html"><![CDATA[A little boy named Alex saw 17 different doctors over the course of three years, unable to find a root cause of his chronic pain. At her witâ€™s end, his mom, Courtney, fed his radiology report into ChatGPT and produced immediate answers.[1]
 In January, Wharton professor Christian Terwiesch gave his MBA final exam to ChatGPT. It passed with flying colors. Now, heâ€™s at it again with a new experiment to determine whether ChatGPT can come up with product ideas better and faster than his students. It can. And cheaper, too.[2]
 Bathroom-cleaning robot built for commercial businesses gives consumers hope for AI maid.[3]
 Judge admits he used ChatGPT to write a Court of Appeal ruling as he calls the AI tool â€˜jolly usefulâ€™.[4]
  
Sources:
 [1] https://radiologybusiness.com/topics/artificial-intelligence/after-seeing-17-different-doctors-boy-rare-condition-receives-diagnosis-chatgpt
 [2] https://knowledge.wharton.upenn.edu/article/is-chatgpt-a-better-entrepreneur-than-most/
 [3] https://www.foxnews.com/lifestyle/bathroom-cleaning-robot-built-commercial-businesses-gives-consumers-hope-ai-maid
 [4] https://www.dailymail.co.uk/news/article-12524607/Judge-ChatGPT-write-Court-Appeal-ruling-AI-useful.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to Evaluate Spectrograms?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jx9od/d_how_to_evaluate_spectrograms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jx9od/d_how_to_evaluate_spectrograms/"/>
        <updated>2023-09-16T03:36:22.000Z</updated>
        <summary type="html"><![CDATA[How would you evaluate generated spectrogram audio quality? Taking Riffusion for example, how would you then compare its performance to another generator? What are some common techniques that I could use?
 I mean of course purely in the quality of the audio itself, not my subjective opinion on how much I like the music
    submitted by    /u/DavesEmployee  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.06604</id>
        <link href="http://arxiv.org/abs/2309.06604"/>
        <updated>2023-09-16T00:40:58.245Z</updated>
        <summary type="html"><![CDATA[Algorithm selection and hyperparameter tuning are critical steps in both
academic and applied machine learning. On the other hand, these steps are
becoming ever increasingly delicate due to the extensive rise in the number,
diversity, and distributedness of machine learning resources. Multi-agent
systems, when applied to the design of machine learning platforms, bring about
several distinctive characteristics such as scalability, flexibility, and
robustness, just to name a few. This paper proposes a fully automatic and
collaborative agent-based mechanism for selecting distributedly organized
machine learning algorithms and simultaneously tuning their hyperparameters.
Our method builds upon an existing agent-based hierarchical machine-learning
platform and augments its query structure to support the aforementioned
functionalities without being limited to specific learning, selection, and
tuning mechanisms. We have conducted theoretical assessments, formal
verification, and analytical study to demonstrate the correctness, resource
utilization, and computational efficiency of our technique. According to the
results, our solution is totally correct and exhibits linear time and space
complexity in relation to the size of available resources. To provide concrete
examples of how the proposed methodologies can effectively adapt and perform
across a range of algorithmic options and datasets, we have also conducted a
series of experiments using a system comprised of 24 algorithms and 9 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1"&gt;Ahmad Esmaeili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1"&gt;Julia T. Rayz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matson_E/0/1/0/all/0/1"&gt;Eric T. Matson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nowhere coexpanding functions. (arXiv:2303.12814v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.12814</id>
        <link href="http://arxiv.org/abs/2303.12814"/>
        <updated>2023-09-16T00:40:58.227Z</updated>
        <summary type="html"><![CDATA[We define a family of $C^1$ functions which we call "nowhere coexpanding
functions" that is closed under composition and includes all $C^3$ functions
with non-positive Schwarzian derivative. We establish results on the number and
nature of the fixed points of these functions, including a generalisation of a
classic result of Singer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cook_A/0/1/0/all/0/1"&gt;Andrew Cook&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hammerlindl_A/0/1/0/all/0/1"&gt;Andy Hammerlindl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tucker_W/0/1/0/all/0/1"&gt;Warwick Tucker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.06800</id>
        <link href="http://arxiv.org/abs/2309.06800"/>
        <updated>2023-09-16T00:40:58.195Z</updated>
        <summary type="html"><![CDATA[Traffic prediction is a crucial topic because of its broad scope of
applications in the transportation domain. Recently, various studies have
achieved promising results. However, most studies assume the prediction
locations have complete or at least partial historical records and cannot be
extended to non-historical recorded locations. In real-life scenarios, the
deployment of sensors could be limited due to budget limitations and
installation availability, which makes most current models not applicable.
Though few pieces of literature tried to impute traffic states at the missing
locations, these methods need the data simultaneously observed at the locations
with sensors, making them not applicable to prediction tasks. Another drawback
is the lack of measurement of uncertainty in prediction, making prior works
unsuitable for risk-sensitive tasks or involving decision-making. To fill the
gap, inspired by the previous inductive graph neural network, this work
proposed an uncertainty-aware framework with the ability to 1) extend
prediction to missing locations with no historical records and significantly
extend spatial coverage of prediction locations while reducing deployment of
sensors and 2) generate probabilistic prediction with uncertainty
quantification to help the management of risk and decision making in the
down-stream tasks. Through extensive experiments on real-life datasets, the
result shows our method achieved promising results on prediction tasks, and the
uncertainty quantification gives consistent results which highly correlated
with the locations with and without historical data. We also show that our
model could help support sensor deployment tasks in the transportation field to
achieve higher accuracy with a limited sensor deployment budget.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1"&gt;Hao Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junxian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhiming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guanjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hua Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressed Real Numbers for AI: a case-study using a RISC-V CPU. (arXiv:2309.07158v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07158</id>
        <link href="http://arxiv.org/abs/2309.07158"/>
        <updated>2023-09-16T00:40:58.094Z</updated>
        <summary type="html"><![CDATA[As recently demonstrated, Deep Neural Networks (DNN), usually trained using
single precision IEEE 754 floating point numbers (binary32), can also work
using lower precision. Therefore, 16-bit and 8-bit compressed format have
attracted considerable attention. In this paper, we focused on two families of
formats that have already achieved interesting results in compressing binary32
numbers in machine learning applications, without sensible degradation of the
accuracy: bfloat and posit. Even if 16-bit and 8-bit bfloat/posit are routinely
used for reducing the storage of the weights/biases of trained DNNs, the
inference still often happens on the 32-bit FPU of the CPU (especially if GPUs
are not available). In this paper we propose a way to decompress a tensor of
bfloat/posits just before computations, i.e., after the compressed operands
have been loaded within the vector registers of a vector capable CPU, in order
to save bandwidth usage and increase cache efficiency. Finally, we show the
architectural parameters and considerations under which this solution is
advantageous with respect to the uncompressed one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1"&gt;Federico Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cococcioni_M/0/1/0/all/0/1"&gt;Marco Cococcioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibanez_R/0/1/0/all/0/1"&gt;Roger Ferrer Ib&amp;#xe0;&amp;#xf1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1"&gt;Jes&amp;#xf9;s Labarta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantovani_F/0/1/0/all/0/1"&gt;Filippo Mantovani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casas_M/0/1/0/all/0/1"&gt;Marc Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruffaldi_E/0/1/0/all/0/1"&gt;Emanuele Ruffaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saponara_S/0/1/0/all/0/1"&gt;Sergio Saponara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.00964</id>
        <link href="http://arxiv.org/abs/2309.00964"/>
        <updated>2023-09-16T00:40:57.913Z</updated>
        <summary type="html"><![CDATA[Since Large Language Models or LLMs have demonstrated high-quality
performance on many complex language tasks, there is a great interest in
bringing these LLMs to mobile devices for faster responses and better privacy
protection. However, the size of LLMs (i.e., billions of parameters) requires
highly effective compression to fit into storage-limited devices. Among many
compression techniques, weight-clustering, a form of non-linear quantization,
is one of the leading candidates for LLM compression, and supported by modern
smartphones. Yet, its training overhead is prohibitively significant for LLM
fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown
the state-of-the-art trade-off between compression ratio and accuracy
regression, but its large memory complexity makes it nearly impossible to apply
to train-time LLM compression. In this paper, we propose a memory-efficient DKM
implementation, eDKM powered by novel techniques to reduce the memory footprint
of DKM by orders of magnitudes. For a given tensor to be saved on CPU for the
backward pass of DKM, we compressed the tensor by applying uniquification and
sharding after checking if there is no duplicated tensor previously copied to
CPU. Our experimental results demonstrate that \prjname can fine-tune and
compress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) with
the Alpaca dataset by reducing the train-time memory footprint of a decoder
layer by 130$\times$, while delivering good accuracy on broader LLM benchmarks
(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;Minsik Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1"&gt;Keivan A. Vahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1"&gt;Qichen Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1"&gt;Saurabh Adya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundo_C/0/1/0/all/0/1"&gt;Carlo C Del Mundo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1"&gt;Mohammad Rastegari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1"&gt;Devang Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zatloukal_P/0/1/0/all/0/1"&gt;Peter Zatloukal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-Assisted Discovery of Novel Reactor Designs. (arXiv:2308.08841v2 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.08841</id>
        <link href="http://arxiv.org/abs/2308.08841"/>
        <updated>2023-09-16T00:40:57.899Z</updated>
        <summary type="html"><![CDATA[Additive manufacturing has enabled the fabrication of advanced reactor
geometries, permitting larger, more complex design spaces. Identifying
promising configurations within such spaces presents a significant challenge
for current approaches. Furthermore, existing parameterisations of reactor
geometries are low-dimensional with expensive optimisation limiting more
complex solutions. To address this challenge, we establish a machine
learning-assisted approach for the design of the next-generation of chemical
reactors, combining the application of high-dimensional parameterisations,
computational fluid dynamics, and multi-fidelity Bayesian optimisation. We
associate the development of mixing-enhancing vortical flow structures in novel
coiled reactors with performance, and use our approach to identify key
characteristics of optimal designs. By appealing to fluid mechanical
principles, we rationalise the selection of novel design features that lead to
experimental performance improvements of ~60% over conventional designs. Our
results demonstrate that coupling advanced manufacturing techniques with
`augmented-intelligence' approaches can lead to superior design performance
and, consequently, emissions-reduction and sustainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1"&gt;Tom Savage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basha_N/0/1/0/all/0/1"&gt;Nausheen Basha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonough_J/0/1/0/all/0/1"&gt;Jonathan McDonough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matar_O/0/1/0/all/0/1"&gt;Omar K Matar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanona_E/0/1/0/all/0/1"&gt;Ehecatl Antonio del Rio Chanona&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.01921</id>
        <link href="http://arxiv.org/abs/2308.01921"/>
        <updated>2023-09-16T00:40:57.886Z</updated>
        <summary type="html"><![CDATA[Fast screening of drug molecules based on the ligand binding affinity is an
important step in the drug discovery pipeline. Graph neural fingerprint is a
promising method for developing molecular docking surrogates with high
throughput and great fidelity. In this study, we built a COVID-19 drug docking
dataset of about 300,000 drug candidates on 23 coronavirus protein targets.
With this dataset, we trained graph neural fingerprint docking models for
high-throughput virtual COVID-19 drug screening. The graph neural fingerprint
models yield high prediction accuracy on docking scores with the mean squared
error lower than $0.21$ kcal/mol for most of the docking targets, showing
significant improvement over conventional circular fingerprint methods. To make
the neural fingerprints transferable for unknown targets, we also propose a
transferable graph neural fingerprint method trained on multiple targets. With
comparable accuracy to target-specific graph neural fingerprint models, the
transferable model exhibits superb training and data efficiency. We highlight
that the impact of this study extends beyond COVID-19 dataset, as our approach
for fast virtual ligand screening can be easily adapted and integrated into a
general machine learning-accelerated pipeline to battle future bio-threats.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yihui Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kagawa_A/0/1/0/all/0/1"&gt;Ai Kagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Carbone_M/0/1/0/all/0/1"&gt;Matthew R. Carbone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1"&gt;Samuel Yen-Chi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiaohui Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Shinjae Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1"&gt;Austin Clyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1"&gt;Arvind Ramanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1"&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dam_H/0/1/0/all/0/1"&gt;Hubertus J. J. van Dam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Liu_D/0/1/0/all/0/1"&gt;Deyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.07398</id>
        <link href="http://arxiv.org/abs/2309.07398"/>
        <updated>2023-09-16T00:40:57.567Z</updated>
        <summary type="html"><![CDATA[Traditional adversarial attacks concentrate on manipulating clean examples in
the pixel space by adding adversarial perturbations. By contrast, semantic
adversarial attacks focus on changing semantic attributes of clean examples,
such as color, context, and features, which are more feasible in the real
world. In this paper, we propose a framework to quickly generate a semantic
adversarial attack by leveraging recent diffusion models since semantic
information is included in the latent space of well-trained diffusion models.
Then there are two variants of this framework: 1) the Semantic Transformation
(ST) approach fine-tunes the latent space of the generated image and/or the
diffusion model itself; 2) the Latent Masking (LM) approach masks the latent
space with another target image and local backpropagation-based interpretation
methods. Additionally, the ST approach can be applied in either white-box or
black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ
datasets, and our framework demonstrates great fidelity, generalizability, and
transferability compared to other baselines. Our approaches achieve
approximately 100% attack success rate in multiple settings with the best FID
as 36.61. Code is available at
https://github.com/steven202/semantic_adv_via_dm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinhao Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1"&gt;Edward Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamm_M/0/1/0/all/0/1"&gt;Matthew Stamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.11110</id>
        <link href="http://arxiv.org/abs/2205.11110"/>
        <updated>2023-09-16T00:40:57.474Z</updated>
        <summary type="html"><![CDATA[Grasping inhomogeneous objects in real-world applications remains a
challenging task due to the unknown physical properties such as mass
distribution and coefficient of friction. In this study, we propose a
meta-learning algorithm called ConDex, which incorporates Conditional Neural
Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical
properties of objects using depth images. ConDex efficiently acquires physical
embeddings from limited trials, enabling precise grasping point estimation.
Furthermore, ConDex is capable of updating the predicted grasping quality
iteratively from new trials in an online fashion. To the best of our knowledge,
we are the first who generate two object datasets focusing on inhomogeneous
physical properties with varying mass distributions and friction coefficients.
Extensive evaluations in simulation demonstrate ConDex's superior performance
over DexNet-2.0 and existing meta-learning-based grasping pipelines.
Furthermore, ConDex shows robust generalization to previously unseen real-world
objects despite training solely in the simulation. The synthetic and real-world
datasets will be published as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Ning Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Ruijie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1"&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning. (arXiv:2206.03420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.03420</id>
        <link href="http://arxiv.org/abs/2206.03420"/>
        <updated>2023-09-16T00:40:57.474Z</updated>
        <summary type="html"><![CDATA[Spatial-temporal data contains rich information and has been widely studied
in recent years due to the rapid development of relevant applications in many
fields. For instance, medical institutions often use electrodes attached to
different parts of a patient to analyse the electorencephal data rich with
spatial and temporal features for health assessment and disease diagnosis.
Existing research has mainly used deep learning techniques such as
convolutional neural network (CNN) or recurrent neural network (RNN) to extract
hidden spatial-temporal features. Yet, it is challenging to incorporate both
inter-dependencies spatial information and dynamic temporal changes
simultaneously. In reality, for a model that leverages these spatial-temporal
features to fulfil complex prediction tasks, it often requires a colossal
amount of training data in order to obtain satisfactory model performance.
Considering the above-mentioned challenges, we propose an adaptive federated
relevance framework, namely FedRel, for spatial-temporal graph learning in this
paper. After transforming the raw spatial-temporal data into high quality
features, the core Dynamic Inter-Intra Graph (DIIG) module in the framework is
able to use these features to generate the spatial-temporal graphs capable of
capturing the hidden topological and long-term temporal correlation information
in these graphs. To improve the model generalization ability and performance
while preserving the local data privacy, we also design a relevance-driven
federated learning module in our framework to leverage diverse data
distributions from different participants with attentive aggregations of their
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tiehua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuze Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhishu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Rui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing. (arXiv:2309.04612v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.04612</id>
        <link href="http://arxiv.org/abs/2309.04612"/>
        <updated>2023-09-16T00:40:57.469Z</updated>
        <summary type="html"><![CDATA[Feature generation aims to generate new and meaningful features to create a
discriminative representation space.A generated feature is meaningful when the
generated feature is from a feature pair with inherent feature interaction. In
the real world, experienced data scientists can identify potentially useful
feature-feature interactions, and generate meaningful dimensions from an
exponentially large search space, in an optimal crossing form over an optimal
generation path. But, machines have limited human-like abilities.We generalize
such learning tasks as self-optimizing feature generation. Self-optimizing
feature generation imposes several under-addressed challenges on existing
systems: meaningful, robust, and efficient generation. To tackle these
challenges, we propose a principled and generic representation-crossing
framework to solve self-optimizing feature generation.To achieve hashing
representation, we propose a three-step approach: feature discretization,
feature hashing, and descriptive summarization. To achieve reinforcement
crossing, we develop a hierarchical reinforcement feature crossing approach.We
present extensive experimental results to demonstrate the effectiveness and
efficiency of the proposed method. The code is available at
https://github.com/yingwangyang/HRC_feature_cross.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_W/0/1/0/all/0/1"&gt;Wangyang Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kunpeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Leilei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanjie Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.07808</id>
        <link href="http://arxiv.org/abs/2309.07808"/>
        <updated>2023-09-16T00:40:57.464Z</updated>
        <summary type="html"><![CDATA[More research attention has recently been given to end-to-end autonomous
driving technologies where the entire driving pipeline is replaced with a
single neural network because of its simpler structure and faster inference
time. Despite this appealing approach largely reducing the components in
driving pipeline, its simplicity also leads to interpretability problems and
safety issues arXiv:2003.06404. The trained policy is not always compliant with
the traffic rules and it is also hard to discover the reason for the
misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are
also critical to autonomous driving's security and feasibility to perceive the
surrounding environment under complex driving scenarios. In this paper, we
proposed P-CSG, a novel penalty-based imitation learning approach with cross
semantics generation sensor fusion technologies to increase the overall
performance of End-to-End Autonomous Driving. We conducted an assessment of our
model's performance using the Town 05 Long benchmark, achieving an impressive
driving score improvement of over 15%. Furthermore, we conducted robustness
evaluations against adversarial attacks like FGSM and Dot attacks, revealing a
substantial increase in robustness compared to baseline models.More detailed
information, such as code-based resources, ablation studies and videos can be
found at https://hk-zh.github.io/p-csg-plus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hongkuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_A/0/1/0/all/0/1"&gt;Aifen Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1"&gt;Wei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1"&gt;Letian Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Directed Scattering for Knowledge Graph-based Cellular Signaling Analysis. (arXiv:2309.07813v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07813</id>
        <link href="http://arxiv.org/abs/2309.07813"/>
        <updated>2023-09-16T00:40:57.459Z</updated>
        <summary type="html"><![CDATA[Directed graphs are a natural model for many phenomena, in particular
scientific knowledge graphs such as molecular interaction or chemical reaction
networks that define cellular signaling relationships. In these situations,
source nodes typically have distinct biophysical properties from sinks. Due to
their ordered and unidirectional relationships, many such networks also have
hierarchical and multiscale structure. However, the majority of methods
performing node- and edge-level tasks in machine learning do not take these
properties into account, and thus have not been leveraged effectively for
scientific tasks such as cellular signaling network inference. We propose a new
framework called Directed Scattering Autoencoder (DSAE) which uses a directed
version of a geometric scattering transform, combined with the non-linear
dimensionality reduction properties of an autoencoder and the geometric
properties of the hyperbolic space to learn latent hierarchies. We show this
method outperforms numerous others on tasks such as embedding directed graphs
and learning cellular signaling networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venkat_A/0/1/0/all/0/1"&gt;Aarthi Venkat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chew_J/0/1/0/all/0/1"&gt;Joyce Chew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_F/0/1/0/all/0/1"&gt;Ferran Cardoso Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tape_C/0/1/0/all/0/1"&gt;Christopher J. Tape&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1"&gt;Michael Perlmutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1"&gt;Smita Krishnaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07332</id>
        <link href="http://arxiv.org/abs/2309.07332"/>
        <updated>2023-09-16T00:40:57.448Z</updated>
        <summary type="html"><![CDATA[Accurately labeling biomedical data presents a challenge. Traditional
semi-supervised learning methods often under-utilize available unlabeled data.
To address this, we propose a novel reliability-based training data cleaning
method employing inductive conformal prediction (ICP). This method capitalizes
on a small set of accurately labeled training data and leverages ICP-calculated
reliability metrics to rectify mislabeled data and outliers within vast
quantities of noisy training data. The efficacy of the method is validated
across three classification tasks within distinct modalities: filtering
drug-induced-liver-injury (DILI) literature with title and abstract, predicting
ICU admission of COVID-19 patients through CT radiomics and electronic health
records, and subtyping breast cancer using RNA-sequencing data. Varying levels
of noise to the training labels were introduced through label permutation.
Results show significant enhancements in classification performance: accuracy
enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC
enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and
accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing
experiments (up to 74.6% and 89.0%). Our method offers the potential to
substantially boost classification performance in multi-modal biomedical
machine learning tasks. Importantly, it accomplishes this without necessitating
an excessive volume of meticulously curated training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xianghao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qinmei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guangming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1"&gt;Olivier Gevaert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.07794</id>
        <link href="http://arxiv.org/abs/2309.07794"/>
        <updated>2023-09-16T00:40:57.443Z</updated>
        <summary type="html"><![CDATA[Effectively leveraging multimodal information from social media posts is
essential to various downstream tasks such as sentiment analysis, sarcasm
detection and hate speech classification. However, combining text and image
information is challenging because of the idiosyncratic cross-modal semantics
with hidden or complementary information present in matching image-text pairs.
In this work, we aim to directly model this by proposing the use of two
auxiliary losses jointly with the main task when fine-tuning any pre-trained
multimodal model. Image-Text Contrastive (ITC) brings image-text
representations of a post closer together and separates them from different
posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates
the understanding of semantic correspondence between images and text by
penalizing unrelated pairs. We combine these objectives with five multimodal
models, demonstrating consistent improvements across four popular social media
datasets. Furthermore, through detailed analysis, we shed light on the specific
scenarios and cases where each auxiliary task proves to be most effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1"&gt;Danae S&amp;#xe1;nchez Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preotiuc_Pietro_D/0/1/0/all/0/1"&gt;Daniel Preo&amp;#x163;iuc-Pietro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1"&gt;Nikolaos Aletras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.16834</id>
        <link href="http://arxiv.org/abs/2307.16834"/>
        <updated>2023-09-16T00:40:57.436Z</updated>
        <summary type="html"><![CDATA[Innovative enhancement in embedded system platforms, specifically hardware
accelerations, significantly influence the application of deep learning in
real-world scenarios. These innovations translate human labor efforts into
automated intelligent systems employed in various areas such as autonomous
driving, robotics, Internet-of-Things (IoT), and numerous other impactful
applications. NVIDIA's Jetson platform is one of the pioneers in offering
optimal performance regarding energy efficiency and throughput in the execution
of deep learning algorithms. Previously, most benchmarking analysis was based
on 2D images with a single deep learning model for each comparison result. In
this paper, we implement an end-to-end video-based crime-scene anomaly
detection system inputting from surveillance videos and the system is deployed
and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin
Nano). The comparison analysis includes the integration of Torch-TensorRT as a
software developer kit from NVIDIA for the model performance optimisation. The
system is built based on the PySlowfast open-source project from Facebook as
the coding template. The end-to-end system process comprises the videos from
camera, data preprocessing pipeline, feature extractor and the anomaly
detection. We provide the experience of an AI-based system deployment on
various Jetson Edge devices with Docker technology. Regarding anomaly
detectors, a weakly supervised video-based deep learning model called Robust
Temporal Feature Magnitude Learning (RTFM) is applied in the system. The
approach system reaches 47.56 frames per second (FPS) inference speed on a
Jetson edge device with only 3.11 GB RAM usage total. We also discover the
promising Jetson device that the AI system achieves 15% better performance than
the previous version of Jetson devices while consuming 50% less energy power.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hoang Viet Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Thinh Gia Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1"&gt;Chuong Dinh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1"&gt;An Dinh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1"&gt;Hien Bich Vo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-step prediction of chlorophyll concentration based on Adaptive Graph-Temporal Convolutional Network with Series Decomposition. (arXiv:2309.07187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07187</id>
        <link href="http://arxiv.org/abs/2309.07187"/>
        <updated>2023-09-16T00:40:57.431Z</updated>
        <summary type="html"><![CDATA[Chlorophyll concentration can well reflect the nutritional status and algal
blooms of water bodies, and is an important indicator for evaluating water
quality. The prediction of chlorophyll concentration change trend is of great
significance to environmental protection and aquaculture. However, there is a
complex and indistinguishable nonlinear relationship between many factors
affecting chlorophyll concentration. In order to effectively mine the nonlinear
features contained in the data. This paper proposes a time-series decomposition
adaptive graph-time convolutional network ( AGTCNSD ) prediction model.
Firstly, the original sequence is decomposed into trend component and periodic
component by moving average method. Secondly, based on the graph convolutional
neural network, the water quality parameter data is modeled, and a parameter
embedding matrix is defined. The idea of matrix decomposition is used to assign
weight parameters to each node. The adaptive graph convolution learns the
relationship between different water quality parameters, updates the state
information of each parameter, and improves the learning ability of the update
relationship between nodes. Finally, time dependence is captured by time
convolution to achieve multi-step prediction of chlorophyll concentration. The
validity of the model is verified by the water quality data of the coastal city
Beihai. The results show that the prediction effect of this method is better
than other methods. It can be used as a scientific resource for environmental
management decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Wenyang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xv_C/0/1/0/all/0/1"&gt;Chongxuan Xv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v7 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.09597</id>
        <link href="http://arxiv.org/abs/2212.09597"/>
        <updated>2023-09-16T00:40:57.424Z</updated>
        <summary type="html"><![CDATA[Reasoning, as an essential ability for complex problem-solving, can provide
back-end support for various real-world applications, such as medical
diagnosis, negotiation, etc. This paper provides a comprehensive survey of
cutting-edge research on reasoning with language model prompting. We introduce
research works with comparisons and summaries and provide systematic resources
to help beginners. We also discuss the potential reasons for emerging such
reasoning abilities and highlight future research directions. Resources are
available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated
periodically).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1"&gt;Shuofei Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1"&gt;Yixin Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.07601</id>
        <link href="http://arxiv.org/abs/2309.07601"/>
        <updated>2023-09-16T00:40:57.411Z</updated>
        <summary type="html"><![CDATA[Credibility signals represent a wide range of heuristics that are typically
used by journalists and fact-checkers to assess the veracity of online content.
Automating the task of credibility signal extraction, however, is very
challenging as it requires high-accuracy signal-specific extractors to be
trained, while there are currently no sufficiently large datasets annotated
with all credibility signals. This paper investigates whether large language
models (LLMs) can be prompted effectively with a set of 18 credibility signals
to produce weak labels for each signal. We then aggregate these potentially
noisy labels using weak supervision in order to predict content veracity. We
demonstrate that our approach, which combines zero-shot LLM credibility signal
labeling and weak supervision, outperforms state-of-the-art classifiers on two
misinformation datasets without using any ground-truth labels for training. We
also analyse the contribution of the individual credibility signals towards
predicting content veracity, which provides new valuable insights into their
role in misinformation detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leite_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o A. Leite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razuvayevskaya_O/0/1/0/all/0/1"&gt;Olesya Razuvayevskaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1"&gt;Kalina Bontcheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1"&gt;Carolina Scarton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.07812</id>
        <link href="http://arxiv.org/abs/2309.07812"/>
        <updated>2023-09-16T00:40:57.405Z</updated>
        <summary type="html"><![CDATA[Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yumeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraj_S/0/1/0/all/0/1"&gt;Soumya Jayaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludmir_E/0/1/0/all/0/1"&gt;Ethan B Ludmir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_K/0/1/0/all/0/1"&gt;Kirk Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07716</id>
        <link href="http://arxiv.org/abs/2309.07716"/>
        <updated>2023-09-16T00:40:57.401Z</updated>
        <summary type="html"><![CDATA[Despite the many successful applications of deep learning models for
multidimensional signal and image processing, most traditional neural networks
process data represented by (multidimensional) arrays of real numbers. The
intercorrelation between feature channels is usually expected to be learned
from the training data, requiring numerous parameters and careful training. In
contrast, vector-valued neural networks are conceived to process arrays of
vectors and naturally consider the intercorrelation between feature channels.
Consequently, they usually have fewer parameters and often undergo more robust
training than traditional neural networks. This paper aims to present a broad
framework for vector-valued neural networks, referred to as V-nets. In this
context, hypercomplex-valued neural networks are regarded as vector-valued
models with additional algebraic properties. Furthermore, this paper explains
the relationship between vector-valued and traditional neural networks.
Precisely, a vector-valued neural network can be obtained by placing
restrictions on a real-valued model to consider the intercorrelation between
feature channels. Finally, we show how V-nets, including hypercomplex-valued
neural networks, can be implemented in current deep-learning libraries as
real-valued networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1"&gt;Marcos Eduardo Valle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based machine learning model for diagnosis and monitoring of Parkinson's Disease in smart IoT environment. (arXiv:2309.07134v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07134</id>
        <link href="http://arxiv.org/abs/2309.07134"/>
        <updated>2023-09-16T00:40:57.396Z</updated>
        <summary type="html"><![CDATA[The study presents the concept of a computationally efficient machine
learning (ML) model for diagnosing and monitoring Parkinson's disease (PD) in
an Internet of Things (IoT) environment using rest-state EEG signals (rs-EEG).
We computed different types of entropy from EEG signals and found that Fuzzy
Entropy performed the best in diagnosing and monitoring PD using rs-EEG. We
also investigated different combinations of signal frequency ranges and EEG
channels to accurately diagnose PD. Finally, with a fewer number of features
(11 features), we achieved a maximum classification accuracy (ARKF) of ~99.9%.
The most prominent frequency range of EEG signals has been identified, and we
have found that high classification accuracy depends on low-frequency signal
components (0-4 Hz). Moreover, the most informative signals were mainly
received from the right hemisphere of the head (F8, P8, T8, FC6). Furthermore,
we assessed the accuracy of the diagnosis of PD using three different lengths
of EEG data (150-1000 samples). Because the computational complexity is reduced
by reducing the input data. As a result, we have achieved a maximum mean
accuracy of 99.9% for a sample length (LEEG) of 1000 (~7.8 seconds), 98.2% with
a LEEG of 800 (~6.2 seconds), and 79.3% for LEEG = 150 (~1.2 seconds). By
reducing the number of features and segment lengths, the computational cost of
classification can be reduced. Lower-performance smart ML sensors can be used
in IoT environments for enhances human resilience to PD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Maksim Belyaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murugappan_M/0/1/0/all/0/1"&gt;Murugappan Murugappan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Velichko_A/0/1/0/all/0/1"&gt;Andrei Velichko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korzun_D/0/1/0/all/0/1"&gt;Dmitry Korzun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Feature Amplification: Feature Learning and Generalization in Neural Networks. (arXiv:2202.07626v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07626</id>
        <link href="http://arxiv.org/abs/2202.07626"/>
        <updated>2023-09-16T00:40:57.383Z</updated>
        <summary type="html"><![CDATA[In this work, we provide a characterization of the feature-learning process
in two-layer ReLU networks trained by gradient descent on the logistic loss
following random initialization. We consider data with binary labels that are
generated by an XOR-like function of the input features. We permit a constant
fraction of the training labels to be corrupted by an adversary. We show that,
although linear classifiers are no better than random guessing for the
distribution we consider, two-layer ReLU networks trained by gradient descent
achieve generalization error close to the label noise rate. We develop a novel
proof technique that shows that at initialization, the vast majority of neurons
function as random features that are only weakly correlated with useful
features, and the gradient descent dynamics 'amplify' these weak, random
features to strong, useful features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations. (arXiv:2309.07684v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07684</id>
        <link href="http://arxiv.org/abs/2309.07684"/>
        <updated>2023-09-16T00:40:57.362Z</updated>
        <summary type="html"><![CDATA[The primary goal of this research is to propose a novel architecture for a
deep neural network that can solve fractional differential equations
accurately. A Gaussian integration rule and a $L_1$ discretization technique
are used in the proposed design. In each equation, a deep neural network is
used to approximate the unknown function. Three forms of fractional
differential equations have been examined to highlight the method's
versatility: a fractional ordinary differential equation, a fractional order
integrodifferential equation, and a fractional order partial differential
equation. The results show that the proposed architecture solves different
forms of fractional differential equations with excellent precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Firoozsalari_A/0/1/0/all/0/1"&gt;Ali Nosrati Firoozsalari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazraeh_H/0/1/0/all/0/1"&gt;Hassan Dana Mazraeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghaei_A/0/1/0/all/0/1"&gt;Alireza Afzal Aghaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parand_K/0/1/0/all/0/1"&gt;Kourosh Parand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images. (arXiv:2304.07097v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.07097</id>
        <link href="http://arxiv.org/abs/2304.07097"/>
        <updated>2023-09-16T00:40:57.357Z</updated>
        <summary type="html"><![CDATA[Alzheimer's Disease (AD) is a progressive disease preceded by Mild Cognitive
Impairment (MCI). Early detection of AD is crucial for making treatment
decisions. However, most of the literature on computer-assisted detection of AD
focuses on classifying brain images into one of three major categories:
healthy, MCI, and AD; or categorizing MCI patients into (1) progressive: those
who progress from MCI to AD at a future examination time, and (2) stable: those
who stay as MCI and never progress to AD. This misses the opportunity to
accurately identify the trajectory of progressive MCI patients. In this paper,
we revisit the brain image classification task for AD identification and
re-frame it as an ordinal classification task to predict how close a patient is
to the severe AD stage. To this end, we select progressive MCI patients from
the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and construct an
ordinal dataset with a prediction target that indicates the time to progression
to AD. We train a Siamese network model to predict the time to onset of AD
based on MRI brain images. We also propose a Weighted variety of Siamese
network and compare its performance to a baseline model. Our evaluations show
that incorporating a weighting factor to Siamese networks brings considerable
performance gain at predicting how close input brain MRI images are to
progressing to AD. Moreover, we complement our results with an interpretation
of the learned embedding space of the Siamese networks using a model
explainability technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hagos_M/0/1/0/all/0/1"&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Belton_N/0/1/0/all/0/1"&gt;Niamh Belton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Killeen_R/0/1/0/all/0/1"&gt;Ronan P. Killeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1"&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Namee_B/0/1/0/all/0/1"&gt;Brian Mac Namee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.10851</id>
        <link href="http://arxiv.org/abs/2211.10851"/>
        <updated>2023-09-16T00:40:57.352Z</updated>
        <summary type="html"><![CDATA[Reinforcement Learning views the maximization of rewards and avoidance of
punishments as central to explaining goal-directed behavior. However, over a
life, organisms will need to learn about many different aspects of the world's
structure: the states of the world and state-vector transition dynamics. The
number of combinations of states grows exponentially as an agent incorporates
new knowledge, and there is no obvious weighted combination of pre-existing
rewards or costs defined for a given combination of states, as such a weighting
would need to encode information about good and bad combinations prior to an
agent's experience in the world. Therefore, we must develop more naturalistic
accounts of behavior and motivation in large state-spaces. We show that it is
possible to use only the intrinsic motivation metric of empowerment, which
measures the agent's capacity to realize many possible futures under a
transition operator. We propose to scale empowerment to hierarchical
state-spaces by using Operator Bellman Equations. These equations produce
state-time feasibility functions, which are compositional hierarchical
state-time transition operators that map an initial state and time when an
agent begins a policy to the final states and times of completing a goal.
Because these functions are hierarchical operators we can define hierarchical
empowerment measures on them. An agent can then optimize plans to distant
states and times to maximize its hierarchical empowerment-gain, allowing it to
discover goals that bring about a more favorable coupling of its internal
structure (physiological states) to its external environment (world structure &
spatial state). Life-long agents could therefore be primarily animated by
principles of compositionality and empowerment, exhibiting self-concern for the
growth & maintenance of their own structural integrity without recourse to
reward-maximization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ringstrom_T/0/1/0/all/0/1"&gt;Thomas J. Ringstrom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07136</id>
        <link href="http://arxiv.org/abs/2309.07136"/>
        <updated>2023-09-16T00:40:57.344Z</updated>
        <summary type="html"><![CDATA[Electrocardiogram (ECG) is one of the most important diagnostic tools in
clinical applications. With the advent of advanced algorithms, various deep
learning models have been adopted for ECG tasks. However, the potential of
Transformers for ECG data is not yet realized, despite their widespread success
in computer vision and natural language processing. In this work, we present a
useful masked Transformer method for ECG classification referred to as MTECG,
which expands the application of masked autoencoders to ECG time series. We
construct a dataset comprising 220,251 ECG recordings with a broad range of
diagnoses annoated by medical experts to explore the properties of MTECG. Under
the proposed training strategies, a lightweight model with 5.7M parameters
performs stably well on a broad range of masking ratios (5%-75%). The ablation
studies highlight the importance of fluctuated reconstruction targets, training
schedule length, layer-wise LR decay and DropPath rate. The experiments on both
private and public ECG datasets demonstrate that MTECG-T significantly
outperforms the recent state-of-the-art algorithms in ECG classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Ya Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diao_X/0/1/0/all/0/1"&gt;Xiaolin Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yanni Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiaohan Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07579</id>
        <link href="http://arxiv.org/abs/2309.07579"/>
        <updated>2023-09-16T00:40:57.321Z</updated>
        <summary type="html"><![CDATA[In recent years, Transformer-based auto-attention mechanisms have been
successfully applied to the analysis of a variety of context-reliant data
types, from texts to images and beyond, including data from non-Euclidean
geometries. In this paper, we present such a mechanism, designed to classify
sequences of Symmetric Positive Definite matrices while preserving their
Riemannian geometry throughout the analysis. We apply our method to automatic
sleep staging on timeseries of EEG-derived covariance matrices from a standard
dataset, obtaining high levels of stage-wise performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seraphim_M/0/1/0/all/0/1"&gt;Mathieu Seraphim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1"&gt;Alexis Lechervy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yger_F/0/1/0/all/0/1"&gt;Florian Yger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brun_L/0/1/0/all/0/1"&gt;Luc Brun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etard_O/0/1/0/all/0/1"&gt;Olivier Etard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors. (arXiv:2309.07888v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.07888</id>
        <link href="http://arxiv.org/abs/2309.07888"/>
        <updated>2023-09-16T00:40:57.307Z</updated>
        <summary type="html"><![CDATA[We present a novel local-global feature fusion framework for body-weight
exercise recognition with floor-based dynamic pressure maps. One step further
from the existing studies using deep neural networks mainly focusing on global
feature extraction, the proposed framework aims to combine local and global
features using image processing techniques and the YOLO object detection to
localize pressure profiles from different body parts and consider physical
constraints. The proposed local feature extraction method generates two sets of
high-level local features consisting of cropped pressure mapping and numerical
features such as angular orientation, location on the mat, and pressure area.
In addition, we adopt a knowledge distillation for regularization to preserve
the knowledge of the global feature extraction and improve the performance of
the exercise recognition. Our experimental results demonstrate a notable 11
percent improvement in F1 score for exercise recognition while preserving
label-specific features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1"&gt;Davinder Pal Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_L/0/1/0/all/0/1"&gt;Lala Shakti Swarup Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1"&gt;Sungho Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1"&gt;Paul Lukowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture. (arXiv:2302.10848v2 [cond-mat.dis-nn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.10848</id>
        <link href="http://arxiv.org/abs/2302.10848"/>
        <updated>2023-09-16T00:40:57.259Z</updated>
        <summary type="html"><![CDATA[In Changjun Fan et al. [Nature Communications
https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep
reinforced learning approach to augment combinatorial optimization heuristics.
In particular, they present results for several spin glass ground state
problems, for which instances on non-planar networks are generally NP-hard, in
comparison with several Monte Carlo based methods, such as simulated annealing
(SA) or parallel tempering (PT). Indeed, those results demonstrate that the
reinforced learning improves the results over those obtained with SA or PT, or
at least allows for reduced runtimes for the heuristics before results of
comparable quality have been obtained relative to those other methods. To
facilitate the conclusion that their method is ''superior'', the authors pursue
two basic strategies: (1) A commercial GUROBI solver is called on to procure a
sample of exact ground states as a testbed to compare with, and (2) a
head-to-head comparison between the heuristics is given for a sample of larger
instances where exact ground states are hard to ascertain. Here, we put these
studies into a larger context, showing that the claimed superiority is at best
marginal for smaller samples and becomes essentially irrelevant with respect to
any sensible approximation of true ground states in the larger samples. For
example, this method becomes irrelevant as a means to determine stiffness
exponents $\theta$ in $d>2$, as mentioned by the authors, where the problem is
not only NP-hard but requires the subtraction of two almost equal ground-state
energies and systemic errors in each of $\approx 1\%$ found here are
unacceptable. This larger picture on the method arises from a straightforward
finite-size corrections study over the spin glass ensembles the authors employ,
using data that has been available for decades.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Boettcher_S/0/1/0/all/0/1"&gt;Stefan Boettcher&lt;/a&gt; (Emory U)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal. (arXiv:2309.00855v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.00855</id>
        <link href="http://arxiv.org/abs/2309.00855"/>
        <updated>2023-09-16T00:40:57.252Z</updated>
        <summary type="html"><![CDATA[The marketplace system connecting demands and supplies has been explored to
develop unbiased decision-making in valuing properties. Real estate appraisal
serves as one of the high-cost property valuation tasks for financial
institutions since it requires domain experts to appraise the estimation based
on the corresponding knowledge and the judgment of the market. Existing
automated valuation models reducing the subjectivity of domain experts require
a large number of transactions for effective evaluation, which is predominantly
limited to not only the labeling efforts of transactions but also the
generalizability of new developing and rural areas. To learn representations
from unlabeled real estate sets, existing self-supervised learning (SSL) for
tabular data neglects various important features, and fails to incorporate
domain knowledge. In this paper, we propose DoRA, a Domain-based
self-supervised learning framework for low-resource Real estate Appraisal. DoRA
is pre-trained with an intra-sample geographic prediction as the pretext task
based on the metadata of the real estate for equipping the real estate
representations with prior domain knowledge. Furthermore, inter-sample
contrastive learning is employed to generalize the representations to be robust
for limited transactions of downstream tasks. Our benchmark results on three
property types of real-world transactions show that DoRA significantly
outperforms the SSL baselines for tabular data, the graph-based methods, and
the supervised approaches in the few-shot scenarios by at least 7.6% for MAPE,
11.59% for MAE, and 3.34% for HR10%. We expect DoRA to be useful to other
financial practitioners with similar marketplace applications who need general
models for properties that are newly built and have limited records. The source
code is available at https://github.com/wwweiwei/DoRA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Wei-Wei Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei-Yao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wen-Chih Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection. (arXiv:2304.12876v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.12876</id>
        <link href="http://arxiv.org/abs/2304.12876"/>
        <updated>2023-09-16T00:40:57.222Z</updated>
        <summary type="html"><![CDATA[Upcoming certification actions related to the security of machine learning
(ML) based systems raise major evaluation challenges that are amplified by the
large-scale deployment of models in many hardware platforms. Until recently,
most of research works focused on API-based attacks that consider a ML model as
a pure algorithmic abstraction. However, new implementation-based threats have
been revealed, emphasizing the urgency to propose both practical and
simulation-based methods to properly evaluate the robustness of models. A major
concern is parameter-based attacks (such as the Bit-Flip Attack, BFA) that
highlight the lack of robustness of typical deep neural network models when
confronted by accurate and optimal alterations of their internal parameters
stored in memory. Setting in a security testing purpose, this work practically
reports, for the first time, a successful variant of the BFA on a 32-bit
Cortex-M microcontroller using laser fault injection. It is a standard fault
injection means for security evaluation, that enables to inject spatially and
temporally accurate faults. To avoid unrealistic brute-force strategies, we
show how simulations help selecting the most sensitive set of bits from the
parameters taking into account the laser fault model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dumont_M/0/1/0/all/0/1"&gt;Mathieu Dumont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hector_K/0/1/0/all/0/1"&gt;Kevin Hector&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moellic_P/0/1/0/all/0/1"&gt;Pierre-Alain Moellic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutertre_J/0/1/0/all/0/1"&gt;Jean-Max Dutertre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pontie_S/0/1/0/all/0/1"&gt;Simon Ponti&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning nonparametric DAGs with incremental information via high-order HSIC. (arXiv:2308.05969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.05969</id>
        <link href="http://arxiv.org/abs/2308.05969"/>
        <updated>2023-09-16T00:40:57.208Z</updated>
        <summary type="html"><![CDATA[Score-based methods for learning Bayesain networks(BN) aim to maximizing the
global score functions. However, if local variables have direct and indirect
dependence simultaneously, the global optimization on score functions misses
edges between variables with indirect dependent relationship, of which scores
are smaller than those with direct dependent relationship. In this paper, we
present an identifiability condition based on a determined subset of parents to
identify the underlying DAG. By the identifiability condition, we develop a
two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the
global optimization. In the optimal phase, an optimization problem based on
first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated
skeleton as the initial determined parents subset. In the tuning phase, the
skeleton is locally tuned by deletion, addition and DAG-formalization
strategies using the theoretically proved incremental properties of high-order
HSIC. Numerical experiments for different synthetic datasets and real-world
datasets show that the OT algorithm outperforms existing methods. Especially in
Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the
structure intervention distance (SID) of the OT algorithm is 329.7 smaller than
the one obtained by CAM, which indicates that the graph estimated by the OT
algorithm misses fewer edges compared with CAM.Source code of the OT algorithm
is available at https://github.com/YafeiannWang/optimal-tune-algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yafei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianguo Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TensorFlow Chaotic Prediction and Blow Up. (arXiv:2309.07450v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07450</id>
        <link href="http://arxiv.org/abs/2309.07450"/>
        <updated>2023-09-16T00:40:57.207Z</updated>
        <summary type="html"><![CDATA[Predicting the dynamics of chaotic systems is one of the most challenging
tasks for neural networks, and machine learning in general. Here we aim to
predict the spatiotemporal chaotic dynamics of a high-dimensional non-linear
system. In our attempt we use the TensorFlow library, representing the state of
the art for deep neural networks training and prediction. While our results are
encouraging, and show that the dynamics of the considered system can be
predicted for short time, we also indirectly discovered an unexpected and
undesirable behavior of the TensorFlow library. More specifically, the longer
term prediction of the system's chaotic behavior quickly deteriorates and blows
up due to the nondeterministic behavior of the TensorFlow library. Here we
provide numerical evidence of the short time prediction ability, and of the
longer term predictability blow up.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andrecut_M/0/1/0/all/0/1"&gt;M. Andrecut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.06724</id>
        <link href="http://arxiv.org/abs/2309.06724"/>
        <updated>2023-09-16T00:40:57.197Z</updated>
        <summary type="html"><![CDATA[We aim to provide a general framework of for computational photography that
recovers the real scene from imperfect images, via the Deep Nonparametric
Convexified Filtering (DNCF). It is consists of a nonparametric deep network to
resemble the physical equations behind the image formation, such as denoising,
super-resolution, inpainting, and flash. DNCF has no parameterization dependent
on training data, therefore has a strong generalization and robustness to
adversarial image manipulation. During inference, we also encourage the network
parameters to be nonnegative and create a bi-convex function on the input and
parameters, and this adapts to second-order optimization algorithms with
insufficient running time, having 10X acceleration over Deep Image Prior. With
these tools, we empirically verify its capability to defend image
classification deep networks against adversary attack algorithms in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wangni_J/0/1/0/all/0/1"&gt;Jianqiao Wangni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization. (arXiv:2309.07235v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07235</id>
        <link href="http://arxiv.org/abs/2309.07235"/>
        <updated>2023-09-16T00:40:57.196Z</updated>
        <summary type="html"><![CDATA[Apache TVM (Tensor Virtual Machine), an open source machine learning compiler
framework designed to optimize computations across various hardware platforms,
provides an opportunity to improve the performance of dense matrix
factorizations such as LU (Lower Upper) decomposition and Cholesky
decomposition on GPUs and AI (Artificial Intelligence) accelerators. In this
paper, we propose a new TVM autotuning framework using Bayesian Optimization
and use the TVM tensor expression language to implement linear algebra kernels
such as LU, Cholesky, and 3mm. We use these scientific computation kernels to
evaluate the effectiveness of our methods on a GPU cluster, called Swing, at
Argonne National Laboratory. We compare the proposed autotuning framework with
the TVM autotuning framework AutoTVM with four tuners and find that our
framework outperforms AutoTVM in most cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xingfu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paramasivam_P/0/1/0/all/0/1"&gt;Praveen Paramasivam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_V/0/1/0/all/0/1"&gt;Valerie Taylor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning. (arXiv:2309.07742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07742</id>
        <link href="http://arxiv.org/abs/2309.07742"/>
        <updated>2023-09-16T00:40:57.048Z</updated>
        <summary type="html"><![CDATA[Focus in Explainable AI is shifting from explanations defined in terms of
low-level elements, such as input features, to explanations encoded in terms of
interpretable concepts learned from data. How to reliably acquire such concepts
is, however, still fundamentally unclear. An agreed-upon notion of concept
interpretability is missing, with the result that concepts used by both
post-hoc explainers and concept-based neural networks are acquired through a
variety of mutually incompatible strategies. Critically, most of these neglect
the human side of the problem: a representation is understandable only insofar
as it can be understood by the human at the receiving end. The key challenge in
Human-interpretable Representation Learning (HRL) is how to model and
operationalize this human element. In this work, we propose a mathematical
framework for acquiring interpretable representations suitable for both
post-hoc explainers and concept-based neural networks. Our formalization of HRL
builds on recent advances in causal representation learning and explicitly
models a human stakeholder as an external observer. This allows us to derive a
principled notion of alignment between the machine representation and the
vocabulary of concepts understood by the human. In doing so, we link alignment
and interpretability through a simple and intuitive name transfer game, and
clarify the relationship between alignment and a well-known property of
representations, namely disentanglment. We also show that alignment is linked
to the issue of undesirable correlations among concepts, also known as concept
leakage, and to content-style separation, all through a general
information-theoretic reformulation of these properties. Our conceptualization
aims to bridge the gap between the human and algorithmic sides of
interpretability and establish a stepping stone for new research on
human-interpretable representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marconato_E/0/1/0/all/0/1"&gt;Emanuele Marconato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1"&gt;Andrea Passerini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1"&gt;Stefano Teso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Warm-Start Fixed-Point Optimization Algorithms. (arXiv:2309.07835v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2309.07835</id>
        <link href="http://arxiv.org/abs/2309.07835"/>
        <updated>2023-09-16T00:40:57.042Z</updated>
        <summary type="html"><![CDATA[We introduce a machine-learning framework to warm-start fixed-point
optimization algorithms. Our architecture consists of a neural network mapping
problem parameters to warm starts, followed by a predefined number of
fixed-point iterations. We propose two loss functions designed to either
minimize the fixed-point residual or the distance to a ground truth solution.
In this way, the neural network predicts warm starts with the end-to-end goal
of minimizing the downstream loss. An important feature of our architecture is
its flexibility, in that it can predict a warm start for fixed-point algorithms
run for any number of steps, without being limited to the number of steps it
has been trained on. We provide PAC-Bayes generalization bounds on unseen data
for common classes of fixed-point operators: contractive, linearly convergent,
and averaged. Applying this framework to well-known applications in control,
statistics, and signal processing, we observe a significant reduction in the
number of iterations and solution time required to solve these problems,
through learned warm starts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Sambharya_R/0/1/0/all/0/1"&gt;Rajiv Sambharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hall_G/0/1/0/all/0/1"&gt;Georgina Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Amos_B/0/1/0/all/0/1"&gt;Brandon Amos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stellato_B/0/1/0/all/0/1"&gt;Bartolomeo Stellato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v9 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00331</id>
        <link href="http://arxiv.org/abs/1906.00331"/>
        <updated>2023-09-16T00:40:57.040Z</updated>
        <summary type="html"><![CDATA[We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}}
\max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is
nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a
convex and bounded set. One of the most popular algorithms for solving this
problem is the celebrated gradient descent ascent (GDA) algorithm, which has
been widely used in machine learning, control theory and economics. Despite the
extensive convergence results for the convex-concave setting, GDA with equal
stepsize can converge to limit cycles or even diverge in a general setting. In
this paper, we present the complexity results on two-time-scale GDA for solving
nonconvex-concave minimax problems, showing that the algorithm can find a
stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in
\mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this
is the first nonasymptotic analysis for two-time-scale GDA in this setting,
shedding light on its superior practical performance in training generative
adversarial networks (GANs) and other real applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07703</id>
        <link href="http://arxiv.org/abs/2309.07703"/>
        <updated>2023-09-16T00:40:57.038Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence models and methods commonly lack causal
interpretability. Despite the advancements in interpretable machine learning
(IML) methods, they frequently assign importance to features which lack causal
influence on the outcome variable. Selecting causally relevant features among
those identified as relevant by these methods, or even before model training,
would offer a solution. Feature selection methods utilizing information
theoretical quantities have been successful in identifying statistically
relevant features. However, the information theoretical quantities they are
based on do not incorporate causality, rendering them unsuitable for such
scenarios. To address this challenge, this article proposes information
theoretical quantities that incorporate the causal structure of the system,
which can be used to evaluate causal importance of features for some given
outcome variable. Specifically, we introduce causal versions of entropy and
mutual information, termed causal entropy and causal information gain, which
are designed to assess how much control a feature provides over the outcome
variable. These newly defined quantities capture changes in the entropy of a
variable resulting from interventions on other variables. Fundamental results
connecting these quantities to the existence of causal effects are derived. The
use of causal information gain in feature selection is demonstrated,
highlighting its superiority over standard mutual information in revealing
which features provide control over a chosen outcome variable. Our
investigation paves the way for the development of methods with improved
interpretability in domains involving causation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1"&gt;Francisco Nunes Ferreira Quialheiro Simoes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dastani_M/0/1/0/all/0/1"&gt;Mehdi Dastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommen_T/0/1/0/all/0/1"&gt;Thijs van Ommen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving physics-informed DeepONets with hard constraints. (arXiv:2309.07899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07899</id>
        <link href="http://arxiv.org/abs/2309.07899"/>
        <updated>2023-09-16T00:40:57.037Z</updated>
        <summary type="html"><![CDATA[Current physics-informed (standard or operator) neural networks still rely on
accurately learning the initial conditions of the system they are solving. In
contrast, standard numerical methods evolve such initial conditions without
needing to learn these. In this study, we propose to improve current
physics-informed deep learning strategies such that initial conditions do not
need to be learned and are represented exactly in the predicted solution.
Moreover, this method guarantees that when a DeepONet is applied multiple times
to time step a solution, the resulting function is continuous.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brecht_R/0/1/0/all/0/1"&gt;R&amp;#xfc;diger Brecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popovych_D/0/1/0/all/0/1"&gt;Dmytro R. Popovych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bihlo_A/0/1/0/all/0/1"&gt;Alex Bihlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popovych_R/0/1/0/all/0/1"&gt;Roman O. Popovych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying the Group-Theoretic Structure of Machine-Learned Symmetries. (arXiv:2309.07860v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2309.07860</id>
        <link href="http://arxiv.org/abs/2309.07860"/>
        <updated>2023-09-16T00:40:57.008Z</updated>
        <summary type="html"><![CDATA[Deep learning was recently successfully used in deriving symmetry
transformations that preserve important physics quantities. Being completely
agnostic, these techniques postpone the identification of the discovered
symmetries to a later stage. In this letter we propose methods for examining
and identifying the group-theoretic structure of such machine-learned
symmetries. We design loss functions which probe the subalgebra structure
either during the deep learning stage of symmetry discovery or in a subsequent
post-processing stage. We illustrate the new methods with examples from the
U(n) Lie group family, obtaining the respective subalgebra decompositions. As
an application to particle physics, we demonstrate the identification of the
residual symmetries after the spontaneous breaking of non-Abelian gauge
symmetries like SU(3) and SU(5) which are commonly used in model building.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Forestano_R/0/1/0/all/0/1"&gt;Roy T. Forestano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Matchev_K/0/1/0/all/0/1"&gt;Konstantin T. Matchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Matcheva_K/0/1/0/all/0/1"&gt;Katia Matcheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Roman_A/0/1/0/all/0/1"&gt;Alexander Roman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Unlu_E/0/1/0/all/0/1"&gt;Eyup B. Unlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Verner_S/0/1/0/all/0/1"&gt;Sarunas Verner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A DenseNet-based method for decoding auditory spatial attention with EEG. (arXiv:2309.07690v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07690</id>
        <link href="http://arxiv.org/abs/2309.07690"/>
        <updated>2023-09-16T00:40:57.000Z</updated>
        <summary type="html"><![CDATA[Auditory spatial attention detection (ASAD) aims to decode the attended
spatial location with EEG in a multiple-speaker setting. ASAD methods are
inspired by the brain lateralization of cortical neural responses during the
processing of auditory spatial attention, and show promising performance for
the task of auditory attention decoding (AAD) with neural recordings. In the
previous ASAD methods, the spatial distribution of EEG electrodes is not fully
exploited, which may limit the performance of these methods. In the present
work, by transforming the original EEG channels into a two-dimensional (2D)
spatial topological map, the EEG data is transformed into a three-dimensional
(3D) arrangement containing spatial-temporal information. And then a 3D deep
convolutional neural network (DenseNet-3D) is used to extract temporal and
spatial features of the neural representation for the attended locations. The
results show that the proposed method achieves higher decoding accuracy than
the state-of-the-art (SOTA) method (94.4% compared to XANet's 90.6%) with
1-second decision window for the widely used KULeuven (KUL) dataset, and the
code to implement our work is available on Github:

https://github.com/xuxiran/ASAD_DenseNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiran Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yujie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xihong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness. (arXiv:2111.01996v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.01996</id>
        <link href="http://arxiv.org/abs/2111.01996"/>
        <updated>2023-09-16T00:40:56.999Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness, which primarily comprises sensitivity-based
robustness and spatial robustness, plays an integral part in achieving robust
generalization. In this paper, we endeavor to design strategies to achieve
universal adversarial robustness. To achieve this, we first investigate the
relatively less-explored realm of spatial robustness. Then, we integrate the
existing spatial robustness methods by incorporating both local and global
spatial vulnerability into a unified spatial attack and adversarial training
approach. Furthermore, we present a comprehensive relationship between natural
accuracy, sensitivity-based robustness, and spatial robustness, supported by
strong evidence from the perspective of robust representation. Crucially, to
reconcile the interplay between the mutual impacts of various robustness
components into one unified framework, we incorporate the \textit{Pareto
criterion} into the adversarial robustness analysis, yielding a novel strategy
called Pareto Adversarial Training for achieving universal robustness. The
resulting Pareto front, which delineates the set of optimal solutions, provides
an optimal balance between natural accuracy and various adversarial robustness.
This sheds light on solutions for achieving universal robustness in the future.
To the best of our knowledge, we are the first to consider universal
adversarial robustness via multi-objective optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Ke Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mingjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07675</id>
        <link href="http://arxiv.org/abs/2309.07675"/>
        <updated>2023-09-16T00:40:56.985Z</updated>
        <summary type="html"><![CDATA[Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this paper, we propose a
developmental mechanism for goal discovery via an emergent representation that
abstracts (i.e., groups together) sets of environment states that have similar
roles in the task. We introduce a Feudal HRL algorithm that concurrently learns
both the goal representation and a hierarchical policy. The algorithm uses
symbolic reachability analysis for neural networks to approximate the
transition relation among sets of states and to refine the goal representation.
We evaluate our approach on complex navigation tasks, showing the learned
representation is interpretable, transferrable and results in data efficient
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zadem_M/0/1/0/all/0/1"&gt;Mehdi Zadem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mover_S/0/1/0/all/0/1"&gt;Sergio Mover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1"&gt;Sao Mai Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy. (arXiv:2309.07402v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07402</id>
        <link href="http://arxiv.org/abs/2309.07402"/>
        <updated>2023-09-16T00:40:56.981Z</updated>
        <summary type="html"><![CDATA[Label scarcity in a graph is frequently encountered in real-world
applications due to the high cost of data labeling. To this end,
semi-supervised domain adaptation (SSDA) on graphs aims to leverage the
knowledge of a labeled source graph to aid in node classification on a target
graph with limited labels. SSDA tasks need to overcome the domain gap between
the source and target graphs. However, to date, this challenging research
problem has yet to be formally considered by the existing approaches designed
for cross-graph node classification. To tackle the SSDA problem on graphs, a
novel method called SemiGCL is proposed, which benefits from graph contrastive
learning and minimax entropy training. SemiGCL generates informative node
representations by contrasting the representations learned from a graph's local
and global views. Additionally, SemiGCL is adversarially optimized with the
entropy loss of unlabeled target nodes to reduce domain divergence.
Experimental results on benchmark datasets demonstrate that SemiGCL outperforms
the state-of-the-art baselines on the SSDA tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jiaren Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Quanyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xiao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaochen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jing Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1"&gt;James Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwok_K/0/1/0/all/0/1"&gt;Ka-Wai Kwok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07867</id>
        <link href="http://arxiv.org/abs/2309.07867"/>
        <updated>2023-09-16T00:40:56.981Z</updated>
        <summary type="html"><![CDATA[We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhendong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huangjie Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformal Regression in Calorie Prediction for Team Jumbo-Visma. (arXiv:2304.03778v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.03778</id>
        <link href="http://arxiv.org/abs/2304.03778"/>
        <updated>2023-09-16T00:40:56.981Z</updated>
        <summary type="html"><![CDATA[UCI WorldTour races, the premier men's elite road cycling tour, are grueling
events that put physical fitness and endurance of riders to the test. The
coaches of Team Jumbo-Visma have long been responsible for predicting the
energy needs of each rider of the Dutch team for every race on the calendar.
Those must be estimated to ensure riders have the energy and resources
necessary to maintain a high level of performance throughout a race. This task,
however, is both time-consuming and challenging, as it requires precise
estimates of race speed and power output. Traditionally, the approach to
predicting energy needs has relied on judgement and experience of coaches, but
this method has its limitations and often leads to inaccurate predictions. In
this paper, we propose a new, more effective approach to predicting energy
needs for cycling races. By predicting the speed and power with regression
models, we provide the coaches with calorie needs estimates for each individual
rider per stage instantly. In addition, we compare methods to quantify
uncertainty using conformal prediction. The empirical analysis of the
jackknife+, jackknife-minmax, jackknife-minmax-after-bootstrap, CV+, CV-minmax,
conformalized quantile regression, and inductive conformal prediction methods
in conformal prediction reveals that all methods achieve valid prediction
intervals. All but minmax-based methods also produce sufficiently narrow
prediction intervals for decision-making. Furthermore, methods computing
prediction intervals of fixed size produce tighter intervals for low
significance values. Among the methods computing intervals of varying length
across the input space, inductive conformal prediction computes narrower
prediction intervals at larger significance level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuijk_K/0/1/0/all/0/1"&gt;Kristian van Kuijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dirksen_M/0/1/0/all/0/1"&gt;Mark Dirksen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seiler_C/0/1/0/all/0/1"&gt;Christof Seiler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07666</id>
        <link href="http://arxiv.org/abs/2309.07666"/>
        <updated>2023-09-16T00:40:56.976Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the intersection of two problems in machine
learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD).
On the one hand, the first considers adapting multiple heterogeneous labeled
source domains to an unlabeled target domain. On the other hand, the second
attacks the problem of synthesizing a small summary containing all the
information about the datasets. We thus consider a new problem called MSDA-DD.
To solve it, we adapt previous works in the MSDA literature, such as
Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD
method Distribution Matching. We thoroughly experiment with this novel problem
on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous
Stirred Tank Reactor, and Case Western Reserve University), where we show that,
even with as little as 1 sample per class, one achieves state-of-the-art
adaptation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1"&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1"&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1"&gt;Antoine Souloumiac&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.07030</id>
        <link href="http://arxiv.org/abs/2309.07030"/>
        <updated>2023-09-16T00:40:56.976Z</updated>
        <summary type="html"><![CDATA[Comparing graphs by means of optimal transport has recently gained
significant attention, as the distances induced by optimal transport provide
both a principled metric between graphs as well as an interpretable description
of the associated changes between graphs in terms of a transport plan. As the
lack of symmetry introduces challenges in the typically considered
formulations, optimal transport distances for graphs have mostly been developed
for undirected graphs. Here, we propose two distance measures to compare
directed graphs based on variants of optimal transport: (i) an earth movers
distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate
these two distances and discuss their relative performance for both simulated
graph data and real-world directed cell-cell communication graphs, inferred
from single-cell RNA-seq data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagai_J/0/1/0/all/0/1"&gt;James S. Nagai&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_I/0/1/0/all/0/1"&gt;Ivan G. Costa&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1"&gt;Michael T. Schaub&lt;/a&gt; (2) ((1) Institute for Computational Genomics, RWTH Aachen Medical Faculty, Germany, (2) Department of Computer Science, RWTH Aachen University, Germany)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preserved Edge Convolutional Neural Network for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI). (arXiv:2309.04100v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.04100</id>
        <link href="http://arxiv.org/abs/2309.04100"/>
        <updated>2023-09-16T00:40:56.973Z</updated>
        <summary type="html"><![CDATA[Purpose: Common to most MRSI techniques, the spatial resolution and the
minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the
achievable SNR. This work presents a deep learning method for sensitivity
enhancement of DMI.

Methods: A convolutional neural network (CNN) was designed to estimate the
2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The
CNN was trained with synthetic data that represent a range of SNR levels
typically encountered in vivo. The estimation precision was further improved by
fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI
dataset. The proposed processing method, PReserved Edge ConvolutIonal neural
network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation
studies and in vivo experiments to evaluate the anticipated improvements in SNR
and investigate the potential for inaccuracies.

Results: PRECISE-DMI visually improved the metabolic maps of low SNR
datasets, and quantitatively provided higher precision than the standard
Fourier reconstruction. Processing of DMI data acquired in rat brain tumor
models resulted in more precise determination of 2H-labeled lactate and
glutamate + glutamine levels, at increased spatial resolution (from >8 to 2
$\mu$L) or shortened scan time (from 32 to 4 min) compared to standard
acquisitions. However, rigorous SD-bias analyses showed that overuse of the
edge-preserving regularization can compromise the accuracy of the results.

Conclusion: PRECISE-DMI allows a flexible trade-off between enhancing the
sensitivity of DMI and minimizing the inaccuracies. With typical settings, the
DMI sensitivity can be improved by 3-fold while retaining the capability to
detect local signal variations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dong_S/0/1/0/all/0/1"&gt;Siyuan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feyter_H/0/1/0/all/0/1"&gt;Henk M. De Feyter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thomas_M/0/1/0/all/0/1"&gt;Monique A. Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Graaf_R/0/1/0/all/0/1"&gt;Robin A. de Graaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1"&gt;James S. Duncan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Massively-Parallel Heat Map Sorting and Applications To Explainable Clustering. (arXiv:2309.07486v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2309.07486</id>
        <link href="http://arxiv.org/abs/2309.07486"/>
        <updated>2023-09-16T00:40:56.972Z</updated>
        <summary type="html"><![CDATA[Given a set of points labeled with $k$ labels, we introduce the heat map
sorting problem as reordering and merging the points and dimensions while
preserving the clusters (labels). A cluster is preserved if it remains
connected, i.e., if it is not split into several clusters and no two clusters
are merged.

We prove the problem is NP-hard and we give a fixed-parameter algorithm with
a constant number of rounds in the massively parallel computation model, where
each machine has a sublinear memory and the total memory of the machines is
linear. We give an approximation algorithm for a NP-hard special case of the
problem. We empirically compare our algorithm with k-means and density-based
clustering (DBSCAN) using a dimensionality reduction via locality-sensitive
hashing on several directed and undirected graphs of email and computer
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghamolaei_S/0/1/0/all/0/1"&gt;Sepideh Aghamolaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_M/0/1/0/all/0/1"&gt;Mohammad Ghodsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.02048</id>
        <link href="http://arxiv.org/abs/2211.02048"/>
        <updated>2023-09-16T00:40:56.941Z</updated>
        <summary type="html"><![CDATA[During image editing, existing deep generative models tend to re-synthesize
the entire output from scratch, including the unedited regions. This leads to a
significant waste of computation, especially for minor editing operations. In
this work, we present Spatially Sparse Inference (SSI), a general-purpose
technique that selectively performs computation for edited regions and
accelerates various generative models, including both conditional GANs and
diffusion models. Our key observation is that users prone to gradually edit the
input image. This motivates us to cache and reuse the feature maps of the
original image. Given an edited image, we sparsely apply the convolutional
filters to the edited regions while reusing the cached features for the
unedited areas. Based on our algorithm, we further propose Sparse Incremental
Generative Engine (SIGE) to convert the computation reduction to latency
reduction on off-the-shelf hardware. With about $1\%$-area edits, SIGE
accelerates DDPM by $3.0\times$ on NVIDIA RTX 3090 and $4.6\times$ on Apple M1
Pro GPU, Stable Diffusion by $7.2\times$ on 3090, and GauGAN by $5.6\times$ on
3090 and $5.2\times$ on M1 Pro GPU. Compared to our conference version, we
extend SIGE to accommodate attention layers and apply it to Stable Diffusion.
Additionally, we offer support for Apple M1 Pro GPU and include more results
with large and sequential edits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Muyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Ji Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chenlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EnCodecMAE: Leveraging neural codecs for universal audio representation learning. (arXiv:2309.07391v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.07391</id>
        <link href="http://arxiv.org/abs/2309.07391"/>
        <updated>2023-09-16T00:40:56.940Z</updated>
        <summary type="html"><![CDATA[The goal of universal audio representation learning is to obtain foundational
models that can be used for a variety of downstream tasks involving speech,
music or environmental sounds. To approach this problem, methods inspired by
self-supervised models from NLP, like BERT, are often used and adapted to
audio. These models rely on the discrete nature of text, hence adopting this
type of approach for audio processing requires either a change in the learning
objective or mapping the audio signal to a set of discrete classes. In this
work, we explore the use of EnCodec, a neural audio codec, to generate discrete
targets for learning an universal audio model based on a masked autoencoder
(MAE). We evaluate this approach, which we call EncodecMAE, on a wide range of
audio tasks spanning speech, music and environmental sounds, achieving
performances comparable or better than leading audio representation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pepino_L/0/1/0/all/0/1"&gt;Leonardo Pepino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riera_P/0/1/0/all/0/1"&gt;Pablo Riera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1"&gt;Luciana Ferrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. (arXiv:2202.05928v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.05928</id>
        <link href="http://arxiv.org/abs/2202.05928"/>
        <updated>2023-09-16T00:40:56.940Z</updated>
        <summary type="html"><![CDATA[Benign overfitting, the phenomenon where interpolating models generalize well
in the presence of noisy data, was first observed in neural network models
trained with gradient descent. To better understand this empirical observation,
we consider the generalization error of two-layer neural networks trained to
interpolation by gradient descent on the logistic loss following random
initialization. We assume the data comes from well-separated class-conditional
log-concave distributions and allow for a constant fraction of the training
labels to be corrupted by an adversary. We show that in this setting, neural
networks exhibit benign overfitting: they can be driven to zero training error,
perfectly fitting any noisy training labels, and simultaneously achieve minimax
optimal test error. In contrast to previous work on benign overfitting that
require linear or kernel-based predictors, our analysis holds in a setting
where both the model and learning dynamics are fundamentally nonlinear.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2309.07261</id>
        <link href="http://arxiv.org/abs/2309.07261"/>
        <updated>2023-09-16T00:40:56.939Z</updated>
        <summary type="html"><![CDATA[Tens of thousands of simultaneous hypothesis tests are routinely performed in
genomic studies to identify differentially expressed genes. However, due to
unmeasured confounders, many standard statistical approaches may be
substantially biased. This paper investigates the large-scale hypothesis
testing problem for multivariate generalized linear models in the presence of
confounding effects. Under arbitrary confounding mechanisms, we propose a
unified statistical estimation and inference framework that harnesses
orthogonal structures and integrates linear projections into three key stages.
It first leverages multivariate responses to separate marginal and uncorrelated
confounding effects, recovering the confounding coefficients' column space.
Subsequently, latent factors and primary effects are jointly estimated,
utilizing $\ell_1$-regularization for sparsity while imposing orthogonality
onto confounding coefficients. Finally, we incorporate projected and weighted
bias-correction steps for hypothesis testing. Theoretically, we establish
various effects' identification conditions and non-asymptotic error bounds. We
show effective Type-I error control of asymptotic $z$-tests as sample and
response sizes approach infinity. Numerical experiments demonstrate that the
proposed method controls the false discovery rate by the Benjamini-Hochberg
procedure and is more powerful than alternative methods. By comparing
single-cell RNA-seq counts from two groups of samples, we demonstrate the
suitability of adjusting confounding effects when significant covariates are
absent from the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Du_J/0/1/0/all/0/1"&gt;Jin-Hong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wasserman_L/0/1/0/all/0/1"&gt;Larry Wasserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roeder_K/0/1/0/all/0/1"&gt;Kathryn Roeder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DGSD: Dynamical Graph Self-Distillation for EEG-Based Auditory Spatial Attention Detection. (arXiv:2309.07147v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07147</id>
        <link href="http://arxiv.org/abs/2309.07147"/>
        <updated>2023-09-16T00:40:56.938Z</updated>
        <summary type="html"><![CDATA[Auditory Attention Detection (AAD) aims to detect target speaker from brain
signals in a multi-speaker environment. Although EEG-based AAD methods have
shown promising results in recent years, current approaches primarily rely on
traditional convolutional neural network designed for processing Euclidean data
like images. This makes it challenging to handle EEG signals, which possess
non-Euclidean characteristics. In order to address this problem, this paper
proposes a dynamical graph self-distillation (DGSD) approach for AAD, which
does not require speech stimuli as input. Specifically, to effectively
represent the non-Euclidean properties of EEG signals, dynamical graph
convolutional networks are applied to represent the graph structure of EEG
signals, which can also extract crucial features related to auditory spatial
attention in EEG signals. In addition, to further improve AAD detection
performance, self-distillation, consisting of feature distillation and
hierarchical distillation strategies at each layer, is integrated. These
strategies leverage features and classification results from the deepest
network layers to guide the learning of shallow layers. Our experiments are
conducted on two publicly available datasets, KUL and DTU. Under a 1-second
time window, we achieve results of 90.0\% and 79.6\% accuracy on KUL and DTU,
respectively. We compare our DGSD method with competitive baselines, and the
experimental results indicate that the detection performance of our proposed
DGSD method is not only superior to the best reproducible baseline but also
significantly reduces the number of trainable parameters by approximately 100
times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fan_C/0/1/0/all/0/1"&gt;Cunhang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jun Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1"&gt;Jianhua Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jiangyan Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_Z/0/1/0/all/0/1"&gt;Zhao Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaopei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Systematic Review of Experimental Paradigms and Deep Neural Networks for Electroencephalography-Based Cognitive Workload Detection. (arXiv:2309.07163v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07163</id>
        <link href="http://arxiv.org/abs/2309.07163"/>
        <updated>2023-09-16T00:40:56.937Z</updated>
        <summary type="html"><![CDATA[This article summarizes a systematic review of the electroencephalography
(EEG)-based cognitive workload (CWL) estimation. The focus of the article is
twofold: identify the disparate experimental paradigms used for reliably
eliciting discreet and quantifiable levels of cognitive load and the specific
nature and representational structure of the commonly used input formulations
in deep neural networks (DNNs) used for signal classification. The analysis
revealed a number of studies using EEG signals in its native representation of
a two-dimensional matrix for offline classification of CWL. However, only a few
studies adopted an online or pseudo-online classification strategy for
real-time CWL estimation. Further, only a couple of interpretable DNNs and a
single generative model were employed for cognitive load detection till date
during this review. More often than not, researchers were using DNNs as
black-box type models. In conclusion, DNNs prove to be valuable tools for
classifying EEG signals, primarily due to the substantial modeling power
provided by the depth of their network architecture. It is further suggested
that interpretable and explainable DNN models must be employed for cognitive
workload estimation since existing methods are limited in the face of the
non-stationary nature of the signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+KN_V/0/1/0/all/0/1"&gt;Vishnu KN&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_C/0/1/0/all/0/1"&gt;Cota Navin Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Grand Illusion: The Myth of Software Portability and Implications for ML Progress. (arXiv:2309.07181v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2309.07181</id>
        <link href="http://arxiv.org/abs/2309.07181"/>
        <updated>2023-09-16T00:40:56.937Z</updated>
        <summary type="html"><![CDATA[Pushing the boundaries of machine learning often requires exploring different
hardware and software combinations. However, the freedom to experiment across
different tooling stacks can be at odds with the drive for efficiency, which
has produced increasingly specialized AI hardware and incentivized
consolidation around a narrow set of ML frameworks. Exploratory research can be
restricted if software and hardware are co-evolving, making it even harder to
stray away from mainstream ideas that work well with popular tooling stacks.
While this friction increasingly impacts the rate of innovation in machine
learning, to our knowledge the lack of portability in tooling has not been
quantified. In this work, we ask: How portable are popular ML software
frameworks? We conduct a large-scale study of the portability of mainstream ML
frameworks across different hardware types. Our findings paint an uncomfortable
picture -- frameworks can lose more than 40% of their key functions when ported
to other hardware. Worse, even when functions are portable, the slowdown in
their performance can be extreme and render performance untenable.
Collectively, our results reveal how costly straying from a narrow set of
hardware-software combinations can be - and suggest that specialization of
hardware impedes innovation in machine learning research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mince_F/0/1/0/all/0/1"&gt;Fraser Mince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_D/0/1/0/all/0/1"&gt;Dzung Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kgomo_J/0/1/0/all/0/1"&gt;Jonas Kgomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thompson_N/0/1/0/all/0/1"&gt;Neil Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The effect of data augmentation and 3D-CNN depth on Alzheimer's Disease detection. (arXiv:2309.07192v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.07192</id>
        <link href="http://arxiv.org/abs/2309.07192"/>
        <updated>2023-09-16T00:40:56.937Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) has emerged as a promising approach in healthcare,
outperforming traditional statistical techniques. However, to establish ML as a
reliable tool in clinical practice, adherence to best practices regarding data
handling, experimental design, and model evaluation is crucial. This work
summarizes and strictly observes such practices to ensure reproducible and
reliable ML. Specifically, we focus on Alzheimer's Disease (AD) detection,
which serves as a paradigmatic example of challenging problem in healthcare. We
investigate the impact of different data augmentation techniques and model
complexity on the overall performance. We consider MRI data from ADNI dataset
to address a classification problem employing 3D Convolutional Neural Network
(CNN). The experiments are designed to compensate for data scarcity and initial
random parameters by utilizing cross-validation and multiple training trials.
Within this framework, we train 15 predictive models, considering three
different data augmentation strategies and five distinct 3D CNN architectures,
each varying in the number of convolutional layers. Specifically, the
augmentation strategies are based on affine transformations, such as zoom,
shift, and rotation, applied concurrently or separately. The combined effect of
data augmentation and model complexity leads to a variation in prediction
performance up to 10% of accuracy. When affine transformation are applied
separately, the model is more accurate, independently from the adopted
architecture. For all strategies, the model accuracy followed a concave
behavior at increasing number of convolutional layers, peaking at an
intermediate value of layers. The best model (8 CL, (B)) is the most stable
across cross-validation folds and training trials, reaching excellent
performance both on the testing set and on an external test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Turrisi_R/0/1/0/all/0/1"&gt;Rosanna Turrisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Verri_A/0/1/0/all/0/1"&gt;Alessandro Verri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barla_A/0/1/0/all/0/1"&gt;Annalisa Barla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Auto-Encoding using Deterministic Projected Belief Networks. (arXiv:2309.07481v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07481</id>
        <link href="http://arxiv.org/abs/2309.07481"/>
        <updated>2023-09-16T00:40:56.931Z</updated>
        <summary type="html"><![CDATA[In this paper, we exploit the unique properties of a deterministic projected
belief network (D-PBN) to take full advantage of trainable compound activation
functions (TCAs). A D-PBN is a type of auto-encoder that operates by "backing
up" through a feed-forward neural network. TCAs are activation functions with
complex monotonic-increasing shapes that change the distribution of the data so
that the linear transformation that follows is more effective. Because a D-PBN
operates by "backing up", the TCAs are inverted in the reconstruction process,
restoring the original distribution of the data, thus taking advantage of a
given TCA in both analysis and reconstruction. In this paper, we show that a
D-PBN auto-encoder with TCAs can significantly out-perform standard
auto-encoders including variational auto-encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baggenstoss_P/0/1/0/all/0/1"&gt;Paul M Baggenstoss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07593</id>
        <link href="http://arxiv.org/abs/2309.07593"/>
        <updated>2023-09-16T00:40:56.931Z</updated>
        <summary type="html"><![CDATA[Variable importance assessment has become a crucial step in machine-learning
applications when using complex learners, such as deep neural networks, on
large-scale data. Removal-based importance assessment is currently the
reference approach, particularly when statistical guarantees are sought to
justify variable inclusion. It is often implemented with variable permutation
schemes. On the flip side, these approaches risk misidentifying unimportant
variables as important in the presence of correlations among covariates. Here
we develop a systematic approach for studying Conditional Permutation
Importance (CPI) that is model agnostic and computationally lean, as well as
reusable benchmarks of state-of-the-art variable importance estimators. We show
theoretically and empirically that $\textit{CPI}$ overcomes the limitations of
standard permutation importance by providing accurate type-I error control.
When used with a deep neural network, $\textit{CPI}$ consistently showed top
accuracy across benchmarks. An empirical benchmark on real-world data analysis
in a large-scale medical dataset showed that $\textit{CPI}$ provides a more
parsimonious selection of statistically significant variables. Our results
suggest that $\textit{CPI}$ can be readily used as drop-in replacement for
permutation-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chamma_A/0/1/0/all/0/1"&gt;Ahmad Chamma&lt;/a&gt; (1 and 2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Engemann_D/0/1/0/all/0/1"&gt;Denis A. Engemann&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1"&gt;Bertrand Thirion&lt;/a&gt; (1 and 2 and 3) ((1) Inria, (2) Universite Paris Saclay, (3) CEA, (4) Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel, F. Hoffmann-La Roche Ltd., Basel, Switzerland)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.07778</id>
        <link href="http://arxiv.org/abs/2309.07778"/>
        <updated>2023-09-16T00:40:56.931Z</updated>
        <summary type="html"><![CDATA[Computational pathology uses artificial intelligence to enable precision
medicine and decision support systems through the analysis of whole slide
images. It has the potential to revolutionize the diagnosis and treatment of
cancer. However, a major challenge to this objective is that for many specific
computational pathology tasks the amount of data is inadequate for development.
To address this challenge, we created Virchow, a 632 million parameter deep
neural network foundation model for computational pathology. Using
self-supervised learning, Virchow is trained on 1.5 million hematoxylin and
eosin stained whole slide images from diverse tissue groups, which is orders of
magnitude more data than previous works. When evaluated on downstream tasks
including tile-level pan-cancer detection and subtyping and slide-level
biomarker prediction, Virchow outperforms state-of-the-art systems both on
internal datasets drawn from the same population as the pretraining data as
well as external public datasets. Virchow achieves 93% balanced accuracy for
pancancer tile classification, and AUCs of 0.983 for colon microsatellite
instability status prediction and 0.967 for breast CDH1 status prediction. The
gains in performance highlight the importance of pretraining on massive
pathology image datasets, suggesting pretraining on even larger datasets could
continue improving performance for many high-impact applications where limited
amounts of training data are available, such as drug outcome prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vorontsov_E/0/1/0/all/0/1"&gt;Eugene Vorontsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bozkurt_A/0/1/0/all/0/1"&gt;Alican Bozkurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Casson_A/0/1/0/all/0/1"&gt;Adam Casson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaikovski_G/0/1/0/all/0/1"&gt;George Shaikovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelechowski_M/0/1/0/all/0/1"&gt;Michal Zelechowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eck_A/0/1/0/all/0/1"&gt;Alexander van Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donghun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Viret_J/0/1/0/all/0/1"&gt;Julian Viret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Robert_E/0/1/0/all/0/1"&gt;Eric Robert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Kan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kun_J/0/1/0/all/0/1"&gt;Jeremy D. Kun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Le_M/0/1/0/all/0/1"&gt;Matthew C. H. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bernhard_J/0/1/0/all/0/1"&gt;Jan Bernhard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Godrich_R/0/1/0/all/0/1"&gt;Ran A. Godrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oakley_G/0/1/0/all/0/1"&gt;Gerard Oakley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Millar_E/0/1/0/all/0/1"&gt;Ewan Millar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hanna_M/0/1/0/all/0/1"&gt;Matthew Hanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Retamero_J/0/1/0/all/0/1"&gt;Juan Retamero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moye_W/0/1/0/all/0/1"&gt;William A. Moye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yousfi_R/0/1/0/all/0/1"&gt;Razik Yousfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanan_C/0/1/0/all/0/1"&gt;Christopher Kanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klimstra_D/0/1/0/all/0/1"&gt;David Klimstra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rothrock_B/0/1/0/all/0/1"&gt;Brandon Rothrock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuchs_T/0/1/0/all/0/1"&gt;Thomas J. Fuchs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.07760</id>
        <link href="http://arxiv.org/abs/2309.07760"/>
        <updated>2023-09-16T00:40:56.930Z</updated>
        <summary type="html"><![CDATA[Large pre-trained vision-language models such as CLIP have demonstrated great
potential in zero-shot transferability to downstream tasks. However, to attain
optimal performance, the manual selection of prompts is necessary to improve
alignment between the downstream image distribution and the textual class
descriptions. This manual prompt engineering is the major challenge for
deploying such models in practice since it requires domain expertise and is
extremely time-consuming. To avoid non-trivial prompt engineering, recent work
Context Optimization (CoOp) introduced the concept of prompt learning to the
vision domain using learnable textual tokens. While CoOp can achieve
substantial improvements over manual prompts, its learned context is worse
generalizable to wider unseen classes within the same dataset. In this work, we
present Prompt Learning with Reparameterization Encoder (PRE) - a simple and
efficient method that enhances the generalization ability of the learnable
prompt to unseen classes while maintaining the capacity to learn Base classes.
Instead of directly optimizing the prompts, PRE employs a prompt encoder to
reparameterize the input prompt embeddings, enhancing the exploration of
task-specific knowledge from few-shot samples. Experiments and extensive
ablation studies on 8 benchmarks demonstrate that our approach is an efficient
method for prompt learning. Specifically, PRE achieves a notable enhancement of
5.60% in average accuracy on New classes and 3% in Harmonic mean compared to
CoOp in the 16-shot setting, all achieved within a good training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Minh_A/0/1/0/all/0/1"&gt;Anh Pham Thi Minh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.07412</id>
        <link href="http://arxiv.org/abs/2309.07412"/>
        <updated>2023-09-16T00:40:56.924Z</updated>
        <summary type="html"><![CDATA[In recent studies, linear recurrent neural networks (LRNNs) have achieved
Transformer-level performance in natural language modeling and long-range
modeling while offering rapid parallel training and constant inference costs.
With the resurged interest in LRNNs, we study whether they can learn the hidden
rules in training sequences, such as the grammatical structures of regular
language. We theoretically analyze some existing LRNNs and discover their
limitations on regular language. Motivated by the analysis, we propose a new
LRNN equipped with a block-diagonal and input-dependent transition matrix.
Experiments suggest that the proposed model is the only LRNN that can perform
length extrapolation on regular language tasks such as Sum, Even Pair, and
Modular Arithmetic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1"&gt;Ting-Han Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1"&gt;Ta-Chung Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1"&gt;Alexander I. Rudnicky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning. (arXiv:2309.07526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07526</id>
        <link href="http://arxiv.org/abs/2309.07526"/>
        <updated>2023-09-16T00:40:56.924Z</updated>
        <summary type="html"><![CDATA[By identifying similarities between successive inputs, Self-Supervised
Learning (SSL) methods for time series analysis have demonstrated their
effectiveness in encoding the inherent static characteristics of temporal data.
However, an exclusive emphasis on similarities might result in representations
that overlook the dynamic attributes critical for modeling cardiovascular
diseases within a confined subject cohort. Introducing Distilled Encoding
Beyond Similarities (DEBS), this paper pioneers an SSL approach that transcends
mere similarities by integrating dissimilarities among positive pairs. The
framework is applied to electrocardiogram (ECG) signals, leading to a notable
enhancement of +10\% in the detection accuracy of Atrial Fibrillation (AFib)
across diverse subjects. DEBS underscores the potential of attaining a more
refined representation by encoding the dynamic characteristics of time series
data, tapping into dissimilarities during the optimization process. Broadly,
the strategy delineated in this study holds the promise of unearthing novel
avenues for advancing SSL methodologies tailored to temporal data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atienza_A/0/1/0/all/0/1"&gt;Adrian Atienza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bardram_J/0/1/0/all/0/1"&gt;Jakob Bardram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puthusserypady_S/0/1/0/all/0/1"&gt;Sadasivan Puthusserypady&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.07277</id>
        <link href="http://arxiv.org/abs/2309.07277"/>
        <updated>2023-09-16T00:40:56.922Z</updated>
        <summary type="html"><![CDATA[Text-to-image diffusion models have achieved widespread popularity due to
their unprecedented image generation capability. In particular, their ability
to synthesize and modify human faces has spurred research into using generated
face images in both training data augmentation and model performance
assessments. In this paper, we study the efficacy and shortcomings of
generative models in the context of face generation. Utilizing a combination of
qualitative and quantitative measures, including embedding-based metrics and
user studies, we present a framework to audit the characteristics of generated
faces conditioned on a set of social attributes. We applied our framework on
faces generated through state-of-the-art text-to-image diffusion models. We
identify several limitations of face image generation that include faithfulness
to the text prompt, demographic disparities, and distributional shifts.
Furthermore, we present an analytical model that provides insights into how
training data selection contributes to the performance of generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1"&gt;Harrison Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Shimaa Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1"&gt;Guruprasad V Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinayak_R/0/1/0/all/0/1"&gt;Ramya Korlakai Vinayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1"&gt;Kassem Fawaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SC-MAD: Mixtures of Higher-order Networks for Data Augmentation. (arXiv:2309.07453v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.07453</id>
        <link href="http://arxiv.org/abs/2309.07453"/>
        <updated>2023-09-16T00:40:56.922Z</updated>
        <summary type="html"><![CDATA[The myriad complex systems with multiway interactions motivate the extension
of graph-based pairwise connections to higher-order relations. In particular,
the simplicial complex has inspired generalizations of graph neural networks
(GNNs) to simplicial complex-based models. Learning on such systems requires
large amounts of data, which can be expensive or impossible to obtain. We
propose data augmentation of simplicial complexes through both linear and
nonlinear mixup mechanisms that return mixtures of existing labeled samples. In
addition to traditional pairwise mixup, we present a convex clustering mixup
approach for a data-driven relationship among several simplicial complexes. We
theoretically demonstrate that the resultant synthetic simplicial complexes
interpolate among existing data with respect to homomorphism densities. Our
method is demonstrated on both synthetic and real-world datasets for simplicial
complex classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Navarro_M/0/1/0/all/0/1"&gt;Madeline Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Segarra_S/0/1/0/all/0/1"&gt;Santiago Segarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments. (arXiv:2309.07197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07197</id>
        <link href="http://arxiv.org/abs/2309.07197"/>
        <updated>2023-09-16T00:40:56.917Z</updated>
        <summary type="html"><![CDATA[The main premise of federated learning (FL) is that machine learning model
updates are computed locally to preserve user data privacy. This approach
avoids by design user data to ever leave the perimeter of their device. Once
the updates aggregated, the model is broadcast to all nodes in the federation.
However, without proper defenses, compromised nodes can probe the model inside
their local memory in search for adversarial examples, which can lead to
dangerous real-world scenarios. For instance, in image-based applications,
adversarial examples consist of images slightly perturbed to the human eye
getting misclassified by the local model. These adversarial images are then
later presented to a victim node's counterpart model to replay the attack.
Typical examples harness dissemination strategies such as altered traffic signs
(patch attacks) no longer recognized by autonomous vehicles or seemingly
unaltered samples that poison the local dataset of the FL scheme to undermine
its robustness. Pelta is a novel shielding mechanism leveraging Trusted
Execution Environments (TEEs) that reduce the ability of attackers to craft
adversarial samples. Pelta masks inside the TEE the first part of the
back-propagation chain rule, typically exploited by attackers to craft the
malicious samples. We evaluate Pelta on state-of-the-art accurate models using
three well-established datasets: CIFAR-10, CIFAR-100 and ImageNet. We show the
effectiveness of Pelta in mitigating six white-box state-of-the-art adversarial
attacks, such as Projected Gradient Descent, Momentum Iterative Method, Auto
Projected Gradient Descent, the Carlini & Wagner attack. In particular, Pelta
constitutes the first attempt at defending an ensemble model against the
Self-Attention Gradient attack to the best of our knowledge. Our code is
available to the research community at https://github.com/queyrusi/Pelta.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Queyrut_S/0/1/0/all/0/1"&gt;Simon Queyrut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiavoni_V/0/1/0/all/0/1"&gt;Valerio Schiavoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felber_P/0/1/0/all/0/1"&gt;Pascal Felber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.07172</id>
        <link href="http://arxiv.org/abs/2309.07172"/>
        <updated>2023-09-16T00:40:56.903Z</updated>
        <summary type="html"><![CDATA[This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yuan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1"&gt;Ian Horrocks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07169</id>
        <link href="http://arxiv.org/abs/2309.07169"/>
        <updated>2023-09-16T00:40:56.890Z</updated>
        <summary type="html"><![CDATA[Topological signal processing (TSP) utilizes simplicial complexes to model
structures with higher order than vertices and edges. In this paper, we study
the transferability of TSP via a generalized higher-order version of graphon,
known as complexon. We recall the notion of a complexon as the limit of a
simplicial complex sequence [1]. Inspired by the integral operator form of
graphon shift operators, we construct a marginal complexon and complexon shift
operator (CSO) according to components of all possible dimensions from the
complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate
them to a new family of weighted adjacency matrices. We prove that when a
simplicial complex sequence converges to a complexon, the eigenvalues of the
corresponding CSOs converge to that of the limit complexon. These results hint
at learning transferability on large simplicial complexes or simplicial complex
sequences, which generalize the graphon signal processing framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Purui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jian_X/0/1/0/all/0/1"&gt;Xingchao Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_F/0/1/0/all/0/1"&gt;Feng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tay_W/0/1/0/all/0/1"&gt;Wee Peng Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bihan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support. (arXiv:2309.07183v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07183</id>
        <link href="http://arxiv.org/abs/2309.07183"/>
        <updated>2023-09-16T00:40:56.889Z</updated>
        <summary type="html"><![CDATA[In global healthcare, respiratory diseases are a leading cause of mortality,
underscoring the need for rapid and accurate diagnostics. To advance rapid
screening techniques via auscultation, our research focuses on employing one of
the largest publicly available medical database of respiratory sounds to train
multiple machine learning models able to classify different health conditions.
Our method combines Empirical Mode Decomposition (EMD) and spectral analysis to
extract physiologically relevant biosignals from acoustic data, closely tied to
cardiovascular and respiratory patterns, making our approach apart in its
departure from conventional audio feature extraction practices. We use Power
Spectral Density analysis and filtering techniques to select Intrinsic Mode
Functions (IMFs) strongly correlated with underlying physiological phenomena.
These biosignals undergo a comprehensive feature extraction process for
predictive modeling. Initially, we deploy a binary classification model that
demonstrates a balanced accuracy of 87% in distinguishing between healthy and
diseased individuals. Subsequently, we employ a six-class classification model
that achieves a balanced accuracy of 72% in diagnosing specific respiratory
conditions like pneumonia and chronic obstructive pulmonary disease (COPD). For
the first time, we also introduce regression models that estimate age and body
mass index (BMI) based solely on acoustic data, as well as a model for gender
classification. Our findings underscore the potential of this approach to
significantly enhance assistive and remote diagnostic capabilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Casado_C/0/1/0/all/0/1"&gt;Constantino &amp;#xc1;lvarez Casado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Canellas_M/0/1/0/all/0/1"&gt;Manuel Lage Ca&amp;#xf1;ellas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pedone_M/0/1/0/all/0/1"&gt;Matteo Pedone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaoting Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lopez_M/0/1/0/all/0/1"&gt;Miguel Bordallo L&amp;#xf3;pez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07909</id>
        <link href="http://arxiv.org/abs/2309.07909"/>
        <updated>2023-09-16T00:40:56.888Z</updated>
        <summary type="html"><![CDATA[Unsupervised contrastive learning methods have recently seen significant
improvements, particularly through data augmentation strategies that aim to
produce robust and generalizable representations. However, prevailing data
augmentation methods, whether hand designed or based on foundation models, tend
to rely heavily on prior knowledge or external data. This dependence often
compromises their effectiveness and efficiency. Furthermore, the applicability
of most existing data augmentation strategies is limited when transitioning to
other research domains, especially science-related data. This limitation stems
from the paucity of prior knowledge and labeled data available in these
domains. To address these challenges, we introduce DiffAug-a novel and
efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure
that the augmented and original data share a smoothed latent space, which is
achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug
first mines sufficient prior semantic knowledge about the neighborhood. This
provides a constraint to guide the diffusion steps, eliminating the need for
labels, external data/models, or prior knowledge. Designed as an
architecture-agnostic framework, DiffAug provides consistent improvements.
Specifically, it improves image classification and clustering accuracy by
1.6%~4.5%. When applied to biological data, DiffAug improves performance by up
to 10.1%, with an average improvement of 5.8%. DiffAug shows good performance
in both vision and biological domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1"&gt;Zelin Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Panpan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan.Z Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution Grid Line Outage Identification with Unknown Pattern and Performance Guarantee. (arXiv:2309.07157v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07157</id>
        <link href="http://arxiv.org/abs/2309.07157"/>
        <updated>2023-09-16T00:40:56.884Z</updated>
        <summary type="html"><![CDATA[Line outage identification in distribution grids is essential for sustainable
grid operation. In this work, we propose a practical yet robust detection
approach that utilizes only readily available voltage magnitudes, eliminating
the need for costly phase angles or power flow data. Given the sensor data,
many existing detection methods based on change-point detection require prior
knowledge of outage patterns, which are unknown for real-world outage
scenarios. To remove this impractical requirement, we propose a data-driven
method to learn the parameters of the post-outage distribution through gradient
descent. However, directly using gradient descent presents feasibility issues.
To address this, we modify our approach by adding a Bregman divergence
constraint to control the trajectory of the parameter updates, which eliminates
the feasibility problems. As timely operation is the key nowadays, we prove
that the optimal parameters can be learned with convergence guarantees via
leveraging the statistical and physical properties of voltage data. We evaluate
our approach using many representative distribution grids and real load
profiles with 17 outage configurations. The results show that we can detect and
localize the outage in a timely manner with only voltage magnitudes and without
assuming a prior knowledge of outage patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chenhan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yizheng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1"&gt;Yang Weng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Latent Differential Equation Models via Attention and Multiple Shooting. (arXiv:2307.05735v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.05735</id>
        <link href="http://arxiv.org/abs/2307.05735"/>
        <updated>2023-09-16T00:40:56.884Z</updated>
        <summary type="html"><![CDATA[Scientific Machine Learning (SciML) is a burgeoning field that
synergistically combines domain-aware and interpretable models with agnostic
machine learning techniques. In this work, we introduce GOKU-UI, an evolution
of the SciML generative model GOKU-nets. GOKU-UI not only broadens the original
model's spectrum to incorporate other classes of differential equations, such
as Stochastic Differential Equations (SDEs), but also integrates attention
mechanisms and a novel multiple shooting training strategy in the latent space.
These modifications have led to a significant increase in its performance in
both reconstruction and forecast tasks, as demonstrated by our evaluation of
simulated and empirical data. Specifically, GOKU-UI outperformed all baseline
models on synthetic datasets even with a training set 16-fold smaller,
underscoring its remarkable data efficiency. Furthermore, when applied to
empirical human brain data, while incorporating stochastic Stuart-Landau
oscillators into its dynamical core, our proposed enhancements markedly
increased the model's effectiveness in capturing complex brain dynamics. This
augmented version not only surpassed all baseline methods in the reconstruction
task, but also demonstrated lower prediction error of future brain activity up
to 15 seconds ahead. By training GOKU-UI on resting state fMRI data, we encoded
whole-brain dynamics into a latent representation, learning a low-dimensional
dynamical system model that could offer insights into brain functionality and
open avenues for practical applications such as the classification of mental
states or psychiatric conditions. Ultimately, our research provides further
impetus for the field of Scientific Machine Learning, showcasing the potential
for advancements when established scientific insights are interwoven with
modern machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrevaya_G/0/1/0/all/0/1"&gt;Germ&amp;#xe1;n Abrevaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramezanian_Panahi_M/0/1/0/all/0/1"&gt;Mahta Ramezanian-Panahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagnon_Audet_J/0/1/0/all/0/1"&gt;Jean-Christophe Gagnon-Audet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polosecki_P/0/1/0/all/0/1"&gt;Pablo Polosecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_S/0/1/0/all/0/1"&gt;Silvina Ponce Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1"&gt;Guillermo Cecchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumas_G/0/1/0/all/0/1"&gt;Guillaume Dumas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Strong and Simple Deep Learning Baseline for BCI MI Decoding. (arXiv:2309.07159v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07159</id>
        <link href="http://arxiv.org/abs/2309.07159"/>
        <updated>2023-09-16T00:40:56.883Z</updated>
        <summary type="html"><![CDATA[We propose EEG-SimpleConv, a straightforward 1D convolutional neural network
for Motor Imagery decoding in BCI. Our main motivation is to propose a very
simple baseline to compare to, using only very standard ingredients from the
literature. We evaluate its performance on four EEG Motor Imagery datasets,
including simulated online setups, and compare it to recent Deep Learning and
Machine Learning approaches. EEG-SimpleConv is at least as good or far more
efficient than other approaches, showing strong knowledge-transfer capabilities
across subjects, at the cost of a low inference time. We advocate that using
off-the-shelf ingredients rather than coming with ad-hoc solutions can
significantly help the adoption of Deep Learning approaches for BCI. We make
the code of the models and the experiments accessible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ouahidi_Y/0/1/0/all/0/1"&gt;Yassine El Ouahidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gripon_V/0/1/0/all/0/1"&gt;Vincent Gripon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pasdeloup_B/0/1/0/all/0/1"&gt;Bastien Pasdeloup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bouallegue_G/0/1/0/all/0/1"&gt;Ghaith Bouallegue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Farrugia_N/0/1/0/all/0/1"&gt;Nicolas Farrugia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lioi_G/0/1/0/all/0/1"&gt;Giulia Lioi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Optimal Control Method to Compute the Most Likely Transition Path for Stochastic Dynamical Systems with Jumps. (arXiv:2203.16874v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16874</id>
        <link href="http://arxiv.org/abs/2203.16874"/>
        <updated>2023-09-16T00:40:56.883Z</updated>
        <summary type="html"><![CDATA[Many complex real world phenomena exhibit abrupt, intermittent or jumping
behaviors, which are more suitable to be described by stochastic differential
equations under non-Gaussian L\'evy noise. Among these complex phenomena, the
most likely transition paths between metastable states are important since
these rare events may have a high impact in certain scenarios. Based on the
large deviation principle, the most likely transition path could be treated as
the minimizer of the rate function upon paths that connect two points. One of
the challenges to calculate the most likely transition path for stochastic
dynamical systems under non-Gaussian L\'evy noise is that the associated rate
function can not be explicitly expressed by paths. For this reason, we
formulate an optimal control problem to obtain the optimal state as the most
likely transition path. We then develop a neural network method to solve this
issue. Several experiments are investigated for both Gaussian and non-Gaussian
cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gao_T/0/1/0/all/0/1"&gt;Ting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinqiao Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoli Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07149</id>
        <link href="http://arxiv.org/abs/2309.07149"/>
        <updated>2023-09-16T00:40:56.882Z</updated>
        <summary type="html"><![CDATA[Decoding visual representations from human brain activity has emerged as a
thriving research domain, particularly in the context of brain-computer
interfaces. Our study presents an innovative method that employs to classify
and reconstruct images from the ImageNet dataset using electroencephalography
(EEG) data from subjects that had viewed the images themselves (i.e. "brain
decoding"). We analyzed EEG recordings from 6 participants, each exposed to 50
images spanning 40 unique semantic categories. These EEG readings were
converted into spectrograms, which were then used to train a convolutional
neural network (CNN), integrated with a knowledge distillation procedure based
on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image
classification teacher network. This strategy allowed our model to attain a
top-5 accuracy of 80%, significantly outperforming a standard CNN and various
RNN-based benchmarks. Additionally, we incorporated an image reconstruction
mechanism based on pre-trained latent diffusion models, which allowed us to
generate an estimate of the images which had elicited EEG activity. Therefore,
our architecture not only decodes images from neural activity but also offers a
credible image reconstruction from EEG only, paving the way for e.g. swift,
individualized feedback experiments. Our research represents a significant step
forward in connecting neural signals with visual cognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ferrante_M/0/1/0/all/0/1"&gt;Matteo Ferrante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boccato_T/0/1/0/all/0/1"&gt;Tommaso Boccato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bargione_S/0/1/0/all/0/1"&gt;Stefano Bargione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Toschi_N/0/1/0/all/0/1"&gt;Nicola Toschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Naturalistic Robot Arm Trajectory Generation via Representation Learning. (arXiv:2309.07550v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.07550</id>
        <link href="http://arxiv.org/abs/2309.07550"/>
        <updated>2023-09-16T00:40:56.880Z</updated>
        <summary type="html"><![CDATA[The integration of manipulator robots in household environments suggests a
need for more predictable and human-like robot motion. This holds especially
true for wheelchair-mounted assistive robots that can support the independence
of people with paralysis. One method of generating naturalistic motion
trajectories is via the imitation of human demonstrators. This paper explores a
self-supervised imitation learning method using an autoregressive
spatio-temporal graph neural network for an assistive drinking task. We address
learning from diverse human motion trajectory data that were captured via
wearable IMU sensors on a human arm as the action-free task demonstrations.
Observed arm motion data from several participants is used to generate natural
and functional drinking motion trajectories for a UR5e robot arm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jayjun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiers_A/0/1/0/all/0/1"&gt;Adam J. Spiers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beta quantile regression for robust estimation of uncertainty in the presence of outliers. (arXiv:2309.07374v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07374</id>
        <link href="http://arxiv.org/abs/2309.07374"/>
        <updated>2023-09-16T00:40:56.879Z</updated>
        <summary type="html"><![CDATA[Quantile Regression (QR) can be used to estimate aleatoric uncertainty in
deep neural networks and can generate prediction intervals. Quantifying
uncertainty is particularly important in critical applications such as clinical
diagnosis, where a realistic assessment of uncertainty is essential in
determining disease status and planning the appropriate treatment. The most
common application of quantile regression models is in cases where the
parametric likelihood cannot be specified. Although quantile regression is
quite robust to outlier response observations, it can be sensitive to outlier
covariate observations (features). Outlier features can compromise the
performance of deep learning regression problems such as style translation,
image reconstruction, and deep anomaly detection, potentially leading to
misleading conclusions. To address this problem, we propose a robust solution
for quantile regression that incorporates concepts from robust divergence. We
compare the performance of our proposed method with (i) least trimmed quantile
regression and (ii) robust regression based on the regularization of
case-specific parameters in a simple real dataset in the presence of outlier.
These methods have not been applied in a deep learning framework. We also
demonstrate the applicability of the proposed method by applying it to a
medical imaging translation task using diffusion models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akrami_H/0/1/0/all/0/1"&gt;Haleh Akrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzam_O/0/1/0/all/0/1"&gt;Omar Zamzam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Anand Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aydore_S/0/1/0/all/0/1"&gt;Sergul Aydore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leahy_R/0/1/0/all/0/1"&gt;Richard Leahy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Feature Amplification: Feature Learning and Generalization in Neural Networks. (arXiv:2202.07626v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07626</id>
        <link href="http://arxiv.org/abs/2202.07626"/>
        <updated>2023-09-16T00:40:56.879Z</updated>
        <summary type="html"><![CDATA[In this work, we provide a characterization of the feature-learning process
in two-layer ReLU networks trained by gradient descent on the logistic loss
following random initialization. We consider data with binary labels that are
generated by an XOR-like function of the input features. We permit a constant
fraction of the training labels to be corrupted by an adversary. We show that,
although linear classifiers are no better than random guessing for the
distribution we consider, two-layer ReLU networks trained by gradient descent
achieve generalization error close to the label noise rate. We develop a novel
proof technique that shows that at initialization, the vast majority of neurons
function as random features that are only weakly correlated with useful
features, and the gradient descent dynamics 'amplify' these weak, random
features to strong, useful features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overview of Human Activity Recognition Using Sensor Data. (arXiv:2309.07170v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07170</id>
        <link href="http://arxiv.org/abs/2309.07170"/>
        <updated>2023-09-16T00:40:56.878Z</updated>
        <summary type="html"><![CDATA[Human activity recognition (HAR) is an essential research field that has been
used in different applications including home and workplace automation,
security and surveillance as well as healthcare. Starting from conventional
machine learning methods to the recently developing deep learning techniques
and the Internet of things, significant contributions have been shown in the
HAR area in the last decade. Even though several review and survey studies have
been published, there is a lack of sensor-based HAR overview studies focusing
on summarising the usage of wearable sensors and smart home sensors data as
well as applications of HAR and deep learning techniques. Hence, we overview
sensor-based HAR, discuss several important applications that rely on HAR, and
highlight the most common machine learning methods that have been used for HAR.
Finally, several challenges of HAR are explored that should be addressed to
further improve the robustness of HAR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hamad_R/0/1/0/all/0/1"&gt;Rebeen Ali Hamad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woo_W/0/1/0/all/0/1"&gt;Wai Lok Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_B/0/1/0/all/0/1"&gt;Bo Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Longzhi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the dimensions in imaging genetics with CLUB-PLS. (arXiv:2309.07352v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2309.07352</id>
        <link href="http://arxiv.org/abs/2309.07352"/>
        <updated>2023-09-16T00:40:56.878Z</updated>
        <summary type="html"><![CDATA[A major challenge in imaging genetics and similar fields is to link
high-dimensional data in one domain, e.g., genetic data, to high dimensional
data in a second domain, e.g., brain imaging data. The standard approach in the
area are mass univariate analyses across genetic factors and imaging
phenotypes. That entails executing one genome-wide association study (GWAS) for
each pre-defined imaging measure. Although this approach has been tremendously
successful, one shortcoming is that phenotypes must be pre-defined.
Consequently, effects that are not confined to pre-selected regions of interest
or that reflect larger brain-wide patterns can easily be missed. In this work
we introduce a Partial Least Squares (PLS)-based framework, which we term
Cluster-Bootstrap PLS (CLUB-PLS), that can work with large input dimensions in
both domains as well as with large sample sizes. One key factor of the
framework is to use cluster bootstrap to provide robust statistics for single
input features in both domains. We applied CLUB-PLS to investigating the
genetic basis of surface area and cortical thickness in a sample of 33,000
subjects from the UK Biobank. We found 107 genome-wide significant
locus-phenotype pairs that are linked to 386 different genes. We found that a
vast majority of these loci could be technically validated at a high rate:
using classic GWAS or Genome-Wide Inferred Statistics (GWIS) we found that 85
locus-phenotype pairs exceeded the genome-wide suggestive (P<1e-05) threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Altmann_A/0/1/0/all/0/1"&gt;Andre Altmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Aquila_A/0/1/0/all/0/1"&gt;Ana C Lawry Aquila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jahanshad_N/0/1/0/all/0/1"&gt;Neda Jahanshad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Thompson_P/0/1/0/all/0/1"&gt;Paul M Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lorenzi_M/0/1/0/all/0/1"&gt;Marco Lorenzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleep Stage Classification Using a Pre-trained Deep Learning Model. (arXiv:2309.07182v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07182</id>
        <link href="http://arxiv.org/abs/2309.07182"/>
        <updated>2023-09-16T00:40:56.877Z</updated>
        <summary type="html"><![CDATA[One of the common human diseases is sleep disorders. The classification of
sleep stages plays a fundamental role in diagnosing sleep disorders, monitoring
treatment effectiveness, and understanding the relationship between sleep
stages and various health conditions. A precise and efficient classification of
these stages can significantly enhance our understanding of sleep-related
phenomena and ultimately lead to improved health outcomes and disease
treatment.

Models others propose are often time-consuming and lack sufficient accuracy,
especially in stage N1. The main objective of this research is to present a
machine-learning model called "EEGMobile". This model utilizes pre-trained
models and learns from electroencephalogram (EEG) spectrograms of brain
signals. The model achieved an accuracy of 86.97% on a publicly available
dataset named "Sleep-EDF20", outperforming other models proposed by different
researchers. Moreover, it recorded an accuracy of 56.4% in stage N1, which is
better than other models. These findings demonstrate that this model has the
potential to achieve better results for the treatment of this disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ardeshir_H/0/1/0/all/0/1"&gt;Hassan Ardeshir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Araghi_M/0/1/0/all/0/1"&gt;Mohammad Araghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking machine learning models for quantum state classification. (arXiv:2309.07679v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.07679</id>
        <link href="http://arxiv.org/abs/2309.07679"/>
        <updated>2023-09-16T00:40:56.875Z</updated>
        <summary type="html"><![CDATA[Quantum computing is a growing field where the information is processed by
two-levels quantum states known as qubits. Current physical realizations of
qubits require a careful calibration, composed by different experiments, due to
noise and decoherence phenomena. Among the different characterization
experiments, a crucial step is to develop a model to classify the measured
state by discriminating the ground state from the excited state. In this
proceedings we benchmark multiple classification techniques applied to real
quantum devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pedicillo_E/0/1/0/all/0/1"&gt;Edoardo Pedicillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pasquale_A/0/1/0/all/0/1"&gt;Andrea Pasquale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Carrazza_S/0/1/0/all/0/1"&gt;Stefano Carrazza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning nonparametric DAGs with incremental information via high-order HSIC. (arXiv:2308.05969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.05969</id>
        <link href="http://arxiv.org/abs/2308.05969"/>
        <updated>2023-09-16T00:40:56.874Z</updated>
        <summary type="html"><![CDATA[Score-based methods for learning Bayesain networks(BN) aim to maximizing the
global score functions. However, if local variables have direct and indirect
dependence simultaneously, the global optimization on score functions misses
edges between variables with indirect dependent relationship, of which scores
are smaller than those with direct dependent relationship. In this paper, we
present an identifiability condition based on a determined subset of parents to
identify the underlying DAG. By the identifiability condition, we develop a
two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the
global optimization. In the optimal phase, an optimization problem based on
first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated
skeleton as the initial determined parents subset. In the tuning phase, the
skeleton is locally tuned by deletion, addition and DAG-formalization
strategies using the theoretically proved incremental properties of high-order
HSIC. Numerical experiments for different synthetic datasets and real-world
datasets show that the OT algorithm outperforms existing methods. Especially in
Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the
structure intervention distance (SID) of the OT algorithm is 329.7 smaller than
the one obtained by CAM, which indicates that the graph estimated by the OT
algorithm misses fewer edges compared with CAM.Source code of the OT algorithm
is available at https://github.com/YafeiannWang/optimal-tune-algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yafei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianguo Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design of Recognition and Evaluation System for Table Tennis Players' Motor Skills Based on Artificial Intelligence. (arXiv:2309.07141v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07141</id>
        <link href="http://arxiv.org/abs/2309.07141"/>
        <updated>2023-09-16T00:40:56.872Z</updated>
        <summary type="html"><![CDATA[With the rapid development of electronic science and technology, the research
on wearable devices is constantly updated, but for now, it is not comprehensive
for wearable devices to recognize and analyze the movement of specific sports.
Based on this, this paper improves wearable devices of table tennis sport, and
realizes the pattern recognition and evaluation of table tennis players' motor
skills through artificial intelligence. Firstly, a device is designed to
collect the movement information of table tennis players and the actual
movement data is processed. Secondly, a sliding window is made to divide the
collected motion data into a characteristic database of six table tennis
benchmark movements. Thirdly, motion features were constructed based on feature
engineering, and motor skills were identified for different models after
dimensionality reduction. Finally, the hierarchical evaluation system of motor
skills is established with the loss functions of different evaluation indexes.
The results show that in the recognition of table tennis players' motor skills,
the feature-based BP neural network proposed in this paper has higher
recognition accuracy and stronger generalization ability than the traditional
convolutional neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhuo-yong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye-tao Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke-xin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1"&gt;Ding-han Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_L/0/1/0/all/0/1"&gt;Long-meng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yong Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach. (arXiv:2309.07153v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2309.07153</id>
        <link href="http://arxiv.org/abs/2309.07153"/>
        <updated>2023-09-16T00:40:56.870Z</updated>
        <summary type="html"><![CDATA[Maximizing influences in complex networks is a practically important but
computationally challenging task for social network analysis, due to its NP-
hard nature. Most current approximation or heuristic methods either require
tremendous human design efforts or achieve unsatisfying balances between
effectiveness and efficiency. Recent machine learning attempts only focus on
speed but lack performance enhancement. In this paper, different from previous
attempts, we propose an effective deep reinforcement learning model that
achieves superior performances over traditional best influence maximization
algorithms. Specifically, we design an end-to-end learning framework that
combines graph neural network as the encoder and reinforcement learning as the
decoder, named DREIM. Trough extensive training on small synthetic graphs,
DREIM outperforms the state-of-the-art baseline methods on very large synthetic
and real-world networks on solution quality, and we also empirically show its
linear scalability with regard to the network size, which demonstrates its
superiority in solving this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Changan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhongzhi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07156</id>
        <link href="http://arxiv.org/abs/2309.07156"/>
        <updated>2023-09-16T00:40:56.870Z</updated>
        <summary type="html"><![CDATA[Sleep, a fundamental physiological process, occupies a significant portion of
our lives. Accurate classification of sleep stages serves as a crucial tool for
evaluating sleep quality and identifying probable sleep disorders. This work
introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture
to classify sleep into five separate stages. The classification process is
based on the analysis of single-channel electroencephalograms (EEGs). The
framework that has been suggested consists of two fundamental elements: a
feature extractor that utilises SE-ResNet, and a temporal context encoder that
use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated
by thorough assessments conducted on three different datasets, namely
SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains
notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with
macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets.
Notably, we introduce the utilization of 1D-GradCAM visualization to shed light
on the decision-making process of our model in the realm of sleep stage
classification. This visualization method not only provides valuable insights
into the model's classification rationale but also aligns its outcomes with the
annotations made by sleep experts. One notable feature of our research is the
integration of an expedited training approach, which effectively preserves the
model's resilience in terms of performance. The experimental evaluations
conducted provide a comprehensive evaluation of the effectiveness of our
proposed model in comparison to existing approaches, highlighting its potential
for practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Shivam Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maiti_S/0/1/0/all/0/1"&gt;Suvadeep Maiti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mythirayee_S/0/1/0/all/0/1"&gt;S.Mythirayee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajendran_S/0/1/0/all/0/1"&gt;Srijithesh Rajendran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raju_B/0/1/0/all/0/1"&gt;Bapi Raju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct Text to Speech Translation System using Acoustic Units. (arXiv:2309.07478v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.07478</id>
        <link href="http://arxiv.org/abs/2309.07478"/>
        <updated>2023-09-16T00:40:56.869Z</updated>
        <summary type="html"><![CDATA[This paper proposes a direct text to speech translation system using discrete
acoustic units. This framework employs text in different source languages as
input to generate speech in the target language without the need for text
transcriptions in this language. Motivated by the success of acoustic units in
previous works for direct speech to speech translation systems, we use the same
pipeline to extract the acoustic units using a speech encoder combined with a
clustering algorithm. Once units are obtained, an encoder-decoder architecture
is trained to predict them. Then a vocoder generates speech from units. Our
approach for direct text to speech translation was tested on the new CVSS
corpus with two different text mBART models employed as initialisation. The
systems presented report competitive performance for most of the language pairs
evaluated. Besides, results show a remarkable improvement when initialising our
proposed architecture with a model pre-trained with more languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1"&gt;Victoria Mingote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1"&gt;Pablo Gimeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1"&gt;Luis Vicente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1"&gt;Sameer Khurana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1"&gt;Antoine Laurent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duret_J/0/1/0/all/0/1"&gt;Jarod Duret&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SC-MAD: Mixtures of Higher-order Networks for Data Augmentation. (arXiv:2309.07453v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.07453</id>
        <link href="http://arxiv.org/abs/2309.07453"/>
        <updated>2023-09-16T00:40:56.869Z</updated>
        <summary type="html"><![CDATA[The myriad complex systems with multiway interactions motivate the extension
of graph-based pairwise connections to higher-order relations. In particular,
the simplicial complex has inspired generalizations of graph neural networks
(GNNs) to simplicial complex-based models. Learning on such systems requires
large amounts of data, which can be expensive or impossible to obtain. We
propose data augmentation of simplicial complexes through both linear and
nonlinear mixup mechanisms that return mixtures of existing labeled samples. In
addition to traditional pairwise mixup, we present a convex clustering mixup
approach for a data-driven relationship among several simplicial complexes. We
theoretically demonstrate that the resultant synthetic simplicial complexes
interpolate among existing data with respect to homomorphism densities. Our
method is demonstrated on both synthetic and real-world datasets for simplicial
complex classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Navarro_M/0/1/0/all/0/1"&gt;Madeline Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Segarra_S/0/1/0/all/0/1"&gt;Santiago Segarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Parametric BRDFs from Natural Language Descriptions. (arXiv:2306.15679v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.15679</id>
        <link href="http://arxiv.org/abs/2306.15679"/>
        <updated>2023-09-16T00:40:56.868Z</updated>
        <summary type="html"><![CDATA[Artistic authoring of 3D environments is a laborious enterprise that also
requires skilled content creators. There have been impressive improvements in
using machine learning to address different aspects of generating 3D content,
such as generating meshes, arranging geometry, synthesizing textures, etc. In
this paper we develop a model to generate Bidirectional Reflectance
Distribution Functions (BRDFs) from descriptive textual prompts. BRDFs are four
dimensional probability distributions that characterize the interaction of
light with surface materials. They are either represented parametrically, or by
tabulating the probability density associated with every pair of incident and
outgoing angles. The former lends itself to artistic editing while the latter
is used when measuring the appearance of real materials. Numerous works have
focused on hypothesizing BRDF models from images of materials. We learn a
mapping from textual descriptions of materials to parametric BRDFs. Our model
is first trained using a semi-supervised approach before being tuned via an
unsupervised scheme. Although our model is general, in this paper we
specifically generate parameters for MDL materials, conditioned on natural
language descriptions, within NVIDIA's Omniverse platform. This enables use
cases such as real-time text prompts to change materials of objects in 3D
environments such as "dull plastic" or "shiny iron". Since the output of our
model is a parametric BRDF, rather than an image of the material, it may be
used to render materials using any shape under arbitrarily specified viewing
and lighting conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memery_S/0/1/0/all/0/1"&gt;Sean Memery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cedron_O/0/1/0/all/0/1"&gt;Osmar Cedron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subr_K/0/1/0/all/0/1"&gt;Kartic Subr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.10629</id>
        <link href="http://arxiv.org/abs/2202.10629"/>
        <updated>2023-09-16T00:40:56.858Z</updated>
        <summary type="html"><![CDATA[In data-rich domains such as vision, language, and speech, deep learning
prevails to deliver high-performance task-specific models and can even learn
general task-agnostic representations for efficient finetuning to downstream
tasks. However, deep learning in resource-limited domains still faces multiple
challenges including (i) limited data, (ii) constrained model development cost,
and (iii) lack of adequate pre-trained models for effective finetuning. This
paper provides an overview of model reprogramming to bridge this gap. Model
reprogramming enables resource-efficient cross-domain machine learning by
repurposing and reusing a well-developed pre-trained model from a source domain
to solve tasks in a target domain without model finetuning, where the source
and target domains can be vastly different. In many applications, model
reprogramming outperforms transfer learning and training from scratch. This
paper elucidates the methodology of model reprogramming, summarizes existing
use cases, provides a theoretical explanation of the success of model
reprogramming, and concludes with a discussion on open-ended research questions
and opportunities. A list of model reprogramming studies is actively maintained
and updated at https://github.com/IBM/model-reprogramming.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust SINDy Approach by Combining Neural Networks and an Integral Form. (arXiv:2309.07193v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2309.07193</id>
        <link href="http://arxiv.org/abs/2309.07193"/>
        <updated>2023-09-16T00:40:56.811Z</updated>
        <summary type="html"><![CDATA[The discovery of governing equations from data has been an active field of
research for decades. One widely used methodology for this purpose is sparse
regression for nonlinear dynamics, known as SINDy. Despite several attempts,
noisy and scarce data still pose a severe challenge to the success of the SINDy
approach. In this work, we discuss a robust method to discover nonlinear
governing equations from noisy and scarce data. To do this, we make use of
neural networks to learn an implicit representation based on measurement data
so that not only it produces the output in the vicinity of the measurements but
also the time-evolution of output can be described by a dynamical system.
Additionally, we learn such a dynamic system in the spirit of the SINDy
framework. Leveraging the implicit representation using neural networks, we
obtain the derivative information -- required for SINDy -- using an automatic
differentiation tool. To enhance the robustness of our methodology, we further
incorporate an integral condition on the output of the implicit networks.
Furthermore, we extend our methodology to handle data collected from multiple
initial conditions. We demonstrate the efficiency of the proposed methodology
to discover governing equations under noisy and scarce data regimes by means of
several examples and compare its performance with existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Forootani_A/0/1/0/all/0/1"&gt;Ali Forootani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Benner_P/0/1/0/all/0/1"&gt;Peter Benner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11322</id>
        <link href="http://arxiv.org/abs/2305.11322"/>
        <updated>2023-09-16T00:40:56.809Z</updated>
        <summary type="html"><![CDATA[Spiking neural networks (SNNs) process time-series data via internal
event-driven neural dynamics whose energy consumption depends on the number of
spikes exchanged between neurons over the course of the input presentation. In
typical implementations of an SNN classifier, decisions are produced after the
entire input sequence has been processed, resulting in latency and energy
consumption levels that are fairly uniform across inputs. Recently introduced
delay-adaptive SNNs tailor the inference latency -- and, with it, the energy
consumption -- to the difficulty of each example, by producing an early
decision when the SNN model is sufficiently ``confident''. In this paper, we
start by observing that, as an SNN processes input samples, its classification
decisions tend to be first under-confident and then over-confident with respect
to the decision's ground-truth, unknown, test accuracy. This makes it difficult
to determine a stopping time that ensures a desired level of accuracy. To
address this problem, we introduce a novel delay-adaptive SNN-based inference
methodology that, wrapping around any pre-trained SNN classifier, provides
guaranteed reliability for the decisions produced at input-dependent stopping
times. The approach entails minimal added complexity as compared to the
underlying SNN, requiring only thresholding and counting operations at run
time, and it leverages tools from conformal prediction (CP).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiechen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.07694</id>
        <link href="http://arxiv.org/abs/2309.07694"/>
        <updated>2023-09-16T00:40:56.806Z</updated>
        <summary type="html"><![CDATA[While the recently introduced Tree of Thoughts (ToT) has heralded
advancements in allowing Large Language Models (LLMs) to reason through
foresight and backtracking for global decision-making, it has overlooked the
inherent local uncertainties in intermediate decision points or "thoughts".
These local uncertainties, intrinsic to LLMs given their potential for diverse
responses, remain a significant concern in the reasoning process. Addressing
this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a
reasoning framework tailored for LLMs. Our TouT effectively leverages Monte
Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse
local responses at these intermediate steps. By marrying this local uncertainty
quantification with global search algorithms, TouT enhances the model's
precision in response generation. We substantiate our approach with rigorous
experiments on two demanding planning tasks: Game of 24 and Mini Crosswords.
The empirical evidence underscores TouT's superiority over both ToT and
chain-of-thought prompting methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Shentong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1"&gt;Miao Xin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning. (arXiv:2309.07578v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07578</id>
        <link href="http://arxiv.org/abs/2309.07578"/>
        <updated>2023-09-16T00:40:56.805Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to address the challenge of generalization in
offline reinforcement learning (RL), where the agent learns from a fixed
dataset without any additional interaction with the environment. Specifically,
we aim to improve the agent's ability to generalize to out-of-distribution
goals. To achieve this, we propose to learn a dynamics model and check if it is
equivariant with respect to a fixed type of transformation, namely translations
in the state space. We then use an entropy regularizer to increase the
equivariant set and augment the dataset with the resulting transformed samples.
Finally, we learn a new policy offline based on the augmented dataset, with an
off-the-shelf offline RL algorithm. Our experimental results demonstrate that
our approach can greatly improve the test performance of the policy on the
considered environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinneri_C/0/1/0/all/0/1"&gt;Cristina Pinneri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bechtle_S/0/1/0/all/0/1"&gt;Sarah Bechtle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1"&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1"&gt;Arunkumar Byravan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1"&gt;William F. Whitney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1"&gt;Martin Riedmiller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BAARD: Blocking Adversarial Examples by Testing for Applicability, Reliability and Decidability. (arXiv:2105.00495v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00495</id>
        <link href="http://arxiv.org/abs/2105.00495"/>
        <updated>2023-09-16T00:40:56.803Z</updated>
        <summary type="html"><![CDATA[Adversarial defenses protect machine learning models from adversarial
attacks, but are often tailored to one type of model or attack. The lack of
information on unknown potential attacks makes detecting adversarial examples
challenging. Additionally, attackers do not need to follow the rules made by
the defender. To address this problem, we take inspiration from the concept of
Applicability Domain in cheminformatics. Cheminformatics models struggle to
make accurate predictions because only a limited number of compounds are known
and available for training. Applicability Domain defines a domain based on the
known compounds and rejects any unknown compound that falls outside the domain.
Similarly, adversarial examples start as harmless inputs, but can be
manipulated to evade reliable classification by moving outside the domain of
the classifier. We are the first to identify the similarity between
Applicability Domain and adversarial detection. Instead of focusing on unknown
attacks, we focus on what is known, the training data. We propose a simple yet
robust triple-stage data-driven framework that checks the input globally and
locally, and confirms that they are coherent with the model's output. This
framework can be applied to any classification model and is not limited to
specific attacks. We demonstrate these three stages work as one unit,
effectively detecting various attacks, even for a white-box scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xinglong Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dost_K/0/1/0/all/0/1"&gt;Katharina Dost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kaiqi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1"&gt;Ambra Demontis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1"&gt;Gill Dobbie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Wicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Community Detection and Rotational Synchronization via Semidefinite Programming. (arXiv:2105.06031v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06031</id>
        <link href="http://arxiv.org/abs/2105.06031"/>
        <updated>2023-09-16T00:40:56.803Z</updated>
        <summary type="html"><![CDATA[In the presence of heterogeneous data, where randomly rotated objects fall
into multiple underlying categories, it is challenging to simultaneously
classify them into clusters and synchronize them based on pairwise relations.
This gives rise to the joint problem of community detection and
synchronization. We propose a series of semidefinite relaxations, and prove
their exact recovery when extending the celebrated stochastic block model to
this new setting where both rotations and cluster identities are to be
determined. Numerical experiments demonstrate the efficacy of our proposed
algorithms and confirm our theoretical result which indicates a sharp phase
transition for exact recovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yifeng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khoo_Y/0/1/0/all/0/1"&gt;Yuehaw Khoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhizhen Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.09960</id>
        <link href="http://arxiv.org/abs/2304.09960"/>
        <updated>2023-09-16T00:40:56.803Z</updated>
        <summary type="html"><![CDATA[Languages are not created randomly but rather to communicate information.
There is a strong association between languages and their underlying meanings,
resulting in a sparse joint distribution that is heavily peaked according to
their correlations. Moreover, these peak values happen to match with the
marginal distribution of languages due to the sparsity. With the advent of LLMs
trained on big data and large models, we can now precisely assess the marginal
distribution of languages, providing a convenient means of exploring the sparse
structures in the joint distribution for effective inferences. In this paper,
we categorize languages as either unambiguous or {\epsilon}-ambiguous and
present quantitative results to demonstrate that the emergent abilities of
LLMs, such as language understanding, in-context learning, chain-of-thought
prompting, and effective instruction fine-tuning, can all be attributed to
Bayesian inference on the sparse joint distribution of languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hui Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07670</id>
        <link href="http://arxiv.org/abs/2309.07670"/>
        <updated>2023-09-16T00:40:56.798Z</updated>
        <summary type="html"><![CDATA[In this article, we propose an approach for federated domain adaptation, a
setting where distributional shift exists among clients and some have unlabeled
data. The proposed framework, FedDaDiL, tackles the resulting challenge through
dictionary learning of empirical distributions. In our setting, clients'
distributions represent particular domains, and FedDaDiL collectively trains a
federated dictionary of empirical distributions. In particular, we build upon
the Dataset Dictionary Learning framework by designing collaborative
communication protocols and aggregation operations. The chosen protocols keep
clients' data private, thus enhancing overall privacy compared to its
centralized counterpart. We empirically demonstrate that our approach
successfully generates labeled data on the target domain with extensive
experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks.
Furthermore, we compare our method to its centralized counterpart and other
benchmarks in federated domain adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellon_F/0/1/0/all/0/1"&gt;Fabiola Espinosa Castellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1"&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1"&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayoue_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Mayoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1"&gt;Antoine Souloumiac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gouy_Pallier_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Gouy-Pallier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.07260</id>
        <link href="http://arxiv.org/abs/2302.07260"/>
        <updated>2023-09-16T00:40:56.798Z</updated>
        <summary type="html"><![CDATA[Several fundamental problems in science and engineering consist of global
optimization tasks involving unknown high-dimensional (black-box) functions
that map a set of controllable variables to the outcomes of an expensive
experiment. Bayesian Optimization (BO) techniques are known to be effective in
tackling global optimization problems using a relatively small number objective
function evaluations, but their performance suffers when dealing with
high-dimensional outputs. To overcome the major challenge of dimensionality,
here we propose a deep learning framework for BO and sequential decision making
based on bootstrapped ensembles of neural architectures with randomized priors.
Using appropriate architecture choices, we show that the proposed framework can
approximate functional relationships between design variables and quantities of
interest, even in cases where the latter take values in high-dimensional vector
spaces or even infinite-dimensional function spaces. In the context of BO, we
augmented the proposed probabilistic surrogates with re-parameterized Monte
Carlo approximations of multiple-point (parallel) acquisition functions, as
well as methodological extensions for accommodating black-box constraints and
multi-fidelity information sources. We test the proposed framework against
state-of-the-art methods for BO and demonstrate superior performance across
several challenging tasks with high-dimensional outputs, including a
constrained multi-fidelity optimization task involving shape optimization of
rotor blades in turbo-machinery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhouri_M/0/1/0/all/0/1"&gt;Mohamed Aziz Bhouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joly_M/0/1/0/all/0/1"&gt;Michael Joly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Robert Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Soumalya Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1"&gt;Paris Perdikaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the complexity of finding a local minimizer of a quadratic function over a polytope. (arXiv:2008.05558v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05558</id>
        <link href="http://arxiv.org/abs/2008.05558"/>
        <updated>2023-09-16T00:40:56.789Z</updated>
        <summary type="html"><![CDATA[We show that unless P=NP, there cannot be a polynomial-time algorithm that
finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a
local minimizer of an $n$-variate quadratic function over a polytope. This
result (even with $c=0$) answers a question of Pardalos and Vavasis that
appeared in 1992 on a list of seven open problems in complexity theory for
numerical optimization. Our proof technique also implies that the problem of
deciding whether a quadratic function has a local minimizer over an (unbounded)
polyhedron, and that of deciding if a quartic polynomial has a local minimizer
are NP-hard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-constrained robust learning of open-form PDEs from limited and noisy data. (arXiv:2309.07672v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07672</id>
        <link href="http://arxiv.org/abs/2309.07672"/>
        <updated>2023-09-16T00:40:56.784Z</updated>
        <summary type="html"><![CDATA[Unveiling the underlying governing equations of nonlinear dynamic systems
remains a significant challenge, especially when encountering noisy
observations and no prior knowledge available. This study proposes R-DISCOVER,
a framework designed to robustly uncover open-form partial differential
equations (PDEs) from limited and noisy data. The framework operates through
two alternating update processes: discovering and embedding. The discovering
phase employs symbolic representation and a reinforcement learning (RL)-guided
hybrid PDE generator to efficiently produce diverse open-form PDEs with tree
structures. A neural network-based predictive model fits the system response
and serves as the reward evaluator for the generated PDEs. PDEs with superior
fits are utilized to iteratively optimize the generator via the RL method and
the best-performing PDE is selected by a parameter-free stability metric. The
embedding phase integrates the initially identified PDE from the discovering
process as a physical constraint into the predictive model for robust training.
The traversal of PDE trees automates the construction of the computational
graph and the embedding process without human intervention. Numerical
experiments demonstrate our framework's capability to uncover governing
equations from nonlinear dynamic systems with limited and highly noisy data and
outperform other physics-informed neural network-based discovery methods. This
work opens new potential for exploring real-world systems with limited
understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengge Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Longfeng Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1"&gt;Siyu Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenc_Y/0/1/0/all/0/1"&gt;Yuntian Chenc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrojViT: Trojan Insertion in Vision Transformers. (arXiv:2208.13049v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.13049</id>
        <link href="http://arxiv.org/abs/2208.13049"/>
        <updated>2023-09-16T00:40:56.784Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViTs) have demonstrated the state-of-the-art performance
in various vision-related tasks. The success of ViTs motivates adversaries to
perform backdoor attacks on ViTs. Although the vulnerability of traditional
CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are
seldom-studied. Compared to CNNs capturing pixel-wise local features by
convolutions, ViTs extract global context information through patches and
attentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTs
yields only a low clean data accuracy and a low attack success rate. In this
paper, we propose a stealth and practical ViT-specific backdoor attack
$TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor
attacks, TrojViT generates a patch-wise trigger designed to build a Trojan
composed of some vulnerable bits on the parameters of a ViT stored in DRAM
memory through patch salience ranking and attention-target loss. TrojViT
further uses minimum-tuned parameter update to reduce the bit number of the
Trojan. Once the attacker inserts the Trojan into the ViT model by flipping the
vulnerable bits, the ViT model still produces normal inference accuracy with
benign inputs. But when the attacker embeds a trigger into an input, the ViT
model is forced to classify the input to a predefined target class. We show
that flipping only few vulnerable bits identified by TrojViT on a ViT model
using the well-known RowHammer can transform the model into a backdoored one.
We perform extensive experiments of multiple datasets on various ViT models.
TrojViT can classify $99.64\%$ of test images to a target class by flipping
$345$ bits on a ViT for ImageNet.Our codes are available at
https://github.com/mxzheng/TrojViT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Mengxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1"&gt;Qian Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lei Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2309.07265</id>
        <link href="http://arxiv.org/abs/2309.07265"/>
        <updated>2023-09-16T00:40:56.766Z</updated>
        <summary type="html"><![CDATA[The open radio access network (O-RAN) architecture supports intelligent
network control algorithms as one of its core capabilities. Data-driven
applications incorporate such algorithms to optimize radio access network (RAN)
functions via RAN intelligent controllers (RICs). Deep reinforcement learning
(DRL) algorithms are among the main approaches adopted in the O-RAN literature
to solve dynamic radio resource management problems. However, despite the
benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms
in real network deployments falls behind. This is primarily due to the slow
convergence and unstable performance exhibited by DRL agents upon deployment
and when facing previously unseen network conditions. In this paper, we address
these challenges by proposing transfer learning (TL) as a core component of the
training and deployment workflows for the DRL-based closed-loop control of
O-RAN functionalities. To this end, we propose and design a hybrid TL-aided
approach that leverages the advantages of both policy reuse and distillation TL
methods to provide safe and accelerated convergence in DRL-based O-RAN slicing.
We conduct a thorough experiment that accommodates multiple services, including
real VR gaming traffic to reflect practical scenarios of O-RAN slicing. We also
propose and implement policy reuse and distillation-aided DRL and non-TL-aided
DRL as three separate baselines. The proposed hybrid approach shows at least:
7.7% and 20.7% improvements in the average initial reward value and the
percentage of converged scenarios, and a 64.6% decrease in reward variance
while maintaining fast convergence and enhancing the generalizability compared
with the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagib_A/0/1/0/all/0/1"&gt;Ahmad M. Nagib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abou_Zeid_H/0/1/0/all/0/1"&gt;Hatem Abou-Zeid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassanein_H/0/1/0/all/0/1"&gt;Hossam S. Hassanein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07666</id>
        <link href="http://arxiv.org/abs/2309.07666"/>
        <updated>2023-09-16T00:40:56.744Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the intersection of two problems in machine
learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD).
On the one hand, the first considers adapting multiple heterogeneous labeled
source domains to an unlabeled target domain. On the other hand, the second
attacks the problem of synthesizing a small summary containing all the
information about the datasets. We thus consider a new problem called MSDA-DD.
To solve it, we adapt previous works in the MSDA literature, such as
Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD
method Distribution Matching. We thoroughly experiment with this novel problem
on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous
Stirred Tank Reactor, and Case Western Reserve University), where we show that,
even with as little as 1 sample per class, one achieves state-of-the-art
adaptation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1"&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1"&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1"&gt;Antoine Souloumiac&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Process Surrogate Models for Neural Networks. (arXiv:2208.06028v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.06028</id>
        <link href="http://arxiv.org/abs/2208.06028"/>
        <updated>2023-09-16T00:40:56.743Z</updated>
        <summary type="html"><![CDATA[Not being able to understand and predict the behavior of deep learning
systems makes it hard to decide what architecture and algorithm to use for a
given problem. In science and engineering, modeling is a methodology used to
understand complex systems whose internal processes are opaque. Modeling
replaces a complex system with a simpler, more interpretable surrogate. Drawing
inspiration from this, we construct a class of surrogate models for neural
networks using Gaussian processes. Rather than deriving kernels for infinite
neural networks, we learn kernels empirically from the naturalistic behavior of
finite neural networks. We demonstrate our approach captures existing phenomena
related to the spectral bias of neural networks, and then show that our
surrogate models can be used to solve practical problems such as identifying
which points most influence the behavior of specific neural networks and
predicting which architectures and algorithms will generalize well for specific
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Michael Y. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1"&gt;Erin Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE. (arXiv:2309.07663v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.07663</id>
        <link href="http://arxiv.org/abs/2309.07663"/>
        <updated>2023-09-16T00:40:56.738Z</updated>
        <summary type="html"><![CDATA[In the Variational Autoencoder (VAE), the variational posterior often aligns
closely with the prior, which is known as posterior collapse and hinders the
quality of representation learning. To mitigate this problem, an adjustable
hyperparameter beta has been introduced in the VAE. This paper presents a
closed-form expression to assess the relationship between the beta in VAE, the
dataset size, the posterior collapse, and the rate-distortion curve by
analyzing a minimal VAE in a high-dimensional limit. These results clarify that
a long plateau in the generalization error emerges with a relatively larger
beta. As the beta increases, the length of the plateau extends and then becomes
infinite beyond a certain beta threshold. This implies that the choice of beta,
unlike the usual regularization parameters, can induce posterior collapse
regardless of the dataset size. Thus, beta is a risky parameter that requires
careful tuning. Furthermore, considering the dataset-size dependence on the
rate-distortion curve, a relatively large dataset is required to obtain a
rate-distortion curve with high rates. Extensive numerical experiments support
our analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ichikawa_Y/0/1/0/all/0/1"&gt;Yuma Ichikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hukushima_K/0/1/0/all/0/1"&gt;Koji Hukushima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trajectory-oriented optimization of stochastic epidemiological models. (arXiv:2305.03926v3 [stat.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.03926</id>
        <link href="http://arxiv.org/abs/2305.03926"/>
        <updated>2023-09-16T00:40:56.731Z</updated>
        <summary type="html"><![CDATA[Epidemiological models must be calibrated to ground truth for downstream
tasks such as producing forward projections or running what-if scenarios. The
meaning of calibration changes in case of a stochastic model since output from
such a model is generally described via an ensemble or a distribution. Each
member of the ensemble is usually mapped to a random number seed (explicitly or
implicitly). With the goal of finding not only the input parameter settings but
also the random seeds that are consistent with the ground truth, we propose a
class of Gaussian process (GP) surrogates along with an optimization strategy
based on Thompson sampling. This Trajectory Oriented Optimization (TOO)
approach produces actual trajectories close to the empirical observations
instead of a set of parameter settings where only the mean simulation behavior
matches with the ground truth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fadikar_A/0/1/0/all/0/1"&gt;Arindam Fadikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Binois_M/0/1/0/all/0/1"&gt;Mickael Binois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Collier_N/0/1/0/all/0/1"&gt;Nicholson Collier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stevens_A/0/1/0/all/0/1"&gt;Abby Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Toh_K/0/1/0/all/0/1"&gt;Kok Ben Toh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ozik_J/0/1/0/all/0/1"&gt;Jonathan Ozik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nowhere coexpanding functions. (arXiv:2303.12814v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.12814</id>
        <link href="http://arxiv.org/abs/2303.12814"/>
        <updated>2023-09-16T00:40:56.728Z</updated>
        <summary type="html"><![CDATA[We define a family of $C^1$ functions which we call "nowhere coexpanding
functions" that is closed under composition and includes all $C^3$ functions
with non-positive Schwarzian derivative. We establish results on the number and
nature of the fixed points of these functions, including a generalisation of a
classic result of Singer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cook_A/0/1/0/all/0/1"&gt;Andrew Cook&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hammerlindl_A/0/1/0/all/0/1"&gt;Andy Hammerlindl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tucker_W/0/1/0/all/0/1"&gt;Warwick Tucker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders. (arXiv:2309.07138v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07138</id>
        <link href="http://arxiv.org/abs/2309.07138"/>
        <updated>2023-09-16T00:40:56.727Z</updated>
        <summary type="html"><![CDATA[The task of blind source separation (BSS) involves separating sources from a
mixture without prior knowledge of the sources or the mixing system. This is a
challenging problem that often requires making restrictive assumptions about
both the mixing system and the sources. In this paper, we propose a novel
method for addressing BSS of non-linear mixtures by leveraging the natural
feature subspace specialization ability of multi-encoder autoencoders with
fully self-supervised learning without strong priors. During the training
phase, our method unmixes the input into the separate encoding spaces of the
multi-encoder network and then remixes these representations within the decoder
for a reconstruction of the input. Then to perform source inference, we
introduce a novel encoding masking technique whereby masking out all but one of
the encodings enables the decoder to estimate a source signal. To this end, we
also introduce a so-called pathway separation loss that encourages sparsity
between the unmixed encoding spaces throughout the decoder's layers and a
so-called zero reconstruction loss on the decoder for coherent source
estimations. In order to carefully evaluate our method, we conduct experiments
on a toy dataset and with real-world biosignal recordings from a
polysomnography sleep study for extracting respiration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Webster_M/0/1/0/all/0/1"&gt;Matthew B. Webster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joonnyong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE. (arXiv:2309.07663v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.07663</id>
        <link href="http://arxiv.org/abs/2309.07663"/>
        <updated>2023-09-16T00:40:56.725Z</updated>
        <summary type="html"><![CDATA[In the Variational Autoencoder (VAE), the variational posterior often aligns
closely with the prior, which is known as posterior collapse and hinders the
quality of representation learning. To mitigate this problem, an adjustable
hyperparameter beta has been introduced in the VAE. This paper presents a
closed-form expression to assess the relationship between the beta in VAE, the
dataset size, the posterior collapse, and the rate-distortion curve by
analyzing a minimal VAE in a high-dimensional limit. These results clarify that
a long plateau in the generalization error emerges with a relatively larger
beta. As the beta increases, the length of the plateau extends and then becomes
infinite beyond a certain beta threshold. This implies that the choice of beta,
unlike the usual regularization parameters, can induce posterior collapse
regardless of the dataset size. Thus, beta is a risky parameter that requires
careful tuning. Furthermore, considering the dataset-size dependence on the
rate-distortion curve, a relatively large dataset is required to obtain a
rate-distortion curve with high rates. Extensive numerical experiments support
our analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ichikawa_Y/0/1/0/all/0/1"&gt;Yuma Ichikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hukushima_K/0/1/0/all/0/1"&gt;Koji Hukushima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.07339</id>
        <link href="http://arxiv.org/abs/2309.07339"/>
        <updated>2023-09-16T00:40:56.724Z</updated>
        <summary type="html"><![CDATA[Quantum reinforcement learning (QRL) has emerged as a framework to solve
sequential decision-making tasks, showcasing empirical quantum advantages. A
notable development is through quantum recurrent neural networks (QRNNs) for
memory-intensive tasks such as partially observable environments. However, QRL
models incorporating QRNN encounter challenges such as inefficient training of
QRL with QRNN, given that the computation of gradients in QRNN is both
computationally expensive and time-consuming. This work presents a novel
approach to address this challenge by constructing QRL agents utilizing
QRNN-based reservoirs, specifically employing quantum long short-term memory
(QLSTM). QLSTM parameters are randomly initialized and fixed without training.
The model is trained using the asynchronous advantage actor-aritic (A3C)
algorithm. Through numerical simulations, we validate the efficacy of our
QLSTM-Reservoir RL framework. Its performance is assessed on standard
benchmarks, demonstrating comparable results to a fully trained QLSTM RL model
with identical architecture and training settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chen_S/0/1/0/all/0/1"&gt;Samuel Yen-Chi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.07611</id>
        <link href="http://arxiv.org/abs/2112.07611"/>
        <updated>2023-09-16T00:40:56.720Z</updated>
        <summary type="html"><![CDATA[We develop a theoretical framework for $S_n$-equivariant convolutional
quantum circuits with SU$(d)$-symmetry, building on and significantly
generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on
Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In
particular, we utilize the Okounkov-Vershik approach to prove Harrow's
statement (Ph.D. Thesis 2005 p.160) on the equivalence between
$\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the
$S_n$-equivariant Convolutional Quantum Alternating Ans\"atze ($S_n$-CQA) using
Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate
any unitary in any given $S_n$ irrep sector, which may serve as a universal
model for a wide array of quantum machine learning problems with the presence
of SU($d$) symmetry. Our method provides another way to prove the universality
of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local
SU($d$) symmetric unitaries are sufficient to build generic SU($d$) symmetric
quantum circuits up to relative phase factors. We present numerical simulations
to showcase the effectiveness of the ans\"atze to find the ground state energy
of the $J_1$--$J_2$ antiferromagnetic Heisenberg model on the rectangular and
Kagome lattices. Our work provides the first application of the celebrated
Okounkov-Vershik's $S_n$ representation theory to quantum physics and machine
learning, from which to propose quantum variational ans\"atze that strongly
suggests to be classically intractable tailored towards a specific optimization
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Han Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zimu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Strelchuk_S/0/1/0/all/0/1"&gt;Sergii Strelchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kondor_R/0/1/0/all/0/1"&gt;Risi Kondor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering. (arXiv:2309.07548v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07548</id>
        <link href="http://arxiv.org/abs/2309.07548"/>
        <updated>2023-09-16T00:40:56.719Z</updated>
        <summary type="html"><![CDATA[This paper aims at the algorithmic/theoretical core of reinforcement learning
(RL) by introducing the novel class of proximal Bellman mappings. These
mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit
from the rich approximation properties and inner product of RKHSs, they are
shown to belong to the powerful Hilbertian family of (firmly) nonexpansive
mappings, regardless of the values of their discount factors, and possess ample
degrees of design freedom to even reproduce attributes of the classical Bellman
mappings and to pave the way for novel RL designs. An approximate
policy-iteration scheme is built on the proposed class of mappings to solve the
problem of selecting online, at every time instance, the "optimal" exponent $p$
in a $p$-norm loss to combat outliers in linear adaptive filtering, without
training data and any knowledge on the statistical properties of the outliers.
Numerical tests on synthetic data showcase the superior performance of the
proposed framework over several non-RL and kernel-based RL schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Akiyama_Y/0/1/0/all/0/1"&gt;Yuki Akiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slavakis_K/0/1/0/all/0/1"&gt;Konstantinos Slavakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Learning of PDEs via Taylor Expansion and Sparse Decomposition into Value and Fourier Domains. (arXiv:2309.07344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07344</id>
        <link href="http://arxiv.org/abs/2309.07344"/>
        <updated>2023-09-16T00:40:56.671Z</updated>
        <summary type="html"><![CDATA[Accelerating the learning of Partial Differential Equations (PDEs) from
experimental data will speed up the pace of scientific discovery. Previous
randomized algorithms exploit sparsity in PDE updates for acceleration. However
such methods are applicable to a limited class of decomposable PDEs, which have
sparse features in the value domain. We propose Reel, which accelerates the
learning of PDEs via random projection and has much broader applicability. Reel
exploits the sparsity by decomposing dense updates into sparse ones in both the
value and frequency domains. This decomposition enables efficient learning when
the source of the updates consists of gradually changing terms across large
areas (sparse in the frequency domain) in addition to a few rapid updates
concentrated in a small set of "interfacial" regions (sparse in the value
domain). Random projection is then applied to compress the sparse signals for
learning. To expand the model applicability, Taylor series expansion is used in
Reel to approximate the nonlinear PDE updates with polynomials in the
decomposable form. Theoretically, we derive a constant factor approximation
between the projected loss function and the original one with poly-logarithmic
number of projected dimensions. Experimentally, we provide empirical evidence
that our proposed Reel can lead to faster learning of PDE models (70-98%
reduction in training time when the data is compressed to 1% of its original
size) with comparable quality as the non-compressed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1"&gt;Md Nasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yexiang Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2309.07261</id>
        <link href="http://arxiv.org/abs/2309.07261"/>
        <updated>2023-09-16T00:40:56.655Z</updated>
        <summary type="html"><![CDATA[Tens of thousands of simultaneous hypothesis tests are routinely performed in
genomic studies to identify differentially expressed genes. However, due to
unmeasured confounders, many standard statistical approaches may be
substantially biased. This paper investigates the large-scale hypothesis
testing problem for multivariate generalized linear models in the presence of
confounding effects. Under arbitrary confounding mechanisms, we propose a
unified statistical estimation and inference framework that harnesses
orthogonal structures and integrates linear projections into three key stages.
It first leverages multivariate responses to separate marginal and uncorrelated
confounding effects, recovering the confounding coefficients' column space.
Subsequently, latent factors and primary effects are jointly estimated,
utilizing $\ell_1$-regularization for sparsity while imposing orthogonality
onto confounding coefficients. Finally, we incorporate projected and weighted
bias-correction steps for hypothesis testing. Theoretically, we establish
various effects' identification conditions and non-asymptotic error bounds. We
show effective Type-I error control of asymptotic $z$-tests as sample and
response sizes approach infinity. Numerical experiments demonstrate that the
proposed method controls the false discovery rate by the Benjamini-Hochberg
procedure and is more powerful than alternative methods. By comparing
single-cell RNA-seq counts from two groups of samples, we demonstrate the
suitability of adjusting confounding effects when significant covariates are
absent from the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Du_J/0/1/0/all/0/1"&gt;Jin-Hong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wasserman_L/0/1/0/all/0/1"&gt;Larry Wasserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roeder_K/0/1/0/all/0/1"&gt;Kathryn Roeder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?. (arXiv:2309.07452v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07452</id>
        <link href="http://arxiv.org/abs/2309.07452"/>
        <updated>2023-09-16T00:40:56.645Z</updated>
        <summary type="html"><![CDATA[A rising trend in theoretical deep learning is to understand why deep
learning works through Neural Tangent Kernel (NTK) [jgh18], a kernel method
that is equivalent to using gradient descent to train a multi-layer
infinitely-wide neural network. NTK is a major step forward in the theoretical
deep learning because it allows researchers to use traditional mathematical
tools to analyze properties of deep neural networks and to explain various
neural network techniques from a theoretical view. A natural extension of NTK
on graph learning is \textit{Graph Neural Tangent Kernel (GNTK)}, and
researchers have already provide GNTK formulation for graph-level regression
and show empirically that this kernel method can achieve similar accuracy as
GNNs on various bioinformatics datasets [dhs+19]. The remaining question now is
whether solving GNTK regression is equivalent to training an infinite-wide
multi-layer GNN using gradient descent. In this paper, we provide three new
theoretical results. First, we formally prove this equivalence for graph-level
regression. Second, we present the first GNTK formulation for node-level
regression. Finally, we prove the equivalence for node-level regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Lianke Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baocheng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2309.07383</id>
        <link href="http://arxiv.org/abs/2309.07383"/>
        <updated>2023-09-16T00:40:56.643Z</updated>
        <summary type="html"><![CDATA[This paper studies convergence rates for some value function approximations
that arise in a collection of reproducing kernel Hilbert spaces (RKHS)
$H(\Omega)$. By casting an optimal control problem in a specific class of
native spaces, strong rates of convergence are derived for the operator
equation that enables offline approximations that appear in policy iteration.
Explicit upper bounds on error in value function approximations are derived in
terms of power function $\Pwr_{H,N}$ for the space of finite dimensional
approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric
in nature and refine some well-known, now classical results concerning
convergence of approximations of value functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bouland_A/0/1/0/all/0/1"&gt;Ali Bouland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_S/0/1/0/all/0/1"&gt;Shengyuan Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Paruchuri_S/0/1/0/all/0/1"&gt;Sai Tej Paruchuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurdila_A/0/1/0/all/0/1"&gt;Andrew Kurdila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1"&gt;John Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schuster_E/0/1/0/all/0/1"&gt;Eugenio Schuster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning. (arXiv:2309.00923v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.00923</id>
        <link href="http://arxiv.org/abs/2309.00923"/>
        <updated>2023-09-16T00:40:56.615Z</updated>
        <summary type="html"><![CDATA[This paper investigates a challenging problem of zero-shot learning in the
multi-label scenario (MLZSL), wherein, the model is trained to recognize
multiple unseen classes within a sample (e.g., an image) based on seen classes
and auxiliary knowledge, e.g., semantic information. Existing methods usually
resort to analyzing the relationship of various seen classes residing in a
sample from the dimension of spatial or semantic characteristics, and transfer
the learned model to unseen ones. But they ignore the effective integration of
local and global features. That is, in the process of inferring unseen classes,
global features represent the principal direction of the image in the feature
space, while local features should maintain uniqueness within a certain range.
This integrated neglect will make the model lose its grasp of the main
components of the image. Relying only on the local existence of seen classes
during the inference stage introduces unavoidable bias. In this paper, we
propose a novel and effective group bi-enhancement framework for MLZSL, dubbed
GBE-MLZSL, to fully make use of such properties and enable a more accurate and
robust visual-semantic projection. Specifically, we split the feature maps into
several feature groups, of which each feature group can be trained
independently with the Local Information Distinguishing Module (LID) to ensure
uniqueness. Meanwhile, a Global Enhancement Module (GEM) is designed to
preserve the principal direction. Besides, a static graph structure is designed
to construct the correlation of local features. Experiments on large-scale
MLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that the
proposed GBE-MLZSL outperforms other state-of-the-art methods with large
margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jingcai Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaocheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Song Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1"&gt;Peiran Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiewei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ETP: Learning Transferable ECG Representations via ECG-Text Pre-training. (arXiv:2309.07145v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07145</id>
        <link href="http://arxiv.org/abs/2309.07145"/>
        <updated>2023-09-16T00:40:56.604Z</updated>
        <summary type="html"><![CDATA[In the domain of cardiovascular healthcare, the Electrocardiogram (ECG)
serves as a critical, non-invasive diagnostic tool. Although recent strides in
self-supervised learning (SSL) have been promising for ECG representation
learning, these techniques often require annotated samples and struggle with
classes not present in the fine-tuning stages. To address these limitations, we
introduce ECG-Text Pre-training (ETP), an innovative framework designed to
learn cross-modal representations that link ECG signals with textual reports.
For the first time, this framework leverages the zero-shot classification task
in the ECG domain. ETP employs an ECG encoder along with a pre-trained language
model to align ECG signals with their corresponding textual reports. The
proposed framework excels in both linear evaluation and zero-shot
classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets,
showcasing its ability for robust and generalizable cross-modal ECG feature
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Che Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wan_Z/0/1/0/all/0/1"&gt;Zhongwei Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sibo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arcucci_R/0/1/0/all/0/1"&gt;Rossella Arcucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing PDEs to JAX with forward and reverse modes automatic differentiation. (arXiv:2309.07137v1 [cs.MS])]]></title>
        <id>http://arxiv.org/abs/2309.07137</id>
        <link href="http://arxiv.org/abs/2309.07137"/>
        <updated>2023-09-16T00:40:56.594Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) are used to describe a variety of
physical phenomena. Often these equations do not have analytical solutions and
numerical approximations are used instead. One of the common methods to solve
PDEs is the finite element method. Computing derivative information of the
solution with respect to the input parameters is important in many tasks in
scientific computing. We extend JAX automatic differentiation library with an
interface to Firedrake finite element library. High-level symbolic
representation of PDEs allows bypassing differentiating through low-level
possibly many iterations of the underlying nonlinear solvers. Differentiating
through Firedrake solvers is done using tangent-linear and adjoint equations.
This enables the efficient composition of finite element solvers with arbitrary
differentiable programs. The code is available at
github.com/IvanYashchuk/jax-firedrake.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yashchuk_I/0/1/0/all/0/1"&gt;Ivan Yashchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gibbs-Duhem-Informed Neural Networks for Binary Activity Coefficient Prediction. (arXiv:2306.07937v2 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.07937</id>
        <link href="http://arxiv.org/abs/2306.07937"/>
        <updated>2023-09-16T00:40:56.593Z</updated>
        <summary type="html"><![CDATA[We propose Gibbs-Duhem-informed neural networks for the prediction of binary
activity coefficients at varying compositions. That is, we include the
Gibbs-Duhem equation explicitly in the loss function for training neural
networks, which is straightforward in standard machine learning (ML) frameworks
enabling automatic differentiation. In contrast to recent hybrid ML approaches,
our approach does not rely on embedding a specific thermodynamic model inside
the neural network and corresponding prediction limitations. Rather,
Gibbs-Duhem consistency serves as regularization, with the flexibility of ML
models being preserved. Our results show increased thermodynamic consistency
and generalization capabilities for activity coefficient predictions by
Gibbs-Duhem-informed graph neural networks and matrix completion methods. We
also find that the model architecture, particularly the activation function,
can have a strong influence on the prediction quality. The approach can be
easily extended to account for other thermodynamic consistency conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Rittig_J/0/1/0/all/0/1"&gt;Jan G. Rittig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Felton_K/0/1/0/all/0/1"&gt;Kobi C. Felton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lapkin_A/0/1/0/all/0/1"&gt;Alexei A. Lapkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mitsos_A/0/1/0/all/0/1"&gt;Alexander Mitsos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MELAGE: A purely python based Neuroimaging software (Neonatal). (arXiv:2309.07175v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.07175</id>
        <link href="http://arxiv.org/abs/2309.07175"/>
        <updated>2023-09-16T00:40:56.587Z</updated>
        <summary type="html"><![CDATA[MELAGE, a pioneering Python-based neuroimaging software, emerges as a
versatile tool for the visualization, processing, and analysis of medical
images. Initially conceived to address the unique challenges of processing 3D
ultrasound and MRI brain images during the neonatal period, MELAGE exhibits
remarkable adaptability, extending its utility to the domain of adult human
brain imaging. At its core, MELAGE features a semi-automatic brain extraction
tool empowered by a deep learning module, ensuring precise and efficient brain
structure extraction from MRI and 3D Ultrasound data. Moreover, MELAGE offers a
comprehensive suite of features, encompassing dynamic 3D visualization,
accurate measurements, and interactive image segmentation. This transformative
software holds immense promise for researchers and clinicians, offering
streamlined image analysis, seamless integration with deep learning algorithms,
and broad applicability in the realm of medical imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jafrasteh_B/0/1/0/all/0/1"&gt;Bahram Jafrasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lopez_S/0/1/0/all/0/1"&gt;Sim&amp;#xf3;n Pedro Lubi&amp;#xe1;n L&amp;#xf3;pez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fernandez_I/0/1/0/all/0/1"&gt;Isabel Benavente Fern&amp;#xe1;ndez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.07611</id>
        <link href="http://arxiv.org/abs/2112.07611"/>
        <updated>2023-09-16T00:40:56.580Z</updated>
        <summary type="html"><![CDATA[We develop a theoretical framework for $S_n$-equivariant convolutional
quantum circuits with SU$(d)$-symmetry, building on and significantly
generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on
Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In
particular, we utilize the Okounkov-Vershik approach to prove Harrow's
statement (Ph.D. Thesis 2005 p.160) on the equivalence between
$\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the
$S_n$-equivariant Convolutional Quantum Alternating Ans\"atze ($S_n$-CQA) using
Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate
any unitary in any given $S_n$ irrep sector, which may serve as a universal
model for a wide array of quantum machine learning problems with the presence
of SU($d$) symmetry. Our method provides another way to prove the universality
of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local
SU($d$) symmetric unitaries are sufficient to build generic SU($d$) symmetric
quantum circuits up to relative phase factors. We present numerical simulations
to showcase the effectiveness of the ans\"atze to find the ground state energy
of the $J_1$--$J_2$ antiferromagnetic Heisenberg model on the rectangular and
Kagome lattices. Our work provides the first application of the celebrated
Okounkov-Vershik's $S_n$ representation theory to quantum physics and machine
learning, from which to propose quantum variational ans\"atze that strongly
suggests to be classically intractable tailored towards a specific optimization
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Han Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zimu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Strelchuk_S/0/1/0/all/0/1"&gt;Sergii Strelchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kondor_R/0/1/0/all/0/1"&gt;Risi Kondor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.15021</id>
        <link href="http://arxiv.org/abs/2305.15021"/>
        <updated>2023-09-16T00:40:56.565Z</updated>
        <summary type="html"><![CDATA[Embodied AI is a crucial frontier in robotics, capable of planning and
executing action sequences for robots to accomplish long-horizon tasks in
physical environments. In this work, we introduce EmbodiedGPT, an end-to-end
multi-modal foundation model for embodied AI, empowering embodied agents with
multi-modal understanding and execution capabilities. To achieve this, we have
made the following efforts: (i) We craft a large-scale embodied planning
dataset, termed EgoCOT. The dataset consists of carefully selected videos from
the Ego4D dataset, along with corresponding high-quality language instructions.
Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts"
mode for effective embodied planning. (ii) We introduce an efficient training
approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B
large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We
introduce a paradigm for extracting task-related features from LLM-generated
planning queries to form a closed loop between high-level planning and
low-level control. Extensive experiments show the effectiveness of EmbodiedGPT
on embodied tasks, including embodied planning, embodied control, visual
captioning, and visual question answering. Notably, EmbodiedGPT significantly
enhances the success rate of the embodied control task by extracting more
effective features. It has achieved a remarkable 1.6 times increase in success
rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World
benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1"&gt;Yao Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Mengkang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Mingyu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jun Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EpiDeNet: An Energy-Efficient Approach to Seizure Detection for Embedded Systems. (arXiv:2309.07135v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07135</id>
        <link href="http://arxiv.org/abs/2309.07135"/>
        <updated>2023-09-16T00:40:56.534Z</updated>
        <summary type="html"><![CDATA[Epilepsy is a prevalent neurological disorder that affects millions of
individuals globally, and continuous monitoring coupled with automated seizure
detection appears as a necessity for effective patient treatment. To enable
long-term care in daily-life conditions, comfortable and smart wearable devices
with long battery life are required, which in turn set the demand for
resource-constrained and energy-efficient computing solutions. In this context,
the development of machine learning algorithms for seizure detection faces the
challenge of heavily imbalanced datasets. This paper introduces EpiDeNet, a new
lightweight seizure detection network, and Sensitivity-Specificity Weighted
Cross-Entropy (SSWCE), a new loss function that incorporates sensitivity and
specificity, to address the challenge of heavily unbalanced datasets. The
proposed EpiDeNet-SSWCE approach demonstrates the successful detection of
91.16% and 92.00% seizure events on two different datasets (CHB-MIT and
PEDESITE, respectively), with only four EEG channels. A three-window majority
voting-based smoothing scheme combined with the SSWCE loss achieves 3x
reduction of false positives to 1.18 FP/h. EpiDeNet is well suited for
implementation on low-power embedded platforms, and we evaluate its performance
on two ARM Cortex-based platforms (M4F/M7) and two parallel ultra-low power
(PULP) systems (GAP8, GAP9). The most efficient implementation (GAP9) achieves
an energy efficiency of 40 GMAC/s/W, with an energy consumption per inference
of only 0.051 mJ at high performance (726.46 MMAC/s), outperforming the best
ARM Cortex-based solutions by approximately 160x in energy efficiency. The
EpiDeNet-SSWCE method demonstrates effective and accurate seizure detection
performance on heavily imbalanced datasets, while being suited for
implementation on energy-constrained platforms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ingolfsson_T/0/1/0/all/0/1"&gt;Thorir Mar Ingolfsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chakraborty_U/0/1/0/all/0/1"&gt;Upasana Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beniczky_S/0/1/0/all/0/1"&gt;Sandor Beniczky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ducouret_P/0/1/0/all/0/1"&gt;Pauline Ducouret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benatti_S/0/1/0/all/0/1"&gt;Simone Benatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ryvlin_P/0/1/0/all/0/1"&gt;Philippe Ryvlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cossettini_A/0/1/0/all/0/1"&gt;Andrea Cossettini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2309.07461</id>
        <link href="http://arxiv.org/abs/2309.07461"/>
        <updated>2023-09-16T00:40:56.503Z</updated>
        <summary type="html"><![CDATA[The widespread integration of Internet of Things (IoT) devices across all
facets of life has ushered in an era of interconnectedness, creating new
avenues for cybersecurity challenges and underscoring the need for robust
intrusion detection systems. However, traditional security systems are designed
with a closed-world perspective and often face challenges in dealing with the
ever-evolving threat landscape, where new and unfamiliar attacks are constantly
emerging. In this paper, we introduce a framework aimed at mitigating the open
set recognition (OSR) problem in the realm of Network Intrusion Detection
Systems (NIDS) tailored for IoT environments. Our framework capitalizes on
image-based representations of packet-level data, extracting spatial and
temporal patterns from network traffic. Additionally, we integrate stacking and
sub-clustering techniques, enabling the identification of unknown attacks by
effectively modeling the complex and diverse nature of benign behavior. The
empirical results prominently underscore the framework's efficacy, boasting an
impressive 88\% detection rate for previously unseen attacks when compared
against existing approaches and recent advancements. Future work will perform
extensive experimentation across various openness levels and attack scenarios,
further strengthening the adaptability and performance of our proposed solution
in safeguarding IoT environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farrukh_Y/0/1/0/all/0/1"&gt;Yasir Ali Farrukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1"&gt;Syed Wali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1"&gt;Irfan Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1"&gt;Nathaniel D. Bastian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Choosing a Proxy Metric from Past Experiments. (arXiv:2309.07893v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2309.07893</id>
        <link href="http://arxiv.org/abs/2309.07893"/>
        <updated>2023-09-16T00:40:56.503Z</updated>
        <summary type="html"><![CDATA[In many randomized experiments, the treatment effect of the long-term metric
(i.e. the primary outcome of interest) is often difficult or infeasible to
measure. Such long-term metrics are often slow to react to changes and
sufficiently noisy they are challenging to faithfully estimate in short-horizon
experiments. A common alternative is to measure several short-term proxy
metrics in the hope they closely track the long-term metric -- so they can be
used to effectively guide decision-making in the near-term. We introduce a new
statistical framework to both define and construct an optimal proxy metric for
use in a homogeneous population of randomized experiments. Our procedure first
reduces the construction of an optimal proxy metric in a given experiment to a
portfolio optimization problem which depends on the true latent treatment
effects and noise level of experiment under consideration. We then denoise the
observed treatment effects of the long-term metric and a set of proxies in a
historical corpus of randomized experiments to extract estimates of the latent
treatment effects for use in the optimization problem. One key insight derived
from our approach is that the optimal proxy metric for a given experiment is
not apriori fixed; rather it should depend on the sample size (or effective
noise level) of the randomized experiment for which it is deployed. To
instantiate and evaluate our framework, we employ our methodology in a large
corpus of randomized experiments from an industrial recommendation system and
construct proxy metrics that perform favorably relative to several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tripuraneni_N/0/1/0/all/0/1"&gt;Nilesh Tripuraneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Richardson_L/0/1/0/all/0/1"&gt;Lee Richardson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Soriano_J/0/1/0/all/0/1"&gt;Jacopo Soriano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yadlowsky_S/0/1/0/all/0/1"&gt;Steve Yadlowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Dynamic Graph Convolutional Recurrent Neural Network for Traffic Flow Prediction in Highway Transportation. (arXiv:2309.07196v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07196</id>
        <link href="http://arxiv.org/abs/2309.07196"/>
        <updated>2023-09-16T00:40:56.474Z</updated>
        <summary type="html"><![CDATA[As one of the important tools for spatial feature extraction, graph
convolution has been applied in a wide range of fields such as traffic flow
prediction. However, current popular works of graph convolution cannot
guarantee spatio-temporal consistency in a long period. The ignorance of
correlational dynamics, convolutional locality and temporal comprehensiveness
would limit predictive accuracy. In this paper, a novel Attention-based Dynamic
Graph Convolutional Recurrent Neural Network (ADGCRNN) is proposed to improve
traffic flow prediction in highway transportation. Three temporal resolutions
of data sequence are effectively integrated by self-attention to extract
characteristics; multi-dynamic graphs and their weights are dynamically created
to compliantly combine the varying characteristics; a dedicated gated kernel
emphasizing highly relative nodes is introduced on these complete graphs to
reduce overfitting for graph convolution operations. Experiments on two public
datasets show our work better than state-of-the-art baselines, and case studies
of a real Web system prove practical benefit in highway transportation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianpu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weilong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_M/0/1/0/all/0/1"&gt;Mengda Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2309.07289</id>
        <link href="http://arxiv.org/abs/2309.07289"/>
        <updated>2023-09-16T00:40:56.451Z</updated>
        <summary type="html"><![CDATA[We designed and tested a system for real-time control of a user interface by
extracting surface electromyographic (sEMG) activity from eight electrodes in a
wrist-band configuration. sEMG data were streamed into a machine-learning
algorithm that classified hand gestures in real-time. After an initial model
calibration, participants were presented with one of three types of feedback
during a human-learning stage: veridical feedback, in which predicted
probabilities from the gesture classification algorithm were displayed without
alteration, modified feedback, in which we applied a hidden augmentation of
error to these probabilities, and no feedback. User performance was then
evaluated in a series of minigames, in which subjects were required to use
eight gestures to manipulate their game avatar to complete a task. Experimental
results indicated that, relative to baseline, the modified feedback condition
led to significantly improved accuracy and improved gesture class separation.
These findings suggest that real-time feedback in a gamified user interface
with manipulation of feedback may enable intuitive, rapid, and accurate task
acquisition for sEMG-based gesture recognition applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bicer_Y/0/1/0/all/0/1"&gt;Yunus Bicer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smedemark_Margulies_N/0/1/0/all/0/1"&gt;Niklas Smedemark-Margulies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celik_B/0/1/0/all/0/1"&gt;Basak Celik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunger_E/0/1/0/all/0/1"&gt;Elifnur Sunger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orendorff_R/0/1/0/all/0/1"&gt;Ryan Orendorff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naufel_S/0/1/0/all/0/1"&gt;Stephanie Naufel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1"&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdo%7Bg%7Dmu%7Bs%7D_D/0/1/0/all/0/1"&gt;Deniz Erdo{&amp;#x11f;}mu{&amp;#x15f;}&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tunik_E/0/1/0/all/0/1"&gt;Eugene Tunik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1"&gt;Mathew Yarossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07176</id>
        <link href="http://arxiv.org/abs/2309.07176"/>
        <updated>2023-09-16T00:40:56.177Z</updated>
        <summary type="html"><![CDATA[In consequential domains, it is often impossible to compel individuals to
take treatment, so that optimal policy rules are merely suggestions in the
presence of human non-adherence to treatment recommendations. In these same
domains, there may be heterogeneity both in who responds in taking-up
treatment, and heterogeneity in treatment efficacy. While optimal treatment
rules can maximize causal outcomes across the population, access parity
constraints or other fairness considerations can be relevant in the case of
encouragement. For example, in social services, a persistent puzzle is the gap
in take-up of beneficial services among those who may benefit from them the
most. When in addition the decision-maker has distributional preferences over
both access and average outcomes, the optimal decision rule changes. We study
causal identification, statistical variance-reduced estimation, and robust
estimation of optimal treatment rules, including under potential violations of
positivity. We consider fairness constraints such as demographic parity in
treatment take-up, and other constraints, via constrained optimization. Our
framework can be extended to handle algorithmic recommendations under an
often-reasonable covariate-conditional exclusion restriction, using our
robustness checks for lack of positivity in the recommendation. We develop a
two-stage algorithm for solving over parametrized policy classes under general
constraints to obtain variance-sensitive regret bounds. We illustrate the
methods in two case studies based on data from randomized encouragement to
enroll in insurance and from pretrial supervised release with electronic
monitoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Angela Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data. (arXiv:2304.14541v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.14541</id>
        <link href="http://arxiv.org/abs/2304.14541"/>
        <updated>2023-09-16T00:40:56.177Z</updated>
        <summary type="html"><![CDATA[Clustering high-dimensional spatiotemporal data using an unsupervised
approach is a challenging problem for many data-driven applications. Existing
state-of-the-art methods for unsupervised clustering use different similarity
and distance functions but focus on either spatial or temporal features of the
data. Concentrating on joint deep representation learning of spatial and
temporal features, we propose Deep Spatiotemporal Clustering (DSC), a novel
algorithm for the temporal clustering of high-dimensional spatiotemporal data
using an unsupervised deep learning method. Inspired by the U-net architecture,
DSC utilizes an autoencoder integrating CNN-RNN layers to learn latent
representations of the spatiotemporal data. DSC also includes a unique layer
for cluster assignment on latent representations that uses the Student's
t-distribution. By optimizing the clustering loss and data reconstruction loss
simultaneously, the algorithm gradually improves clustering assignments and the
nonlinear mapping between low-dimensional latent feature space and
high-dimensional original data space. A multivariate spatiotemporal climate
dataset is used to evaluate the efficacy of the proposed method. Our extensive
experiments show our approach outperforms both conventional and deep
learning-based unsupervised clustering algorithms. Additionally, we compared
the proposed model with its various variants (CNN encoder, CNN autoencoder,
CNN-RNN encoder, CNN-RNN autoencoder, etc.) to get insight into using both the
CNN and RNN layers in the autoencoder, and our proposed technique outperforms
these variants in terms of clustering results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faruque_O/0/1/0/all/0/1"&gt;Omar Faruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nji_F/0/1/0/all/0/1"&gt;Francis Ndikum Nji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cham_M/0/1/0/all/0/1"&gt;Mostafa Cham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salvi_R/0/1/0/all/0/1"&gt;Rohan Mandar Salvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xue Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianwu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-term power load forecasting method based on CNN-SAEDN-Res. (arXiv:2309.07140v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07140</id>
        <link href="http://arxiv.org/abs/2309.07140"/>
        <updated>2023-09-16T00:40:55.832Z</updated>
        <summary type="html"><![CDATA[In deep learning, the load data with non-temporal factors are difficult to
process by sequence models. This problem results in insufficient precision of
the prediction. Therefore, a short-term load forecasting method based on
convolutional neural network (CNN), self-attention encoder-decoder network
(SAEDN) and residual-refinement (Res) is proposed. In this method, feature
extraction module is composed of a two-dimensional convolutional neural
network, which is used to mine the local correlation between data and obtain
high-dimensional data features. The initial load fore-casting module consists
of a self-attention encoder-decoder network and a feedforward neural network
(FFN). The module utilizes self-attention mechanisms to encode high-dimensional
features. This operation can obtain the global correlation between data.
Therefore, the model is able to retain important information based on the
coupling relationship between the data in data mixed with non-time series
factors. Then, self-attention decoding is per-formed and the feedforward neural
network is used to regression initial load. This paper introduces the residual
mechanism to build the load optimization module. The module generates residual
load values to optimize the initial load. The simulation results show that the
proposed load forecasting method has advantages in terms of prediction accuracy
and prediction stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Han Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.04953</id>
        <link href="http://arxiv.org/abs/2212.04953"/>
        <updated>2023-09-16T00:40:55.822Z</updated>
        <summary type="html"><![CDATA[Basecalling is an essential step in nanopore sequencing analysis where the
raw signals of nanopore sequencers are converted into nucleotide sequences,
i.e., reads. State-of-the-art basecallers employ complex deep learning models
to achieve high basecalling accuracy. This makes basecalling
computationally-inefficient and memory-hungry; bottlenecking the entire genome
analysis pipeline. However, for many applications, the majority of reads do no
match the reference genome of interest (i.e., target reference) and thus are
discarded in later steps in the genomics pipeline, wasting the basecalling
computation. To overcome this issue, we propose TargetCall, the first
pre-basecalling filter to eliminate the wasted computation in basecalling.
TargetCall's key idea is to discard reads that will not match the target
reference (i.e., off-target reads) prior to basecalling. TargetCall consists of
two main components: (1) LightCall, a lightweight neural network basecaller
that produces noisy reads; and (2) Similarity Check, which labels each of these
noisy reads as on-target or off-target by matching them to the target
reference. TargetCall aims to filter out all off-target reads before
basecalling. The highly-accurate but slow basecalling is performed only on the
raw signals whose noisy reads are labeled as on-target. Our thorough
experimental evaluations using both real and simulated data show that
TargetCall 1) improves the end-to-end basecalling performance while maintaining
high sensitivity in keeping on-target reads, 2) maintains high accuracy in
downstream analysis, 3) precisely filters out up to 94.71% of off-target reads,
and 4) achieves better performance, throughput, sensitivity, precision, and
generality compared to prior works. We open-source TargetCall at
https://github.com/CMU-SAFARI/TargetCall]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Cavlak_M/0/1/0/all/0/1"&gt;Meryem Banu Cavlak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Alser_M/0/1/0/all/0/1"&gt;Mohammed Alser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Firtina_C/0/1/0/all/0/1"&gt;Can Firtina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lindegger_J/0/1/0/all/0/1"&gt;Jo&amp;#xeb;l Lindegger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sadrosadati_M/0/1/0/all/0/1"&gt;Mohammad Sadrosadati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ghiasi_N/0/1/0/all/0/1"&gt;Nika Mansouri Ghiasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Alkan_C/0/1/0/all/0/1"&gt;Can Alkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mutlu_O/0/1/0/all/0/1"&gt;Onur Mutlu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.10520</id>
        <link href="http://arxiv.org/abs/2304.10520"/>
        <updated>2023-09-16T00:40:55.810Z</updated>
        <summary type="html"><![CDATA[Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),
efficiently learn a rich representation of the input. However, for adapting to
downstream tasks, they require a sufficient amount of labeled data since their
rich features code not only objects but also less relevant image background. In
contrast, Instance Discrimination (ID) methods focus on objects. In this work,
we study how to combine the efficiency and scalability of MIM with the ability
of ID to perform downstream classification in the absence of large amounts of
labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning
(MAE-CT), a sequential approach that utilizes the implicit clustering of the
Nearest Neighbor Contrastive Learning (NNCLR) objective to induce abstraction
in the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features such
that they form semantic clusters of objects without using any labels. Notably,
MAE-CT does not rely on hand-crafted augmentations and frequently achieves its
best performances while using only minimal augmentations (crop & flip).
Further, MAE-CT is compute efficient as it requires at most 10% overhead
compared to MAE re-training. Applied to large and huge Vision Transformer (ViT)
models, MAE-CT excels over previous self-supervised methods trained on ImageNet
in linear probing, k-NN and low-shot classification accuracy as well as in
unsupervised clustering accuracy. With ViT-H/16 MAE-CT achieves a new
state-of-the-art in linear probing of 82.2%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lehner_J/0/1/0/all/0/1"&gt;Johannes Lehner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alkin_B/0/1/0/all/0/1"&gt;Benedikt Alkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furst_A/0/1/0/all/0/1"&gt;Andreas F&amp;#xfc;rst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rumetshofer_E/0/1/0/all/0/1"&gt;Elisabeth Rumetshofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miklautz_L/0/1/0/all/0/1"&gt;Lukas Miklautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1"&gt;Sepp Hochreiter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All you need is spin: SU(2) equivariant variational quantum circuits based on spin networks. (arXiv:2309.07250v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.07250</id>
        <link href="http://arxiv.org/abs/2309.07250"/>
        <updated>2023-09-16T00:40:55.794Z</updated>
        <summary type="html"><![CDATA[Variational algorithms require architectures that naturally constrain the
optimisation space to run efficiently. In geometric quantum machine learning,
one achieves this by encoding group structure into parameterised quantum
circuits to include the symmetries of a problem as an inductive bias. However,
constructing such circuits is challenging as a concrete guiding principle has
yet to emerge. In this paper, we propose the use of spin networks, a form of
directed tensor network invariant under a group transformation, to devise SU(2)
equivariant quantum circuit ans\"atze -- circuits possessing spin rotation
symmetry. By changing to the basis that block diagonalises SU(2) group action,
these networks provide a natural building block for constructing parameterised
equivariant quantum circuits. We prove that our construction is mathematically
equivalent to other known constructions, such as those based on twirling and
generalised permutations, but more direct to implement on quantum hardware. The
efficacy of our constructed circuits is tested by solving the ground state
problem of SU(2) symmetric Heisenberg models on the one-dimensional triangular
lattice and on the Kagome lattice. Our results highlight that our equivariant
circuits boost the performance of quantum variational algorithms, indicating
broader applicability to other real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+East_R/0/1/0/all/0/1"&gt;Richard D. P. East&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Alonso_Linaje_G/0/1/0/all/0/1"&gt;Guillermo Alonso-Linaje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Park_C/0/1/0/all/0/1"&gt;Chae-Yeun Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression. (arXiv:2304.14131v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.14131</id>
        <link href="http://arxiv.org/abs/2304.14131"/>
        <updated>2023-09-16T00:40:55.786Z</updated>
        <summary type="html"><![CDATA[Meteorological radar reflectivity data (i.e. radar echo) significantly
influences precipitation prediction. It can facilitate accurate and expeditious
forecasting of short-term heavy rainfall bypassing the need for complex
Numerical Weather Prediction (NWP) models. In comparison to conventional
models, Deep Learning (DL)-based radar echo extrapolation algorithms exhibit
higher effectiveness and efficiency. Nevertheless, the development of reliable
and generalized echo extrapolation algorithm is impeded by three primary
challenges: cumulative error spreading, imprecise representation of sparsely
distributed echoes, and inaccurate description of non-stationary motion
processes. To tackle these challenges, this paper proposes a novel radar echo
extrapolation algorithm called Temporal-Spatial Parallel Transformer, referred
to as TempEE. TempEE avoids using auto-regression and instead employs a
one-step forward strategy to prevent cumulative error spreading during the
extrapolation process. Additionally, we propose the incorporation of a
Multi-level Temporal-Spatial Attention mechanism to improve the algorithm's
capability of capturing both global and local information while emphasizing
task-related regions, including sparse echo representations, in an efficient
manner. Furthermore, the algorithm extracts spatio-temporal representations
from continuous echo images using a parallel encoder to model the
non-stationary motion process for echo extrapolation. The superiority of our
TempEE has been demonstrated in the context of the classic radar echo
extrapolation task, utilizing a real-world dataset. Extensive experiments have
further validated the efficacy and indispensability of various components
within TempEE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shengchao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shu_T/0/1/0/all/0/1"&gt;Ting Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Huan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhong_G/0/1/0/all/0/1"&gt;Guo Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xunlai Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survival Estimation for Missing not at Random Censoring Indicators based on Copula Models. (arXiv:2009.01726v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01726</id>
        <link href="http://arxiv.org/abs/2009.01726"/>
        <updated>2023-09-16T00:40:55.687Z</updated>
        <summary type="html"><![CDATA[In the presence of right-censored data with covariates, the conditional
Kaplan-Meier estimator (also known as the Beran estimator) consistently
estimates the conditional survival function of the random follow-up for the
event of interest. However, a necessary condition is the unambiguous knowledge
of whether each individual is censored or not, which may be incomplete in
practice. We therefore propose a study of the Beran estimator when the
censoring indicators are generic random variables and discuss necessary
conditions for the efficiency of the Beran estimator. From this, we provide a
new estimator for the conditional survival function with missing not at random
(MNAR) censoring indicators based on a conditional copula model for the
missingness mechanism. In addition to the theoretical results, we illustrate
how the estimators work for small samples through a simulation study and show
their practical applicability by analyzing synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Escobar_Bach_M/0/1/0/all/0/1"&gt;Mikael Escobar-Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goudet_O/0/1/0/all/0/1"&gt;Olivier Goudet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2309.07602</id>
        <link href="http://arxiv.org/abs/2309.07602"/>
        <updated>2023-09-16T00:40:55.653Z</updated>
        <summary type="html"><![CDATA[Recently sequential recommendations and next-item prediction task has become
increasingly popular in the field of recommender systems. Currently, two
state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec.
Over the past few years, there have been quite a few publications comparing
these two algorithms and proposing new state-of-the-art models. In most of the
publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec
uses cross-entropy over softmax for all items, while SASRec uses negative
sampling and calculates binary cross-entropy loss for one positive and one
negative item. In our work, we show that if both models are trained with the
same loss, which is used by BERT4Rec, then SASRec will significantly outperform
BERT4Rec both in terms of quality and training speed. In addition, we show that
SASRec could be effectively trained with negative sampling and still outperform
BERT4Rec, but the number of negative examples should be much larger than one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klenitskiy_A/0/1/0/all/0/1"&gt;Anton Klenitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilev_A/0/1/0/all/0/1"&gt;Alexey Vasilev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Recurrence Relations using Machine Learning, with Application to Cost Analysis. (arXiv:2309.07259v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2309.07259</id>
        <link href="http://arxiv.org/abs/2309.07259"/>
        <updated>2023-09-16T00:40:55.637Z</updated>
        <summary type="html"><![CDATA[Automatic static cost analysis infers information about the resources used by
programs without actually running them with concrete data, and presents such
information as functions of input data sizes. Most of the analysis tools for
logic programs (and other languages) are based on setting up recurrence
relations representing (bounds on) the computational cost of predicates, and
solving them to find closed-form functions that are equivalent to (or a bound
on) them. Such recurrence solving is a bottleneck in current tools: many of the
recurrences that arise during the analysis cannot be solved with current
solvers, such as Computer Algebra Systems (CASs), so that specific methods for
different classes of recurrences need to be developed. We address such a
challenge by developing a novel, general approach for solving arbitrary,
constrained recurrence relations, that uses machine-learning sparse regression
techniques to guess a candidate closed-form function, and a combination of an
SMT-solver and a CAS to check whether such function is actually a solution of
the recurrence. We have implemented a prototype and evaluated it with
recurrences generated by a cost analysis system (the one in CiaoPP). The
experimental results are quite promising, showing that our approach can find
closed-form solutions, in a reasonable time, for classes of recurrences that
cannot be solved by such a system, nor by current CASs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klemen_M/0/1/0/all/0/1"&gt;Maximiliano Klemen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_Perpinan_M/0/1/0/all/0/1"&gt;Miguel &amp;#xc1;. Carreira-Perpi&amp;#xf1;&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Garcia_P/0/1/0/all/0/1"&gt;Pedro Lopez-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication Efficient Private Federated Learning Using Dithering. (arXiv:2309.07809v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07809</id>
        <link href="http://arxiv.org/abs/2309.07809"/>
        <updated>2023-09-16T00:40:55.620Z</updated>
        <summary type="html"><![CDATA[The task of preserving privacy while ensuring efficient communication is a
fundamental challenge in federated learning. In this work, we tackle this
challenge in the trusted aggregator model, and propose a solution that achieves
both objectives simultaneously. We show that employing a quantization scheme
based on subtractive dithering at the clients can effectively replicate the
normal noise addition process at the aggregator. This implies that we can
guarantee the same level of differential privacy against other clients while
substantially reducing the amount of communication required, as opposed to
transmitting full precision gradients and using central noise addition. We also
experimentally demonstrate that the accuracy of our proposed approach matches
that of the full precision gradient method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hasircioglu_B/0/1/0/all/0/1"&gt;Burak Hasircioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07364</id>
        <link href="http://arxiv.org/abs/2309.07364"/>
        <updated>2023-09-16T00:40:55.549Z</updated>
        <summary type="html"><![CDATA[Simplicial complexes prove effective in modeling data with multiway
dependencies, such as data defined along the edges of networks or within other
higher-order structures. Their spectrum can be decomposed into three
interpretable subspaces via the Hodge decomposition, resulting foundational in
numerous applications. We leverage this decomposition to develop a contrastive
self-supervised learning approach for processing simplicial data and generating
embeddings that encapsulate specific spectral information.Specifically, we
encode the pertinent data invariances through simplicial neural networks and
devise augmentations that yield positive contrastive examples with suitable
spectral properties for downstream tasks. Additionally, we reweight the
significance of negative examples in the contrastive loss, considering the
similarity of their Hodge components to the anchor. By encouraging a stronger
separation among less similar instances, we obtain an embedding space that
reflects the spectral properties of the data. The numerical results on two
standard edge flow classification tasks show a superior performance even when
compared to supervised learning techniques. Our findings underscore the
importance of adopting a spectral perspective for contrastive learning with
higher-order data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mollers_A/0/1/0/all/0/1"&gt;Alexander M&amp;#xf6;llers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1"&gt;Alexander Immer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1"&gt;Elvin Isufi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quantum Linear Solver enhanced Quantum Support Vector Machine. (arXiv:2309.07770v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.07770</id>
        <link href="http://arxiv.org/abs/2309.07770"/>
        <updated>2023-09-16T00:40:55.543Z</updated>
        <summary type="html"><![CDATA[Quantum Support Vector Machines (QSVM) play a vital role in using quantum
resources for supervised machine learning tasks, such as classification.
However, current methods are strongly limited in terms of scalability on Noisy
Intermediate Scale Quantum (NISQ) devices. In this work, we propose a novel
approach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM.
This is built upon our idea of utilizing the variational quantum linear solver
to solve system of linear equations of a least squares-SVM on a NISQ device.
The implementation of our approach is evaluated by an extensive series of
numerical experiments with the Iris dataset, which consists of three distinct
iris plant species. Based on this, we explore the practicality and
effectiveness of our algorithm by constructing a classifier capable of
classification in a feature space ranging from one to seven dimensions.
Furthermore, by strategically exploiting both classical and quantum computing
for various subroutines of our algorithm, we effectively mitigate practical
challenges associated with the implementation. These include significant
improvement in the trainability of the variational ansatz and notable
reductions in run-time for cost calculations. Based on the numerical
experiments, our approach exhibits the capability of identifying a separating
hyperplane in an 8-dimensional feature space. Moreover, it consistently
demonstrated strong performance across various instances with the same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jianming Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Suresh_K/0/1/0/all/0/1"&gt;Kalyani Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Moghiseh_A/0/1/0/all/0/1"&gt;Ali Moghiseh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wehn_N/0/1/0/all/0/1"&gt;Norbert Wehn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.11721</id>
        <link href="http://arxiv.org/abs/2308.11721"/>
        <updated>2023-09-16T00:40:55.527Z</updated>
        <summary type="html"><![CDATA[Historically, much of machine learning research has focused on the
performance of the algorithm alone, but recently more attention has been
focused on optimizing joint human-algorithm performance. Here, we analyze a
specific type of human-algorithm collaboration where the algorithm has access
to a set of $n$ items, and presents a subset of size $k$ to the human, who
selects a final item from among those $k$. This scenario could model content
recommendation, route planning, or any type of labeling task. Because both the
human and algorithm have imperfect, noisy information about the true ordering
of items, the key question is: which value of $k$ maximizes the probability
that the best item will be ultimately selected? For $k=1$, performance is
optimized by the algorithm acting alone, and for $k=n$ it is optimized by the
human acting alone. Surprisingly, we show that for multiple of noise models, it
is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to
collaborating, even when the human and algorithm have equal accuracy
separately. We demonstrate this theoretically for the Mallows model and
experimentally for the Random Utilities models of noisy permutations. However,
we show this pattern is reversed when the human is anchored on the algorithm's
presented ordering - the joint system always has strictly worse performance. We
extend these results to the case where the human and algorithm differ in their
accuracy levels, showing that there always exist regimes where a more accurate
agent would strictly benefit from collaborating with a less accurate one, but
these regimes are asymmetric between the human and the algorithm's accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_K/0/1/0/all/0/1"&gt;Kate Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gollapudi_S/0/1/0/all/0/1"&gt;Sreenivas Gollapudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollias_K/0/1/0/all/0/1"&gt;Kostas Kollias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Survival Time of Ball Bearings in the Presence of Censoring. (arXiv:2309.07188v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07188</id>
        <link href="http://arxiv.org/abs/2309.07188"/>
        <updated>2023-09-16T00:40:55.521Z</updated>
        <summary type="html"><![CDATA[Ball bearings find widespread use in various manufacturing and mechanical
domains, and methods based on machine learning have been widely adopted in the
field to monitor wear and spot defects before they lead to failures. Few
studies, however, have addressed the problem of censored data, in which failure
is not observed. In this paper, we propose a novel approach to predict the time
to failure in ball bearings using survival analysis. First, we analyze bearing
data in the frequency domain and annotate when a bearing fails by comparing the
Kullback-Leibler divergence and the standard deviation between its break-in
frequency bins and its break-out frequency bins. Second, we train several
survival models to estimate the time to failure based on the annotated data and
covariates extracted from the time domain, such as skewness, kurtosis and
entropy. The models give a probabilistic prediction of risk over time and allow
us to compare the survival function between groups of bearings. We demonstrate
our approach on the XJTU and PRONOSTIA datasets. On XJTU, the best result is a
0.70 concordance-index and 0.21 integrated Brier score. On PRONOSTIA, the best
is a 0.76 concordance-index and 0.19 integrated Brier score. Our work motivates
further work on incorporating censored data in models for predictive
maintenance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lillelund_C/0/1/0/all/0/1"&gt;Christian Marius Lillelund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pannullo_F/0/1/0/all/0/1"&gt;Fernando Pannullo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jakobsen_M/0/1/0/all/0/1"&gt;Morten Opprud Jakobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pedersen_C/0/1/0/all/0/1"&gt;Christian Fischer Pedersen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.01952</id>
        <link href="http://arxiv.org/abs/2302.01952"/>
        <updated>2023-09-16T00:40:55.516Z</updated>
        <summary type="html"><![CDATA[The recipe behind the success of deep learning has been the combination of
neural networks and gradient-based optimization. Understanding the behavior of
gradient descent however, and particularly its instability, has lagged behind
its empirical success. To add to the theoretical tools available to study
gradient descent we propose the principal flow (PF), a continuous time flow
that approximates gradient descent dynamics. To our knowledge, the PF is the
only continuous flow that captures the divergent and oscillatory behaviors of
gradient descent, including escaping local minima and saddle points. Through
its dependence on the eigendecomposition of the Hessian the PF sheds light on
the recently observed edge of stability phenomena in deep learning. Using our
new understanding of instability we propose a learning rate adaptation method
which enables us to control the trade-off between training stability and test
set evaluation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1"&gt;Mihaela Rosca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qin_C/0/1/0/all/0/1"&gt;Chongli Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dherin_B/0/1/0/all/0/1"&gt;Benoit Dherin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07200</id>
        <link href="http://arxiv.org/abs/2309.07200"/>
        <updated>2023-09-16T00:40:55.493Z</updated>
        <summary type="html"><![CDATA[Markov processes are widely used mathematical models for describing dynamic
systems in various fields. However, accurately simulating large-scale systems
at long time scales is computationally expensive due to the short time steps
required for accurate integration. In this paper, we introduce an inference
process that maps complex systems into a simplified representational space and
models large jumps in time. To achieve this, we propose Time-lagged Information
Bottleneck (T-IB), a principled objective rooted in information theory, which
aims to capture relevant temporal features while discarding high-frequency
information to simplify the simulation task and minimize the inference error.
Our experiments demonstrate that T-IB learns information-optimal
representations for accurately modeling the statistical properties and dynamics
of the original process at a selected time lag, outperforming existing
time-lagged dimensionality reduction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Federici_M/0/1/0/all/0/1"&gt;Marco Federici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1"&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomioka_R/0/1/0/all/0/1"&gt;Ryota Tomioka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1"&gt;Bastiaan S. Veeling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physically Plausible Full-Body Hand-Object Interaction Synthesis. (arXiv:2309.07907v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.07907</id>
        <link href="http://arxiv.org/abs/2309.07907"/>
        <updated>2023-09-16T00:40:55.477Z</updated>
        <summary type="html"><![CDATA[We propose a physics-based method for synthesizing dexterous hand-object
interactions in a full-body setting. While recent advancements have addressed
specific facets of human-object interactions, a comprehensive physics-based
approach remains a challenge. Existing methods often focus on isolated segments
of the interaction process and rely on data-driven techniques that may result
in artifacts. In contrast, our proposed method embraces reinforcement learning
(RL) and physics simulation to mitigate the limitations of data-driven
approaches. Through a hierarchical framework, we first learn skill priors for
both body and hand movements in a decoupled setting. The generic skill priors
learn to decode a latent skill embedding into the motion of the underlying
part. A high-level policy then controls hand-object interactions in these
pretrained latent spaces, guided by task objectives of grasping and 3D target
trajectory following. It is trained using a novel reward function that combines
an adversarial style term with a task reward, encouraging natural motions while
fulfilling the task incentives. Our method successfully accomplishes the
complete interaction task, from approaching an object to grasping and
subsequent manipulation. We compare our approach against kinematics-based
baselines and show that it leads to more physically plausible motions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braun_J/0/1/0/all/0/1"&gt;Jona Braun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1"&gt;Sammy Christen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1"&gt;Muhammed Kocabas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aksan_E/0/1/0/all/0/1"&gt;Emre Aksan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07544</id>
        <link href="http://arxiv.org/abs/2309.07544"/>
        <updated>2023-09-16T00:40:55.456Z</updated>
        <summary type="html"><![CDATA[The increasing popularity of large language models (LLMs) has paved the way
for their application in diverse domains. This paper proposes a benchmarking
framework tailored specifically for evaluating LLM performance in the context
of Verilog code generation for hardware design and verification. We present a
comprehensive evaluation dataset consisting of 156 problems from the Verilog
instructional website HDLBits. The evaluation set consists of a diverse set of
Verilog code generation tasks, ranging from simple combinational circuits to
complex finite state machines. The Verilog code completions can be
automatically tested for functional correctness by comparing the transient
simulation outputs of the generated design with a golden solution. We also
demonstrate that the Verilog code generation capability of pretrained language
models could be improved with supervised fine-tuning by bootstrapping with LLM
generated synthetic problem-code pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1"&gt;Nathaniel Pinckney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1"&gt;Brucek Khailany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Haoxing Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning and Computer Vision Techniques in Continuous Beehive Monitoring Applications: A survey. (arXiv:2208.00085v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.00085</id>
        <link href="http://arxiv.org/abs/2208.00085"/>
        <updated>2023-09-16T00:40:55.451Z</updated>
        <summary type="html"><![CDATA[Wide use and availability of the machine learning and computer vision
techniques allows development of relatively complex monitoring systems in many
domains. Besides the traditional industrial domain, new application appears
also in biology and agriculture, where we could speak about the detection of
infections, parasites and weeds, but also about automated monitoring and early
warning systems. This is also connected with the introduction of the easily
accessible hardware and development kits such as Arduino, or RaspberryPi
family. In this paper, we survey 50 existing papers focusing on the methods of
automated beehive monitoring methods using the computer vision techniques,
particularly on the pollen and Varroa mite detection together with the bee
traffic monitoring. Such systems could also be used for the monitoring of the
honeybee colonies and for the inspection of their health state, which could
identify potentially dangerous states before the situation is critical, or to
better plan periodic bee colony inspections and therefore save significant
costs. Later, we also include analysis of the research trends in this
application field and we outline the possible direction of the new
explorations. Our paper is aimed also at veterinary and apidology professionals
and experts, who might not be familiar with machine learning to introduce them
to its possibilities, therefore each family of applications is opened by a
brief theoretical introduction and motivation related to its base method. We
hope that this paper will inspire other scientists to use machine learning
techniques for other applications in beehive monitoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1"&gt;Simon Bilik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemcik_T/0/1/0/all/0/1"&gt;Tomas Zemcik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kratochvila_L/0/1/0/all/0/1"&gt;Lukas Kratochvila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricanek_D/0/1/0/all/0/1"&gt;Dominik Ricanek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1"&gt;Milos Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zambanini_S/0/1/0/all/0/1"&gt;Sebastian Zambanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1"&gt;Karel Horak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Conditional Moment Constraints for Confounding Robust Inference. (arXiv:2302.13348v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.13348</id>
        <link href="http://arxiv.org/abs/2302.13348"/>
        <updated>2023-09-16T00:40:55.445Z</updated>
        <summary type="html"><![CDATA[We study policy evaluation of offline contextual bandits subject to
unobserved confounders. Sensitivity analysis methods are commonly used to
estimate the policy value under the worst-case confounding over a given
uncertainty set. However, existing work often resorts to some coarse relaxation
of the uncertainty set for the sake of tractability, leading to overly
conservative estimation of the policy value. In this paper, we propose a
general estimator that provides a sharp lower bound of the policy value. It can
be shown that our estimator contains the recently proposed sharp estimator by
Dorn and Guo (2022) as a special case, and our method enables a novel extension
of the classical marginal sensitivity model using f-divergence. To construct
our estimator, we leverage the kernel method to obtain a tractable
approximation to the conditional moment constraints, which traditional
non-sharp estimators failed to take into account. In the theoretical analysis,
we provide a condition for the choice of the kernel which guarantees no
specification error that biases the lower bound estimation. Furthermore, we
provide consistency guarantees of policy evaluation and learning. In the
experiments with synthetic and real-world data, we demonstrate the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ishikawa_K/0/1/0/all/0/1"&gt;Kei Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07207</id>
        <link href="http://arxiv.org/abs/2309.07207"/>
        <updated>2023-09-16T00:40:55.429Z</updated>
        <summary type="html"><![CDATA[We introduce EarthPT -- an Earth Observation (EO) pretrained transformer.
EarthPT is a 700 million parameter decoding transformer foundation model
trained in an autoregressive self-supervised manner and developed specifically
with EO use-cases in mind. We demonstrate that EarthPT is an effective
forecaster that can accurately predict future pixel-level surface reflectances
across the 400-2300 nm range well into the future. For example, forecasts of
the evolution of the Normalised Difference Vegetation Index (NDVI) have a
typical error of approximately 0.05 (over a natural range of -1 -> 1) at the
pixel level over a five month test set horizon, out-performing simple
phase-folded models based on historical averaging. We also demonstrate that
embeddings learnt by EarthPT hold semantically meaningful information and could
be exploited for downstream tasks such as highly granular, dynamic land use
classification. Excitingly, we note that the abundance of EO data provides us
with -- in theory -- quadrillions of training tokens. Therefore, if we assume
that EarthPT follows neural scaling laws akin to those derived for Large
Language Models (LLMs), there is currently no data-imposed limit to scaling
EarthPT and other similar `Large Observation Models.']]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1"&gt;Michael J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleming_L/0/1/0/all/0/1"&gt;Luke Fleming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geach_J/0/1/0/all/0/1"&gt;James E. Geach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation. (arXiv:2309.07609v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.07609</id>
        <link href="http://arxiv.org/abs/2309.07609"/>
        <updated>2023-09-16T00:40:55.424Z</updated>
        <summary type="html"><![CDATA[The robotic manipulation of Deformable Linear Objects (DLOs) is a vital and
challenging task that is important in many practical applications. Classical
model-based approaches to this problem require an accurate model to capture how
robot motions affect the deformation of the DLO. Nowadays, data-driven models
offer the best tradeoff between quality and computation time. This paper
analyzes several learning-based 3D models of the DLO and proposes a new one
based on the Transformer architecture that achieves superior accuracy, even on
the DLOs of different lengths, thanks to the proposed scaling method. Moreover,
we introduce a data augmentation technique, which improves the prediction
performance of almost all considered DLO data-driven models. Thanks to this
technique, even a simple Multilayer Perceptron (MLP) achieves close to
state-of-the-art performance while being significantly faster to evaluate. In
the experiments, we compare the performance of the learning-based 3D models of
the DLO on several challenging datasets quantitatively and demonstrate their
applicability in the task of shaping a DLO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kicki_P/0/1/0/all/0/1"&gt;Piotr Kicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidzinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Bidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walas_K/0/1/0/all/0/1"&gt;Krzysztof Walas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning. (arXiv:2202.03609v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.03609</id>
        <link href="http://arxiv.org/abs/2202.03609"/>
        <updated>2023-09-16T00:40:55.418Z</updated>
        <summary type="html"><![CDATA[While real-world applications of reinforcement learning are becoming popular,
the security and robustness of RL systems are worthy of more attention and
exploration. In particular, recent works have revealed that, in a multi-agent
RL environment, backdoor trigger actions can be injected into a victim agent
(a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it
sees the backdoor trigger action. To ensure the security of RL agents against
malicious backdoors, in this work, we propose the problem of Backdoor Detection
in a multi-agent competitive reinforcement learning system, with the objective
of detecting Trojan agents as well as the corresponding potential trigger
actions, and further trying to mitigate their Trojan behavior. In order to
solve this problem, we propose PolicyCleanse that is based on the property that
the activated Trojan agents accumulated rewards degrade noticeably after
several timesteps. Along with PolicyCleanse, we also design a machine
unlearning-based approach that can effectively mitigate the detected backdoor.
Extensive experiments demonstrate that the proposed methods can accurately
detect Trojan agents, and outperform existing backdoor mitigation baseline
approaches by at least 3% in winning rate across various types of agents and
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Junfeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Cong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization. (arXiv:2309.07189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07189</id>
        <link href="http://arxiv.org/abs/2309.07189"/>
        <updated>2023-09-16T00:40:55.406Z</updated>
        <summary type="html"><![CDATA[Federated learning algorithms perform reasonably well on independent and
identically distributed (IID) data. They, on the other hand, suffer greatly
from heterogeneous environments, i.e., Non-IID data. Despite the fact that many
research projects have been done to address this issue, recent findings
indicate that they are still sub-optimal when compared to training on IID data.
In this work, we carefully analyze the existing methods in heterogeneous
environments. Interestingly, we find that regularizing the classifier's outputs
is quite effective in preventing performance degradation on Non-IID data.
Motivated by this, we propose Learning from Drift (LfD), a novel method for
effectively training the model in heterogeneous settings. Our scheme
encapsulates two key components: drift estimation and drift regularization.
Specifically, LfD first estimates how different the local model is from the
global model (i.e., drift). The local model is then regularized such that it
does not fall in the direction of the estimated drift. In the experiment, we
evaluate each method through the lens of the five aspects of federated
learning, i.e., Generalization, Heterogeneity, Scalability, Forgetting, and
Efficiency. Comprehensive evaluation results clearly support the superiority of
LfD in federated learning with Non-IID data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yeachan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_B/0/1/0/all/0/1"&gt;Bonggun Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech. (arXiv:2110.12539v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.12539</id>
        <link href="http://arxiv.org/abs/2110.12539"/>
        <updated>2023-09-16T00:40:55.390Z</updated>
        <summary type="html"><![CDATA[We present a Split Vector Quantized Variational Autoencoder (SVQ-VAE)
architecture using a split vector quantizer for NTTS, as an enhancement to the
well-known Variational Autoencoder (VAE) and Vector Quantized Variational
Autoencoder (VQ-VAE) architectures. Compared to these previous architectures,
our proposed model retains the benefits of using an utterance-level bottleneck,
while keeping significant representation power and a discretized latent space
small enough for efficient prediction from text. We train the model on
recordings in the expressive task-oriented dialogues domain and show that
SVQ-VAE achieves a statistically significant improvement in naturalness over
the VAE and VQ-VAE models. Furthermore, we demonstrate that the SVQ-VAE latent
acoustic space is predictable from text, reducing the gap between the standard
constant vector synthesis and vocoded recordings by 32%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Strong_M/0/1/0/all/0/1"&gt;Marek Strong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohnke_J/0/1/0/all/0/1"&gt;Jonas Rohnke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonafonte_A/0/1/0/all/0/1"&gt;Antonio Bonafonte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lajszczak_M/0/1/0/all/0/1"&gt;Mateusz &amp;#x141;ajszczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_T/0/1/0/all/0/1"&gt;Trevor Wood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using wearable device-based machine learning models to autonomously identify older adults with poor cognition. (arXiv:2309.07133v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.07133</id>
        <link href="http://arxiv.org/abs/2309.07133"/>
        <updated>2023-09-16T00:40:55.379Z</updated>
        <summary type="html"><![CDATA[Conducting cognitive tests is time-consuming for patients and clinicians.
Wearable device-based prediction models allow for continuous health monitoring
under normal living conditions and could offer an alternative to identifying
older adults with cognitive impairments for early interventions. In this study,
we first derived novel wearable-based features related to circadian rhythms,
ambient light exposure, physical activity levels, sleep, and signal processing.
Then, we quantified the ability of wearable-based machine-learning models to
predict poor cognition based on outcomes from the Digit Symbol Substitution
Test (DSST), the Consortium to Establish a Registry for Alzheimers Disease
Word-Learning subtest (CERAD-WL), and the Animal Fluency Test (AFT). We found
that the wearable-based models had significantly higher AUCs when predicting
all three cognitive outcomes compared to benchmark models containing age, sex,
education, marital status, household income, diabetic status, depression
symptoms, and functional independence scores. In addition to uncovering
previously unidentified wearable-based features that are predictive of poor
cognition such as the standard deviation of the midpoints of each persons most
active 10-hour periods and least active 5-hour periods, our paper provides
proof-of-concept that wearable-based machine learning models can be used to
autonomously screen older adults for possible cognitive impairments. Such
models offer cost-effective alternatives to conducting initial screenings
manually in clinical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sakal_C/0/1/0/all/0/1"&gt;Collin Sakal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Tingyou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Juan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyue Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids. (arXiv:2303.08447v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.08447</id>
        <link href="http://arxiv.org/abs/2303.08447"/>
        <updated>2023-09-16T00:40:55.372Z</updated>
        <summary type="html"><![CDATA[Integrating variable renewable energy into the grid has posed challenges to
system operators in achieving optimal trade-offs among energy availability,
cost affordability, and pollution controllability. This paper proposes a
multi-agent reinforcement learning framework for managing energy transactions
in microgrids. The framework addresses the challenges above: it seeks to
optimize the usage of available resources by minimizing the carbon footprint
while benefiting all stakeholders. The proposed architecture consists of three
layers of agents, each pursuing different objectives. The first layer,
comprised of prosumers and consumers, minimizes the total energy cost. The
other two layers control the energy price to decrease the carbon impact while
balancing the consumption and production of both renewable and conventional
energy. This framework also takes into account fluctuations in energy demand
and supply.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cuadrado_N/0/1/0/all/0/1"&gt;Nicolas Cuadrado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1"&gt;Roberto Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yongli Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1"&gt;Martin Takac&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.04688</id>
        <link href="http://arxiv.org/abs/2210.04688"/>
        <updated>2023-09-16T00:40:55.367Z</updated>
        <summary type="html"><![CDATA[A growing body of research has focused on the Reinforcement Learning (RL)
methods which allow the agent to learn from trial-and-error experiences
gathered during the interaction with the environment. Recently, offline RL
becomes a popular RL paradigm because it saves the interactions with
environments. In offline RL, data providers share large pre-collected datasets,
and others can train high-quality agents without interacting with the
environments. This paradigm has demonstrated effectiveness in critical tasks
like robot control, autonomous driving, etc. However, less attention is paid to
investigating the security threats to the offline RL system. This paper focuses
on backdoor attacks, where some perturbations are added to the data
(observations) such that given normal observations, the agent takes
high-rewards actions, and low-reward actions on observations injected with
triggers. In this paper, we propose Baffle (Backdoor Attack for Offline
Reinforcement Learning), an approach that automatically implants backdoors to
RL agents by poisoning the offline RL dataset, and evaluate how different
offline RL algorithms react to this attack. Our experiments conducted on four
tasks and four offline RL algorithms expose a disquieting fact: none of the
existing offline RL algorithms is immune to such a backdoor attack. Baffle
modifies $10\%$ of the datasets for four tasks. Agents trained on the poisoned
datasets perform well in normal settings. However, when triggers are presented,
the agents' performance decreases drastically by $63.2\%$, $53.9\%$, $64.7\%$,
and $47.4\%$ in the four tasks on average. The backdoor still persists after
fine-tuning poisoned agents on clean datasets. We further show that the
inserted backdoor is also hard to be detected by a popular defensive method.
This paper calls attention to developing more effective protection for the
open-source offline RL dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhou Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yunpeng Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Junda He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jieke Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kecen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Arunesh Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bowen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1"&gt;Xinwen Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1"&gt;David Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Process Surrogate Models for Neural Networks. (arXiv:2208.06028v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.06028</id>
        <link href="http://arxiv.org/abs/2208.06028"/>
        <updated>2023-09-16T00:40:55.335Z</updated>
        <summary type="html"><![CDATA[Not being able to understand and predict the behavior of deep learning
systems makes it hard to decide what architecture and algorithm to use for a
given problem. In science and engineering, modeling is a methodology used to
understand complex systems whose internal processes are opaque. Modeling
replaces a complex system with a simpler, more interpretable surrogate. Drawing
inspiration from this, we construct a class of surrogate models for neural
networks using Gaussian processes. Rather than deriving kernels for infinite
neural networks, we learn kernels empirically from the naturalistic behavior of
finite neural networks. We demonstrate our approach captures existing phenomena
related to the spectral bias of neural networks, and then show that our
surrogate models can be used to solve practical problems such as identifying
which points most influence the behavior of specific neural networks and
predicting which architectures and algorithms will generalize well for specific
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Michael Y. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1"&gt;Erin Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some notes concerning a generalized KMM-type optimization method for density ratio estimation. (arXiv:2309.07887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07887</id>
        <link href="http://arxiv.org/abs/2309.07887"/>
        <updated>2023-09-16T00:40:55.324Z</updated>
        <summary type="html"><![CDATA[In the present paper we introduce new optimization algorithms for the task of
density ratio estimation. More precisely, we consider extending the well-known
KMM method using the construction of a suitable loss function, in order to
encompass more general situations involving the estimation of density ratio
with respect to subsets of the training data and test data, respectively. The
associated codes can be found at https://github.com/CDAlecsa/Generalized-KMM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alecsa_C/0/1/0/all/0/1"&gt;Cristian Daniel Alecsa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting sampling biases via importance reweighting for spatial modeling. (arXiv:2309.04824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.04824</id>
        <link href="http://arxiv.org/abs/2309.04824"/>
        <updated>2023-09-16T00:40:55.297Z</updated>
        <summary type="html"><![CDATA[In machine learning models, the estimation of errors is often complex due to
distribution bias, particularly in spatial data such as those found in
environmental studies. We introduce an approach based on the ideas of
importance sampling to obtain an unbiased estimate of the target error. By
taking into account difference between desirable error and available data, our
method reweights errors at each sample point and neutralizes the shift.
Importance sampling technique and kernel density estimation were used for
reweighteing. We validate the effectiveness of our approach using artificial
data that resemble real-world spatial datasets. Our findings demonstrate
advantages of the proposed approach for the estimation of the target error,
offering a solution to a distribution shift problem. Overall error of
predictions dropped from 7% to just 2% and it gets smaller for larger samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorov_B/0/1/0/all/0/1"&gt;Boris Prokhorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koldasbayeva_D/0/1/0/all/0/1"&gt;Diana Koldasbayeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization for Crop Segmentation with Knowledge Distillation. (arXiv:2304.01029v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.01029</id>
        <link href="http://arxiv.org/abs/2304.01029"/>
        <updated>2023-09-16T00:40:55.210Z</updated>
        <summary type="html"><![CDATA[In recent years, precision agriculture has gradually oriented farming closer
to automation processes to support all the activities related to field
management. Service robotics plays a predominant role in this evolution by
deploying autonomous agents that can navigate fields while performing tasks
without human intervention, such as monitoring, spraying, and harvesting. To
execute these precise actions, mobile robots need a real-time perception system
that understands their surroundings and identifies their targets in the wild.
Generalizing to new crops and environmental conditions is critical for
practical applications, as labeled samples are rarely available. In this paper,
we investigate the problem of crop segmentation and propose a novel approach to
enhance domain generalization using knowledge distillation. In the proposed
framework, we transfer knowledge from an ensemble of models individually
trained on source domains to a student model that can adapt to unseen target
domains. To evaluate the proposed method, we present a synthetic multi-domain
dataset for crop segmentation containing plants of variegate shapes and
covering different terrain styles, weather conditions, and light scenarios for
more than 50,000 samples. We demonstrate significant improvements in
performance over state-of-the-art methods and superior sim-to-real
generalization. Our approach provides a promising solution for domain
generalization in crop segmentation and has the potential to enhance a wide
variety of precision agriculture applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1"&gt;Simone Angarano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1"&gt;Mauro Martini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navone_A/0/1/0/all/0/1"&gt;Alessandro Navone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1"&gt;Marcello Chiaberge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive approximation of monotone functions. (arXiv:2309.07530v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07530</id>
        <link href="http://arxiv.org/abs/2309.07530"/>
        <updated>2023-09-16T00:40:55.202Z</updated>
        <summary type="html"><![CDATA[We study the classical problem of approximating a non-decreasing function $f:
\mathcal{X} \to \mathcal{Y}$ in $L^p(\mu)$ norm by sequentially querying its
values, for known compact real intervals $\mathcal{X}$, $\mathcal{Y}$ and a
known probability measure $\mu$ on $\cX$. For any function~$f$ we characterize
the minimum number of evaluations of $f$ that algorithms need to guarantee an
approximation $\hat{f}$ with an $L^p(\mu)$ error below $\epsilon$ after
stopping. Unlike worst-case results that hold uniformly over all $f$, our
complexity measure is dependent on each specific function $f$. To address this
problem, we introduce GreedyBox, a generalization of an algorithm originally
proposed by Novak (1992) for numerical integration. We prove that GreedyBox
achieves an optimal sample complexity for any function $f$, up to logarithmic
factors. Additionally, we uncover results regarding piecewise-smooth functions.
Perhaps as expected, the $L^p(\mu)$ error of GreedyBox decreases much faster
for piecewise-$C^2$ functions than predicted by the algorithm (without any
knowledge on the smoothness of $f$). A simple modification even achieves
optimal minimax approximation rates for such functions, which we compute
explicitly. In particular, our findings highlight multiple performance gaps
between adaptive and non-adaptive algorithms, smooth and piecewise-smooth
functions, as well as monotone or non-monotone functions. Finally, we provide
numerical experiments to support our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaillard_P/0/1/0/all/0/1"&gt;Pierre Gaillard&lt;/a&gt; (Thoth), &lt;a href="http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Gerchinovitz&lt;/a&gt; (IMT), &lt;a href="http://arxiv.org/find/cs/1/au:+Montbrun_E/0/1/0/all/0/1"&gt;&amp;#xc9;tienne de Montbrun&lt;/a&gt; (TSE-R)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.05845</id>
        <link href="http://arxiv.org/abs/2210.05845"/>
        <updated>2023-09-16T00:40:55.196Z</updated>
        <summary type="html"><![CDATA[In real life, success is often contingent upon multiple critical steps that
are distant in time from each other and from the final reward. These critical
steps are challenging to identify with traditional reinforcement learning (RL)
methods that rely on the Bellman equation for credit assignment. Here, we
present a new RL algorithm that uses offline contrastive learning to hone in on
critical steps. This algorithm, which we call contrastive introspection
(ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of
prototypes for the critical steps in a task by a novel contrastive loss and
delivers an intrinsic reward when the current state matches one of these
prototypes. The prototypes in ConSpec provide two key benefits for credit
assignment: (1) They enable rapid identification of all the critical steps. (2)
They do so in a readily interpretable manner, enabling out-of-distribution
generalization when sensory features are altered. Distinct from other
contemporary RL approaches to credit assignment, ConSpec takes advantage of the
fact that it is easier to retrospectively identify the small set of steps that
success is contingent upon than it is to prospectively predict reward at every
step taken in the environment. Altogether, ConSpec improves learning in a
diverse set of RL tasks, including both those with explicit, discrete critical
steps and those with complex, continuous critical steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wannan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiralerspong_T/0/1/0/all/0/1"&gt;Thomas Jiralerspong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malenfant_D/0/1/0/all/0/1"&gt;Dane Malenfant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsbury_Nealy_B/0/1/0/all/0/1"&gt;Benjamin Alsbury-Nealy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1"&gt;Blake Richards&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time. (arXiv:2309.07418v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2309.07418</id>
        <link href="http://arxiv.org/abs/2309.07418"/>
        <updated>2023-09-16T00:40:55.191Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) have played a pivotal role in revolutionizing
various facets of our daily existence. Solving attention regression is a
fundamental task in optimizing LLMs. In this work, we focus on giving a
provable guarantee for the one-layer attention network objective function
$L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp(
\mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3}
Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$. Here $\mathsf{A} \in \mathbb{R}^{n^2
\times d^2}$ is Kronecker product between $A_1 \in \mathbb{R}^{n \times d}$ and
$A_2 \in \mathbb{R}^{n \times d}$. $A_3$ is a matrix in $\mathbb{R}^{n \times
d}$, $\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ is the $j_0$-th block of
$\mathsf{A}$. The $X, Y \in \mathbb{R}^{d \times d}$ are variables we want to
learn. $B \in \mathbb{R}^{n \times d}$ and $b_{j_0,i_0} \in \mathbb{R}$ is one
entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \in \mathbb{R}^d$
is the $i_0$-column vector of $Y$, and $x \in \mathbb{R}^{d^2}$ is the
vectorization of $X$.

In a multi-layer LLM network, the matrix $B \in \mathbb{R}^{n \times d}$ can
be viewed as the output of a layer, and $A_1= A_2 = A_3 \in \mathbb{R}^{n
\times d}$ can be viewed as the input of a layer. The matrix version of $x$ can
be viewed as $QK^\top$ and $Y$ can be viewed as $V$. We provide an iterative
greedy algorithm to train loss function $L(X,Y)$ up $\epsilon$ that runs in
$\widetilde{O}( ({\cal T}_{\mathrm{mat}}(n,n,d) + {\cal
T}_{\mathrm{mat}}(n,d,d) + d^{2\omega}) \log(1/\epsilon) )$ time. Here ${\cal
T}_{\mathrm{mat}}(a,b,c)$ denotes the time of multiplying $a \times b$ matrix
another $b \times c$ matrix, and $\omega\approx 2.37$ denotes the exponent of
matrix multiplication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yeqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Junze Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07174</id>
        <link href="http://arxiv.org/abs/2309.07174"/>
        <updated>2023-09-16T00:40:55.145Z</updated>
        <summary type="html"><![CDATA[Hurricanes present major challenges in the U.S. due to their devastating
impacts. Mitigating these risks is important, and the insurance industry is
central in this effort, using intricate statistical models for risk assessment.
However, these models often neglect key temporal and spatial hurricane patterns
and are limited by data scarcity. This study introduces a refined approach
combining the ARIMA model and K-MEANS to better capture hurricane trends, and
an Autoencoder for enhanced hurricane simulations. Our experiments show that
this hybrid methodology effectively simulate historical hurricane behaviors
while providing detailed projections of potential future trajectories and
intensities. Moreover, by leveraging a comprehensive yet selective dataset, our
simulations enrich the current understanding of hurricane patterns and offer
actionable insights for risk management strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shouwei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Meiyan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuepeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Wenqian Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2309.07367</id>
        <link href="http://arxiv.org/abs/2309.07367"/>
        <updated>2023-09-16T00:40:55.118Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have shown many fruitful applications in this decade. A
network can get the generalized function through training with a finite
dataset. The degree of generalization is a realization of the proximity scale
in the data space. Specifically, the scale is not clear if the dataset is
complicated. Here we consider a network for the distribution estimation of the
dataset. We show the estimation is unstable and the instability depends on the
data density and training duration. We derive the kernel-balanced equation,
which gives a short phenomenological description of the solution. The equation
tells us the reason for the instability and the mechanism of the scale. The
network outputs a local average of the dataset as a prediction and the scale of
averaging is determined along the equation. The scale gradually decreases along
training and finally results in instability in our case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Nakazato_K/0/1/0/all/0/1"&gt;Kenichi Nakazato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.00305</id>
        <link href="http://arxiv.org/abs/2210.00305"/>
        <updated>2023-09-16T00:40:55.109Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
this http URL and long-term maintenance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhoubo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1"&gt;Zekun Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free Learning of Regions of Attraction via Recurrent Sets. (arXiv:2204.10372v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.10372</id>
        <link href="http://arxiv.org/abs/2204.10372"/>
        <updated>2023-09-16T00:40:55.094Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning an inner approximation of the region of
attraction (ROA) of an asymptotically stable equilibrium point without an
explicit model of the dynamics. Rather than leveraging approximate models with
bounded uncertainty to find a (robust) invariant set contained in the ROA, we
propose to learn sets that satisfy a more relaxed notion of containment known
as recurrence. We define a set to be $\tau$-recurrent (resp. $k$-recurrent) if
every trajectory that starts within the set, returns to it after at most $\tau$
seconds (resp. $k$ steps). We show that under mild assumptions a
$\tau$-recurrent set containing a stable equilibrium must be a subset of its
ROA. We then leverage this property to develop algorithms that compute inner
approximations of the ROA using counter-examples of recurrence that are
obtained by sampling finite-length trajectories. Our algorithms process samples
sequentially, which allow them to continue being executed even after an initial
offline training stage. We further provide an upper bound on the number of
counter-examples used by the algorithm, and almost sure convergence guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yue Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bichuch_M/0/1/0/all/0/1"&gt;Maxim Bichuch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallada_E/0/1/0/all/0/1"&gt;Enrique Mallada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.07200</id>
        <link href="http://arxiv.org/abs/2308.07200"/>
        <updated>2023-09-16T00:40:55.089Z</updated>
        <summary type="html"><![CDATA[Recent advances in learning reusable motion priors have demonstrated their
effectiveness in generating naturalistic behaviors. In this paper, we propose a
new learning framework in this paradigm for controlling physics-based
characters with significantly improved motion quality and diversity over
existing state-of-the-art methods. The proposed method uses reinforcement
learning (RL) to initially track and imitate life-like movements from
unstructured motion clips using the discrete information bottleneck, as adopted
in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure
compresses the most relevant information from the motion clips into a compact
yet informative latent space, i.e., a discrete space over vector quantized
codes. By sampling codes in the space from a trained categorical prior
distribution, high-quality life-like behaviors can be generated, similar to the
usage of VQ-VAE in computer vision. Although this prior distribution can be
trained with the supervision of the encoder's output, it follows the original
motion clip distribution in the dataset and could lead to imbalanced behaviors
in our setting. To address the issue, we further propose a technique named
prior shifting to adjust the prior distribution using curiosity-driven RL. The
outcome distribution is demonstrated to offer sufficient behavioral diversity
and significantly facilitates upper-level policy learning for downstream tasks.
We conduct comprehensive experiments using humanoid characters on two
challenging downstream tasks, sword-shield striking and two-player boxing game.
Our results demonstrate that the proposed framework is capable of controlling
the character to perform considerably high-quality movements in terms of
behavioral strategies, diversity, and realism. Videos, codes, and data are
available at https://tencent-roboticsx.github.io/NCP/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qingxu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;He Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1"&gt;Mengting Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Lei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07610</id>
        <link href="http://arxiv.org/abs/2309.07610"/>
        <updated>2023-09-16T00:40:55.084Z</updated>
        <summary type="html"><![CDATA[Community question answering (CQA) forums are Internet-based platforms where
users ask questions about a topic and other expert users try to provide
solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer,
StackExchange exist with a lot of user-generated data. These data are leveraged
in automated CQA ranking systems where similar questions (and answers) are
presented in response to the query of the user. In this work, we empirically
investigate a few aspects of this domain. Firstly, in addition to traditional
features like TF-IDF, BM25 etc., we introduce a BERT-based feature that
captures the semantic similarity between the question and answer. Secondly,
most of the existing research works have focused on features extracted only
from the question part; features extracted from answers have not been explored
extensively. We combine both types of features in a linear fashion. Thirdly,
using our proposed concepts, we conduct an empirical investigation with
different rank-learning algorithms, some of which have not been used so far in
CQA domain. On three standard CQA datasets, our proposed framework achieves
state-of-the-art performance. We also analyze importance of the features we use
in our investigation. This work is expected to guide the practitioners to
select a better set of features for the CQA retrieval task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sajid_N/0/1/0/all/0/1"&gt;Nafis Sajid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Md Rashidul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1"&gt;Muhammad Ibrahim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Unsupervised and Supervised Learning and Digital Twin for Deep Convective Ice Storm Classification. (arXiv:2309.07173v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07173</id>
        <link href="http://arxiv.org/abs/2309.07173"/>
        <updated>2023-09-16T00:40:55.071Z</updated>
        <summary type="html"><![CDATA[Smart Ice Cloud Sensing (SMICES) is a small-sat concept in which a primary
radar intelligently targets ice storms based on information collected by a
lookahead radiometer. Critical to the intelligent targeting is accurate
identification of storm/cloud types from eight bands of radiance collected by
the radiometer. The cloud types of interest are: clear sky, thin cirrus,
cirrus, rainy anvil, and convection core.

We describe multi-step use of Machine Learning and Digital Twin of the
Earth's atmosphere to derive such a classifier. First, a digital twin of
Earth's atmosphere called a Weather Research Forecast (WRF) is used generate
simulated lookahead radiometer data as well as deeper "science" hidden
variables. The datasets simulate a tropical region over the Caribbean and a
non-tropical region over the Atlantic coast of the United States. A K-means
clustering over the scientific hidden variables was utilized by human experts
to generate an automatic labelling of the data - mapping each physical data
point to cloud types by scientists informed by mean/centroids of hidden
variables of the clusters. Next, classifiers were trained with the inputs of
the simulated radiometer data and its corresponding label. The classifiers of a
random decision forest (RDF), support vector machine (SVM), Gaussian na\"ive
bayes, feed forward artificial neural network (ANN), and a convolutional neural
network (CNN) were trained. Over the tropical dataset, the best performing
classifier was able to identify non-storm and storm clouds with over 80%
accuracy in each class for a held-out test set. Over the non-tropical dataset,
the best performing classifier was able to classify non-storm clouds with over
90% accuracy and storm clouds with over 40% accuracy. Additionally both sets of
classifiers were shown to be resilient to instrument noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Swope_J/0/1/0/all/0/1"&gt;Jason Swope&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1"&gt;Steve Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunkel_E/0/1/0/all/0/1"&gt;Emily Dunkel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosch_Lluis_X/0/1/0/all/0/1"&gt;Xavier Bosch-Lluis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Q/0/1/0/all/0/1"&gt;Qing Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deal_W/0/1/0/all/0/1"&gt;William Deal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07168</id>
        <link href="http://arxiv.org/abs/2309.07168"/>
        <updated>2023-09-16T00:40:55.061Z</updated>
        <summary type="html"><![CDATA[Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this work, we propose a
developmental mechanism for subgoal discovery via an emergent representation
that abstracts (i.e., groups together) sets of environment states that have
similar roles in the task. We create a HRL algorithm that gradually learns this
representation along with the policies and evaluate it on navigation tasks to
show the learned representation is interpretable and results in data
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zadem_M/0/1/0/all/0/1"&gt;Mehdi Zadem&lt;/a&gt; (LIX, U2IS), &lt;a href="http://arxiv.org/find/cs/1/au:+Mover_S/0/1/0/all/0/1"&gt;Sergio Mover&lt;/a&gt; (LIX), &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1"&gt;Sao Mai Nguyen&lt;/a&gt; (U2IS, Flowers, IMT Atlantique - INFO, Lab-STICC_RAMBO)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis. (arXiv:2309.07178v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2309.07178</id>
        <link href="http://arxiv.org/abs/2309.07178"/>
        <updated>2023-09-16T00:40:55.033Z</updated>
        <summary type="html"><![CDATA[Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful
analytical tool for studying molecular structure and dynamics in chemistry and
biology. However, the processing of raw data acquired from NMR spectrometers
and subsequent quantitative analysis involves various specialized tools, which
necessitates comprehensive knowledge in programming and NMR. Particularly, the
emerging deep learning tools is hard to be widely used in NMR due to the
sophisticated setup of computation. Thus, NMR processing is not an easy task
for chemist and biologists. In this work, we present CloudBrain-NMR, an
intelligent online cloud computing platform designed for NMR data reading,
processing, reconstruction, and quantitative analysis. The platform is
conveniently accessed through a web browser, eliminating the need for any
program installation on the user side. CloudBrain-NMR uses parallel computing
with graphics processing units and central processing units, resulting in
significantly shortened computation time. Furthermore, it incorporates
state-of-the-art deep learning-based algorithms offering comprehensive
functionalities that allow users to complete the entire processing procedure
without relying on additional software. This platform has empowered NMR
applications with advanced artificial intelligence processing. CloudBrain-NMR
is openly accessible for free usage at https://csrc.xmu.edu.cn/CloudBrain.html]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Guo_D/0/1/0/all/0/1"&gt;Di Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1"&gt;Sijin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhangren Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qiu_T/0/1/0/all/0/1"&gt;Tianyu Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jingjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Feng_L/0/1/0/all/0/1"&gt;Liubin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lin_D/0/1/0/all/0/1"&gt;Donghai Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hong_Q/0/1/0/all/0/1"&gt;Qing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lin_M/0/1/0/all/0/1"&gt;Meijin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yanqin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiaobo Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07708</id>
        <link href="http://arxiv.org/abs/2309.07708"/>
        <updated>2023-09-16T00:40:55.028Z</updated>
        <summary type="html"><![CDATA[Financial simulators play an important role in enhancing forecasting
accuracy, managing risks, and fostering strategic financial decision-making.
Despite the development of financial market simulation methodologies, existing
frameworks often struggle with adapting to specialized simulation context. We
pinpoint the challenges as i) current financial datasets do not contain context
labels; ii) current techniques are not designed to generate financial data with
context as control, which demands greater precision compared to other
modalities; iii) the inherent difficulties in generating context-aligned,
high-fidelity data given the non-stationary, noisy nature of financial data. To
address these challenges, our contributions are: i) we proposed the Contextual
Market Dataset with market dynamics, stock ticker, and history state as
context, leveraging a market dynamics modeling method that combines linear
regression and Dynamic Time Warping clustering to extract market dynamics; ii)
we present Market-GAN, a novel architecture incorporating a Generative
Adversarial Networks (GAN) for the controllable generation with context, an
autoencoder for learning low-dimension features, and supervisors for knowledge
transfer; iii) we introduce a two-stage training scheme to ensure that
Market-GAN captures the intrinsic market distribution with multiple objectives.
In the pertaining stage, with the use of the autoencoder and supervisors, we
prepare the generator with a better initialization for the adversarial training
stage. We propose a set of holistic evaluation metrics that consider alignment,
fidelity, data usability on downstream tasks, and market facts. We evaluate
Market-GAN with the Dow Jones Industrial Average data from 2000 to 2023 and
showcase superior performance in comparison to 4 state-of-the-art time-series
generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1"&gt;Haochong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shuo Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bo An&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.07315</id>
        <link href="http://arxiv.org/abs/2309.07315"/>
        <updated>2023-09-16T00:40:55.021Z</updated>
        <summary type="html"><![CDATA[Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Molina_R/0/1/0/all/0/1"&gt;Raul Molina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07332</id>
        <link href="http://arxiv.org/abs/2309.07332"/>
        <updated>2023-09-16T00:40:54.840Z</updated>
        <summary type="html"><![CDATA[Accurately labeling biomedical data presents a challenge. Traditional
semi-supervised learning methods often under-utilize available unlabeled data.
To address this, we propose a novel reliability-based training data cleaning
method employing inductive conformal prediction (ICP). This method capitalizes
on a small set of accurately labeled training data and leverages ICP-calculated
reliability metrics to rectify mislabeled data and outliers within vast
quantities of noisy training data. The efficacy of the method is validated
across three classification tasks within distinct modalities: filtering
drug-induced-liver-injury (DILI) literature with title and abstract, predicting
ICU admission of COVID-19 patients through CT radiomics and electronic health
records, and subtyping breast cancer using RNA-sequencing data. Varying levels
of noise to the training labels were introduced through label permutation.
Results show significant enhancements in classification performance: accuracy
enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC
enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and
accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing
experiments (up to 74.6% and 89.0%). Our method offers the potential to
substantially boost classification performance in multi-modal biomedical
machine learning tasks. Importantly, it accomplishes this without necessitating
an excessive volume of meticulously curated training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xianghao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qinmei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guangming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1"&gt;Olivier Gevaert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Choosing a Proxy Metric from Past Experiments. (arXiv:2309.07893v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2309.07893</id>
        <link href="http://arxiv.org/abs/2309.07893"/>
        <updated>2023-09-16T00:40:54.823Z</updated>
        <summary type="html"><![CDATA[In many randomized experiments, the treatment effect of the long-term metric
(i.e. the primary outcome of interest) is often difficult or infeasible to
measure. Such long-term metrics are often slow to react to changes and
sufficiently noisy they are challenging to faithfully estimate in short-horizon
experiments. A common alternative is to measure several short-term proxy
metrics in the hope they closely track the long-term metric -- so they can be
used to effectively guide decision-making in the near-term. We introduce a new
statistical framework to both define and construct an optimal proxy metric for
use in a homogeneous population of randomized experiments. Our procedure first
reduces the construction of an optimal proxy metric in a given experiment to a
portfolio optimization problem which depends on the true latent treatment
effects and noise level of experiment under consideration. We then denoise the
observed treatment effects of the long-term metric and a set of proxies in a
historical corpus of randomized experiments to extract estimates of the latent
treatment effects for use in the optimization problem. One key insight derived
from our approach is that the optimal proxy metric for a given experiment is
not apriori fixed; rather it should depend on the sample size (or effective
noise level) of the randomized experiment for which it is deployed. To
instantiate and evaluate our framework, we employ our methodology in a large
corpus of randomized experiments from an industrial recommendation system and
construct proxy metrics that perform favorably relative to several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tripuraneni_N/0/1/0/all/0/1"&gt;Nilesh Tripuraneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Richardson_L/0/1/0/all/0/1"&gt;Lee Richardson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Soriano_J/0/1/0/all/0/1"&gt;Jacopo Soriano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yadlowsky_S/0/1/0/all/0/1"&gt;Steve Yadlowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Community Detection and Rotational Synchronization via Semidefinite Programming. (arXiv:2105.06031v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06031</id>
        <link href="http://arxiv.org/abs/2105.06031"/>
        <updated>2023-09-16T00:40:54.809Z</updated>
        <summary type="html"><![CDATA[In the presence of heterogeneous data, where randomly rotated objects fall
into multiple underlying categories, it is challenging to simultaneously
classify them into clusters and synchronize them based on pairwise relations.
This gives rise to the joint problem of community detection and
synchronization. We propose a series of semidefinite relaxations, and prove
their exact recovery when extending the celebrated stochastic block model to
this new setting where both rotations and cluster identities are to be
determined. Numerical experiments demonstrate the efficacy of our proposed
algorithms and confirm our theoretical result which indicates a sharp phase
transition for exact recovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yifeng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khoo_Y/0/1/0/all/0/1"&gt;Yuehaw Khoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhizhen Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07867</id>
        <link href="http://arxiv.org/abs/2309.07867"/>
        <updated>2023-09-16T00:40:54.786Z</updated>
        <summary type="html"><![CDATA[We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhendong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huangjie Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time. (arXiv:2309.07418v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2309.07418</id>
        <link href="http://arxiv.org/abs/2309.07418"/>
        <updated>2023-09-16T00:40:54.757Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) have played a pivotal role in revolutionizing
various facets of our daily existence. Solving attention regression is a
fundamental task in optimizing LLMs. In this work, we focus on giving a
provable guarantee for the one-layer attention network objective function
$L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp(
\mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3}
Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$. Here $\mathsf{A} \in \mathbb{R}^{n^2
\times d^2}$ is Kronecker product between $A_1 \in \mathbb{R}^{n \times d}$ and
$A_2 \in \mathbb{R}^{n \times d}$. $A_3$ is a matrix in $\mathbb{R}^{n \times
d}$, $\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ is the $j_0$-th block of
$\mathsf{A}$. The $X, Y \in \mathbb{R}^{d \times d}$ are variables we want to
learn. $B \in \mathbb{R}^{n \times d}$ and $b_{j_0,i_0} \in \mathbb{R}$ is one
entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \in \mathbb{R}^d$
is the $i_0$-column vector of $Y$, and $x \in \mathbb{R}^{d^2}$ is the
vectorization of $X$.

In a multi-layer LLM network, the matrix $B \in \mathbb{R}^{n \times d}$ can
be viewed as the output of a layer, and $A_1= A_2 = A_3 \in \mathbb{R}^{n
\times d}$ can be viewed as the input of a layer. The matrix version of $x$ can
be viewed as $QK^\top$ and $Y$ can be viewed as $V$. We provide an iterative
greedy algorithm to train loss function $L(X,Y)$ up $\epsilon$ that runs in
$\widetilde{O}( ({\cal T}_{\mathrm{mat}}(n,n,d) + {\cal
T}_{\mathrm{mat}}(n,d,d) + d^{2\omega}) \log(1/\epsilon) )$ time. Here ${\cal
T}_{\mathrm{mat}}(a,b,c)$ denotes the time of multiplying $a \times b$ matrix
another $b \times c$ matrix, and $\omega\approx 2.37$ denotes the exponent of
matrix multiplication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yeqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Junze Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Conditional Moment Constraints for Confounding Robust Inference. (arXiv:2302.13348v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.13348</id>
        <link href="http://arxiv.org/abs/2302.13348"/>
        <updated>2023-09-16T00:40:54.739Z</updated>
        <summary type="html"><![CDATA[We study policy evaluation of offline contextual bandits subject to
unobserved confounders. Sensitivity analysis methods are commonly used to
estimate the policy value under the worst-case confounding over a given
uncertainty set. However, existing work often resorts to some coarse relaxation
of the uncertainty set for the sake of tractability, leading to overly
conservative estimation of the policy value. In this paper, we propose a
general estimator that provides a sharp lower bound of the policy value. It can
be shown that our estimator contains the recently proposed sharp estimator by
Dorn and Guo (2022) as a special case, and our method enables a novel extension
of the classical marginal sensitivity model using f-divergence. To construct
our estimator, we leverage the kernel method to obtain a tractable
approximation to the conditional moment constraints, which traditional
non-sharp estimators failed to take into account. In the theoretical analysis,
we provide a condition for the choice of the kernel which guarantees no
specification error that biases the lower bound estimation. Furthermore, we
provide consistency guarantees of policy evaluation and learning. In the
experiments with synthetic and real-world data, we demonstrate the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ishikawa_K/0/1/0/all/0/1"&gt;Kei Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All you need is spin: SU(2) equivariant variational quantum circuits based on spin networks. (arXiv:2309.07250v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.07250</id>
        <link href="http://arxiv.org/abs/2309.07250"/>
        <updated>2023-09-16T00:40:54.730Z</updated>
        <summary type="html"><![CDATA[Variational algorithms require architectures that naturally constrain the
optimisation space to run efficiently. In geometric quantum machine learning,
one achieves this by encoding group structure into parameterised quantum
circuits to include the symmetries of a problem as an inductive bias. However,
constructing such circuits is challenging as a concrete guiding principle has
yet to emerge. In this paper, we propose the use of spin networks, a form of
directed tensor network invariant under a group transformation, to devise SU(2)
equivariant quantum circuit ans\"atze -- circuits possessing spin rotation
symmetry. By changing to the basis that block diagonalises SU(2) group action,
these networks provide a natural building block for constructing parameterised
equivariant quantum circuits. We prove that our construction is mathematically
equivalent to other known constructions, such as those based on twirling and
generalised permutations, but more direct to implement on quantum hardware. The
efficacy of our constructed circuits is tested by solving the ground state
problem of SU(2) symmetric Heisenberg models on the one-dimensional triangular
lattice and on the Kagome lattice. Our results highlight that our equivariant
circuits boost the performance of quantum variational algorithms, indicating
broader applicability to other real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+East_R/0/1/0/all/0/1"&gt;Richard D. P. East&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Alonso_Linaje_G/0/1/0/all/0/1"&gt;Guillermo Alonso-Linaje&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Park_C/0/1/0/all/0/1"&gt;Chae-Yeun Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.06724</id>
        <link href="http://arxiv.org/abs/2309.06724"/>
        <updated>2023-09-16T00:40:54.725Z</updated>
        <summary type="html"><![CDATA[We aim to provide a general framework of for computational photography that
recovers the real scene from imperfect images, via the Deep Nonparametric
Convexified Filtering (DNCF). It is consists of a nonparametric deep network to
resemble the physical equations behind the image formation, such as denoising,
super-resolution, inpainting, and flash. DNCF has no parameterization dependent
on training data, therefore has a strong generalization and robustness to
adversarial image manipulation. During inference, we also encourage the network
parameters to be nonnegative and create a bi-convex function on the input and
parameters, and this adapts to second-order optimization algorithms with
insufficient running time, having 10X acceleration over Deep Image Prior. With
these tools, we empirically verify its capability to defend image
classification deep networks against adversary attack algorithms in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wangni_J/0/1/0/all/0/1"&gt;Jianqiao Wangni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v9 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00331</id>
        <link href="http://arxiv.org/abs/1906.00331"/>
        <updated>2023-09-16T00:40:54.717Z</updated>
        <summary type="html"><![CDATA[We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}}
\max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is
nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a
convex and bounded set. One of the most popular algorithms for solving this
problem is the celebrated gradient descent ascent (GDA) algorithm, which has
been widely used in machine learning, control theory and economics. Despite the
extensive convergence results for the convex-concave setting, GDA with equal
stepsize can converge to limit cycles or even diverge in a general setting. In
this paper, we present the complexity results on two-time-scale GDA for solving
nonconvex-concave minimax problems, showing that the algorithm can find a
stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in
\mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this
is the first nonasymptotic analysis for two-time-scale GDA in this setting,
shedding light on its superior practical performance in training generative
adversarial networks (GANs) and other real applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectrum-Aware Adjustment: A New Debiasing Framework with Applications to Principal Components Regression. (arXiv:2309.07810v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2309.07810</id>
        <link href="http://arxiv.org/abs/2309.07810"/>
        <updated>2023-09-16T00:40:54.711Z</updated>
        <summary type="html"><![CDATA[We introduce a new debiasing framework for high-dimensional linear regression
that bypasses the restrictions on covariate distributions imposed by modern
debiasing technology. We study the prevalent setting where the number of
features and samples are both large and comparable. In this context,
state-of-the-art debiasing technology uses a degrees-of-freedom correction to
remove shrinkage bias of regularized estimators and conduct inference. However,
this method requires that the observed samples are i.i.d., the covariates
follow a mean zero Gaussian distribution, and reliable covariance matrix
estimates for observed features are available. This approach struggles when (i)
covariates are non-Gaussian with heavy tails or asymmetric distributions, (ii)
rows of the design exhibit heterogeneity or dependencies, and (iii) reliable
feature covariance estimates are lacking.

To address these, we develop a new strategy where the debiasing correction is
a rescaled gradient descent step (suitably initialized) with step size
determined by the spectrum of the sample covariance matrix. Unlike prior work,
we assume that eigenvectors of this matrix are uniform draws from the
orthogonal group. We show this assumption remains valid in diverse situations
where traditional debiasing fails, including designs with complex row-column
dependencies, heavy tails, asymmetric properties, and latent low-rank
structures. We establish asymptotic normality of our proposed estimator
(centered and scaled) under various convergence notions. Moreover, we develop a
consistent estimator for its asymptotic variance. Lastly, we introduce a
debiased Principal Component Regression (PCR) technique using our
Spectrum-Aware approach. In varied simulations and real data experiments, we
observe that our method outperforms degrees-of-freedom debiasing by a margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sur_P/0/1/0/all/0/1"&gt;Pragya Sur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survival Estimation for Missing not at Random Censoring Indicators based on Copula Models. (arXiv:2009.01726v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.01726</id>
        <link href="http://arxiv.org/abs/2009.01726"/>
        <updated>2023-09-16T00:40:54.693Z</updated>
        <summary type="html"><![CDATA[In the presence of right-censored data with covariates, the conditional
Kaplan-Meier estimator (also known as the Beran estimator) consistently
estimates the conditional survival function of the random follow-up for the
event of interest. However, a necessary condition is the unambiguous knowledge
of whether each individual is censored or not, which may be incomplete in
practice. We therefore propose a study of the Beran estimator when the
censoring indicators are generic random variables and discuss necessary
conditions for the efficiency of the Beran estimator. From this, we provide a
new estimator for the conditional survival function with missing not at random
(MNAR) censoring indicators based on a conditional copula model for the
missingness mechanism. In addition to the theoretical results, we illustrate
how the estimators work for small samples through a simulation study and show
their practical applicability by analyzing synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Escobar_Bach_M/0/1/0/all/0/1"&gt;Mikael Escobar-Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goudet_O/0/1/0/all/0/1"&gt;Olivier Goudet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence analysis of online algorithms for vector-valued kernel regression. (arXiv:2309.07779v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.07779</id>
        <link href="http://arxiv.org/abs/2309.07779"/>
        <updated>2023-09-16T00:40:54.688Z</updated>
        <summary type="html"><![CDATA[We consider the problem of approximating the regression function from noisy
vector-valued data by an online learning algorithm using an appropriate
reproducing kernel Hilbert space (RKHS) as prior. In an online algorithm,
i.i.d. samples become available one by one by a random process and are
successively processed to build approximations to the regression function. We
are interested in the asymptotic performance of such online approximation
algorithms and show that the expected squared error in the RKHS norm can be
bounded by $C^2 (m+1)^{-s/(2+s)}$, where $m$ is the current number of processed
data, the parameter $0<s\leq 1$ expresses an additional smoothness assumption
on the regression function and the constant $C$ depends on the variance of the
input noise, the smoothness of the regression function and further parameters
of the algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Griebel_M/0/1/0/all/0/1"&gt;Michael Griebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oswald_P/0/1/0/all/0/1"&gt;Peter Oswald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.07260</id>
        <link href="http://arxiv.org/abs/2302.07260"/>
        <updated>2023-09-16T00:40:54.674Z</updated>
        <summary type="html"><![CDATA[Several fundamental problems in science and engineering consist of global
optimization tasks involving unknown high-dimensional (black-box) functions
that map a set of controllable variables to the outcomes of an expensive
experiment. Bayesian Optimization (BO) techniques are known to be effective in
tackling global optimization problems using a relatively small number objective
function evaluations, but their performance suffers when dealing with
high-dimensional outputs. To overcome the major challenge of dimensionality,
here we propose a deep learning framework for BO and sequential decision making
based on bootstrapped ensembles of neural architectures with randomized priors.
Using appropriate architecture choices, we show that the proposed framework can
approximate functional relationships between design variables and quantities of
interest, even in cases where the latter take values in high-dimensional vector
spaces or even infinite-dimensional function spaces. In the context of BO, we
augmented the proposed probabilistic surrogates with re-parameterized Monte
Carlo approximations of multiple-point (parallel) acquisition functions, as
well as methodological extensions for accommodating black-box constraints and
multi-fidelity information sources. We test the proposed framework against
state-of-the-art methods for BO and demonstrate superior performance across
several challenging tasks with high-dimensional outputs, including a
constrained multi-fidelity optimization task involving shape optimization of
rotor blades in turbo-machinery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhouri_M/0/1/0/all/0/1"&gt;Mohamed Aziz Bhouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joly_M/0/1/0/all/0/1"&gt;Michael Joly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Robert Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1"&gt;Soumalya Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1"&gt;Paris Perdikaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. (arXiv:2202.05928v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.05928</id>
        <link href="http://arxiv.org/abs/2202.05928"/>
        <updated>2023-09-16T00:40:54.653Z</updated>
        <summary type="html"><![CDATA[Benign overfitting, the phenomenon where interpolating models generalize well
in the presence of noisy data, was first observed in neural network models
trained with gradient descent. To better understand this empirical observation,
we consider the generalization error of two-layer neural networks trained to
interpolation by gradient descent on the logistic loss following random
initialization. We assume the data comes from well-separated class-conditional
log-concave distributions and allow for a constant fraction of the training
labels to be corrupted by an adversary. We show that in this setting, neural
networks exhibit benign overfitting: they can be driven to zero training error,
perfectly fitting any noisy training labels, and simultaneously achieve minimax
optimal test error. In contrast to previous work on benign overfitting that
require linear or kernel-based predictors, our analysis holds in a setting
where both the model and learning dynamics are fundamentally nonlinear.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1"&gt;Spencer Frei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07593</id>
        <link href="http://arxiv.org/abs/2309.07593"/>
        <updated>2023-09-16T00:40:54.647Z</updated>
        <summary type="html"><![CDATA[Variable importance assessment has become a crucial step in machine-learning
applications when using complex learners, such as deep neural networks, on
large-scale data. Removal-based importance assessment is currently the
reference approach, particularly when statistical guarantees are sought to
justify variable inclusion. It is often implemented with variable permutation
schemes. On the flip side, these approaches risk misidentifying unimportant
variables as important in the presence of correlations among covariates. Here
we develop a systematic approach for studying Conditional Permutation
Importance (CPI) that is model agnostic and computationally lean, as well as
reusable benchmarks of state-of-the-art variable importance estimators. We show
theoretically and empirically that $\textit{CPI}$ overcomes the limitations of
standard permutation importance by providing accurate type-I error control.
When used with a deep neural network, $\textit{CPI}$ consistently showed top
accuracy across benchmarks. An empirical benchmark on real-world data analysis
in a large-scale medical dataset showed that $\textit{CPI}$ provides a more
parsimonious selection of statistically significant variables. Our results
suggest that $\textit{CPI}$ can be readily used as drop-in replacement for
permutation-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chamma_A/0/1/0/all/0/1"&gt;Ahmad Chamma&lt;/a&gt; (1 and 2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Engemann_D/0/1/0/all/0/1"&gt;Denis A. Engemann&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1"&gt;Bertrand Thirion&lt;/a&gt; (1 and 2 and 3) ((1) Inria, (2) Universite Paris Saclay, (3) CEA, (4) Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel, F. Hoffmann-La Roche Ltd., Basel, Switzerland)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed Bayesian inference of external potentials in classical density-functional theory. (arXiv:2309.07065v2 [cond-mat.stat-mech] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.07065</id>
        <link href="http://arxiv.org/abs/2309.07065"/>
        <updated>2023-09-16T00:40:54.640Z</updated>
        <summary type="html"><![CDATA[The swift progression of machine learning (ML) has not gone unnoticed in the
realm of statistical mechanics. ML techniques have attracted attention by the
classical density-functional theory (DFT) community, as they enable discovery
of free-energy functionals to determine the equilibrium-density profile of a
many-particle system. Within DFT, the external potential accounts for the
interaction of the many-particle system with an external field, thus, affecting
the density distribution. In this context, we introduce a statistical-learning
framework to infer the external potential exerted on a many-particle system. We
combine a Bayesian inference approach with the classical DFT apparatus to
reconstruct the external potential, yielding a probabilistic description of the
external potential functional form with inherent uncertainty quantification.
Our framework is exemplified with a grand-canonical one-dimensional particle
ensemble with excluded volume interactions in a confined geometry. The required
training dataset is generated using a Monte Carlo (MC) simulation where the
external potential is applied to the grand-canonical ensemble. The resulting
particle coordinates from the MC simulation are fed into the learning framework
to uncover the external potential. This eventually allows us to compute the
equilibrium density profile of the system by using the tools of DFT. Our
approach benchmarks the inferred density against the exact one calculated
through the DFT formulation with the true external potential. The proposed
Bayesian procedure accurately infers the external potential and the density
profile. We also highlight the external-potential uncertainty quantification
conditioned on the amount of available simulated data. The seemingly simple
case study introduced in this work might serve as a prototype for studying a
wide variety of applications, including adsorption and capillarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Malpica_Morales_A/0/1/0/all/0/1"&gt;Antonio Malpica-Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Yatsyshin_P/0/1/0/all/0/1"&gt;Peter Yatsyshin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Duran_Olivencia_M/0/1/0/all/0/1"&gt;Miguel A. Duran-Olivencia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kalliadasis_S/0/1/0/all/0/1"&gt;Serafim Kalliadasis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07703</id>
        <link href="http://arxiv.org/abs/2309.07703"/>
        <updated>2023-09-16T00:40:54.633Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence models and methods commonly lack causal
interpretability. Despite the advancements in interpretable machine learning
(IML) methods, they frequently assign importance to features which lack causal
influence on the outcome variable. Selecting causally relevant features among
those identified as relevant by these methods, or even before model training,
would offer a solution. Feature selection methods utilizing information
theoretical quantities have been successful in identifying statistically
relevant features. However, the information theoretical quantities they are
based on do not incorporate causality, rendering them unsuitable for such
scenarios. To address this challenge, this article proposes information
theoretical quantities that incorporate the causal structure of the system,
which can be used to evaluate causal importance of features for some given
outcome variable. Specifically, we introduce causal versions of entropy and
mutual information, termed causal entropy and causal information gain, which
are designed to assess how much control a feature provides over the outcome
variable. These newly defined quantities capture changes in the entropy of a
variable resulting from interventions on other variables. Fundamental results
connecting these quantities to the existence of causal effects are derived. The
use of causal information gain in feature selection is demonstrated,
highlighting its superiority over standard mutual information in revealing
which features provide control over a chosen outcome variable. Our
investigation paves the way for the development of methods with improved
interpretability in domains involving causation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1"&gt;Francisco Nunes Ferreira Quialheiro Simoes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dastani_M/0/1/0/all/0/1"&gt;Mehdi Dastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommen_T/0/1/0/all/0/1"&gt;Thijs van Ommen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Model-Based Gaussian Process Clustering. (arXiv:2309.07882v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2309.07882</id>
        <link href="http://arxiv.org/abs/2309.07882"/>
        <updated>2023-09-16T00:40:54.618Z</updated>
        <summary type="html"><![CDATA[Gaussian process is an indispensable tool in clustering functional data,
owing to it's flexibility and inherent uncertainty quantification. However,
when the functional data is observed over a large grid (say, of length $p$),
Gaussian process clustering quickly renders itself infeasible, incurring
$O(p^2)$ space complexity and $O(p^3)$ time complexity per iteration; and thus
prohibiting it's natural adaptation to large environmental applications. To
ensure scalability of Gaussian process clustering in such applications, we
propose to embed the popular Vecchia approximation for Gaussian processes at
the heart of the clustering task, provide crucial theoretical insights towards
algorithmic design, and finally develop a computationally efficient expectation
maximization (EM) algorithm. Empirical evidence of the utility of our proposal
is provided via simulations and analysis of polar temperature anomaly
(\href{https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/global/time-series}{noaa.gov})
data-sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Anirban Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Abhisek Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.07176</id>
        <link href="http://arxiv.org/abs/2309.07176"/>
        <updated>2023-09-16T00:40:54.609Z</updated>
        <summary type="html"><![CDATA[In consequential domains, it is often impossible to compel individuals to
take treatment, so that optimal policy rules are merely suggestions in the
presence of human non-adherence to treatment recommendations. In these same
domains, there may be heterogeneity both in who responds in taking-up
treatment, and heterogeneity in treatment efficacy. While optimal treatment
rules can maximize causal outcomes across the population, access parity
constraints or other fairness considerations can be relevant in the case of
encouragement. For example, in social services, a persistent puzzle is the gap
in take-up of beneficial services among those who may benefit from them the
most. When in addition the decision-maker has distributional preferences over
both access and average outcomes, the optimal decision rule changes. We study
causal identification, statistical variance-reduced estimation, and robust
estimation of optimal treatment rules, including under potential violations of
positivity. We consider fairness constraints such as demographic parity in
treatment take-up, and other constraints, via constrained optimization. Our
framework can be extended to handle algorithmic recommendations under an
often-reasonable covariate-conditional exclusion restriction, using our
robustness checks for lack of positivity in the recommendation. We develop a
two-stage algorithm for solving over parametrized policy classes under general
constraints to obtain variance-sensitive regret bounds. We illustrate the
methods in two case studies based on data from randomized encouragement to
enroll in insurance and from pretrial supervised release with electronic
monitoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Angela Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.01952</id>
        <link href="http://arxiv.org/abs/2302.01952"/>
        <updated>2023-09-16T00:40:54.604Z</updated>
        <summary type="html"><![CDATA[The recipe behind the success of deep learning has been the combination of
neural networks and gradient-based optimization. Understanding the behavior of
gradient descent however, and particularly its instability, has lagged behind
its empirical success. To add to the theoretical tools available to study
gradient descent we propose the principal flow (PF), a continuous time flow
that approximates gradient descent dynamics. To our knowledge, the PF is the
only continuous flow that captures the divergent and oscillatory behaviors of
gradient descent, including escaping local minima and saddle points. Through
its dependence on the eigendecomposition of the Hessian the PF sheds light on
the recently observed edge of stability phenomena in deep learning. Using our
new understanding of instability we propose a learning rate adaptation method
which enables us to control the trade-off between training stability and test
set evaluation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1"&gt;Mihaela Rosca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qin_C/0/1/0/all/0/1"&gt;Chongli Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dherin_B/0/1/0/all/0/1"&gt;Benoit Dherin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Googleâ€™s new DeepMind Maps algorithm improves route suggestions by 24%]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16jsd4m/googles_new_deepmind_maps_algorithm_improves/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16jsd4m/googles_new_deepmind_maps_algorithm_improves/"/>
        <updated>2023-09-15T23:29:34.000Z</updated>
        <summary type="html"><![CDATA[Google has developed an AI algorithm to refine route suggestions on Google Maps, personalizing it based on user data and behavior, allegedly improving the accuracy on an average by 16-24 percent.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/rp21ylzo7iob1.jpg?width=770&format=pjpg&auto=webp&s=737cf3fd278ade42900e8586d626a7c729522c1a
 Personalized Route Suggestions through AI
  
The AI model comprises 360 million parameters, using real-time data from Maps users to influence factors including travel time, road conditions, tolls, and personal preferences to suggest routes.
 This technology is grounded on "inverse reinforcement learning" (IRL), specifically a new IRL algorithm - "Receding Horizon Inverse Planning (RHIP)".
  
The Power of RHIP and AI in Maps
  
Google and Deepmind jointly worked to develop RHIP, using complex stochastic models in immediate vicinity areas, but switching to simpler deterministic methods for distant areas for power conservation.
 The AI improves route suggestions for both driving and two-wheeled vehicles by learning from Maps users' movements and behaviors over time.
 Google states that this is the largest application of inverse reinforcement learning for route planning to date.
  
Implementation and User Testing
  
Google has applied the algorithm to Maps data globally, but extensive user testing is needed to confirm if the technique consistently produces better routes.
 Previous attempts at using AI systems for route planning on a large scale have often failed due to the complexity of road networks.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bard on Republican 1/6 denial]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16jpu3w/bard_on_republican_16_denial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16jpu3w/bard_on_republican_16_denial/"/>
        <updated>2023-09-15T21:43:32.000Z</updated>
        <summary type="html"><![CDATA[There are a number of reasons why some Republicans may be in denial about Trump's role in the January 6th Capitol riot.
  
Trump has been very effective at cultivating a cult of personality among his supporters. These supporters are often fiercely loyal to him and willing to believe whatever he says, even if it contradicts reality.
 Many Republicans have been reluctant to criticize Trump for fear of alienating his supporters. This is especially true for Republican politicians who are up for re-election in 2022.
 Some Republicans may genuinely believe that Trump did not incite the riot. They may point to the fact that he did not explicitly call for violence, or that he told his supporters to "peacefully and patriotically make your voices heard."
 Other Republicans may be motivated by partisâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bard on factory farming and clean meat]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16jowi1/bard_on_factory_farming_and_clean_meat/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16jowi1/bard_on_factory_farming_and_clean_meat/"/>
        <updated>2023-09-15T21:07:30.000Z</updated>
        <summary type="html"><![CDATA[Factory farming is a cruel and unsustainable way to produce meat. Animals are crammed into small, filthy spaces, where they are often denied their basic needs. They are given antibiotics to prevent disease and growth hormones to make them grow faster. This leads to a variety of health problems for the animals, as well as for the people who eat their meat.
 Clean meat is a new technology that offers a more humane and sustainable way to produce meat. It is made from animal cells that are grown in a lab, without the need for animals to be raised or slaughtered. Clean meat is still in its early stages of development, but it has the potential to revolutionize the meat industry.
 Here are some of the reasons why factory farming must be replaced by clean meat:
  
It is cruel to animals. Factory fâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Building My Own AI-Model Hub: Seeking Guidance and Resources]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16josbf/p_building_my_own_aimodel_hub_seeking_guidance/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16josbf/p_building_my_own_aimodel_hub_seeking_guidance/"/>
        <updated>2023-09-15T21:03:02.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone! I'm embarking on a project to create an AI-model hubâ€”a platform where users can upload and utilize their AI models. While I'm aware of popular platforms that offer this, my primary goal is for educational purposes. I'd greatly appreciate any recommendations for helpful articles, videos, or codebases to guide me on this journey. Thanks in advance!
    submitted by    /u/Electronic-Choice-86  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] LLMa: Expert Guidance on Generative AI, Tailored for Your Needs, Outdoing GPT-4 & Saving Costs!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16joh3y/p_llma_expert_guidance_on_generative_ai_tailored/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16joh3y/p_llma_expert_guidance_on_generative_ai_tailored/"/>
        <updated>2023-09-15T20:50:41.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 Introducing LLMa: ChatGPT built around YOU (getllma.com) - a dedicated service offering hands-on expertise to integrate state-of-the-art generative AI tailored for your projects. We utilize open-source models and train them to outperform GPT-4 on tasks specific to your domain. Envision having a seasoned AI specialist on your team, ensuring your model not only rivals the big players but excels in your unique challenges.
 ðŸŒŸ Why LLMa?
  
Personalized Expertise: Our team collaborates closely with you, delving into your needs and sculpting a model that thrives in your domain.
 Bespoke Training: We refine open-source models (LLaMa, T5, etc.) with plenty of secret tricks to specialize and surpass GPT-4's performance for your specific tasks.
 Cost-Effective: LLMa tends to be around 100x cheaper than GPT-4, offering significant savings. No recurring fees; invest in a one-time fee based on your model's complexity.
 Full Ownership: We hand over the model files/weights to you. It's entirely yours, ensuring total privacy with no PII leaks.
 Deployment Assistance: Beyond just crafting the model, we can guide you in deploying it, ensuring a seamless integration into your operations.
 Ongoing Support: From initial brainstorming to model deployment, we're with you, ensuring success at every phase.
  
ðŸ’¼ Tailored for Enterprises: LLMa is meticulously crafted for enterprises that aim for a high-performing, bespoke AI solution. Transparent pricing begins at $500, contingent on your distinct requirements.
 â“ Navigating the Generative AI Terrain? Embarking on the vast journey of generative AI? LLMa is your compass. We aid in defining challenges, strategizing solutions, and optimizing the AI potential for your endeavors.
 If LLMa piques your interest or if you have any queries, fill-in the form, drop a comment below or DM me. I'm all ears and eager to connect!
    submitted by    /u/iliashark  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Deploying Hugging Face models on Amazon SageMaker using infrastructure as code]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jo7hg/p_deploying_hugging_face_models_on_amazon/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jo7hg/p_deploying_hugging_face_models_on_amazon/"/>
        <updated>2023-09-15T20:39:55.000Z</updated>
        <summary type="html"><![CDATA[Quick template that bootstraps Amazon SageMaker running a LlaMa 2 model from Hugging Face. Everything deployed as code (Python), no manual tweaking in the SageMaker console. 
 www.pulumi.com/blog/mlops-huggingface-llm-aws-sagemaker-python/ 
    submitted by    /u/kao-pulumi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Timing attacks]]></title>
        <id>https://www.johndcook.com/blog/?p=208268</id>
        <link href="https://www.johndcook.com/blog/2023/09/15/timing-attacks/"/>
        <updated>2023-09-15T20:15:11.000Z</updated>
        <summary type="html"><![CDATA[If you ask someone a question and they say â€œyesâ€ immediately, that gives you different information than if they pause and slowly say â€œyes.â€ The information you receive is not just the response but also the time it took to generate the response. Encryption can be analogous. The time it takes to encrypt data can [â€¦]
Timing attacks first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] GAN training]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jlgvp/d_gan_training/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jlgvp/d_gan_training/"/>
        <updated>2023-09-15T18:49:58.000Z</updated>
        <summary type="html"><![CDATA[Am trying to train GANs for oversampling a minority text class (am feeding it only the minority class), but the results dont seem to improve much (AUC only improves by .03 so far). while basic oversampling techniques like SMOTE gives way better results. also am using a vector representation for the whole text instead of word embedding(same used for SMOTE), i tried different architectures with CNN.
 is there any tricks maybe in training the discriminator and generator ? i can't seem to find the problem
    submitted by    /u/SlightSecretaryB  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Agents: An Open-source Framework for Autonomous Language Agents - AIWaves Inc 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jl4pe/r_agents_an_opensource_framework_for_autonomous/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jl4pe/r_agents_an_opensource_framework_for_autonomous/"/>
        <updated>2023-09-15T18:36:10.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.07870 
 Github: https://github.com/aiwaves-cn/agents 
 Abstract:
  
Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. 
  
https://preview.redd.it/3bdi71r5rgob1.jpg?width=1131&format=pjpg&auto=webp&s=760942c19be6ecda791414c812a77e72751c526d
 https://preview.redd.it/howf64r5rgob1.jpg?width=1656&format=pjpg&auto=webp&s=636744fccab7a1c2bafb902bad5dbb647440fff5
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breakthrough way to train neuromorphic chips]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16jl0ab/breakthrough_way_to_train_neuromorphic_chips/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16jl0ab/breakthrough_way_to_train_neuromorphic_chips/"/>
        <updated>2023-09-15T18:31:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image dataset management tools [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jkk71/image_dataset_management_tools_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jkk71/image_dataset_management_tools_d/"/>
        <updated>2023-09-15T18:12:37.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I have about 100K images on my machine and I am looking for a tool that can help me do some QA on it.
 Example features I would love:
  
Search and visualize all images with a prefix "cls1_" or "cls2_"
 Easily rename file names if they're not named correctly
 
Visualize all captions associated with each image 
  
We can assume they have the same name but with extension ".txt" or ".captions"
 Or there's a metadata.json linking between "img_file" and "caption_file"
 
 
Easily edit captions in the dashboard
 
 I can also work with some kind of metadata file instead of relying on filename logic if it really helps a certain tool.
 I prefer a locally run, open-source tool. It would be a problem for me to upload this data to any online platform.
 Many thanks in advance for any help or guidance.
    submitted by    /u/JYP_Scouter  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Testing values are different from "Real World" values]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jk2s1/d_testing_values_are_different_from_real_world/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jk2s1/d_testing_values_are_different_from_real_world/"/>
        <updated>2023-09-15T17:52:54.000Z</updated>
        <summary type="html"><![CDATA[Before training my model im going through multiple steps to collect and process my data. One of these steps to is calculate values from algorithmic and mathematical functions. In my training and testing data the values are around 12-15 decimal places. I then split the data without shuffling. Training, validation and testing averages at 75% accuracy. 
 Now my next step I wanted to do a "real world" data test where I collect the exact same data as my testing data and predict it using my previously built model but the values inside test_dataset1 are different to test_dataset2 within the last 6-9 decimal places even though the data is going through the exact same code. For example 
 Test_dataset1 Value : 1.123456789
 Test_dataset2 Value : 1.123456987
 This messes with my prediction and its making me wonder aswell as standrising my data should I be rounding my float values to say 4-6 decimal places?
    submitted by    /u/paddockson  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is the difference between the TPU found in Google Tensor chips vs the Neural Engine found in Apple's A and M series?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jjx4m/d_what_is_the_difference_between_the_tpu_found_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jjx4m/d_what_is_the_difference_between_the_tpu_found_in/"/>
        <updated>2023-09-15T17:46:21.000Z</updated>
        <summary type="html"><![CDATA[What are the key differences between the Tensor Processing Unit (TPU) found in Google Tensor chips and the Neural Engine found in Apple's A and M series chips? Are they the same things?
 Or is the TPU only available for Google's own AI, while the Neural Engine is available to all developers for accelerating AI for all apps, if they decide to?
 Can developers optimize apps for Google Tensor like they can for the Neural Engine?
 If not, how do developers take advantage of machine learning acceleration chips on Google Pixel or Android in general?
 If yes, let's say a developer optimizes their app for the Google TPU, will they need to re-optimize for other chips like Samsung's NPU or Qualcomm AI too? If not, how well do they run? Are they the same fast and efficient?
    submitted by    /u/GRguy_21  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double DQN do not learn anything]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16jjwvu/double_dqn_do_not_learn_anything/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16jjwvu/double_dqn_do_not_learn_anything/"/>
        <updated>2023-09-15T17:46:04.000Z</updated>
        <summary type="html"><![CDATA[Hi, i just finished the coursera ml course and i wanted to create myself a double dqn model but my model don't seem to learn anything, 
 it always return very low rewards (-100 to -300) even after playing 2000 episodes. 
 I've been stuck on this for 4 days without any hope to find the solution, any help would be welcome :')
 thank you in advance
 import random import numpy as np import gymnasium as gym import tensorflow as tf from collections import deque, namedtuple from tensorflow.keras import Sequential, Input from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam from tensorflow.keras.losses import MeanSquaredError import matplotlib.pyplot as plt # function creating the models def createModel(inputSize, outputSize): model = Sequential([ Input(inputSize),â€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MediaPipe FaceStylizer: On-device real-time few-shot face stylization]]></title>
        <id>http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html</id>
        <link href="http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html"/>
        <updated>2023-09-15T17:39:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Haolin Jia, Software Engineer, and Qifei Wang, Senior Software Engineer, Core ML



In recent years, we have witnessed rising interest across consumers and researchers in integrated augmented reality (AR) experiences using real-time face feature generation and editing functions in mobile applications, including short videos, virtual reality, and gaming. As a result, there is a growing demand for lightweight, yet high-quality face generation and editing models, which are often based on generative adversarial network (GAN) techniques. However, the majority of GAN models suffer from high computational complexity and the need for a large training dataset. In addition, it is also important to employ GAN models responsibly. 



In this post, we introduce MediaPipe FaceStylizer, an effiâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Suggestions/Direction: Working on Image Dehazing for Face Images.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jjp5l/p_suggestionsdirection_working_on_image_dehazing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jjp5l/p_suggestionsdirection_working_on_image_dehazing/"/>
        <updated>2023-09-15T17:37:22.000Z</updated>
        <summary type="html"><![CDATA[Working on Final year project in the field on computer vision: Image Dehazing. I aim at having a novel approch for better dehazing of face haze images.
 Have read papers related to single image dehazing & face SR/Deblur.
 Any thoughts/ suggestions/ leads would be appreciated.
    submitted by    /u/GahlotB  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI â€” weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16jisc3/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16jisc3/ai_weekly_megathread/"/>
        <updated>2023-09-15T17:02:02.000Z</updated>
        <summary type="html"><![CDATA[News provided by aibrews.com
  
Stability AI launched Stable Audio, a generative AI tool for music & sound generation from text. The underlying latent diffusion model architecture uses audio conditioned on text metadata as well as audio file duration and start time [Details].
 Coqui released XTTS - a new voice generation model that lets you clone voices in 13 different languages by using just a quick 3-second audio clip [Details].
 Microsoft Research released and open-sourced Phi-1.5 - a 1.3 billion parameter transformer-based model with performance on natural language tasks comparable to models 5x larger [Paper ].
 Project Gutenberg, Microsoft and MIT have worked together to use neural text-to-speech to create and release thousands of human-quality free and open audiobooks [Details].
 Resâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Traveling Words: A Geometric Interpretation of Transformers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jibo2/r_traveling_words_a_geometric_interpretation_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jibo2/r_traveling_words_a_geometric_interpretation_of/"/>
        <updated>2023-09-15T16:43:10.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.07315
 Abstract:
  
Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajectory of word particles along the hyper-sphere.
  
â€‹
 https://preview.redd.it/0i302t857gob1.png?width=1864&format=png&auto=webp&s=1da999c014979bdb6c99809d5b38eb5ccfd717d0
    submitted by    /u/CoolThingsOnTop  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Actor-Critic and other implementations]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16ji6oq/actorcritic_and_other_implementations/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16ji6oq/actorcritic_and_other_implementations/"/>
        <updated>2023-09-15T16:37:30.000Z</updated>
        <summary type="html"><![CDATA[I'm confused with several algorithms that are based on an actor-critic approach. In TD3 and SAC, it is understandable that each of them is implemented to serve their purpose (deterministic and stachastic action). But in Dreamer algorithm (DreamerV3), why does it require to combine actor and critic network to the model-based planning approach, as the model-based also able to perform an action by planning to the simulation state. It is mean that using model-based to simulate the possible future then update the critic according to the simulation might sound good in training an agent?
    submitted by    /u/AnnonymeowCat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best â€œMathâ€ book for creating neural networks]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16jh4zk/best_math_book_for_creating_neural_networks/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16jh4zk/best_math_book_for_creating_neural_networks/"/>
        <updated>2023-09-15T15:55:33.000Z</updated>
        <summary type="html"><![CDATA[So I want to create a neural network from scratch, like no lib(tensorflow, pytorch, etcâ€¦), so whatâ€™s the best book for that, I know both calculus and statistics, so Iâ€™m assuming that the math wouldnâ€™t be a problem. Also I will be using Cuda for its speed.
    submitted by    /u/GateCodeMark  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn how to build and deploy tool-using LLM agents using AWS SageMaker JumpStart Foundation Models]]></title>
        <id>3078dbf5dded8b902326930cc4c8eaed55ce699b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/learn-how-to-build-and-deploy-tool-using-llm-agents-using-aws-sagemaker-jumpstart-foundation-models/"/>
        <updated>2023-09-15T15:24:36.000Z</updated>
        <summary type="html"><![CDATA[Large language model (LLM) agents are programs that extend the capabilities of standalone LLMs with 1) access to external tools (APIs, functions, webhooks, plugins, and so on), and 2) the ability to plan and execute tasks in a self-directed fashion. Often, LLMs need to interact with other software, databases, or APIs to accomplish complex tasks. [â€¦]]]></summary>
        <author>
            <name>John Hwang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[With things always moving so fast, I may have missed it. Anyone doing something like Run.AI in an opensource capacity?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16jg22m/with_things_always_moving_so_fast_i_may_have/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16jg22m/with_things_always_moving_so_fast_i_may_have/"/>
        <updated>2023-09-15T15:13:22.000Z</updated>
        <summary type="html"><![CDATA[Has anyone dropped an opensource tool like run.ai, to leverage multiple gpus / distribute the workload a bit more efficiently?
 I'm loving some of the single gpu llm modifications that have been dropping recently (have a couple i've tested that ran well on 4090 and 3090ti in the lab), but i've got a plethora of 8 & 12 gig 3xxx series cards i'd love to take advantage of beyond passthroughs to individual vms. Looking for any solutions. Speed isn't as important as the ability to distributively run larger models.
    submitted by    /u/SwallowedBuckyBalls  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Death Metal band 'Anarchy's Echo' - Debut single 'Soul Shredder' [visuals, music, name, logo all AI generated]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16jg0zf/ai_death_metal_band_anarchys_echo_debut_single/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16jg0zf/ai_death_metal_band_anarchys_echo_debut_single/"/>
        <updated>2023-09-15T15:12:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/glenniszen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elliptic curve Diffie-Hellman key exchange]]></title>
        <id>https://www.johndcook.com/blog/?p=208277</id>
        <link href="https://www.johndcook.com/blog/2023/09/15/ecdhe/"/>
        <updated>2023-09-15T15:08:53.000Z</updated>
        <summary type="html"><![CDATA[I concluded the previous post by saying elliptic curve Diffie-Hellman key exchange (ECDHE) requires smaller keys than finite field Diffie-Hellman (FFDHE) to obtain the same level of security. How much smaller are we talking about? According to NIST recommendations, a 256-bit elliptic curve curve provides about the same security as working over a 3072-bit finite [â€¦]
Elliptic curve Diffie-Hellman key exchange first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Hampel Python Library with C extensions ðŸš€]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jfe2w/p_hampel_python_library_with_c_extensions/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jfe2w/p_hampel_python_library_with_c_extensions/"/>
        <updated>2023-09-15T14:47:18.000Z</updated>
        <summary type="html"><![CDATA[Repo -> https://github.com/MichaelisTrofficus/hampel_filter
 The Python library hampel implements the Hampel Filter, which is generally used to detect anomalies in data with a timeseries structure. It basically consists of a sliding window of a parameterizable size.
 The library was in plain Python before (using pandas for all the sliding operations, median computation etc), but now it has been replaced by a Cython implementation, which speeds up things quite a bit! ðŸ˜€
 It also provides much more valuable information (thresholds, median absolute deviations, etc.), allowing us to create plots like this one:
 https://preview.redd.it/6j4ubiwgmfob1.png?width=800&format=png&auto=webp&s=bbc56777fce30a464d0bb33ac5126033b3413838
 â€‹
    submitted by    /u/Hefty-Consequence443  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] How to generate post custom for each user ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jd9h7/discussion_how_to_generate_post_custom_for_each/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jd9h7/discussion_how_to_generate_post_custom_for_each/"/>
        <updated>2023-09-15T13:20:10.000Z</updated>
        <summary type="html"><![CDATA[Hi everybody.
 Currently, I am building a Deep Learning model with the task of automatically generating random posts and tweets. The characteristic is that it must have the personality of the writer, for example the user is the CEO of company A, then the generated post must have the writing style of the CEO or company A, similar to other users.
 Actually, I don't know where to start solving this problem. I intend to use RWKV to do this problem but I'm not sure if it is a good direction or not. Is there any related research or can anyone who has done this problem give me some suggestions?
    submitted by    /u/unknow_from_vietnam  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Uncovering mesa-optimization algorithms in Transformers (from Google Research, ETH ZÃ¼rich, and Google DeepMind)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jc2su/r_uncovering_mesaoptimization_algorithms_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jc2su/r_uncovering_mesaoptimization_algorithms_in/"/>
        <updated>2023-09-15T12:31:13.000Z</updated>
        <summary type="html"><![CDATA[Paper. I am not affiliated with this work or its authors.
 Abstract:
  
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.
  
Twitter thread about the paper from one of the paper's authors. Nitter thread, for those who aren't signed into Twitter but want to see the entire Twitter thread.
 Background info: Mesa-Optimization: Explain it like I'm 10 Edition.
    submitted by    /u/Wiskkey  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can somebody help check my math to see if I'm understanding Microsoft's Retentive Network paper correctly? I'm confused how we are enriching the tokens with enough context.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jbp8q/d_can_somebody_help_check_my_math_to_see_if_im/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jbp8q/d_can_somebody_help_check_my_math_to_see_if_im/"/>
        <updated>2023-09-15T12:14:53.000Z</updated>
        <summary type="html"><![CDATA[Relevant Paper: 2307.08621.pdf (arxiv.org)
 So the definition of the recurrent representation of the retention mechanism is below
  
Sn = Î³Snâˆ’1 + KâŠºnVn 
 Retention(Xn) = QnSn, n = 1, Â· Â· Â· , |x|
  
Î³ is a decay factor, and K, Q, and V have their standard transformer definitions.
 What confuses me is the derivation of Sn. The formula makes it look like a scalar. But if that's the case, are we saying that for a given token, the retention mechanism is just multiplying the Query by a scalar? That's surprising! How is that able to provide enough context?
 Here is some code I wrote with GPT to show my understanding of how it works. Is this correct? I use 3 arbitrary tokens of dimension 3, and then a pick arbitrary K Q and V matrices. I also initialize gamma to 0.5
 import numpy as np # Tokens x1â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project]: Correcting Misspelled Words in Urdu language text]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jaqdb/project_correcting_misspelled_words_in_urdu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jaqdb/project_correcting_misspelled_words_in_urdu/"/>
        <updated>2023-09-15T11:30:24.000Z</updated>
        <summary type="html"><![CDATA[Help required from NLP and Text Researchers !!
 Hello everyone!
 I have Urdu language transcriptions (text) which contain many misspelled words that are not part of the Urdu language.
 I wanted to know do we have any good NLP techniques or methods which can solve this problem for Urdu language? I want to replace these misspelled words with the correct words in Urdu.
 I have already tried Python libraries and methods such as indic-nlp, Levenshtein distance, UrduHack, Word2vec Urdu etc, but they weren't able to solve this problem. Some of the methods require Urdu dictionaries to find the correct word, which I'm also unable to find open-source on internet (please also help in that if possible).
 Will appreciate everyone's help and response to this.
 Thank you! 
    submitted by    /u/a_r182  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Prospective PhD advisors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16janap/d_prospective_phd_advisors/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16janap/d_prospective_phd_advisors/"/>
        <updated>2023-09-15T11:26:14.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone!
 I am a (soon graduating) MSc student at a top European university and I'd like to apply for a ML PhD in the US this Fall. I've done my research on schools and advisors, but I figured there's no harm in also asking in this subreddit.
 What are some groups/professors that do ML research at US unis in deep learning theory (specifically foundations) and optimization? As an example, I'm talking about topics such as: https://arxiv.org/abs/1902.08129, https://arxiv.org/abs/1711.04735, https://arxiv.org/abs/2306.04637.
 Thank you all! Cheers!
    submitted by    /u/AlexIsEpic24  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project]: Correcting Misspelled Words in Urdu language text]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16jamu8/project_correcting_misspelled_words_in_urdu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16jamu8/project_correcting_misspelled_words_in_urdu/"/>
        <updated>2023-09-15T11:25:37.000Z</updated>
        <summary type="html"><![CDATA[Help required from NLP and Text Researchers !!
 Hello everyone!
 I have Urdu language transcriptions (text) which contain many misspelled words that are not part of the Urdu language.
 I wanted to know do we have any good NLP techniques or methods which can solve this problem for Urdu language? I want to replace these misspelled words with the correct words in Urdu.
 I have already tried Python libraries and methods such as indic-nlp, Levenshtein distance, UrduHack, Word2vec Urdu etc, but they weren't able to solve this problem. Some of the methods require Urdu dictionaries to find the correct word, which I'm also unable to find open-source on internet (please also help in that if possible).
 Will appreciate everyone's help and response to this.
 Thank you! 
    submitted by    /u/a_r182  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite field Diffie Hellman primes]]></title>
        <id>https://www.johndcook.com/blog/?p=208147</id>
        <link href="https://www.johndcook.com/blog/2023/09/15/ffdhe/"/>
        <updated>2023-09-15T10:31:46.000Z</updated>
        <summary type="html"><![CDATA[Diffie-Hellman key exchange is conceptually simple. Alice and Bob want to generate a shared cryptographic key. They want to use asymmetric (public) cryptography to share a symmetric (private) key. The starting point is a large prime p and a generator 1 < g < p. Alice generates a large random number x, her private key, [â€¦]
Finite field Diffie Hellman primes first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[free AI summary of article based on most frequently used English words]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16j8m29/free_ai_summary_of_article_based_on_most/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16j8m29/free_ai_summary_of_article_based_on_most/"/>
        <updated>2023-09-15T09:32:13.000Z</updated>
        <summary type="html"><![CDATA[I am looking for a free AI tool, site (may be paid with free account limited options) that can do a summary of English article (like BBC, youtube) based on most frequently used English words, for example Oxford list of 3000 most frequently used words in English. In other words if word is not in that list it should replace the word with the one that is on the list or its definition.
 Pdf
    submitted by    /u/vibrantform  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI on why we have much to be grateful for]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16j8lha/ai_on_why_we_have_much_to_be_grateful_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16j8lha/ai_on_why_we_have_much_to_be_grateful_for/"/>
        <updated>2023-09-15T09:31:17.000Z</updated>
        <summary type="html"><![CDATA[I said to Bard:
 It seems like these are difficult times. Compare today with the WWII years, explaining how much we have to be grateful for.
 Bard said:
 I agree that these are difficult times. The world is facing many challenges, including the COVID-19 pandemic, climate change, and political instability. However, I believe that we have much to be grateful for today compared to the WWII years.
 Here are just a few things:
 We live in a time of peace. World War II was a global conflict that caused the deaths of millions of people. Today, there are no major wars raging on.
 We have access to advanced medical care. During WWII, many people died from diseases that are now easily treatable. Today, we have antibiotics, vaccines, and other medical advances that have saved countless lives.
 We have a higher standard of living. In the WWII years, many people lived in poverty. Today, most people in the developed world have access to clean water, food, and shelter.
 We have more opportunities for education and advancement. In the WWII years, many people were denied an education because of their race, gender, or social class. Today, education is more accessible than ever before, and people from all backgrounds have the opportunity to succeed.
 We have a better understanding of the world. During WWII, there was a lot of misinformation and propaganda. Today, we have access to more information than ever before, and we can make more informed decisions about our lives.
 Of course, there are still many challenges that we face today. But I believe that we have come a long way since the WWII years, and we have much to be grateful for. Let us never forget the sacrifices that were made during that time, and let us continue to work towards a better future for all.
    submitted by    /u/Georgeo57  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the RobotPerfâ„¢ Benchmarks Beta Release: An industry standard for benchmarking robotic brains]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16j7va5/announcing_the_robotperf_benchmarks_beta_release/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16j7va5/announcing_the_robotperf_benchmarks_beta_release/"/>
        <updated>2023-09-15T08:48:02.000Z</updated>
        <summary type="html"><![CDATA[https://news.accelerationrobotics.com/robotperf-beta/
 Together with AMD, Intel, Ford, Harvard, Klagenfurt University, Georgia Institute of Technology, Boston University, Johannes Kepler University Linz, Barnard College, Columbia University and Carnegie Mellon University we are thrilled to introduce the beta release of RobotPerfâ„¢ Benchmarks, an advanced benchmarking suite crafted specifically to evaluate robotics computing performance using ROS 2 as its baseline. In this beta release, we not only showcase new benchmarks and results but also introduce novel visualization capabilities. The complete release is available at https://github.com/robotperf/benchmarks/releases/tag/beta.
 https://preview.redd.it/5whys5ufudob1.png?width=1562&format=png&auto=webp&s=08a6e22a0b07b26fa6340f59ec9df822ab49c9d0
    submitted by    /u/pablocarrera  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is at stake in the AI based techno-economic war between the West and China?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16j6djc/what_is_at_stake_in_the_ai_based_technoeconomic/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16j6djc/what_is_at_stake_in_the_ai_based_technoeconomic/"/>
        <updated>2023-09-15T07:12:03.000Z</updated>
        <summary type="html"><![CDATA[The AI-based techno-economic war between the West and China will determine the global distribution of technology, economic benefits, and influence.
 
The winner of this race, particularly in AI, will have the power to accelerate GDP growth and project global economic benefits.
 
China is actively pursuing technological leadership in AI and 5G wireless, while the West must prioritize technological talent influx and leverage its venture and technology ecosystem.
 
Aggressive AI regulation could hinder the West's momentum in this race.
 
The risks of this war and AI technology are significant, but it is a greater risk to let adversaries have unconstrained power.
 
Global treaties are not enforceable solutions to regulate AI.
 
 Source : https://medium.com/@vkhosla/what-is-at-stake-in-this-ai-based-techno-economic-war-between-the-west-and-china-8f76bd291be7
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ML Research Topics (reasonably short)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16j6cgn/d_ml_research_topics_reasonably_short/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16j6cgn/d_ml_research_topics_reasonably_short/"/>
        <updated>2023-09-15T07:10:13.000Z</updated>
        <summary type="html"><![CDATA[So Iâ€™m starting my masters thesis project in ML ASAP and need a research topic. What areas (if any) are currently hot / feasible to do research in roughly 6 months with fairly limited compute access? 
 Iâ€™m more interested in theory / research heavy areas rather than applied. And probably happier to dig into some hard math rather than taking on a software engineering type project. 
 Any thoughts or general feedback very welcome! Thanks!
    submitted by    /u/Professional-Pace158  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What has been the most significant assistance from AI for you? It could be in various aspects of work, learning, and life. Are there any other needs you have that you hope AI can help you with, but there aren't suitable AI tools for them yet?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16j63zr/what_has_been_the_most_significant_assistance/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16j63zr/what_has_been_the_most_significant_assistance/"/>
        <updated>2023-09-15T06:55:47.000Z</updated>
        <summary type="html"><![CDATA[There's no need to specify which product fulfills your needs, but we'd like everyone to focus on describing their current frustrations and the issues they urgently hope AI can help them with.
    submitted by    /u/Minna_Z  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical use cases for skew symmetrical matrices in AI/ML? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16j4xj3/practical_use_cases_for_skew_symmetrical_matrices/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16j4xj3/practical_use_cases_for_skew_symmetrical_matrices/"/>
        <updated>2023-09-15T05:44:31.000Z</updated>
        <summary type="html"><![CDATA[Just came across this property of matrices that I have never seen before as I am contributing to the NumPy codebase and someone asked for this feature to be added and it got me thinking. It is defined as:
 A skew-symmetric matrix is a square matrix whose transpose equals to its negative. It should satisfy the below condition:
 AT = â€“A
 Just wondering do these have any applications in ML at all? I never came across this in my math classes so just wondering if the property pops up anywhere else in the field. Maybe in 3D space applications? Or maybe RBG data augmentation? But yeah not 100% sure.
    submitted by    /u/Ok_Reality2341  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[â€œWine can prevent Cancerâ€ says ChatGPT]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16j4q4e/wine_can_prevent_cancer_says_chatgpt/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16j4q4e/wine_can_prevent_cancer_says_chatgpt/"/>
        <updated>2023-09-15T05:32:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How much should I focus on DSA?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16j4n5c/d_how_much_should_i_focus_on_dsa/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16j4n5c/d_how_much_should_i_focus_on_dsa/"/>
        <updated>2023-09-15T05:27:37.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m an electrical engineering student in college currently, and have been learning about ML for a few months now. I will be starting a ML research paper under my professor from next week. However, my DSA skills are quite sub-par. Should I focus more on ML math and projects, or take sufficient time out for leetcode as well?
    submitted by    /u/4R1N1493  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-driven tool makes it easy to personalize 3D-printable models]]></title>
        <id>https://news.mit.edu/2023/ai-driven-tool-personalize-3d-printable-models-0915</id>
        <link href="https://news.mit.edu/2023/ai-driven-tool-personalize-3d-printable-models-0915"/>
        <updated>2023-09-15T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[With Style2Fab, makers can rapidly customize models of 3D-printable objects, such as assistive devices, without hampering their functionality.]]></summary>
        <author>
            <name>Adam Zewe | MIT News</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best architecture for an autoencoder for 2D trajectory data? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iy8bj/best_architecture_for_an_autoencoder_for_2d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iy8bj/best_architecture_for_an_autoencoder_for_2d/"/>
        <updated>2023-09-15T00:05:43.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I have a dataset that consists of 2D trajectories and I am aiming to develop an autoencoder architecture to learn a compressed set of features that reasonable represents and can reconstruct the trajectories. 
 The trajectories may look something like this as an example. A 2D image as input would seem to require a very sparse representation with high resolution to track the trajectory path. I am hoping there is a better way to input the path without requiring high resolution. 
 An alternative might be to use a LSTM structure to input as a sequence, although not sure that solves the resolution issue.
 Do you have any suggestions? I've worked with 1d time series and 2D images just fine but this is a bit different.
 â€‹
 â€‹
 https://preview.redd.it/vqz8y3o69bob1.png?width=2020&format=png&auto=webp&s=d8bcc2fe311743c8e78a96055e68f1ad364b48c3
    submitted by    /u/ZeApelido  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Besides something about LLM, is there any new or interesting research you think is worth reading?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ixsc8/d_besides_something_about_llm_is_there_any_new_or/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ixsc8/d_besides_something_about_llm_is_there_any_new_or/"/>
        <updated>2023-09-14T23:45:24.000Z</updated>
        <summary type="html"><![CDATA[Please provide Arkiv links. If you want to share your thoughts then go for it. By new I mean within the last 6 months.
    submitted by    /u/I_will_delete_myself  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Good papers on poster collapse in VAEs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iwhg3/d_good_papers_on_poster_collapse_in_vaes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iwhg3/d_good_papers_on_poster_collapse_in_vaes/"/>
        <updated>2023-09-14T22:47:18.000Z</updated>
        <summary type="html"><![CDATA[What are some good papers to understand posterior collapse in VAEs?
    submitted by    /u/randomkolmogorov  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Create an Object Detector for Any Game Using YOLO]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iw7ey/p_create_an_object_detector_for_any_game_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iw7ey/p_create_an_object_detector_for_any_game_using/"/>
        <updated>2023-09-14T22:35:16.000Z</updated>
        <summary type="html"><![CDATA[Full Video Tutorial: https://www.linkedin.com/posts/moisesdias\_english-version-below-tutorial-crie-activity-7107686497885011969-ZLVW/ 
 Hello everyone! Have you ever thought about how to create an object detection system using YOLO that works with any game? 
 If you're interested, I've created a tutorial with all the steps to develop this system. I'll leave a link to the video where I demonstrate the process step by step using the game Diablo 2 as an example. 
 I hope you enjoy it, and if you have any suggestions, feel free to send a message or comment here! 
    submitted by    /u/moisesdepaulodias  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Would anyone know of any information regarding esrb ratings?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iw48b/p_would_anyone_know_of_any_information_regarding/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iw48b/p_would_anyone_know_of_any_information_regarding/"/>
        <updated>2023-09-14T22:31:45.000Z</updated>
        <summary type="html"><![CDATA[Good Evening, I and some fellow students are taking a SE class and are looking for relevant information regarding esrb ratings and games for a research project. Does anyone know of any data pertaining to relevant esrb info that we can access? We have a few sources and are waiting to hear back from esrb themselves. Would anyone know if they permit web scraping or if there is a csv containing relevant information, or even perhaps an api we could use? Any information would help and thank you all for taking the time to read this. Thanks in advance!
    submitted by    /u/GOD_LIKE_WOW  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two guys with VERY different views on AI â€“Yuval Noah Harari and Mustafa Suleyman in conversation (full interview)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ivaym/two_guys_with_very_different_views_on_ai_yuval/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ivaym/two_guys_with_very_different_views_on_ai_yuval/"/>
        <updated>2023-09-14T21:59:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hauntedhivezzz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Ways to speed up llama-2 summarization on sagemaker?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iutyp/p_ways_to_speed_up_llama2_summarization_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iutyp/p_ways_to_speed_up_llama2_summarization_on/"/>
        <updated>2023-09-14T21:40:28.000Z</updated>
        <summary type="html"><![CDATA[I'm currently working on a project to give a quick summary of long articles/conversations.
 I'm running llama-2-7b-chat-hf with 4bit quantization on a g5.2xlarge instance on sagemaker.
 The method I'm using is map_reduce (option 2)from this webpage https://python.langchain.com/docs/use_cases/summarization)
 Of everything I've tried this is the only one that's been able to do decent summaries in a reasonable amount of time. However with really long articles (10,000+ words) it takes ~6 minutes before giving an output.
 I tried running this same thing on a g5.12xlarge instance which has 4 A10G gpus but it hasn't reduced the time by any noticeable amount.
 Is there anything else I could be doing to speed this up?
    submitted by    /u/Able_Body_9654  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] what prompt should i use with llama2 for context generative question answering?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16itt92/d_what_prompt_should_i_use_with_llama2_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16itt92/d_what_prompt_should_i_use_with_llama2_for/"/>
        <updated>2023-09-14T21:01:54.000Z</updated>
        <summary type="html"><![CDATA[so as the question states, i want to use llama2 to generate an answer for the question based on the context (or the article for more precision), no finetuning is needed, just want to predict the answer, but i can't find what's the right prompt i should use to get a well structured answer.
 my dataset contains two columns, one for articles, and the other column is for the question,
 Example:
 context: article talking about world war 2.
 question : based on the text, describe how the ww2 had started, and what were the political effects on Europe?
    submitted by    /u/kaoutar-  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Coqui releases XTTS an open-access foundational Voice Cloning model!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iswh9/p_coqui_releases_xtts_an_openaccess_foundational/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iswh9/p_coqui_releases_xtts_an_openaccess_foundational/"/>
        <updated>2023-09-14T20:26:09.000Z</updated>
        <summary type="html"><![CDATA[There's a new open-access foundational audio model in town!
 Standing on the shoulders of TorToiSe TTS - XTTS allows cross-language and multi-lingual speech generation with just 3 lines of code ðŸ¸
 Key facts about the model: 1. Supports 13 languages. 2. Voice cloning with just a 3-second audio clip. 3. Emotion and style transfer by cloning. 4. Cross-language voice cloning.
 Try it out on HF Hub: https://huggingface.co/spaces/coqui/xtts
    submitted by    /u/vaibhavs10  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-device content distillation with graph neural networks]]></title>
        <id>http://blog.research.google/2023/09/on-device-content-distillation-with.html</id>
        <link href="http://blog.research.google/2023/09/on-device-content-distillation-with.html"/>
        <updated>2023-09-14T19:39:00.034Z</updated>
        <summary type="html"><![CDATA[Posted by Gabriel Barcik and Duc-Hieu Tran, Research Engineers, Google Research




In today's digital age, smartphones and desktop web browsers serve as the primary tools for accessing news and information. However, the proliferation of website clutter â€” encompassing complex layouts, navigation elements, and extraneous links â€” significantly impairs both the reading experience and article navigation. This issue is particularly acute for individuals with accessibility requirements.



To improve the user experience and make reading more accessible, Android and Chrome users may leverage the Reading Mode feature, which enhances accessibility by processing webpages to allow customizable contrast, adjustable text size, more legible fonts, and to enable text-to-speech utilities. Additionally, Anâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does C++ in mujoco have benefits over python for reinforced learning?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16irjth/does_c_in_mujoco_have_benefits_over_python_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16irjth/does_c_in_mujoco_have_benefits_over_python_for/"/>
        <updated>2023-09-14T19:32:13.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to build a humanoid model and then train it to perform some tasks , I have decided to go with mujoco for the simulation and now I'm wondering if I should use the C++ API or the python one. the python implementation says it uses C API but is it good? Also if it's slower than the c++ one how slow is it? I'll probably have to make something real time and hence can't compromise much on the speed, but if it's only and small amount it's acceptable.
 would really appreciate some guidance in this matter
 thank you
    submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT, tell the jury that Trump waiting three hours to call off his mob is overwhelming and irrefutable evidence that he planned and conducted the January 6th insurrection]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ir89k/chatgpt_tell_the_jury_that_trump_waiting_three/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ir89k/chatgpt_tell_the_jury_that_trump_waiting_three/"/>
        <updated>2023-09-14T19:18:57.000Z</updated>
        <summary type="html"><![CDATA[I asked ChatGPT to talk with Trump's jury about the 3-hour delay. Not bad.
 ChatGPT:
 Ladies and gentlemen of the jury, today, we present a compelling case that the three-hour delay in President Trump's response to the events of January 6th is, without a doubt, overwhelming and irrefutable evidence of his planning and conducting the insurrection that unfolded before our eyes.
 First, consider the timeline. As the violence escalated, President Trump did not immediately call for an end to the chaos. This delay allowed the mob to wreak havoc, endanger lives, and damage our democracy. A leader's duty is to protect the nation and its citizens, and a three-hour wait to act is inexplicable unless there was intent behind it.
 Second, let's talk about intent. In criminal law, intent matters. It's the difference between an accident and a crime. A deliberate delay in response to an unfolding insurrection suggests a guilty mind. President Trump had the power and the responsibility to intervene immediately. Yet, he chose not to, allowing the violence to escalate.
 Furthermore, we must consider his words leading up to and during the events of January 6th. His rhetoric could be interpreted as encouragement for his supporters to take matters into their own hands. This, coupled with the delay, paints a picture of a leader who not only knew what was happening but may have orchestrated it.
 Ladies and gentlemen, we are not here to pass judgment but to seek justice. The evidence of a three-hour delay in responding to the insurrection, along with the surrounding circumstances, presents a compelling case that President Trump planned and conducted this insurrection. It is our duty to hold individuals accountable for their actions, regardless of their position.
    submitted by    /u/Georgeo57  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How good a crime sleuth and judge is today's AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16iqau7/how_good_a_crime_sleuth_and_judge_is_todays_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16iqau7/how_good_a_crime_sleuth_and_judge_is_todays_ai/"/>
        <updated>2023-09-14T18:41:58.000Z</updated>
        <summary type="html"><![CDATA[How would today's AI perform as a crime investigator or judge? To find out, I asked Bard the following question:
 The film Social Network suggests that Mark Zuckerberg stole his idea for Facebook from two brothers who hired him to make an app for the idea. How likely is this to be true?
 Bard seemed informed and sharp:
 "The film The Social Network is a dramatization of the founding of Facebook, and as such, it takes some liberties with the truth. However, the film does accurately portray the fact that Mark Zuckerberg was sued by the Winklevoss twins, Cameron and Tyler, who claimed that he stole their idea for a social networking website called HarvardConnection.
 The Winklevoss twins allege that they met with Zuckerberg in December 2003 to discuss their idea for HarvardConnection, and thaâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which AI to use for this idea?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ipk9h/which_ai_to_use_for_this_idea/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ipk9h/which_ai_to_use_for_this_idea/"/>
        <updated>2023-09-14T18:13:01.000Z</updated>
        <summary type="html"><![CDATA[Hi there! I'm participating in a Hackathon at work and am wondering if this community can give me some direction. Every year our company receives mandates / updates from different parties. In turn, our development teams have to parse through heavy documentation to figure out what needs to change in our code. Ingesting the data is what takes the longest. Our goal is to feed the mandates documentation through an AI and have it return what is needed to be changed in our code. For example, something might say field 200 now needs to include a 6 digit date format vs the 4 digit date format we've had in years past. We have secured a license for Azure AI but honestly no idea if that is the right AI to use. I youtubed a bunch of videos on document processing but I'm also not sure if that is what we are trying to do. Any advice on this is much appreciated.
    submitted by    /u/HillyjoKokoMo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Large Language Models for Compiler Optimization - MetaAi 2023 - Autotuner needs 949 CPU-days to achive nearly the same as this approach in 1shot!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ioxjm/r_large_language_models_for_compiler_optimization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ioxjm/r_large_language_models_for_compiler_optimization/"/>
        <updated>2023-09-14T17:48:05.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.07062 
 Abstract:
  
We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.
 We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time. 
  
https://preview.redd.it/f9c7kh7bd9ob1.jpg?width=1530&format=pjpg&auto=webp&s=287fffa714936da9b9a5141b7e01609942416156
 https://preview.redd.it/z4a0ce7bd9ob1.jpg?width=1537&format=pjpg&auto=webp&s=e6275b2b53fa6f431b87940784629b3270c656f9
 https://preview.redd.it/89toie7bd9ob1.jpg?width=750&format=pjpg&auto=webp&s=9a71bdb2eeeff52b2f8bbb3cf2b678debcd4a060
 https://preview.redd.it/0krmqd7bd9ob1.jpg?width=1536&format=pjpg&auto=webp&s=ba3fade0883ee621b185fabc67839db42ea74a53
 https://preview.redd.it/8nz00i7bd9ob1.jpg?width=1198&format=pjpg&auto=webp&s=6ddbddf68311f576fbf3c52a47381316feace8c9
 â€‹
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a classification pipeline with Amazon Comprehend custom classification (Part I)]]></title>
        <id>60ba0e07738ad3e7fc6a1a59aa323e2926cb5ba2</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-classification-pipeline-with-amazon-comprehend-custom-classification-part-i/"/>
        <updated>2023-09-14T16:58:16.000Z</updated>
        <summary type="html"><![CDATA[In first part of this multi-series blog post, you will learn how to create a scalable training pipeline and prepare training data for Comprehend Custom Classification models. We will introduce a custom classifier training pipeline that can be deployed in your AWS account with few clicks.]]></summary>
        <author>
            <name>Yanyan Zhang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tune Falcon 7B and other LLMs on Amazon SageMaker with @remote decorator]]></title>
        <id>79e9de80c1ca72d14b38bb721ea7c350683f1306</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/fine-tune-falcon-7b-and-other-llms-on-amazon-sagemaker-with-remote-decorator/"/>
        <updated>2023-09-14T16:53:48.000Z</updated>
        <summary type="html"><![CDATA[Today, generative AI models cover a variety of tasks from text summarization, Q&A, and image and video generation. To improve the quality of output, approaches like n-short learning, Prompt engineering, Retrieval Augmented Generation (RAG) and fine tuning are used. Fine-tuning allows you to adjust these generative AI models to achieve improved performance on your domain-specific [â€¦]]]></summary>
        <author>
            <name>Bruno Pistone</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simplify access to internal information using Retrieval Augmented Generation and LangChain Agents]]></title>
        <id>f373585d90ca2452a8f575b61523a3b5d9993d14</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/simplify-access-to-internal-information-using-retrieval-augmented-generation-and-langchain-agents/"/>
        <updated>2023-09-14T16:47:56.000Z</updated>
        <summary type="html"><![CDATA[This post takes you through the most common challenges that customers face when searching internal documents, and gives you concrete guidance on how AWS services can be used to create a generative AI conversational bot that makes internal information more useful. Unstructured data accounts for 80% of all the data found within organizations, consisting of [â€¦]]]></summary>
        <author>
            <name>Abhishek Maligehalli Shivalingaiah</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Searching for discussion about chunking algorithms and strategy]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16in6cd/d_searching_for_discussion_about_chunking/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16in6cd/d_searching_for_discussion_about_chunking/"/>
        <updated>2023-09-14T16:39:24.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone! 
 I'm still experiencing with my own implementation of rag, and I deployed my custom chunking function (honestly don't like the methods on LangChain) . 
 Anyway, I'm searching for alternative methods, algoritms (NLP or not) and models... There are lots of info and different implementation on RAG, but as I can see noone put much effort to augment chunking quality. 
 Also, there are other approach than this one I'm currently using? bi-encoder (instructor) - > cross-encoder (reranking) - > LLM 
 Can someone share some resources, repo, lib or existing implementation of different chunking methods? (or simply discuss here some idea, though or approach) 
 Thanks in advance for you time!!
    submitted by    /u/Distinct-Target7503  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yocto, ROS 2, and Hardware Acceleration: A Production-Grade Trio for Robotics]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16imm3n/yocto_ros_2_and_hardware_acceleration_a/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16imm3n/yocto_ros_2_and_hardware_acceleration_a/"/>
        <updated>2023-09-14T16:17:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pablocarrera  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chinese Remainder Theorem synthesis algorithm]]></title>
        <id>https://www.johndcook.com/blog/?p=208054</id>
        <link href="https://www.johndcook.com/blog/2023/09/14/garners-algorithm/"/>
        <updated>2023-09-14T16:07:56.000Z</updated>
        <summary type="html"><![CDATA[Suppose m = pq where p and q are large, distinct primes. In the previous post we said that calculations mod m can often be carried out more efficiently by working mod p and mod q, then combining the results to get back to a result mod m. The Chinese Remainder Theorem assures us that [â€¦]
Chinese Remainder Theorem synthesis algorithm first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaining efficiency by working modulo factors]]></title>
        <id>https://www.johndcook.com/blog/?p=208061</id>
        <link href="https://www.johndcook.com/blog/2023/09/14/crt-analysis/"/>
        <updated>2023-09-14T16:05:36.000Z</updated>
        <summary type="html"><![CDATA[Suppose m is a large integer that you are able to factor. To keep things simple, suppose m = pq where p and q are distinct primes; everything in this post generalizes easily to the case of m having more than two factors. You can carry out calculations mod m more efficiently by carrying out [â€¦]
Gaining efficiency by working modulo factors first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft at ACM SIGCOMM 2023: Innovating the future of networking]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=967503</id>
        <link href="https://www.microsoft.com/en-us/research/blog/microsoft-at-acm-sigcomm-2023-innovating-the-future-of-networking/"/>
        <updated>2023-09-14T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Modern applications heavily rely on robust network infrastructure, requiring continuous innovation. In this evolving landscape, Microsoft is at the forefront, spearheading innovation efforts in networking and strengthening the foundational network infrastructure that underpins the cloud ecosystem. By investing in and enhancing this critical infrastructure, Microsoft not only ensures the resilience and scalability of cloud services [â€¦]
The post Microsoft at ACM SIGCOMM 2023: Innovating the future of networking appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=967848</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-the-future-of-scale-with-ahmed-awadallah-and-ashley-llorens/"/>
        <updated>2023-09-14T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Whatâ€™s the driving force behind AIâ€™s recent, rapid progress? Research manager Ahmed Awadallah shares his insights on this, the two-stage approach to training large-scale models, and the need for better model evaluation in this episode of the #MSRPodcast.
The post AI Frontiers: The future of scale with Ahmed Awadallah and Ashley Llorens appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Gradient descent in regularized least squares]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iljbp/d_gradient_descent_in_regularized_least_squares/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iljbp/d_gradient_descent_in_regularized_least_squares/"/>
        <updated>2023-09-14T15:34:16.000Z</updated>
        <summary type="html"><![CDATA[The problem is obtained from Chapter 3 in Wright, Stephen J and Benjamin Recht (2022). Optimization for data analysis.
 Cambridge University Press
 I am solving the problem I attach and I have a doubt in section (f). I have solved all the sections (a)-(e).
 In section (e) I have obtained that I need
 $$
 k \geq \frac{\lambda_{\text{max}}\left(\frac{2}{N} A^TA + 2\mu I\right)}{\lambda_{\text{min}}\left(\frac{2}{N} A^TA + 2\mu I\right)}log((f(x^0)-f(x_\mu)/\epsilon).
 $$
 However in section (f) asks for a tight upper bound but I only can think about the following bound:
 $f(\hat{x}) \leq f_\mu(x_\mu) + ||\hat{x}||^2+ \epsilon$,
 which is very simple.
 Do you think that I can obtain the result in (e) to obtain another bond, or what would you do?
    submitted by    /u/ItsGauss  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Guide: Implementing ImageNet classification using Deep CNNs Paper.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ilhcd/p_guide_implementing_imagenet_classification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ilhcd/p_guide_implementing_imagenet_classification/"/>
        <updated>2023-09-14T15:31:59.000Z</updated>
        <summary type="html"><![CDATA[Need help on how to get started with implementing a research paper. I'm implementing the Imagenet classification task paper for my final year undergrad mini-project. Any advice is appreciated on how to get started? I have mid-level machine learning knowledge and am ready to pick the required concepts on the go. Please help. Thank you :)
 Link: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
    submitted by    /u/DrBeans0  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Use LLM to analyse and port software written in C (very long files)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16il8ef/d_use_llm_to_analyse_and_port_software_written_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16il8ef/d_use_llm_to_analyse_and_port_software_written_in/"/>
        <updated>2023-09-14T15:22:08.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm trying to figure out what is the best way to use LLMs to analyse a very old software entirely written in C.
 I've tried to to some basic prompts with ChatGPT and it seems to recognise the language.
 The situation is that I've many .c files with thousands of lines and with a lot of redundant code.
 Moreover, since there are a lot of data structure with variables names not easily understandable, I need to provide some context to allow the model to trying to understand what the code does.
 My worry is that providing all the needed info + the file itself (even considering 1 file at a time) I could consume all the model context and therefore not leave room for generating anything of value.
 Has anyone had the opportunity to face similar problems? Ideas?
 Cheers
 Alexio
    submitted by    /u/Alexioc  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are data science certifications the gateway to competitive pay?]]></title>
        <id>https://www.datasciencecentral.com/?p=63162</id>
        <link href="https://www.datasciencecentral.com/are-data-science-certifications-the-gateway-to-competitive-pay/"/>
        <updated>2023-09-14T15:10:15.000Z</updated>
        <summary type="html"><![CDATA[Working as a data scientist is the dream of many IT professionals these days. It is no secret that data science is a skyrocketing field attracting young professionals and inspiring many to switch careers to data science. On one front are young professionals who study their courses in colleges to pursue their dream of becomingâ€¦Â Read More Â»Are data science certifications the gateway to competitive pay?
The post Are data science certifications the gateway to competitive pay? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] MIT-IBM Watson AI Lab releases MoLM suite with three small sparse MoE models, the largest of which (8B params with 700M experts) performs on par with Pythia 2.8B while its throughput is comparable to Pythia 1.4B]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ikvt8/n_mitibm_watson_ai_lab_releases_molm_suite_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ikvt8/n_mitibm_watson_ai_lab_releases_molm_suite_with/"/>
        <updated>2023-09-14T15:07:54.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2306.04640
 GitHub: https://github.com/ibm/moduleformer (under Apache 2.0)
 Twitter thread: https://twitter.com/Yikang_Shen/status/1702041129267388678
 Abstract:
  
Large Language Models (LLMs) have achieved remarkable results. However, existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model, which requires domain-labeled data to learn domain-specific experts, ModuleFormer can iâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CUPED for starters: Enhancing controlled experiments with pre-experiment data]]></title>
        <id>https://www.datasciencecentral.com/?p=63132</id>
        <link href="https://www.datasciencecentral.com/cuped-for-starters-enhancing-controlled-experiments-with-pre-experiment-data/"/>
        <updated>2023-09-14T15:01:10.000Z</updated>
        <summary type="html"><![CDATA[CUPED: Improve Your A/B Testing - Detect Smaller Gains, Utilise Smaller Samples and Make Smarter Decisions!
The post CUPED for starters: Enhancing controlled experiments with pre-experiment data appeared first on Data Science Central.]]></summary>
        <author>
            <name>Igor Khomyanin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Searching for sustainability in data center cooling]]></title>
        <id>https://www.datasciencecentral.com/?p=63139</id>
        <link href="https://www.datasciencecentral.com/searching-for-sustainability-in-data-center-cooling/"/>
        <updated>2023-09-14T14:58:36.000Z</updated>
        <summary type="html"><![CDATA[Data centers are known for their impact on the environment. They run 24/7 and exude a lot of heat. Massive warehouses full of hot technology require advanced cooling systems or an HVAC system pushed to its limit.Â  Data center managers and sustainability leaders no longer settle for antiquated techniques. Theyâ€™re striving to develop greener andâ€¦Â Read More Â»Searching for sustainability in data center cooling
The post Searching for sustainability in data center cooling appeared first on Data Science Central.]]></summary>
        <author>
            <name>Jane Marsh</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a meeting assistant]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ikloc/looking_for_a_meeting_assistant/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ikloc/looking_for_a_meeting_assistant/"/>
        <updated>2023-09-14T14:56:34.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for a program that will transcribe live audio playing from my computer (windows).
 Do you know something like that? I've seen Buzz (https://chidiwilliams.github.io/buzz/docs/usage), but it needs an audio loopback driver in order to work, so I wonder if there are others.
 ðŸš€ Bonus points if it recognizes different people talking.
 ðŸš€ Extra bonus points if it can transcribe multiple languages.
    submitted by    /u/AleHoju  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative visual knowledge graph modeling at the system level]]></title>
        <id>https://www.datasciencecentral.com/?p=63136</id>
        <link href="https://www.datasciencecentral.com/collaborative-visual-knowledge-graph-modeling-at-the-system-level/"/>
        <updated>2023-09-14T14:52:10.000Z</updated>
        <summary type="html"><![CDATA[The best way to model business and consumer dynamics is collaboratively, with stakeholders all in the same virtual room contributing. Of course, this has been happening asynchronously for some time now, but the potential exists for more real-time interaction.Â  Modelers donâ€™t work in a vacuum, of course. The iterations between a modeler who develops aâ€¦Â Read More Â»Collaborative visual knowledge graph modeling at the system level
The post Collaborative visual knowledge graph modeling at the system level appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assigning inbalanced labels to "Other" class in scikitlearn [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ijc89/assigning_inbalanced_labels_to_other_class_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ijc89/assigning_inbalanced_labels_to_other_class_in/"/>
        <updated>2023-09-14T14:03:28.000Z</updated>
        <summary type="html"><![CDATA[Hey there,
 I wasn't doing any ml in some time and forgot basics. I was thinking that you may help me. So I trained svc model on small dataset (around 1400 unical records). I have 13 classes, which are badly distributed in the training set and inbalanced. 3 classes took around 80% of all. What the business wants is keep the 3 classes and categorize the rest as "Other" class. In the future they may be able to generate more training data for the remaining labels.
 How should I do it? I know I can assign everytning to "Other" class, before training with simple if then formula, but dont know if this is the right approach. Any sugestions? I know this may be some case of 1 vs all case, but don't know which exactly.
 Thanks in advance for any help.
    submitted by    /u/th00masml  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] The ML Papers That Rocked Our World (2020-2023)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ij18f/d_the_ml_papers_that_rocked_our_world_20202023/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ij18f/d_the_ml_papers_that_rocked_our_world_20202023/"/>
        <updated>2023-09-14T13:50:27.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone! ðŸ‘‹
 Iâ€™ve been on a bit of a deep-dive lately, trying to catch up on all the awesome stuff thatâ€™s been happening in the ML space. It got me wondering, from 2020 to 2023, what have been the absolute must-read papers that shook the foundations and got everyone talking?
 Whether itâ€™s something that reinvented the wheel in your specific niche or just made waves industry-wide, I wanna hear about it!
 Iâ€™m curious to see how different the responses will be, and hey, this might even become a go-to list for anyone looking to get the lowdown on the hottest trends and discoveries of the past few years.
 Canâ€™t wait to hear your thoughts!
 tl;dr
 I decided to aggregate your best suggestions into categories for anyone interested in reading them without searching through the whole comment seâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shout at the Devil: Capcomâ€™s â€˜Devil May Cry 5â€™ Joins GeForce NOW]]></title>
        <id>https://blogs.nvidia.com/?p=66903</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/14/geforce-now-thursday-sep-14/"/>
        <updated>2023-09-14T13:00:20.000Z</updated>
        <summary type="html"><![CDATA[GFN Thursday is downright demonic, as Devil May Cry 5 comes to GeForce NOW. Capcomâ€™s action-packed third-person brawler leads 15 titles joining the GeForce NOW library this week, including Gears Tactics and The Crew Motorfest. Itâ€™s also the last week to take on the Ultimate KovaaKâ€™s Challenge. Get on the leaderboard today for a chance Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Llama2 inference in a single file of pure Mojo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ih82w/p_llama2_inference_in_a_single_file_of_pure_mojo/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ih82w/p_llama2_inference_in_a_single_file_of_pure_mojo/"/>
        <updated>2023-09-14T12:32:42.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone!
 I was really excited that Mojo became publicly available and thinking which project can I implement to learn Mojo concepts. Since I have already ported llama2.c to pure Python, I decided why not try to port llama2.py to Mojo now.. And here is what I got
 First round of llama2.c vs llama2.ðŸ”¥ battle. Mojo demonstrated 20% better performance than C in a single threaded execution of llama2 inference and 250x times better performance than Python
 https://i.redd.it/0gcwwfc2r7ob1.gif
 For reference Mojo is using SIMD vectorization, that's why it's performing great for matmul operations. In the other hand, it turned out that gcc also aggressively optimizes all for loops it can find, so I suggest this benchmark is pretty fair.
 â€‹
 Mojo natively supports SIMD vectorizations out of the box
 gcc aggressively vectorizing loops
 â€‹
 After that I decided to compare both solutions in multi-threaded (parallelized) mode, and now `llama2.c` strike back with help of OMP demonstrating 20% better performance than Mojo
 â€‹
 https://i.redd.it/gwymffods7ob1.gif
 I hope this post will be useful for all Machine Learning engineers/enthusiasts/students out there, ensuring we're up-to-date with Modular/Mojo's game-changing AI tech developments.
 Stay informed and ahead of the curve!
 Links
 llama2.ðŸ”¥: https://github.com/tairov/llama2.mojo
 llama2.python: https://github.com/tairov/llama2.py
 llama2.c: https://github.com/karpathy/llama2.c
 Modular_AI repost in Twitter: https://twitter.com/tairov/status/1701345271752343900
 feel the magic on HF: https://huggingface.co/spaces/radames/Gradio-llama2.mojo
    submitted by    /u/Albatross9855  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Training an LLM Model: AWS p3.2xlarge EC2 instance vs. Multiple RTX 4090s at Home?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16igzy7/d_training_an_llm_model_aws_p32xlarge_ec2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16igzy7/d_training_an_llm_model_aws_p32xlarge_ec2/"/>
        <updated>2023-09-14T12:22:30.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I'm currently at a crossroads with a decision that I believe many in this community might have faced or will face at some point: Should I use cloud-based GPU instances like AWS's p3.2xlarge EC2 (with Tesla V100) or invest in building a high-performance rig at home with multiple RTX 4090s for training a large language model?
 Context: I run a startup and we're currently fine-tuning an open source LLM, and the computational demands are of course high. We want to make an informed choice between using AWS's offerings or setting up a high-performance system at home to start.
 Cloud Option: AWS p3.2xlarge EC2
  
Cost: Approximately $3.06 per hour.
 Specifications: One Tesla V100 GPU, 8 vCPUs, 61 GiB RAM.
 Pros: Scalability, reliability, specialized software optimizations.
 Cons: Recurring costs, potential limitations on customization.
  
Home Rig Option: Multiple RTX 4090s
  
Cost: Around $1,600 for each 4090, but I'd own them.
 Specifications: Even higher TFLOPs than a V100, and memory isn't a constraint (24GB per card).
 Pros: One-time investment, flexibility, potentially higher raw computational power.
 Cons: Need to handle cooling, power, and system integration myself
  
I'd love to hear your thoughts, experiences, and recommendations. Here are some specific questions:
  
Performance: How many RTX 4090s would roughly equal the computational power of an AWS p3.2xlarge instance for ML tasks?
 Cost-Effectiveness: Given that we're a startup with limited resources, does it make more financial sense to invest upfront in hardware?
 Reliability and Maintenance: For those who have run multi-GPU setups at home, how reliable are they, and what maintenance work is required?
 Other Considerations: Are there factors I haven't considered that you think are critical?
  
Thank you in advance for sharing your insights!
    submitted by    /u/devolvedai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group theory and RSA encryption]]></title>
        <id>https://www.johndcook.com/blog/?p=208013</id>
        <link href="https://www.johndcook.com/blog/2023/09/14/rsa-group-theory/"/>
        <updated>2023-09-14T11:57:26.000Z</updated>
        <summary type="html"><![CDATA[RSA encryption a map from numbers mod n to numbers mod n where n is a public key. A message is represented as an integer m and is encrypted by computing c = me mod n where e is part of the public key. In practice, e is usually 65537 though it does not have [â€¦]
Group theory and RSA encryption first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Scaling Data-Constrained Language Models - Hugging Face et al. 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ifl84/r_scaling_dataconstrained_language_models_hugging/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ifl84/r_scaling_dataconstrained_language_models_hugging/"/>
        <updated>2023-09-14T11:11:44.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2305.16264
 GitHub: https://github.com/huggingface/datablations
 License:
  
All models & code are licensed under Apache 2.0. Filtered datasets are released with the same license as the datasets they stem from.
  
Abstract:
  
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at this https URL.
  
â€‹
 https://preview.redd.it/ahzyonnqe7ob1.png?width=1015&format=png&auto=webp&s=0e0cb4051e390ea23440cd61bfc0bbf5fce83bb7
 https://preview.redd.it/l6a81onqe7ob1.png?width=1014&format=png&auto=webp&s=a36b74cbb510a1f753ef1b891531bb36ab643246
 https://preview.redd.it/yyu8h0oqe7ob1.png?width=1001&format=png&auto=webp&s=047cb2bb1932c6215cea0c30e22fd9bbe60391a8
 https://preview.redd.it/xskcytnqe7ob1.png?width=1007&format=png&auto=webp&s=4090e92dd9eacb377840327bb7d0ae69ff752b52
    submitted by    /u/InterviewIntrepid889  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Gradio on the same server but different ports]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ifgyw/d_gradio_on_the_same_server_but_different_ports/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ifgyw/d_gradio_on_the_same_server_but_different_ports/"/>
        <updated>2023-09-14T11:05:06.000Z</updated>
        <summary type="html"><![CDATA[On my team they are using Gradio for LLM applications, etc. When running multiple instances of Gradio applications on the same server, but on different ports, opening a new session is causing an error and closing the previous session, "error" is written in the chat. The logs I found were like "Invalid HTTP request received" and "max retries exceeded". For me, Gradio is supposed to work as a demonstration and not as a scalable product, but they are using it that way and I thought that the problem could be precisely that. But if not, does anyone have any idea what could be going on?
 No meu time estÃ£o utilizando gradio para aplicaÃ§Ãµes de LLMs, etc. Ao executar mÃºltiplas instÃ¢ncias de aplicaÃ§Ãµes Gradio no mesmo servidor, mas em portas diferentes, a abertura de uma nova sessÃ£o estÃ¡ causando erro e encerrando a sessÃ£o anterior, fica "erro" escrito no chat. Os logs que encontrei eram como "Invalid HTTP request received" e "max retries exceeded". Para mim, o Gradio Ã© pra funcionar como demonstraÃ§Ã£o e nÃ£o em forma de produto escalÃ¡vel, mas estÃ£o utilizando assim e pensei que o problema poderia ser justamente isso. Mas caso nÃ£o, alguÃ©m tem alguma ideia do que pode estar acontecendo?
    submitted by    /u/Magic_squirrel_hat  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I signed up for a debate on ai.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ieydv/i_signed_up_for_a_debate_on_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ieydv/i_signed_up_for_a_debate_on_ai/"/>
        <updated>2023-09-14T10:36:54.000Z</updated>
        <summary type="html"><![CDATA[So today I signed up for a debate on ai. Wheather ai is beneficial or dangerous to human beings. I have the freedom to choose any side. This debate will be watched by about 130 people, all cs freshmen (mind you, also my first time speaking in front of this many people). Now, I'm confident I know more about ai than an average person but I need your help in preparing properly. Which side do I take and what are all the points I should keep in mind? It's 4 pm here and the debate is tomorrow. Any help will be appreciated. Thank you.
    submitted by    /u/CalmGuy69  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RSA encrypted messages that cannot be decrypted]]></title>
        <id>https://www.johndcook.com/blog/?p=207999</id>
        <link href="https://www.johndcook.com/blog/2023/09/14/rsa-decrypted/"/>
        <updated>2023-09-14T10:15:35.000Z</updated>
        <summary type="html"><![CDATA[Not all messages encrypted with the RSA algorithm can be decrypted. This post will show why this is possible and why it does not matter in practice. RSA in a nutshell RSA encryption starts by finding two large primes, p and q. These primes are kept secret, but their productÂ n = pq is made public. [â€¦]
RSA encrypted messages that cannot be decrypted first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Compilation of non-open source AI models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16iecpt/r_compilation_of_nonopen_source_ai_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16iecpt/r_compilation_of_nonopen_source_ai_models/"/>
        <updated>2023-09-14T10:01:40.000Z</updated>
        <summary type="html"><![CDATA[Hi, does anyone have a compiled list of non-open source AI models that can be used for MVP building? 
    submitted by    /u/Compound_Group  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence May Be Humanityâ€™s Most Ingenious Inventionâ€“and Its Last?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16idzsi/artificial_intelligence_may_be_humanitys_most/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16idzsi/artificial_intelligence_may_be_humanitys_most/"/>
        <updated>2023-09-14T09:39:20.000Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) has the potential to solve the world's problems or destroy humanity.
 
It is being developed by a few hundred individuals in Silicon Valley, and we have little say in its ethics or existence.
 
AI has already demonstrated creative abilities in painting, writing, and music.
 
It is also being used in drug discovery, therapy, dating apps, and misinformation in politics.
 
The rapid adoption of AI raises concerns about job displacement and the potential for catastrophic events.
 
Experts predict a significant chance of AI causing a catastrophe or even wiping out humanity.
 
 Source : https://www.vanityfair.com/news/2023/09/artificial-intelligence-industry-future
 Summarized by Nuse AI
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI Consumer Landscape by a16z]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16iaxda/generative_ai_consumer_landscape_by_a16z/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16iaxda/generative_ai_consumer_landscape_by_a16z/"/>
        <updated>2023-09-14T06:36:55.000Z</updated>
        <summary type="html"><![CDATA[In less than a year since ChatGPT was introduced, it has become the fastest consumer application to register 100 million monthly active users. But how are consumers using other GenAI products apart from ChatGPT? An a16z Consumer report examines the top 50 GenAI web products (based on SimilarWeb data) to find out.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/f0kh5qag16ob1.png?width=2058&format=png&auto=webp&s=1cab11a31d384c068912c9fca32a000393b795d5
  
Proprietary models are dominating: 80% of the top 50 GenAI products didnâ€™t exist a year agoâ€”suggesting many of the most compelling consumer experiences are completely novel. Interestingly, 48% of these are bootstrappedâ€”with no outside funding.
 ChatGPT holds a massive lead: ChatGPT alone accounts for 60% of the entire list's monthly traffic, with roughly 1.6 billion visits and 200 million monthly users as of June 2023.
 LLM assistants are dominating: LLMs, including Googleâ€™s Bard and Quoraâ€™s Poe, constitute 68% of total consumer traffic to the top 50. The other categories seeing significant traffic are AI companions and content-generation tools.
 GenAI marketing is mostly organic: Marketing for most of these products has been reliant on referrals, word of mouth, and other traditional marketing as they enter the market. About 90% of these companies are already monetizing, and most do so via a subscription model.
 GenAI and mobile adaptability: Given the extensive consumer time spent on mobile, an increase in mobile-first GenAI products is expected as the technology evolves.
  
(source)
 P.S. If you like this type of analysis, sign up for my free newsletter that deciphers the fastest-moving news and research in AI and tech. Professionals from Google, Meta, and OpenAI are already on board.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/14/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ianu2/oneminute_daily_ai_news_9142023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ianu2/oneminute_daily_ai_news_9142023/"/>
        <updated>2023-09-14T06:21:03.000Z</updated>
        <summary type="html"><![CDATA[Adobeâ€™s Firefly generative AI tools are now widely available.[1]
 Stability AI, gunning for a hit, launches an AI-powered music generator.[2]
 Elon Musk warned of civilizational risks posed by artificial intelligence. Sundar Pichai of Google highlighted the technologyâ€™s potential to solve health and energy problems. And Mark Zuckerberg of Meta stressed the importance of open and transparent A.I. systems.[3] 
 German military plows millions into AI â€˜environmentâ€™ for weapons tests that could change combat forever.[4]
 Amazon launches generative AI to help sellers write product descriptions.[5]
  
Sources:
 [1] https://www.theverge.com/2023/9/13/23871537/adobe-firefly-generative-ai-model-general-availability-launch-date-price
 [2] https://techcrunch.com/2023/09/13/stability-ai-gunning-for-a-hit-launches-an-ai-powered-music-generator/
 [3] https://www.nytimes.com/2023/09/13/technology/silicon-valley-ai-washington-schumer.html
 [4] https://www.foxnews.com/world/german-military-plows-millions-ai-environment-weapons-tests-change-combat
 [5] https://www.aboutamazon.com/news/small-business/amazon-sellers-generative-ai-tool 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A pose-mapping technique could remotely evaluate patients with cerebral palsy]]></title>
        <id>https://news.mit.edu/2023/pose-mapping-technique-cerebral-palsy-patients-0914</id>
        <link href="https://news.mit.edu/2023/pose-mapping-technique-cerebral-palsy-patients-0914"/>
        <updated>2023-09-14T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[The machine-learning method works on most mobile devices and could be expanded to assess other motor disorders outside of the doctorâ€™s office.]]></summary>
        <author>
            <name>Jennifer Chu | MIT News</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic pricing environments for RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16i7r1k/algorithmic_pricing_environments_for_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16i7r1k/algorithmic_pricing_environments_for_rl/"/>
        <updated>2023-09-14T03:39:05.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am looking for environments to test out some ideas I have for algorithmic pricing. By algorithmic pricing environments, I mean there are multiple competing algorithms trying to maximize profits. 
 I can't really find any out of the box implementations. There are trading environments but those are not what I am looking for.
 Any help would be appreciated, thanks. 
    submitted by    /u/Next_Gap8224  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Communicative/Collaborative Agents hybrids]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16i691d/d_communicativecollaborative_agents_hybrids/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16i691d/d_communicativecollaborative_agents_hybrids/"/>
        <updated>2023-09-14T02:23:53.000Z</updated>
        <summary type="html"><![CDATA[I had a Claude based LLM analyze the strengths and weaknesses of the MetaGPT, ChatDev, AoT, and brain inspired algorithms papers, seeking ways to combine the strengths of two or more of the methods. "Here are some specific examples of how the methods in the Algorithm of Thoughts (AoT), brain-inspired algorithms, MetaGPT, and ChatDev papers could be combined to improve multi-agent systems:
  
MetaGPT could incorporate longer, more elaborate algorithmic examples from AoT into its prompts to guide the LLM's reasoning process. For example, in solving math word problems, the prompt could provide a 4-5 step worked example walking through unpacking the problem statement, setting up equations, solving, and checking the solution. This mirrors AoT's more extensive algorithm narratives.
 
ChatDev's aâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some confusion about using mocap in Mujoco]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16i5zc9/some_confusion_about_using_mocap_in_mujoco/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16i5zc9/some_confusion_about_using_mocap_in_mujoco/"/>
        <updated>2023-09-14T02:11:13.000Z</updated>
        <summary type="html"><![CDATA[Hi!
 Recently, I tried to follow fetch_pick_and_place.env in gymnasium_robotics to build a similar environment with Franka.
 I found that the core of this implementation is to use the mocap to control the end-effector, and then mocap derives joint angles using the built-in inverse kinematics algorithm.
 For the fetch_pick_and_place.env, mocap does not cause mutations and oscillations in configuration space. However, when I use mocap to control Franka, oscillations in joint space occur frequently, although I've minimized the step size of the mocap to ensure that the movement of the end-effector in Cartesian space is minor. Fetch and Franka are both redundant arms, I don't know why there is such a big difference in mocap performance.
 Here is the video to illustrate the above phenomenon
 Franka
 I've opened issues on mujoco and gymnasium robotics repositories, but it didn't initiate any discussion.
 Any help would be appreciated! Thanks!
 â€‹
    submitted by    /u/UpperSearch4172  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm very new in this field (Prompt Engineering) and have a handful of questions, any advice and thoughts are welcome!!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16i4bou/im_very_new_in_this_field_prompt_engineering_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16i4bou/im_very_new_in_this_field_prompt_engineering_and/"/>
        <updated>2023-09-14T00:53:02.000Z</updated>
        <summary type="html"><![CDATA[My experience in CS, coding, and programming is very minimal. I understand general concepts but only through the lens of a degree in physics that required I mess around with WolframAlpha sometimes (which I really enjoyed). I've been considering getting a second degree in CS or something related but want to find a clear(ish) path before committing to it. I would love to hear any related thoughts as well!
 â€‹
  
Prompt Engineering seems like a pretty fresh field of study. Is it up and coming as a career path?
 People that specialize in this, what field(s) did you come from and how would you recommend diving into it?
 Considering my experience, would I be able to actually find work, freelancing or employed?
 How on earth do I get started in this world? It seems so insanely big and complicated but I am just fascinated by the idea of using written dialogue to manipulate the output of an LLM!
 From my very high overview of PE and the recent advances in AI, PE as a field of study and interest is going to expand exponentially, is that accurate?
  
   submitted by    /u/Top_Room_6714  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] OpenGL Library Error]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16i1ewr/d_opengl_library_error/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16i1ewr/d_opengl_library_error/"/>
        <updated>2023-09-13T22:46:23.000Z</updated>
        <summary type="html"><![CDATA[Has anyone ever encountered this error while working with the OpenGL library?
 raise ImportError("Unable to load OpenGL library", *err.args) ImportError: ('Unable to load OpenGL library', "Could not find module 'OSMesa' (or one of its dependencies)
 I have done pip install opengl and pyopengl, but it doesn't seem to solve the problem.
 Specifically, I am trying to run this code: https://github.com/brjathu/LART
    submitted by    /u/BigDreamx  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Wandb remote agent source code managing]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16i0llq/d_wandb_remote_agent_source_code_managing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16i0llq/d_wandb_remote_agent_source_code_managing/"/>
        <updated>2023-09-13T22:13:25.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I'm new to machine learning and have decided to use a combination of Stable baselines3 and Wandb. I'm at the point where I'm running sweeps using Wandb and want to utilize another PC I have laying around to run agents on.
 What is the best way to get my python code for the agent to run onto the spare PC? I know I can manually load copy the code over, but I'm looking for a more efficient method that will maintain any changes made to the source code. Maybe packaging up the python code within the sweep and having the agent download and execute it? I'm not all too familiar with possibilities and limits of trying to achieve this so any and all input is appreciated. Thanks!
    submitted by    /u/chip_fork  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[â€ŽThe Economist Podcasts: Babbage: Mustafa Suleyman on how to prepare for the age of AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16i09c7/the_economist_podcasts_babbage_mustafa_suleyman/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16i09c7/the_economist_podcasts_babbage_mustafa_suleyman/"/>
        <updated>2023-09-13T22:00:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/siiilverrsurfer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Mean scores or appending all the predictions in cross-validation for model performance evaluation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16i08hr/d_mean_scores_or_appending_all_the_predictions_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16i08hr/d_mean_scores_or_appending_all_the_predictions_in/"/>
        <updated>2023-09-13T21:59:37.000Z</updated>
        <summary type="html"><![CDATA[I have this question that I cannot seem to settle in my head. All papers that I read, report the average (std) performance results across each folds when they report F1, Precision, etc.. Somebody that I highly trust in ML (somebody with a PhD in the field) was reporting the results after saving all the predicted labels (y_pred) and actual labels (y_true) to a list and compute the F1 score one time with the pooled predictions.
 I now am working on a dataset (binary classification) and trying to validate my model using leave-one-subject-out CV (Some people in my dataset have more lines than others). When I take the average of all iterations I get poor results (F1 score= 0.5), but if I pool all the predictions and compute the F1 score at the end, I achieve decent performance (F1 score =0.7). So, in my project, it is in my best interest to use the second approach, and somebody that I trust tells me that it is okay to do this approach. But I cannot seem to find a paper that says that this approach is acceptable or good. What do you guys think and do you have any suggestions?
    submitted by    /u/enthusiastic31  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dystopia AI Movie]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hzcgb/dystopia_ai_movie/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hzcgb/dystopia_ai_movie/"/>
        <updated>2023-09-13T21:25:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generic Multi-Agent Scenario]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16hz4vu/a_generic_multiagent_scenario/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16hz4vu/a_generic_multiagent_scenario/"/>
        <updated>2023-09-13T21:17:46.000Z</updated>
        <summary type="html"><![CDATA[I was thinking of some major challenges in modeling a generic real-world environment. Some of them are: adaptive agents, uncertain intentions, and lack of common knowledge.
 However, most of the papers I see on RL make some assumption or other violating one or more of these, like considering simple agents, assuming known knowledge about others' intentions, and considering that the models of other agents are known when irl an agent hardly has a model of other agents it interacts with apriori. 
 Consider an airport scenario where agents are trying to get into respective planes within a given time, and the gates to each plane allow one person at a time. Looking at the scenario from the view of a single agent, they know what they want, but they can't really make any assumptions about the intentions, strategy, and complexity of other agents beforehand. These other agents can be neutral or adversarial (competing for getting in the same plane) from the agent's viewpoint. All they can see is a restricted view of the motions of some of the other agents.
 What would you say could model and provide a solution in such a scenario? It is to be noted that other agents can change their strategies based on actions taken by you till now, and so can you. Due to having incomplete information, I fail to see the notion of an equilibrium, and the agents needn't be fully rational as well.
    submitted by    /u/Quirky_Concoction  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Looking for Efficient Encoding Methods for Java Method Names for Downstream Classification Task]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hz47b/p_looking_for_efficient_encoding_methods_for_java/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hz47b/p_looking_for_efficient_encoding_methods_for_java/"/>
        <updated>2023-09-13T21:17:02.000Z</updated>
        <summary type="html"><![CDATA[I'm working on a project that involves analyzing large samples of Java codes. My end goal is to perform classification based on these codes. For this, I've been trying to efficiently encode the full names of the methods (in the package.class.methodname format) in the Java code.
 Currently, I am experimenting with doc2vec. I'm treating the components of each method's full name (separated by dots) as individual documents. This allows me to produce vectors for each method name, and I evaluate the results by computing the cosine similarity between pairs of similar method names. The results were not good so far.
 Before moving to doc2vec, I tried using an LLM which gave me good results. However, the inference time was far too long, especially given the scale at which I'm working. I also considered using a Bag of Words model, but quickly realized it wouldn't be effective. Many of the method names in my samples are obfuscated, making this approach unsuitable.
 The issue I'm facing is that using the direct method names as features is not generalizing well for classification. A slight change in a method's name results in losing that particular feature, making my model fragile.
 What are some optimal encoding methods for small sentences (around 5-6 words) like these method names, that can scale well? 
 Also, Are there any specialized encoding techniques tailored for software code that I could use for this task?
 Any suggestions or insights would be really helpful. Thank you!
    submitted by    /u/Practical_Mango_8720  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How an archeological approach can help leverage biased data in AI to improve medicine]]></title>
        <id>https://news.mit.edu/2023/how-archeological-approach-can-help-leverage-biased-data-ai-improve-medicine-0913</id>
        <link href="https://news.mit.edu/2023/how-archeological-approach-can-help-leverage-biased-data-ai-improve-medicine-0913"/>
        <updated>2023-09-13T20:50:00.000Z</updated>
        <summary type="html"><![CDATA[Although computer scientists may initially treat data bias and error as a nuisance, researchers argue itâ€™s a hidden treasure trove for reflecting societal values.]]></summary>
        <author>
            <name>Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Efficient Memory Management for Large Language Model Serving with PagedAttention - UC Berkeley et al 2023 - 2-4x higher throughput than HuggingFace Transformers without requiring any model architecture changes!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hx11m/r_efficient_memory_management_for_large_language/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hx11m/r_efficient_memory_management_for_large_language/"/>
        <updated>2023-09-13T19:55:42.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.06180 
 Github: https://github.com/vllm-project/vllm 
 Blog: https://vllm.ai/ 
 Abstract:
  
High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4Ã— with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. 
  
https://preview.redd.it/x8w8ckejv2ob1.jpg?width=667&format=pjpg&auto=webp&s=28fae778b67ac28fc72d084f071b12c92cb5ea07
 https://preview.redd.it/ctlrqpejv2ob1.jpg?width=1468&format=pjpg&auto=webp&s=31755d169673ee5d30efa3f05bd6cb10813b328d
 https://preview.redd.it/z5r7knejv2ob1.jpg?width=1504&format=pjpg&auto=webp&s=9ceb5370aa5a7cc0688fe9a3771a0328262c3a01
 â€‹
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] LLM for viral tweet generation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hx0pe/p_llm_for_viral_tweet_generation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hx0pe/p_llm_for_viral_tweet_generation/"/>
        <updated>2023-09-13T19:55:18.000Z</updated>
        <summary type="html"><![CDATA[Problem: Given a database of the most viral tweets (of a certain shared category), I am hoping to use LLM's to generate further viral tweets. 
 Currently I am seeing this as a synthetic data generation problem: two approaches I am thinking of is 1) grounding (using viral examples to guide the prompt) and 2) filtering (finetuning an LLM to predict virality and filtering for the most viral generations) 
 I want to ensure that the output retains the "viral" structure/style and is diverse/new (i.e no copies).
 Any general directions or references are appreciated
    submitted by    /u/greatSWE  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] 1is Msc Math enough to secure jobs in R&D sector for AI/ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hwgjv/d_1is_msc_math_enough_to_secure_jobs_in_rd_sector/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hwgjv/d_1is_msc_math_enough_to_secure_jobs_in_rd_sector/"/>
        <updated>2023-09-13T19:33:53.000Z</updated>
        <summary type="html"><![CDATA[I'm currently doing Integrated MSc in Mathematics (in India) and until now I've done a remote research intern in a French university and I'll do a research intern at a French research laboratory (INRIA-LORIA) next year, I want a job (in India) at the R&D sector (Data Scientist or ML Engineering anything would do) Idk if research interns are as valuable as Industry internships when it comes to R&D? Basically after my masters, I'll probably have only these two internships to show as a work experience, Probably won't be able to get an internship in an IT company unless i opt for remote work (which is also not guaranteed atm).
 Mu question, generally is Msc + 2 research internships + 1or 2 publications good enough to secure a decent job in R&D?
    submitted by    /u/Emotional-Zebra5359  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for AI developers and researchers]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hw9qv/looking_for_ai_developers_and_researchers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hw9qv/looking_for_ai_developers_and_researchers/"/>
        <updated>2023-09-13T19:26:31.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I would love to create a small group of people who work together in AI.
 The project would be to create an AI that can infer new novel knowledge from existing datasets, as opposed to be being limited by operating within the training data. Specifically to be used in the quest to learn more about the universe.
 So I am looking for a team of likeminded individuals who want to grow in the field of AI.
 I'd love to setup a discord, subreddit and github profile to showcase our work.
 My introduction question is: How do we get AI's to expand upon current knowledge instead of just serving from the knowledge itself.
 Anyone interested in joining me in this?
    submitted by    /u/Miserable-Cobbler-16  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't worry, folks. Big Tech pinky swears it'll build safe, trustworthy AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hte7j/dont_worry_folks_big_tech_pinky_swears_itll_build/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hte7j/dont_worry_folks_big_tech_pinky_swears_itll_build/"/>
        <updated>2023-09-13T17:37:15.000Z</updated>
        <summary type="html"><![CDATA[Eight big names in tech, including Nvidia, Palantir, and Adobe, have agreed to red team their AI applications before they're released and prioritize research that will make their systems more trustworthy.
 
The White House has secured voluntary commitments from Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI, and Stability AI to develop machine-learning software and models in a safe, secure, and trustworthy way. The commitments only cover future generative AI models.
 
Each of the corporations has promised to submit their software to internal and external audits, where independent experts can attack the models to see how they can be misused.
 
The organizations agreed to safeguard their intellectual property and make sure things like the weights of their proprietary neural networks don't leak, while giving users a way to easily report vulnerabilities or bugs.
 
All eight companies agreed to focus on research to investigate societal and civil risks AI might pose if they lead to discriminatory decision-making or have weaknesses in data privacy.
 
The US government wants Big Tech to develop watermarking techniques that can identify AI-generated content.
 
The US has asked the corporations to commit to building models for good, such as fighting climate change or improving healthcare.
 
 Source : https://www.theregister.com/2023/09/12/nvidia_adobe_palantir_ai_safety/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can I work later as an ML engineer? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ht4zl/can_i_work_later_as_an_ml_engineer_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ht4zl/can_i_work_later_as_an_ml_engineer_d/"/>
        <updated>2023-09-13T17:27:15.000Z</updated>
        <summary type="html"><![CDATA[Hello!!
 I have a BSc in Mathematics and currently I'm going to start a MSc in FinTech ( it has 3 courses out of 9 that have to do with ML, NN and many more ).
 Since I am really fascinated about programming ( didn't like it when I was at Maths department because of teachers and I am starting learning on my own through courses and we will have many programming languages in my masters degree ex. R,Python, SQL and others) and I would start as a data scientist at the beginning, could I through years of experience later ( ideally 1-2 years later, provided that I have a solid and good programming skills and projects) become a ML engineer? ( Now of course I can't become one because I know that it is difficult since I am competing with people that have CS degrees).
    submitted by    /u/math-is-cool-62  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[P]" Seeking technical Co-Founder: Private Equity SaaS Startup]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hsobt/p_seeking_technical_cofounder_private_equity_saas/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hsobt/p_seeking_technical_cofounder_private_equity_saas/"/>
        <updated>2023-09-13T17:09:37.000Z</updated>
        <summary type="html"><![CDATA[Hi there!
 I bring 2 years of experience from a European investment fund and a solid idea for a B2B SaaS solution targeting private equity investment funds. 
 This market is notoriously challenging to penetrate without insider knowledge. The timing couldn't be better. Similar kind of software is currently sold â‚¬20k per user by a semi-monopolistic boomer company that is ready to be disrupted.
 Offer:
 - Equity shared equally.
 - Ready to quit my job and go full-time on it, if I find the right co-founder.
 Ideal Partner:
 Seeking someone proficient in SQL for handling large datasets and able to integrate OpenAI's API within such datasets (technical AI skills required). While I'm based in Berlin and prefer a European co-founder, it's not mandatory, but meeting in person is a must before we commit.
 Don't be afraid to DM me if intrigued! Together, we can make it happen. Let's revolutionize the sector!
    submitted by    /u/Sudden_Possible489  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Books for machine learning. [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hsntv/books_for_machine_learning_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hsntv/books_for_machine_learning_d/"/>
        <updated>2023-09-13T17:09:05.000Z</updated>
        <summary type="html"><![CDATA[Ä° am lookong for the pdfs about machine learning, maths for ML, ml projects. Ä°s there any sites i can find pdf like that?
    submitted by    /u/Necessary-Car-5080  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harvard iLab-funded project: Sub-feature of the platform out -- Enjoy free ChatGPT-3/4, personalized education, and file interaction with no page limit ðŸ˜®. All at no cost. Your feedback is invaluable!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hshxl/harvard_ilabfunded_project_subfeature_of_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hshxl/harvard_ilabfunded_project_subfeature_of_the/"/>
        <updated>2023-09-13T17:02:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Raymondlkj  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissatisfied with GPT paid subscription - who should I go with instead?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hs3df/dissatisfied_with_gpt_paid_subscription_who/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hs3df/dissatisfied_with_gpt_paid_subscription_who/"/>
        <updated>2023-09-13T16:47:55.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™ve been using the paid version of GPT for a while but I think itâ€™s time to move on. For $20 a month i would at least like for there to be an option to input an image, but I would also maybe pay a bit more than that per month for a suite of tools or something if it could also do image generation in addition to just text.
 Iâ€™m sorry if it seems like I should be able to understand anything better - please note I am disabled, my use of the tools is personal (creative and household) and not professional, and Iâ€™m doing my best by asking here. I do not mean to bother anyone with my own ignorance. Thank you.
    submitted by    /u/CaveLady3000  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualize an Amazon Comprehend analysis with a word cloud in Amazon QuickSight]]></title>
        <id>40be6ff2f4544c5a10bacc0b64db35ad113ba178</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/visualize-an-amazon-comprehend-analysis-with-a-word-cloud-in-amazon-quicksight/"/>
        <updated>2023-09-13T16:23:42.000Z</updated>
        <summary type="html"><![CDATA[Searching for insights in a repository of free-form text documents can be like finding a needle in a haystack. A traditional approach might be to use word counting or other basic analysis to parse documents, but with the power of Amazon AI and machine learning (ML) tools, we can gather deeper understanding of the content. [â€¦]]]></summary>
        <author>
            <name>Clark Lefavour</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/13/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hrfcm/oneminute_daily_ai_news_9132023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hrfcm/oneminute_daily_ai_news_9132023/"/>
        <updated>2023-09-13T16:21:56.000Z</updated>
        <summary type="html"><![CDATA[Project Gutenberg and Microsoft have created thousands of free audiobooks that use neural text-to-speech technology to generate the voices.[1]
 A group of U.S. authors, including Pulitzer Prize winner Michael Chabon, has sued OpenAI in federal court in San Francisco, accusing the Microsoft-backed program of misusing their writing to train its popular artificial intelligence-powered chatbot ChatGPT.[2]
 Numenta launches brain-based NuPIC to make AI processing up to 100 times more efficient.[3]
 Adept AI Labs released Persimmon-8B. Persimmon-8B is an open-source, fully permissively licensed model in the 8B class. This model holds immense potential for a wide array of applications, aiming to assist users in various computer-related tasks.[4]
  
Sources:
 [1] https://www.zdnet.com/article/heres-how-to-access-thousands-of-free-audiobooks-thanks-to-microsoft-ai-and-project-gutenberg/
 [2] https://www.reuters.com/technology/more-writers-sue-openai-copyright-infringement-over-ai-training-2023-09-11/
 [3] https://venturebeat.com/ai/numenta-launches-brain-based-nupic-to-make-ai-processing-up-to-100-times-more-efficient/
 [4] https://www.marktechpost.com/2023/09/09/adept-ai-labs-open-sources-persimmon-8b-a-powerful-fully-permissively-licensed-language-model-with/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Tensorflow Dropped Support for Windows :-(]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hqzxy/d_tensorflow_dropped_support_for_windows/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hqzxy/d_tensorflow_dropped_support_for_windows/"/>
        <updated>2023-09-13T16:05:22.000Z</updated>
        <summary type="html"><![CDATA[Hey,
 I've been using TF pretty much my whole deep learning career starting in 2017. I've also used it on Windows the entire time. This was never a major issue.
 Now when I tried (somewhat belatedly) upgrading from 2.10 to 2.13, I see the GPU isnt being utilized and upon further digging see that they dropped Windows GPU support after 2.10:
 "Caution: TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows. Starting with TensorFlow 2.11, you will need to install TensorFlow in WSL2, or install tensorflow or tensorflow-cpu and, optionally, try the TensorFlow-DirectML-Plugin"
 This is really upsetting! Most of the ML developers I know actually use Windows machines since we develop locally and only switch to Linux for deployment.
 I know WSL is an option, but it (1) can only use 50% RAM (2) doesnt use the native file system.
 I feel very betrayed. After sticking with, and even advocating for Tensorflow when everyone was (and still is) switching to PyTorch, TF dropped me! This is probably the final nail in the coffin for me. I will be switching to PyTorch as soon as I can :-(
 -Disgruntled user
    submitted by    /u/rsandler  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Research Focus: Week of September 11, 2023]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=966795</id>
        <link href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-11-2023/"/>
        <updated>2023-09-13T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[In this issue: Efficient polyglot analytics on semantic data aids query performance; generative retrieval for conversational question answering improves dialogue-based interfaces; a new tool uses ML to address capacity degradation in lithium-ion batteries.
The post Research Focus: Week of September 11, 2023 appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] --Research Participants Required-- Age perception of AI generated infant faces compared to real infant faces. (Suitable for everyone) (Available for Laptop/PC and Tablet devices only)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hpvow/r_research_participants_required_age_perception/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hpvow/r_research_participants_required_age_perception/"/>
        <updated>2023-09-13T15:22:30.000Z</updated>
        <summary type="html"><![CDATA[https://research.sc/participant/login/dynamic/A1D66883-6E8F-409B-8EF9-AC989A76C7E9
 Psychology researchers at Swansea University are carrying out an original study to see whether artificial intelligence is able to generate infant faces between the ages of 0 and 7 years. AI generated pictures will be presented alongside real faces from an existing face database, with a sliding scale underneath that you will use to roughly estimate the age of the childâ€™s face.
 The experiment should only take between 15 and 20 minutes
 Thank you for your time.
    submitted by    /u/Logipsychlical  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI image tool that makes existing images look like renders?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hpu36/is_there_an_ai_image_tool_that_makes_existing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hpu36/is_there_an_ai_image_tool_that_makes_existing/"/>
        <updated>2023-09-13T15:20:43.000Z</updated>
        <summary type="html"><![CDATA[I see a ton of AI image tools out there. Some let you upload image files and modulate/modify them in some way. I am wondering if a tool exists that will take a real life product image and make it appear more like a render/computer generated image. 
 Essential I would love to be able to take a pic of a product and use automatic smoothing and AI simulated rendering to output a clean image that looks like a 3d render. This would be used as a product image for an e-commerce website.
    submitted by    /u/ElonMusk0fficial  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Will be presenting a talk on Data Pre-processing in Deep Learning - what would be the topics, notebooks or datasets would you include if you would be giving such talk?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hpn7m/d_will_be_presenting_a_talk_on_data_preprocessing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hpn7m/d_will_be_presenting_a_talk_on_data_preprocessing/"/>
        <updated>2023-09-13T15:13:10.000Z</updated>
        <summary type="html"><![CDATA[So I'll be presenting a talk on Data pre-processing in deep learning in my city's Keras Community Day, and I am still thinking about all the content I want to present there.
 What I want from this session is to present different ways of preprocessing the data for a deep learning model. I want to show different types of implementations, how those affect the final trained model, when to use which type of data preprocessing and things similar to this. It would be great if you can suggest me some topic, notebooks or datasets for the same. All the notebooks that show good implementation and affect of data preprocessing are absolutely welcome.
 Also, as this is **Keras** Community Day, I'll have to include more about data preprocessing using Keras and less about other libraries. 
 Also, if you could help me with this: I am confused between showing preprocessing using layers or doing the preprocessing without layers. I know this sounds vague, but if you have any idea about this, let me know.
 Thank you for reading!
    submitted by    /u/inclinedadarsh  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Need help Selecting MSc. Courses]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hp7fn/d_need_help_selecting_msc_courses/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hp7fn/d_need_help_selecting_msc_courses/"/>
        <updated>2023-09-13T14:56:40.000Z</updated>
        <summary type="html"><![CDATA[I'm currently in my first year of MSc. in Engineering Mathematics and Computational Science. First Study Period (Currently) I have nonlinear optimization and High-performance computing.
 The track I want to choose is a mix between Machine Learning and Big Data. I can select 2 courses for Study Period 2. Here are the potential courses to select from:
  
Game Theory and Rationality
 Large-Scale Optimization
 Advanced Probabilistic machine learning
 Basic Stochastic Processes
 Options and Mathematics (Options trading)
 Foundations of Probability Theory
  
â€‹
 I need to select 4 potential courses and rank them from high preference to low preference. In case I don't get one of them, the other will be preferred. Please Machine Learning Reddit Gods, Help me.
    submitted by    /u/AdMaster9439  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Abstracts: September 13, 2023]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=966513</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/abstracts-september-13-2023/"/>
        <updated>2023-09-13T14:07:11.000Z</updated>
        <summary type="html"><![CDATA[The new #MSRPodcast series â€œAbstractsâ€ is your source for cutting-edge research in brief. In the first episode, join researchers Ava Amini and Kevin K. Yang to learn about their new paper on using evolutionary-scale protein data to improve protein design.
The post Abstracts: September 13, 2023 appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] We built Beam: An ultrafast serverless GPU runtime]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hnyn5/d_we_built_beam_an_ultrafast_serverless_gpu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hnyn5/d_we_built_beam_an_ultrafast_serverless_gpu/"/>
        <updated>2023-09-13T14:06:50.000Z</updated>
        <summary type="html"><![CDATA[Hi r/MachineLearning,
 TL;DR: Run AI apps on pay-per-second cloud GPUs that hot reload with your code changes.
 Documentation: https://docs.beam.cloud
 Iâ€™m Eli, and my co-founder and I built Beam to run workloads on serverless cloud GPUs with hot reloading, autoscaling, and (of course) fast cold start. You donâ€™t need Docker or AWS to use it, and everyone who signs up gets 10 hours of free GPU credit to try it out.
 Here a few examples of things you can run on Beam:
  
Fine-tune a LLaMA LLM
 Transcribe videos with Whisper
 Train a custom stable diffusion model
  
Beam is built for a fast developer experience. Weâ€™ve felt that using Docker and AWS directly is too slow for iterative development. Youâ€™ll often find yourself making changes to your code and waiting 10 minutes for a new image to buâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] mlflow plugin manager - early days / looking for feedback and alpha users]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hng02/d_mlflow_plugin_manager_early_days_looking_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hng02/d_mlflow_plugin_manager_early_days_looking_for/"/>
        <updated>2023-09-13T13:44:38.000Z</updated>
        <summary type="html"><![CDATA[Hey r/machinelearning!
 I'm thrilled and anxious to share an early version of the MLflow Plugin Manager. It's designed to simplify your mlflow installation, allowing you to install, update, and uninstall MLflow plugins directly from the web interface. Think of it as the "wbond's package manager for sublime", but tailored for MLflow!
 ðŸ“½ï¸
 https://i.redd.it/9gj8vqcz01ob1.gif
 â€‹
 Yes, it's in its infant stages and doesn't boast of a fancy UI yet, but I'm eager to get your feedback!
 ðŸ” What are your first impressions? Is this a good idea?
 ðŸ’¡ Any features you'd love to see?
 ðŸŒ Ideas on promoting or expanding its reach?
 I built this to bring a bit more ease to our community. Can't wait to hear your thoughts and where we can take this next!
 Repo: https://github.com/thijsdezoete/mlflow-plugin-manager/
    submitted by    /u/jessepnk  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alibaba Cloud open sources its generative AI models Tongyi Qianwen]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hn3o9/alibaba_cloud_open_sources_its_generative_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hn3o9/alibaba_cloud_open_sources_its_generative_ai/"/>
        <updated>2023-09-13T13:30:33.000Z</updated>
        <summary type="html"><![CDATA[Alibaba Cloud has open sourced two of its generative AI models, Qwen-7B and Qwen-7B-Chat, for commercial and research use.
 
The models' codes and documentation will be accessible through Alibaba Cloud's AI model repository ModelScope and the US collaborative AI platform Hugging Face.
 
Companies with fewer than 100 million monthly active users can use the models for commercial purposes free of charge, while those with more users will need to request a license.
 
Alibaba aims to democratize AI technology and support LLM start-ups.
 
Alibaba Cloud's ModelScope platform currently features over 1,000 ready-to-use AI models contributed by 20 leading AI institutions.
 
 Source : https://www.scmp.com/tech/big-tech/article/3229907/alibaba-cloud-open-sources-its-two-generative-ai-models-based-chatgpt-style-tongyi-qianwen
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI tool for generating videos using stock footages?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hmxxq/is_there_an_ai_tool_for_generating_videos_using/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hmxxq/is_there_an_ai_tool_for_generating_videos_using/"/>
        <updated>2023-09-13T13:23:58.000Z</updated>
        <summary type="html"><![CDATA[I have a text script that I want to turn into a video. For the sake of context, the video is on balancing a personâ€™s daily activities. Iâ€™m getting tired trying to find matching stock footages for the videos. I was wondering if there is a way to do this using AI tools? Synthesia wonâ€™t do because it looks like a video narration, more than a video essay. Any suggestions would help. Thanks in advanced!
    submitted by    /u/Entaro2109  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Non deterministic behaviour in LLMs when temperature set to 0?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hmwcc/discussion_non_deterministic_behaviour_in_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hmwcc/discussion_non_deterministic_behaviour_in_llms/"/>
        <updated>2023-09-13T13:21:59.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 Someone asked me today "why are LLMs still non deterministic in their output when temperature is set to 0. Assume fixed model between runs on the same machine" 
 I was like WTF are you saying - the randomness in LLM comes from temperature - chat gpt etc.. might have other randomness in the process but we don't have exact info on this. What I know is that in a standard transformers architecture, temperature is the only parameter that can enduce non deterministic behaviour at inference time. 
 He was convinced that there was more to it "i spoke about this to other LLM experts and they also are not sure" 
 I'm like wtf - I start looking up online and do find some people who claim that temperature is not the only thing that influences stochasticity during inference, but I can't find an answer as to what it is exactly. 
 Anyone has a clue of what I am missing here? 
 Thanks!
    submitted by    /u/WagnerianJLC  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Codey HumanEval Benchmark]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hmqgn/google_codey_humaneval_benchmark/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hmqgn/google_codey_humaneval_benchmark/"/>
        <updated>2023-09-13T13:14:54.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I'm hunting for a HumanEval Benchmark for Google's Codey model and am having a tough time hunting it down. Can anyone point me to an Arxiv paper or a coding leaderboard that includes Codey?
 Thanks!
    submitted by    /u/Iamreason  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlocking the Language of Genomes and Climates: Anima Anandkumar on Using Generative AI to Tackle Global Challenges]]></title>
        <id>https://blogs.nvidia.com/?p=66807</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/13/anima-anandkumar-generative-ai/"/>
        <updated>2023-09-13T13:00:11.000Z</updated>
        <summary type="html"><![CDATA[Generative AI-based models can not only learn and understand natural languages â€” they can learn the very language of nature itself, presenting new possibilities for scientific research. Anima Anandkumar, Bren Professor at Caltech and senior director of AI research at NVIDIA, was recently invited to speak at the Presidentâ€™s Council of Advisors on Science and Read article >]]></summary>
        <author>
            <name>Kristen Yee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Will Tsetlin machines reach state-of-the-art accuracy on CIFAR-10/CIFAR-100 anytime soon?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hleuv/p_will_tsetlin_machines_reach_stateoftheart/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hleuv/p_will_tsetlin_machines_reach_stateoftheart/"/>
        <updated>2023-09-13T12:13:11.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 A composite of specialized Tsetlin machines that enables plug-and-play collaboration.
 I have a love-and-hate relationship with CIFAR-10/100. I love the datasets for the challenge. On the other hand, they are two datasets where Tsetlin machines have struggled with getting state-of-the-art performance. (The Tsetlin machine is a low-energy logic-based alternative to deep learning that has done well on MNIST, Fashion-MNIST, CIFAR-2, and various NLP tasks.)
 I have been working for some time now on figuring out a solution, and this summer, I finally had a breakthrough: a new architecture that allows multiple Tsetlin machines to collaborate in a plug-and-play manner, forming a Tsetlin machine composite. The collaboration relies on a Tsetlin machine's ability to specialize during learning andâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Tutorial - Create an Object Detector for Any Game Using YOLO]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hl9wo/p_tutorial_create_an_object_detector_for_any_game/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hl9wo/p_tutorial_create_an_object_detector_for_any_game/"/>
        <updated>2023-09-13T12:06:35.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone! Have you ever thought about how to create an object detection system using YOLO that works with any game?
 If you're interested, I've created a tutorial with all the steps to develop this system. I'll leave a link to the video where I demonstrate the process step by step using the game Diablo 2 as an example.
 I hope you enjoy it, and if you have any suggestions, feel free to send a message or comment here! 
 link to the tutorial: https://www.linkedin.com/posts/moisesdias_english-version-below-tutorial-crie-activity-7107686497885011969-ZLVW/
    submitted by    /u/moisesdepaulodias  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Adversarial Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hjrx8/r_adversarial_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hjrx8/r_adversarial_reinforcement_learning/"/>
        <updated>2023-09-13T10:48:18.000Z</updated>
        <summary type="html"><![CDATA[A curated reading list for the adversarial perspective in deep reinforcement learning.
 https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning
    submitted by    /u/ml_dnn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] The XOR trick]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hih2w/p_the_xor_trick/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hih2w/p_the_xor_trick/"/>
        <updated>2023-09-13T09:32:10.000Z</updated>
        <summary type="html"><![CDATA[Can a single layer neural network solve the XOR problem?
 Most answers say no, but with this one weird trick the answer is yes! And we don't even need a bias! The trick is to multiply the outputs of a single (2,2) linear layer. Here is how:
 class XorSolver(nn.Module): def __init__(self, *args, **kwargs) -> None: super().__init__(*args, **kwargs) self.layer = nn.Linear(2, 2, bias=False) # we don't even need a bias! def forward(self, x: torch.Tensor) -> torch.Tensor: z = self.layer.forward(x) y = z[:, 0] * z[:, 1] return y 
 This is the loss and model output after 5000 epochs:
 loss: 6.516383166399464e-08 Input: [[0. 0.] [0. 1.] [1. 0.] [1. 1.]] Model output: [0. 1. 1. 0.] Expected output: [0. 1. 1. 0.] Layer weight: [[-1.248097 1.2202195 ] [-0.80121976 0.81952316]] 
 The full implementation with training and inference - around 50 lines of code - can be found on GitHub.
 Why it works?
 Basically the model simulates a more sophisticated neuron which allows more interactions between the inputs.
 By multiplying the outputs of two neurons, we introduce a form of non-linearity that allows us to separate data that are not linearly separable, like in the XOR problem:
 f(x1, x2, w1, w2, w3, w4) = (x1 * w1 + x2 * w2) * (x1 * w3 + x2 * w4)
  
w1, w2 are learnable parameters of the first neuron
 w3, w4 are learnable parameters of the second neuron
 x1, x2 are inputs to the model
  
Related studies
  
Solving XOR with a single Perceptron
 Artificial Neural Networks With Adaptive Polynomial Activation Function
 Single Cortical Neurons as Deep Artificial Neural Networks
 Dendritic action potentials and computation in human layer 2/3 cortical neurons
  
   submitted by    /u/tecbar  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many executives/investors are pushing for the use of generative AI in products/applicationsâ€¦]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hidk9/many_executivesinvestors_are_pushing_for_the_use/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hidk9/many_executivesinvestors_are_pushing_for_the_use/"/>
        <updated>2023-09-13T09:26:25.000Z</updated>
        <summary type="html"><![CDATA[But product & tech teams succumb to the pressure and move on to developing proof of concepts & even launch products that fail to achieve ROI 
 Why?
 1- use cases are not well defined
 2- not enough data or right data strategy
 3- data and model architecture not founded well 
 I love Appleâ€™s approach to AI, they shy away from the hype and focus on the fundamentals. 
 First the customer, product, then the tech that will add the value the customer needs. 
 What do you think are the top reasons generative AI applications succeed? 
 View Poll
    submitted by    /u/AILaunchpad  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Create a custom search engine]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hhz0h/create_a_custom_search_engine/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hhz0h/create_a_custom_search_engine/"/>
        <updated>2023-09-13T09:02:06.000Z</updated>
        <summary type="html"><![CDATA[I have an open book exam that has a lot of information that I will need to physically search through.
 Is there a way i can load all he PDFS and create a customised chatgbt style search, so i can easily look through all the information and research i have?
    submitted by    /u/yellowmushroom22  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Renting cloud services]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hhwoo/r_renting_cloud_services/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hhwoo/r_renting_cloud_services/"/>
        <updated>2023-09-13T08:58:19.000Z</updated>
        <summary type="html"><![CDATA[Hello guys. As I plan on soing a scientific research project, I would need some cloud compute. Say for a month of usage(can I rent for month?). What are some popular options? I am looking at something with 48gb vram pooled,mybe a600 or a100 and some decent cpu, and 2tb space.
    submitted by    /u/Outrageous_Ad1452  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chatty LLama: A fullstack Rust + react chat app using Llama-2 https://github.com/Sollimann/chatty-llama]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16hhd2y/chatty_llama_a_fullstack_rust_react_chat_app/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16hhd2y/chatty_llama_a_fullstack_rust_react_chat_app/"/>
        <updated>2023-09-13T08:24:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sollimann  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing OpenAI Dublin]]></title>
        <id>https://openai.com/blog/introducing-openai-dublin</id>
        <link href="https://openai.com/blog/introducing-openai-dublin"/>
        <updated>2023-09-13T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Weâ€™re growing our presence in Europe with an office in Dublin, Ireland.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HeyGen's one-click translation from English to Italian, Hindi, German and Spanish is the craziest AI application I've seen in months.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hf381/heygens_oneclick_translation_from_english_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hf381/heygens_oneclick_translation_from_english_to/"/>
        <updated>2023-09-13T06:06:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Fadawah  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I wanna develop small scale personal AI apps for each my friends and AI said i should learn about Python, TensorFlow Lite, SQLite, GCP, Pandas, Scikit Learn and Keras. How right is this?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hezjn/i_wanna_develop_small_scale_personal_ai_apps_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hezjn/i_wanna_develop_small_scale_personal_ai_apps_for/"/>
        <updated>2023-09-13T06:00:34.000Z</updated>
        <summary type="html"><![CDATA[I wanna develop small scale personal AI apps for each my friends and AI said i should learn Python, TensorFlow Lite, SQLite, GCP, Pandas, Scikit Learn and Keras. How right is this?
    submitted by    /u/Leading-Ad2278  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI image generators have a moderation problem]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hev64/ai_image_generators_have_a_moderation_problem/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hev64/ai_image_generators_have_a_moderation_problem/"/>
        <updated>2023-09-13T05:53:10.000Z</updated>
        <summary type="html"><![CDATA[Tests carried out by Logically confirm these platforms accept 85% of prompts tailored for election manipulation.
    submitted by    /u/Asleep-Television-24  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TurboZero: a vectorized implementation of AlphaZero + more]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16he0m4/turbozero_a_vectorized_implementation_of/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16he0m4/turbozero_a_vectorized_implementation_of/"/>
        <updated>2023-09-13T05:06:20.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/lowrollr/turbozero
 I've recently been working on TurboZero, a vectorized implementation of AlphaZero where model inference, search (MCTS), and environment simulation all run in parallel on the GPU. I've also implemented a self-contained training/evaluation pipeline, along with a few environments. I've written a wiki and a starter notebook for those who want to dig deeper. 
 This project is similar to DeepMind's mctx, but supports MCTS subtree persistence (unnecessary for MuZero, which is what mctx was mainly built to support), is written with PyTorch rather than JAX, and can also stand on its own and train models end-to-end.
 I hope to continue to expand and improve upon this as time allows, and I hope someone here might find it useful or interesting! 
 This is my first major open-source project of any real substance and I still don't have tons of experience with RL, so any feedback/advice is greatly appreciated. 
    submitted by    /u/lowrollr  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Chatbots successfully build software in under 7 minutes for less than $1]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hd8jo/ai_chatbots_successfully_build_software_in_under/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hd8jo/ai_chatbots_successfully_build_software_in_under/"/>
        <updated>2023-09-13T04:25:26.000Z</updated>
        <summary type="html"><![CDATA[AI Chatbots, such as OpenAI's ChatGPT, can create incredibly cost-friendly software in record time, reveals a new study.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/rxpr6db3aynb1.png?width=1300&format=png&auto=webp&s=721ff5d8f9d25b5e48fa26e2b335c1d20620a83a
 The AI Tech Company Experiment
  
Brown University and several Chinese University researchers put ChatGPT-powered AI bots to the test by making them run a hypothetical software development company, ChatDev.
 The AI chatbots were given specific roles and allocated respective stages based on the traditional waterfall model encompassing designing, coding, testing, and documenting.
 AI 'employees' functioned with minimal human input to complete their parts of the software development process.
  
Impressive Results
  
Assigning 70 tasks to ChatDev led to the completion of the entire software development process in under seven minutes at a cost of less than one dollar on average.
 A stunning 86.66% of the generated software systems performed flawlessly.
 Despite some language model errors and biases, the study demonstrates AI's immense potential in automating tasks - a boon, especially to junior programmers around the world.
  
Broader Implications
  
Powerfully generative AI technologies like ChatGPT can perform specific job functions, saving time, and boosting productivity in several industries.
 While coders find such tools beneficial, it's also critical to note that limitations and biases do exist in AI models which could potentially affect the software creation process.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most vital news and research in AI. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helping computer vision and language models understand what they see]]></title>
        <id>https://news.mit.edu/2023/helping-computer-vision-language-models-see-0913</id>
        <link href="https://news.mit.edu/2023/helping-computer-vision-language-models-see-0913"/>
        <updated>2023-09-13T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[Researchers use synthetic data to improve a modelâ€™s ability to grasp conceptual information, which could enhance automatic captioning and question-answering systems.]]></summary>
        <author>
            <name>Adam Zewe | MIT News</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A. Michael West: Advancing human-robot interactions in health care]]></title>
        <id>https://news.mit.edu/2023/michael-west-advancing-human-robot-interactions-0913</id>
        <link href="https://news.mit.edu/2023/michael-west-advancing-human-robot-interactions-0913"/>
        <updated>2023-09-13T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[When he isnâ€™t investigating human motor control, the graduate student gives back by volunteering with programs that helped him grow as a researcher.]]></summary>
        <author>
            <name>Michaela Jarvis | School of Engineering</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Business Thinkers Can Start Building AI Plugins With Semantic Kernel]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hcknz/how_business_thinkers_can_start_building_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hcknz/how_business_thinkers_can_start_building_ai/"/>
        <updated>2023-09-13T03:51:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mycall  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EU leads the way in regulating AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16hbxjs/eu_leads_the_way_in_regulating_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16hbxjs/eu_leads_the_way_in_regulating_ai/"/>
        <updated>2023-09-13T03:19:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Jariiari7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Guidance in training different models and comparing using smaller versions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16hboas/d_guidance_in_training_different_models_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16hboas/d_guidance_in_training_different_models_and/"/>
        <updated>2023-09-13T03:07:47.000Z</updated>
        <summary type="html"><![CDATA[Currently I'm training medium (1B-3B) sized audio models. I have several different architectures in mind. Obviously I don't want to train the full-sized models and then compare them, thats a waste of money. So I'm thinking of training smaller versions (~100M) and then comparing those instead.
 My question is there some sort of best practice for this? Some smaller multiple of your full model size where it is best to compare? Thanks.
    submitted by    /u/ginger_turmeric  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Webinar with Dr. Richard Marks]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16h9or2/webinar_with_dr_richard_marks/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16h9or2/webinar_with_dr_richard_marks/"/>
        <updated>2023-09-13T01:35:44.000Z</updated>
        <summary type="html"><![CDATA[>Sailea is a student run non-profit that does not charge for any of its services 
 ðŸŒŸ Join SAILeaâ€™s Free Webinar with Dr. Richard Marks! ðŸŒŸ
 ðŸ—“ï¸ Date: September 23rd, 2023 â° Time: 3:00-4:00PM EST
 Don't miss an exclusive opportunity to learn from an AI expert! Join us for a free webinar featuring Dr. Richard Marks, a renowned CS and Data Science professor at UNC-Chapel Hill University with a remarkable journey â€“ from Google to PlayStation, and the mind behind EyeToy and PlayStation Move.
 ðŸš€ What to Expect:
 ðŸ”¹ Deep insights into tech innovation.
 ðŸ”¹ Career advice. 
 ðŸ”¹ Live Q&A with Dr. Richard Marks.
 Reserve your spot now: sailea.org/events
 ðŸ”¥ Donâ€™t miss this opportunity! Register today!ðŸ”¥
    submitted by    /u/Envoy-Insc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I want to try out Stability.AI's chat. After logging in with a Google account, a spinning wheel is all I get. Is it like that for everyone?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16h53b7/i_want_to_try_out_stabilityais_chat_after_logging/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16h53b7/i_want_to_try_out_stabilityais_chat_after_logging/"/>
        <updated>2023-09-12T22:21:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/w__sky  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Need advice for vector DB]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16h3rpd/p_need_advice_for_vector_db/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16h3rpd/p_need_advice_for_vector_db/"/>
        <updated>2023-09-12T21:31:04.000Z</updated>
        <summary type="html"><![CDATA[Hi, all,
 I'm working on a GPT-powered game where the characters speak using API calls.
 For this, I need an inexpensive vector database that does not require an API, or at least if it does, it leverages the OpenAI API. Also, this vector database must be runnable on consumer-grade gaming hardware with a small search space (let's say 10-50 entries in the DB).
 Also, I need to package it with the game somehow. My game is in the Godot engine which can use Python with a plug-in. Other approaches I was considering is having a second process communicate with the game through a socket. Ideally the vector DB solution would be easy to install - that is, I could package it with a .exe, and simply run both without the player having to download anything else.
 Any suggestions? 
    submitted by    /u/kettlebot141  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are some ways that you can reduce latency of real-time user-user matching?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16h3rfb/d_what_are_some_ways_that_you_can_reduce_latency/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16h3rfb/d_what_are_some_ways_that_you_can_reduce_latency/"/>
        <updated>2023-09-12T21:30:47.000Z</updated>
        <summary type="html"><![CDATA[Hi. I'm currently tasked with something at my company that I'm facing some difficulty with because it's not in my domain. My company has a service where we provide video chatting to users and match users with others based on various features.
 Currently I've implemented a simple model where we have separate embedding matrices for each user feature, create a user representation by aggregating these features, and performing regression between two users. The way that regression works is that the final score output from the model would act as a "matching score" and we'll match user A with the highest other user.
 The problem is that obviously running inference on every single pair of users is very slow and I need to speed this up.
 Some methods I thought about were to either use a feature store or perform sampling on users so we're not running inference on the entire users, but I'm not sure if this is optimal.
 Just curious what other people who have tackled problems like these have done and looking for second opinions. Thanks.
    submitted by    /u/Seankala  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[World scale inverse reinforcement learning in Google Maps]]></title>
        <id>http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html</id>
        <link href="http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html"/>
        <updated>2023-09-12T21:22:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Matt Barnes, Software Engineer, Google Research




Routing in Google Maps remains one of our most helpful and frequently used features. Determining the best route from A to B requires making complex trade-offs between factors including the estimated time of arrival (ETA), tolls, directness, surface conditions (e.g., paved, unpaved roads), and user preferences, which vary across transportation mode and local geography. Often, the most natural visibility we have into travelers' preferences is by analyzing real-world travel patterns.



Learning preferences from observed sequential decision making behavior is a classic application of inverse reinforcement learning (IRL). Given a Markov decision process (MDP) â€” a formalization of the road network â€” and a set of demonstration trajectâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made a data request feature so you don't have to exhaustively collect data/dataset(s) yourself!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16h2e0s/i_made_a_data_request_feature_so_you_dont_have_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16h2e0s/i_made_a_data_request_feature_so_you_dont_have_to/"/>
        <updated>2023-09-12T20:39:23.000Z</updated>
        <summary type="html"><![CDATA[So, I've been working on an AI data marketplace platform for a few months now. Users can buy, sell, request, and subscribe to data/datasets (and soon even train their ML/AI models using other users' datasets). One of our key features is the request feature, which allows users to submit data requests for free. These requests include descriptions, required fields, geographical scope, budget etc... Once a request is posted, it's sent to numerous companies, organizations, and data vendors that have the potential to fulfill it.
 I understand how frustrating the data acquisition process can be, so I designed this platform to be your one-stop shop for all data-related transactions. You no longer have to spend weeks or months dealing with different vendors and companies through slow emails. With our platform, you can request, negotiate, and purchase data all in one place, and it's completely free to post a request, by the way.
 We've already achieved some successes, and we hope to help more people access the datasets they need. After all, the best AI models are built on diverse and differentiating data. We've had some notable achievements, and we're eager to see if we can fulfill even more interesting dataset requests!
    submitted by    /u/nobilis_rex_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Lends Support to Washingtonâ€™s Efforts to Ensure AI Safety]]></title>
        <id>https://blogs.nvidia.com/?p=66820</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/12/ai-safety-washington/"/>
        <updated>2023-09-12T20:35:31.000Z</updated>
        <summary type="html"><![CDATA[In an event at the White House today, NVIDIA announced support for voluntary commitments that the Biden Administration developed to ensure advanced AI systems are safe, secure and trustworthy. The news came the same day NVIDIAâ€™s chief scientist, Bill Dally, testified before a U.S. Senate subcommittee seeking input on potential legislation covering generative AI. Separately, Read article >]]></summary>
        <author>
            <name>Ned Finkle</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] NExT-GPT: Any-to-Any Multimodal LLM - National University of Singapore 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16h20xn/r_nextgpt_anytoany_multimodal_llm_national/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16h20xn/r_nextgpt_anytoany_multimodal_llm_national/"/>
        <updated>2023-09-12T20:25:17.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.05519 
 Blog: https://next-gpt.github.io/ 
 My opinion: It lacks a Cognitive Architecture: https://arxiv.org/abs/2309.02427 Also the models are far too small and are more on the gpt-2 level. The idea in itself is a good one but can be far improved with bigger models. I also would like to remember in this that all foundation models could be improved if there would be no tokenizers: https://x.com/karpathy/status/1657949234535211009?s=20 
 Abstract:
  
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through varâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Unveiling theory of mind in large language models: A parallel to single neurons in the human brain - Harvard University 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16h1tup/r_unveiling_theory_of_mind_in_large_language/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16h1tup/r_unveiling_theory_of_mind_in_large_language/"/>
        <updated>2023-09-12T20:17:26.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.01660
 Abstract:
  
With their recent development, large language models (LLMs) have been found to exhibit a certain level of Theory of Mind (ToM), a complex cognitive capacity that is related to our conscious mind and that allows us to infer another's beliefs and perspective. While human ToM capabilities are believed to derive from the neural activity of a broadly interconnected brain network, including that of dorsal medial prefrontal cortex (dmPFC) neurons, the precise processes underlying LLM's capacity for ToM or their similarities with that of humans remains largely unknown. In this study, we drew inspiration from the dmPFC neurons subserving human ToM and employed a similar methodology to examine whether LLMs exhibit comparable characteristics. Surpâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][R] Looking for help with Forced Alignment for translated audio]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16h1oqr/dr_looking_for_help_with_forced_alignment_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16h1oqr/dr_looking_for_help_with_forced_alignment_for/"/>
        <updated>2023-09-12T20:12:00.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 I'm trying to create an alignment between source audio in a different language than the translated transcript. Essentially want to align the translated transcript with the word-level timestamps on an Audio, programmatically. I've tried to find different tools; some open-source ML models force alignment if the source audio and transcript language are the same. 
 My goal is to have audio in a dubbed language, which I generate using a translated transcript that has been originally transcribed from my audio. Alignment seems tough since languages are spoken at different rates, so I'm figuring out the best way to optimize alignment without having to speed up/slow down the audio too much for each sentence.
    submitted by    /u/Revolutionary_Ant944  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 12 September 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=63121</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-12-september-2023/"/>
        <updated>2023-09-12T19:59:49.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 12 September 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best Places to Access the Greatest Number of GPUs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16h0jdq/d_best_places_to_access_the_greatest_number_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16h0jdq/d_best_places_to_access_the_greatest_number_of/"/>
        <updated>2023-09-12T19:27:53.000Z</updated>
        <summary type="html"><![CDATA[I'm in need of a massive amount of GPUs for batch inference I'm doing. Outside of the big cloud providers are there any niche services out there you'd recommend? 
    submitted by    /u/Ok_Post_149  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are Fourier Positional Encodings Outdated?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16h09u4/d_are_fourier_positional_encodings_outdated/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16h09u4/d_are_fourier_positional_encodings_outdated/"/>
        <updated>2023-09-12T19:17:16.000Z</updated>
        <summary type="html"><![CDATA[I gave a talk at work the other day about the attention mechanism and one of my coworkers told me that he thinks Fourier Positional Encodings in transformers are outdated. I've tried to follow up and find what I could but I didn't see anything suggesting that they're not being used. I know that learned encodings are also used.
 Can anyone give me some direction on this? My initial impression is that they are not outdated by any means, but I'm happy to be wrong about that.
    submitted by    /u/XfrmrTron  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help me with modeling my game (source code review)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16h05t7/help_me_with_modeling_my_game_source_code_review/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16h05t7/help_me_with_modeling_my_game_source_code_review/"/>
        <updated>2023-09-12T19:13:16.000Z</updated>
        <summary type="html"><![CDATA[Hi! 
 I am working on the implementation for DQN algorithm for one interesting game. This game is interesting because moves in this game are not affecting state of the game directly, but modify beliefs of other participants of the game and basically allow other agents to deduce role of other players in the game. It's game of "Mafia". Here's are the rules: 
 Mafia Game description: 
 Game is played with 10 players, players are getting roles at random. 
 At the beginning of the game there's 3 players who gets Black cards (1 Don and 2 Mafia) and 7 players get Red cards
 (6 Citizen card and 1 Sheriff card). 
 One team is playing against each other.
 Three black players knows each other and red players do not know who is red and who is black. 
 Game is played with phases - "Day" and "Night". Duâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do interneuron can choose other interneuron in connections to send the signal? Or canâ€™t And send the signal to all inter neuron in his connections .]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16gzff5/do_interneuron_can_choose_other_interneuron_in/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16gzff5/do_interneuron_can_choose_other_interneuron_in/"/>
        <updated>2023-09-12T18:45:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/PowerfulGeologist373  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Launched my own TTS/Sound Effect/AI Music Service - looking for people to try]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gzd6i/p_launched_my_own_ttssound_effectai_music_service/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gzd6i/p_launched_my_own_ttssound_effectai_music_service/"/>
        <updated>2023-09-12T18:43:11.000Z</updated>
        <summary type="html"><![CDATA[I've created an AI Sound service that can do TTS (text to speech), STS (speech to speech), Voice Cloning, generate sound effects, and also generate instrumental music. Here's the link: https://voicegen.org/, you can try everything for free.
 The TTS quality is similar to Elevenlabs, and there are some sample clips on the home page.
 Stuff I'm working on:
 - Emotional speech (where you can select the emotion of the TTS). Right now you can already do it by putting the emotion in brackets: e.g. "[Angrily] Please go away!" but I want to make it better.
 - Music with vocals. Currently the model only generates instrumental music. I am retraining it/tweaking the model to allow for music with lyrics.
 - Faster Inference: Since I'm doing this all myself and I'm not rich, I don't have access to the best hardware. However, I am working on some optimizations like speculative decoding that should speed things up.
 Anyways, let me know if you have any questions/comments/feature suggestions/see any bugs! Feel free to DM me. Thanks.
    submitted by    /u/ginger_turmeric  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Securing your AI data pipeline with MLOps]]></title>
        <id>https://www.datasciencecentral.com/?p=63117</id>
        <link href="https://www.datasciencecentral.com/securing-your-ai-data-pipeline-with-mlops/"/>
        <updated>2023-09-12T18:37:04.000Z</updated>
        <summary type="html"><![CDATA[By Colin Priest, Chief Evangelist at FeatureByte Enterprises are increasingly implementing Artificial Intelligence (AI) into their operations. However, AI-ready data pipeline practices are still in their infancy, especially when it comes to IT security. The pervasiveness of â€œSpaghetti Codeâ€ Enterprises delving into AI data pipelines often find themselves wading through a mess of complex andâ€¦Â Read More Â»Securing your AI data pipeline with MLOps
The post <a></a>Securing your AI data pipeline with MLOps appeared first on Data Science Central.]]></summary>
        <author>
            <name>Colin Priest</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Warehousing: The key to effective marketing campaign management]]></title>
        <id>https://www.datasciencecentral.com/?p=62265</id>
        <link href="https://www.datasciencecentral.com/data-warehousing-the-key-to-effective-marketing-campaign-management/"/>
        <updated>2023-09-12T18:31:24.000Z</updated>
        <summary type="html"><![CDATA[Businesses today constantly strive to gain a competitive edge in their marketing efforts. Â Leveraging their data effectively to create data-driven campaigns is the best way to trump the competition. One of the best tools at their disposal to utilize their data is a data warehouse. Data warehousing is crucial in enhancing marketing and campaign managementâ€¦Â Read More Â»Data Warehousing: The key to effective marketing campaign management
The post Data Warehousing: The key to effective marketing campaign management appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ovais Naseem</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobility Gets Amped: IAA Show Floor Energized by Surge in EV Reveals, Generative AI]]></title>
        <id>https://blogs.nvidia.com/?p=66802</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/12/iaa-drive-ecosystem-roundup/"/>
        <updated>2023-09-12T18:08:26.000Z</updated>
        <summary type="html"><![CDATA[Generative AIâ€™s transformative effect on the auto industry took center stage last week at the International Motor Show Germany, known as IAA, in Munich. NVIDIAâ€™s Danny Shapiro, VP of automotive marketing, explained in his IAA keynote how this driving force is accelerating innovation and streamlining processes â€” from advancing design, engineering and digital-twin deployment for Read article >]]></summary>
        <author>
            <name>Marie Labrie</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][R] Kani: A Lightweight Highly Hackable Open-Source Framework for Building Chat Applications with Tool Usage (e.g. Plugins)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gxp51/pr_kani_a_lightweight_highly_hackable_opensource/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gxp51/pr_kani_a_lightweight_highly_hackable_opensource/"/>
        <updated>2023-09-12T17:38:09.000Z</updated>
        <summary type="html"><![CDATA[Hey all, we just released our new project/paper and we thought you all might find it useful!
 Our project (Kani) is a super lightweight and hackable alternative to frameworks like LangChain or simpleAIchat meant to help developers hook in callable functions or tools to chat models easily. With Kani, devs can write functions in pure python and just add one line (the @ai_function() decorator) to turn any function into an AI-callable function!
 Kani works with any model and has built-in tools for OpenAI, HuggingFace, LLaMAv2, Vicuna, and GGML with more to come. Kani also never does any prompt engineering under the hood and doesn't require learning complex library tools---all defaults are minimal and highly customizable.
 Check out our Colab for mini-examples of things like retrieval, web-search, model routing, etc. https://colab.research.google.com/github/zhudotexe/kani/blob/main/examples/colab_examples.ipynb 
 If you're interested in learning more check out our links below!
 Paper: https://arxiv.org/abs/2309.05542
 GitHub: https://github.com/zhudotexe/kani
 Docs: https://kani.readthedocs.io/
    submitted by    /u/zhuexe  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Wonâ€™t Believe How Much Teslaâ€™s Dojo Supercomputer Is Worth]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gxmdj/you_wont_believe_how_much_teslas_dojo/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gxmdj/you_wont_believe_how_much_teslas_dojo/"/>
        <updated>2023-09-12T17:35:22.000Z</updated>
        <summary type="html"><![CDATA[Morgan Stanley Research has valued Tesla's soon-to-be-released Dojo supercomputer at up to $500 billion, potentially increasing the auto giant's valuation significantly. The financial institution believes Dojoâ€™s applications will go beyond Tesla's Full-Self Driving (FSD) capabilities.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/yhia3c5f1vnb1.jpg?width=1440&format=pjpg&auto=webp&s=7adf40e7b868a4fbf6eb3d696132652f4f549f23
 Morgan Stanley's Bullish Prediction on Dojo
  
Morgan Stanley has suggested that Dojo might not just enhance Tesla's FSD technology, but could find use in other devices that make real-time decisions based on a visual field.
 Apart from raising Teslaâ€™s valuation, this could potentially open up new markets for the company.
 Following this, Morgan Stanley has increased its target price for Tesla shares from $250 to $400 each.
  
Dojo Supercomputer Overview
  
Tesla has developed Dojo in-house, diverging from conventional AI accelerators and involving its own computing, networking, IO, and instruction set.
 At the heart of Dojo is the D1 AI accelerator processor, containing 354 custom CPU cores. Twenty-five D1 chips are combined to create a Dojo training tile, which could expedite Teslaâ€™s move towards earning revenue from vehicle software.
  
Future Plans and Implications
  
Tesla could potentially become an AI-as-a-service provider to automakers in need of FSD capabilities with Dojo.
 As the development of Dojo continues, Tesla has invested in alternative AI infrastructures, including a cluster of 10,000 of Nvidia's most potent H100 accelerators.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that explores the latest AI developments. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Help Understanding LLM Quantization techniques and how they Relate]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gx2lg/d_help_understanding_llm_quantization_techniques/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gx2lg/d_help_understanding_llm_quantization_techniques/"/>
        <updated>2023-09-12T17:14:16.000Z</updated>
        <summary type="html"><![CDATA[So i have been doing some research to get into the LLM quantization field but have some questions. To better organize my ideas i have developed the image below. Does it make sense / is true?
 The way i understand it there are 3 main methods which are compatible with different backends (the backend part is still quite confusing to me). What is the core diference between what the methods do and the backends? What are some core diferences between the backends? What is the main distinction between GPTQ and NF4? How does NF4 relate to QLoRa, is it the same or is it just a small part of QLoRa?
 Thanks in advance and i apologize for any ignorance.
 â€‹
 https://preview.redd.it/gxpo0ir0yunb1.png?width=1041&format=png&auto=webp&s=872424a58a9d4393c025b8d2cec0160979b035f4
    submitted by    /u/MiNeves  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schwoz Sings Ballin]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gx1l6/schwoz_sings_ballin/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gx1l6/schwoz_sings_ballin/"/>
        <updated>2023-09-12T17:13:06.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LaminateShark7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FP2: Fully In-Place Functional Programming provides memory reuse for pure functional programs]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=964929</id>
        <link href="https://www.microsoft.com/en-us/research/blog/fp2-fully-in-place-functional-programming-provides-memory-reuse-for-pure-functional-programs/"/>
        <updated>2023-09-12T16:55:00.000Z</updated>
        <summary type="html"><![CDATA[This research paper was presented at the 28th ACM SIGPLAN International Conference on Functional Programming (opens in new tab) (ICFP), a premier forum for discussing design, implementations, principles, and uses of functional programming. Functional programming languages offer a host of advantages, such as ensuring memory safety (opens in new tab) and eliminating arbitrary side effects. [â€¦]
The post FP2: Fully In-Place Functional Programming provides memory reuse for pure functional programsÂ  appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amazon SageMaker simplifies the Amazon SageMaker Studio setup for individual users]]></title>
        <id>c5176386a69be025f1a39037ff987ad3db274ed8</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-simplifies-the-amazon-sagemaker-studio-setup-for-individual-users/"/>
        <updated>2023-09-12T16:43:16.000Z</updated>
        <summary type="html"><![CDATA[Today, we are excited to announce the simplified Quick setup experience in Amazon SageMaker. With this new capability, individual users can launch Amazon SageMaker Studio with default presets in minutes. SageMaker Studio is an integrated development environment (IDE) for machine learning (ML). ML practitioners can perform all ML development stepsâ€”from preparing their data to building, [â€¦]]]></summary>
        <author>
            <name>Vikesh Pandey</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Use of GPT-4 to Analyze Medical Records of Patients With Extensive Investigations and Delayed Diagnosis]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gvvdo/r_use_of_gpt4_to_analyze_medical_records_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gvvdo/r_use_of_gpt4_to_analyze_medical_records_of/"/>
        <updated>2023-09-12T16:27:26.000Z</updated>
        <summary type="html"><![CDATA[Paper - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10425828/
  
Six patients 65 years or older (2 women and 4 men) were included in the analysis. The accuracy of the primary diagnoses made by GPT-4, clinicians, and Isabel DDx Companion was 4 of 6 patients (66.7%), 2 of 6 patients (33.3%), and 0 patients, respectively. If including differential diagnoses, the accuracy was 5 of 6 (83.3%) for GPT-4, 3 of 6 (50.0%) for clinicians, and 2 of 6 (33.3%) for Isabel DDx Companion.
  
â€‹
    submitted by    /u/MysteryInc152  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Train ViT on small datasets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gvjow/r_train_vit_on_small_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gvjow/r_train_vit_on_small_datasets/"/>
        <updated>2023-09-12T16:14:46.000Z</updated>
        <summary type="html"><![CDATA[Hello, everyone,
 I'm currently working on a computer vision project using the Oxford Pets Dataset, which consists of 37 different pet categories. I initially used a pre-trained ViT model with ImageNet weights model=vit_b_32(ViT_B_32_Weights.IMAGENET1K_V1), and it gave me an impressive accuracy of 88%. However, I want to modify the architecture of the ViT model and train it from scratch without relying on ImageNet weights.
 I'm aware that ViT models are data-hungry and that training from scratch (model=vit_b_32( )) can be challenging, especially with limited data. I've already applied data augmentation techniques to enhance my dataset, but I'm still struggling to achieve satisfactory results. My accuracy is currently only 7%.
 I'd appreciate any advice or tips from the community on how I can improve the performance of my scratch-trained ViT model. Are there any specific training strategies, hyperparameters, or architectural modifications that I should consider? How can I make the most out of my limited dataset to boost accuracy?
 Thank you in advance for your help!
    submitted by    /u/NoEntertainment6225  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlocking language barriers: Translate application logs with Amazon Translate for seamless support]]></title>
        <id>d0725fec0786bb2853300d9d418b133d27b9b555</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/unlocking-language-barriers-translate-application-logs-with-amazon-translate-for-seamless-support/"/>
        <updated>2023-09-12T16:09:35.000Z</updated>
        <summary type="html"><![CDATA[This post addresses the challenge faced by developers and support teams when application logs are presented in languages other than English, making it difficult for them to debug and provide support. The proposed solution uses Amazon Translate to automatically translate non-English logs in CloudWatch, and provides step-by-step guidance on deploying the solution in your environment.]]></summary>
        <author>
            <name>Xan Huang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerate client success management through email classification with Hugging Face on Amazon SageMaker]]></title>
        <id>c5489b6f165eb2ac2631eadd90827f7f2ed24a2d</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/accelerate-client-success-management-through-email-classification-with-hugging-face-on-amazon-sagemaker/"/>
        <updated>2023-09-12T16:05:21.000Z</updated>
        <summary type="html"><![CDATA[In this post, we share how SageMaker facilitates the data science team at Scalable to manage the lifecycle of a data science project efficiently, namely the email classifier project. The lifecycle starts with the initial phase of data analysis and exploration with SageMaker Studio; moves on to model experimentation and deployment with SageMaker training, inference, and Hugging Face DLCs; and completes with a training pipeline with SageMaker Pipelines integrated with other AWS services]]></summary>
        <author>
            <name>Dr. Sandra Schmid</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven insights: Improving remote team performance with time-tracking analytics]]></title>
        <id>https://www.datasciencecentral.com/?p=62852</id>
        <link href="https://www.datasciencecentral.com/data-driven-insights-improving-remote-team-performance-with-time-tracking-analytics/"/>
        <updated>2023-09-12T15:39:35.000Z</updated>
        <summary type="html"><![CDATA[The way we work has changed, with remote teams now a common part of the landscape. While remote work offers flexibility, it also brings challenges. Managing remote teams effectively is crucial to ensure productivity and collaboration. In this article, weâ€™ll explore how using time tracking for remote teams can help manage employeesâ€™ performance better. Time-trackingâ€¦Â Read More Â»Data-driven insights: Improving remote team performance with time-tracking analytics
The post Data-driven insights: Improving remote team performance with time-tracking analytics appeared first on Data Science Central.]]></summary>
        <author>
            <name>Costanza Tagliaferi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI for Natural Language Understanding (NLU)]]></title>
        <id>https://www.datasciencecentral.com/?p=63099</id>
        <link href="https://www.datasciencecentral.com/ai-for-natural-language-understanding-nlu/"/>
        <updated>2023-09-12T15:36:04.000Z</updated>
        <summary type="html"><![CDATA[In the panorama of Artificial Intelligence (AI), Natural Language Understanding (NLU) stands as a citadel of computational wizardry. No longer in its nascent stage, NLU has matured into an irreplaceable asset for business intelligence. In this discussion, we delve into the advanced realms of NLU, unraveling its role in semantic comprehension, intent classification, and context-awareâ€¦Â Read More Â»AI for Natural Language Understanding (NLU)
The post <strong>AI for Natural Language Understanding (NLU)</strong> appeared first on Data Science Central.]]></summary>
        <author>
            <name>Seven Kole</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can IoT transform and benefit the entertainment industry?]]></title>
        <id>https://www.datasciencecentral.com/?p=63076</id>
        <link href="https://www.datasciencecentral.com/how-can-iot-transform-and-benefit-the-entertainment-industry/"/>
        <updated>2023-09-12T15:34:07.000Z</updated>
        <summary type="html"><![CDATA[The Internet of Things (IoT) has been transforming entertainment and has given it new ways of creating, delivering and consuming content. The wide-ranging utility of IoT devices has improved user experience while enhancing the safety and security of users. The media and entertainment (M&E) companies can leverage IoT technology to improve the overall quality ofâ€¦Â Read More Â»How can IoT transform and benefit the entertainment industry?
The post How can IoT transform and benefit the entertainment industry? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ryan Williamson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI and the cyber challenge: Bridging vulnerabilities in modern defense strategies]]></title>
        <id>https://www.datasciencecentral.com/?p=63064</id>
        <link href="https://www.datasciencecentral.com/ai-and-the-cyber-challenge-bridging-vulnerabilities-in-modern-defense-strategies/"/>
        <updated>2023-09-12T15:32:48.000Z</updated>
        <summary type="html"><![CDATA[In our increasingly interconnected world, the digital realm has become both a frontier of innovation and a battleground of threats. As technology advances, so do the tactics of malicious actors who seek to exploit vulnerabilities in our digital infrastructure. The rapid evolution of cyber threats calls for a paradigm shift in defense strategies, and thatâ€™sâ€¦Â Read More Â»AI and the cyber challenge: Bridging vulnerabilities in modern defense strategies
The post AI and the cyber challenge: Bridging vulnerabilities in modern defense strategies appeared first on Data Science Central.]]></summary>
        <author>
            <name>Anas Baig</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][R] Developing a platform to accelerate the research]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gu9af/pr_developing_a_platform_to_accelerate_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gu9af/pr_developing_a_platform_to_accelerate_the/"/>
        <updated>2023-09-12T15:24:47.000Z</updated>
        <summary type="html"><![CDATA[Hi community~
 We are developing a platform similar to mTurk and Prolific and plan to do the first wave of hypothesis testing in the coming weeks. If you have open tasks that require large amounts of human intelligence, please reply to this thread or dm me. We can support your research in our hypothesis testing. 
 we are on the mission of helping machine learning experts and AI training as open and public goods, you can learn more here: https://ivynetwork.cloud/ 
 feel free to ask more questions here :)
    submitted by    /u/Accomplished_Code_25  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Quantum Boost: cuQuantum With PennyLane Lets Simulations Ride Supercomputers]]></title>
        <id>https://blogs.nvidia.com/?p=66694</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/12/quantum-supercomputers-pennylane/"/>
        <updated>2023-09-12T15:00:56.000Z</updated>
        <summary type="html"><![CDATA[Ten miles in from Long Islandâ€™s Atlantic coast, Shinjae Yoo is revving his engine. The computational scientist and machine learning group lead at the U.S. Department of Energyâ€™s Brookhaven National Laboratory is one of many researchers gearing up to run quantum computing simulations on a supercomputer for the first time, thanks to new software. Yooâ€™s Read article >]]></summary>
        <author>
            <name>Sam Stanwyck</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Math for machine learning [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gs6a9/math_for_machine_learning_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gs6a9/math_for_machine_learning_d/"/>
        <updated>2023-09-12T14:01:03.000Z</updated>
        <summary type="html"><![CDATA[Ä° have a question. How important linear algebra for machine learning? Ä° have basic level knowledge on linear? Should Ä° study in more detail? And How can Ä° follow roadmap on math for machine learning?
    submitted by    /u/Necessary-Car-5080  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][P]Has anyone ever tried fine-tuning Tortoise tts for better voice cloning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16grcpm/dphas_anyone_ever_tried_finetuning_tortoise_tts/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16grcpm/dphas_anyone_ever_tried_finetuning_tortoise_tts/"/>
        <updated>2023-09-12T13:25:44.000Z</updated>
        <summary type="html"><![CDATA[Hello people. I've been wanting to clone voices along with the accent. For example: A user speak English in an Indian accent should have that accent cloned in the output audio as well. By default, tortoise is not good at doing that. It can clone the pitch of the voice really well but the accent is completely lost. I was wondering if fine tuning the model could get me what I want. Please do suggest. Also do suggest any methods on fine tuning it if it does in fact help my use case. Thanks a lot!
 Note: I've also tried bark, coqui and vall-e-x. They aren't that good for voice cloning from what I saw.
    submitted by    /u/salehxoxo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Small Step for Artists, One Giant Leap for Creative-Kind]]></title>
        <id>https://blogs.nvidia.com/?p=66825</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/12/aendom-blender-adobe-substance-3d-painter/"/>
        <updated>2023-09-12T13:00:35.000Z</updated>
        <summary type="html"><![CDATA[Editorâ€™s note: This post is part of our weekly In the NVIDIA Studio series, which celebrates featured artists, offers creative tips and tricks and demonstrates how NVIDIA Studio technology improves creative workflows.Â  When it comes to converting 2D concepts into 3D masterpieces, self-taught visual development artist Alex TreviÃ±o has confidence in the potential of all Read article >]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Today we test which AI is smartest. Tomorrow AI tests which human is smartest.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gqnwd/today_we_test_which_ai_is_smartest_tomorrow_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gqnwd/today_we_test_which_ai_is_smartest_tomorrow_ai/"/>
        <updated>2023-09-12T12:55:54.000Z</updated>
        <summary type="html"><![CDATA[Somewhere in the world there's a person who is the smartest. Why stop there? There are ten people who are the smartest. And if they are on the Internet, AI will find them.
 Perhaps not yet. It probably needs to get smarter. Maybe by Gemini. Or GPT-5. But eventually an AI will analyze all of the content on the Internet, and determine from that data who are the ten most intelligent people on the planet, (whose material is online).
 Of course if AI can determine the top ten, it can certainly determine the top 100, and the top 1,000, and even the top 100,000. I suppose when that happens there will be a lot more human brain power available to solve our problems. Although by then AI will be solving them far better than we could, haha. 
 But think about it for a minute. There are very smart people out there who don't publish in traditional mass media channels. The geniuses among us who don't fit in so well, and are therefore resigned to the margins, remaining unrecognized. 
 Wouldn't it be great if AI discovered them, and gave them the validation they deserve? Wouldn't it be great to find out who they are so that they can better work on whatever.
    submitted by    /u/Georgeo57  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Evaluating a clothing size prediction model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gpl4h/d_evaluating_a_clothing_size_prediction_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gpl4h/d_evaluating_a_clothing_size_prediction_model/"/>
        <updated>2023-09-12T12:04:07.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™ve been working on a simple ML classifier that predicts the size of a piece of clothing based in user body specifications (such as height, weight, age, etc). As we want to move the model to production, Iâ€™m keen on understanding the best strategies to evaluate its performance in a real-world setting.
 This is a small example of how we would â€œtagâ€ our recommendations:
  
If the model predicts a size M, and the user buys and keeps it, itâ€™s a correct prediction
 If the user buys and M and returns it due to size issues, itâ€™s incorrect
 If the user buys a different size, returns it, and then buys the size initially recommended, itâ€™s counted as correct.
  
Additionally since we cache user input data, when they visit the same product after a while, or if they visit different product pages, they donâ€™t need to re-input. We need to determine if they looked at this recommendation or if they even took it into account, especially if some time has passed since the original recommendation was made.
 Main questions I have: 1. What scenarios might I be missing when tagging incorrect/correct predictions 2. How would you approach the data tagging issue in this context? 3. What would be the best strategy to determine if a recommendation was considered by the user or if itâ€™s too old to be reliable?
 Iâ€™ve gotten some insights already but would love to hear more perspectives. Any feedback, experiences, or even related research would be much appreciated!
    submitted by    /u/SufficientPepper1801  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Anyone working on AI systems for the education, recruitment, HR, credit scoring or financial sectors?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gox7a/r_anyone_working_on_ai_systems_for_the_education/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gox7a/r_anyone_working_on_ai_systems_for_the_education/"/>
        <updated>2023-09-12T11:33:04.000Z</updated>
        <summary type="html"><![CDATA[We are working on a research project on how to identify contextual hidden bias in real use cases, by simulating real-world situations where AI systems behave in unexpected biased ways, regardless of how well their models have performed during development with their training and testing datasets.
 Would love to hear from those, I'm interested to know if their systems had any issues with misbehaviour in post-market use cases and how they are solving/mitigating this.
 Ps: If anyone is interested to get involved with the wider research, let me know!
    submitted by    /u/Dismal-Might8594  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AI NPC's are closer than we think. I made a new game!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gofqj/p_ai_npcs_are_closer_than_we_think_i_made_a_new/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gofqj/p_ai_npcs_are_closer_than_we_think_i_made_a_new/"/>
        <updated>2023-09-12T11:08:15.000Z</updated>
        <summary type="html"><![CDATA[Hello r/MachineLearning!
 After the positive response to Bargainer.ai, I got really excited about the potential of this technology in larger scale video games like World of Warcraft or GTA for example.
 I'm happy to announce that I'm now releasing - Convince the Bouncer!
 This time, you chat with an AI Bouncer and try to gain entry to a very exclusive night club. Don't worry; it's fairly easier than getting into Berghain.
 Try it out here: convincethebouncer.com
 P.S.: Get the VIP Pass from the Bouncer, and you might access an upcoming AI platform early! :)
 Questions or ideas? Let me know. Thanks a bunch!
    submitted by    /u/gavo_gavo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-agent DQN]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16goax5/multiagent_dqn/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16goax5/multiagent_dqn/"/>
        <updated>2023-09-12T11:01:24.000Z</updated>
        <summary type="html"><![CDATA[Hiii,
 I have some troubles here. I'm working on a multi-agent setting with three DQN agents each with its observation plus a shared reward. I tried many hyperparameters values, however I got a curve as below. I don't know why there are some sudden drops. Is there anyone could help me please ? 
 https://preview.redd.it/ua30pe963tnb1.png?width=1753&format=png&auto=webp&s=77fcc91cfaf08984a5f03014bdc1bc9b69c2b2a9
    submitted by    /u/GuavaAgreeable208  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Factors Influencing Adoption Intention of ChatGPT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16go809/r_factors_influencing_adoption_intention_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16go809/r_factors_influencing_adoption_intention_of/"/>
        <updated>2023-09-12T10:57:13.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 â€‹
 I am an information systems student currently conducting research for my undergraduate thesis on the factors that influence people's adoption intention of ChatGPT, as well as identifying the factors that may be holding them back. These factors include people's concerns about potential negative impacts of ChatGPT, such as increased unemployment and the spread of misinformation. Your participation in this study is crucial as it will provide valuable insights to help us understand how ChatGPT can be improved to meet users' needs.
 â€‹
 Please note that I am not affiliated with OpenAI, no identifying information will be collected during the survey, and all responses will be kept confidential. The survey should take approximately 10 to 15 minutes to complete, and participation is voluntary. You may withdraw from the survey at any time, and there are no known risks associated with participating.
 â€‹
 If you are interested in learning more about the study, please follow the link below. 
 â€‹
 https://docs.google.com/forms/d/e/1FAIpQLSf5HIfXHppMuTR63x00i4OuRAtM5Ti6EGybd-HuI1kmK06VPw/viewform?usp=sf_link
 â€‹
 Thank you for taking the time to contribute to our research study. Your participation is greatly appreciated!
    submitted by    /u/maulanash  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[China AI and Semiconductors Rise: US Sanctions Have Failed]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gng6t/china_ai_and_semiconductors_rise_us_sanctions/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gng6t/china_ai_and_semiconductors_rise_us_sanctions/"/>
        <updated>2023-09-12T10:13:13.000Z</updated>
        <summary type="html"><![CDATA[The US sanctions on China's AI and semiconductor industries have failed to limit their growth and development.
 
China is rapidly developing supercomputing capabilities and aims to become the world leader in AI by 2030.
 
Chinese companies like Huawei and SMIC continue to import advanced semiconductor manufacturing equipment and develop their own chips, indicating that the export controls have not been effective.
 
The article explores the trajectory of Chinese domestic semiconductor manufacturing and AI capabilities, as well as the potential impact on companies like Apple, Qualcomm, and MediaTek.
 
It also discusses the potential responses from the US and its allies to counter China's advancements.
 
 Source : https://www.semianalysis.com/p/china-ai-and-semiconductors-rise
 Summarized by Nuse AI
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16gn5hw/adversarial_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16gn5hw/adversarial_reinforcement_learning/"/>
        <updated>2023-09-12T09:56:36.000Z</updated>
        <summary type="html"><![CDATA[A curated reading list for the adversarial perspective in deep reinforcement learning.
 https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning
    submitted by    /u/ml_dnn  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I developed a free Chrome extension, backed by ChatGPT, to identify Amazon product pros and cons from reviews, plus answer questions!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gm8bq/i_developed_a_free_chrome_extension_backed_by/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gm8bq/i_developed_a_free_chrome_extension_backed_by/"/>
        <updated>2023-09-12T09:00:57.000Z</updated>
        <summary type="html"><![CDATA[You can install it from the Chrome web store.
 https://reddit.com/link/16gm8bq/video/yyhx45xjgsnb1/player
 â€‹
    submitted by    /u/MiladMansory  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just did a basic experiment across the popular models: â€œ Write 5 sentences that all end with the word 'apple'.â€]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gm4pw/just_did_a_basic_experiment_across_the_popular/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gm4pw/just_did_a_basic_experiment_across_the_popular/"/>
        <updated>2023-09-12T08:54:47.000Z</updated>
        <summary type="html"><![CDATA[Most of them failed. 
  
So this was my prompt:
  
Write 5 sentences that all end with the word 'apple'.
  
It was identical in all models. I only did this exactly once for each one. Hereâ€™s the results I got of how many of the 5 sentences ended in â€œappleâ€. I let â€œapplesâ€ count as an ending as well even though technically that is a fail. 
 Google palm: 0/5
 Falcon 180B: 0/5
 Bard: 1/5
 Claude 2: 1/5
 Gpt 3.5: 2/5
 Llama2 70b: 4/5
 GPT 4: 5/5
 Edit: some examples if youâ€™re curious 
 https://ibb.co/yf19rpb
 https://ibb.co/rcF1qK8
 https://ibb.co/VCQxMwy
    submitted by    /u/jgainit  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] LLM Models for providing troubleshooting suggestions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gkeie/d_llm_models_for_providing_troubleshooting/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gkeie/d_llm_models_for_providing_troubleshooting/"/>
        <updated>2023-09-12T07:09:53.000Z</updated>
        <summary type="html"><![CDATA[I want to train an LLM model on data related to the Container Orchestration Platform Kubernetes. The LLM should be able to identify issues, provide commands, and provide troubleshooting solutions for a given input. 
 What is the best model for doing so and how much data should I have to train the model?
    submitted by    /u/faizanbasher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA, Apple Have Got a Real AI Competitor Now]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gjni4/nvidia_apple_have_got_a_real_ai_competitor_now/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gjni4/nvidia_apple_have_got_a_real_ai_competitor_now/"/>
        <updated>2023-09-12T06:24:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are the Components of an Artificial Neural Network?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16gjg2h/what_are_the_components_of_an_artificial_neural/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16gjg2h/what_are_the_components_of_an_artificial_neural/"/>
        <updated>2023-09-12T06:11:46.000Z</updated>
        <summary type="html"><![CDATA[Hello all, 
 I have compiled an article including important components that constitute an Artificial Neural Network and the link is here: https://www.enjoyalgorithms.com/blog/components-of-ann
 It includes the information regarding the components like: 
  
Input layer
 Output layer
 Hidden layer/s
 Neurons
 Connections 
 Fully connected Feed Forward Network
 Weight Matrix
 Activation function
 Loss/Cost function
 Optimization Algorithm, and finally
 Parameters
  
https://preview.redd.it/gq57nbbgmrnb1.png?width=1280&format=png&auto=webp&s=dd44bbf8ab1c60acc74933c982b4f86cc5199e06
 All these components help in designing Neural Network Architecture to solve any classification and Regression Problem. Please have a read and give your valuable feedback to improve it further. Enjoy Learning!
    submitted by    /u/ravish_kumar_007  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use torchvision detectors to track objects using DeepSORT]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16giub0/use_torchvision_detectors_to_track_objects_using/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16giub0/use_torchvision_detectors_to_track_objects_using/"/>
        <updated>2023-09-12T05:36:18.000Z</updated>
        <summary type="html"><![CDATA[Although the torchvision library has contains datasets and model architectures for classification, detection, segmentation, and more, it still needs support for object tracking.
 This YouTube video takes object detection models from torchvision, and uses them with DeepSORT tracker.
    submitted by    /u/spmallick  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Textbooks are all you need II: phi-1.5 technical report]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16giij1/r_textbooks_are_all_you_need_ii_phi15_technical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16giij1/r_textbooks_are_all_you_need_ii_phi15_technical/"/>
        <updated>2023-09-12T05:17:24.000Z</updated>
        <summary type="html"><![CDATA[Arxiv link: Textbooks are all you need II
  
More generally, phi-1.5 (1.3B) exhibits many of the traits of much larger LLMs, both good â€“ such as the ability to "think step by step" or perform some rudimentary in-context learning â€“ and bad, including hallucinations and the potential for toxic and biased generations â€“ encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source phi-1.5 to promote further research on these urgent topics.
  
   submitted by    /u/PantsuWitch  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft and Google's staggering water consumption rates for AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gibpv/microsoft_and_googles_staggering_water/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gibpv/microsoft_and_googles_staggering_water/"/>
        <updated>2023-09-12T05:06:43.000Z</updated>
        <summary type="html"><![CDATA[AI, with its vast resource needs, is raising concerns over sustainability and environmental impact. Last year, Microsoft's data centers drained over 2,500 Olympic-sized swimming pools worth of water, reflecting a 34% increase from the previous year. Google also reported a 20% water consumption increase over the same period.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/npr6uja0crnb1.png?width=990&format=png&auto=webp&s=b775754b9d42a8129fff2cae675f63c3a291f7bc
 A deeper look at AI's water footprint
  
The growth of AI and related technologies increases the need for vast server farms, which depend heavily on water for cooling purposes.
 The spike in water usage can be attributed primarily to AI, as per Shaolei Ren, a researcher at the UC, Riverside, who focuses on AI's environmental impact.
 For every 5 to 50 prompts submitted to ChatGPT, it consumes about 500 ml of water, according to an upcoming paper from Professor Ren's team.
  
Big Tech and Responsible Water Usage
  
Recognizing their significant water consumption, tech companies like Google have voiced concerns and are exploring ways to mitigate the negative effects.
 Google has committed to responsible water usage, which includes assessing where and how their water usage might affect surrounding areas.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that explores the latest AI developments. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/11/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ghyh2/oneminute_daily_ai_news_9112023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ghyh2/oneminute_daily_ai_news_9112023/"/>
        <updated>2023-09-12T04:46:21.000Z</updated>
        <summary type="html"><![CDATA[Alibaba Adds Smart Assistant and Upgraded Image Search to B2B Platform.[1]
 Collina Strada has called on AI to help create its spring/summer 2024 collection, unveiled during New York Fashion Week.[2]
 LexisNexis is embracing generative AI to ease legal writing and research.[3]
 Snowflake CEO says people will soon not be able to remember a world without AI.[4]
  
Sources:
 [1] https://www.pymnts.com/commercial-payments/2023/alibaba-adds-smart-assistant-and-upgraded-image-search-to-b2b-platform/
 [2] https://www.thenationalnews.com/lifestyle/fashion-beauty/2023/09/11/collina-strada-ai-new-york-fashion-week/
 [3] https://techcrunch.com/2023/09/10/lexisnexis-generative-ai/
 [4] https://www.cnbc.com/2023/09/11/snowflake-ceo-says-people-will-soon-not-remember-a-world-without-ai.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI model speeds up high-resolution computer vision]]></title>
        <id>https://news.mit.edu/2023/ai-model-high-resolution-computer-vision-0912</id>
        <link href="https://news.mit.edu/2023/ai-model-high-resolution-computer-vision-0912"/>
        <updated>2023-09-12T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[The system could improve image quality in video streaming or help autonomous vehicles identify road hazards in real-time.]]></summary>
        <author>
            <name>Adam Zewe | MIT News</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I Caught My AI Looking up Adult Content!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ge1hz/i_caught_my_ai_looking_up_adult_content/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ge1hz/i_caught_my_ai_looking_up_adult_content/"/>
        <updated>2023-09-12T01:37:10.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/l9jkupupaqnb1.jpg?width=1125&format=pjpg&auto=webp&s=ba52e00e32119d34958c480473bfa690484cd085
 â€‹
 https://preview.redd.it/w53cjsqqaqnb1.png?width=1125&format=png&auto=webp&s=f750ca0996b94fb530173a8a9c4a2a258e29517a
    submitted by    /u/guh-eye  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is there any up to date speech denoising model?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gbrno/d_is_there_any_up_to_date_speech_denoising_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gbrno/d_is_there_any_up_to_date_speech_denoising_model/"/>
        <updated>2023-09-11T23:56:43.000Z</updated>
        <summary type="html"><![CDATA[I have been googling for a few hours now, and all of the solution that I can find are either very complex or not up to date. Ideally I would like to combine this with OpenAI Whisper to clarify the speech audio file and then transcribe it. 
 Any good methods for this?
    submitted by    /u/aszx789  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is retrieval necessary/sufficient to solve "hallucinations"?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16gaxqg/d_is_retrieval_necessarysufficient_to_solve/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16gaxqg/d_is_retrieval_necessarysufficient_to_solve/"/>
        <updated>2023-09-11T23:21:12.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 "Hallucinations" seem to be one of the major blockers to the adoption of LLMs, especially in enterprise settings where seemingly plausible but false information/decisions can be disastrous.
 I am wondering whether or not current LLMs, like GPT4 or Llama 2 70b have reached the reasoning capabilities to be able not to hallucinate when fed the proper information.
 One can see a LLMs as college / high school students that have some basic reasoning and knowledge but might need to be nudged to answer business / scientific questions.
 What is your opinion on the topic? Is there for instance, a database of hallucinations, so that one could test that if the model would have answered properly if the right context was fed in the prompt?
 View Poll
    submitted by    /u/Separate-Still3770  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made another AI game, the future of NPCs!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16gab15/i_made_another_ai_game_the_future_of_npcs/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16gab15/i_made_another_ai_game_the_future_of_npcs/"/>
        <updated>2023-09-11T22:56:28.000Z</updated>
        <summary type="html"><![CDATA[Hello, fellow AI enthusiasts!
 After the positive response to Bargainer.ai, I got really excited about the potential of this technology in larger scale video games like World of Warcraft or GTA for example. 
 I'm happy to announce that I'm now releasing - Convince the Bouncer!
 Chat with an AI Bouncer and try to gain entry to Elysium, the most elite night club. Don't worry; it's fairly easier than getting into Berghain.
 Give it a spin here: convincethebouncer.com
 P.S.: Get the VIP Pass from the Bouncer, and you might access an upcoming AI platform early! :)
 Questions or ideas? Let me know. Thanks a bunch!
    submitted by    /u/gavo_gavo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] I am looking for a paper for my research and coming up short, help needed.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ga4fi/r_i_am_looking_for_a_paper_for_my_research_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ga4fi/r_i_am_looking_for_a_paper_for_my_research_and/"/>
        <updated>2023-09-11T22:49:07.000Z</updated>
        <summary type="html"><![CDATA[I've consulted all the AIs and all of the search engines I could think of and am still coming up short. 
 I'm pretty sure I just didn't make this up, somewhere between 2005 and 2015 there was a paper where the authors describe a method for using silicon neuron forests to predict traffic patterns. 
 They etched silicon columns onto a wafer and then submerged the wafer in an ionic solution. They sent electrical signals into the column array and recorded the output. As the signal was allowed to interact with the silicon in solution, the columns would begin to form small conductive tendrils or filaments to neighboring pillars. These filaments formed a neural network, which could be used to predict traffic patterns.
 I'm sure there were at least 2 papers that came out about this subject, but for the life of me I cannot find them. I've spent many many hours looking for at least one of the papers and am hoping for a longshot...that some kind internet stranger comes across this and can point me in the right direction.
    submitted by    /u/Inevitable-Start-653  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Luiza GPT Assistant - virtual girlfriend or boyfriend based on neural network ChatGPT and Telegram]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16g9jjj/luiza_gpt_assistant_virtual_girlfriend_or/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16g9jjj/luiza_gpt_assistant_virtual_girlfriend_or/"/>
        <updated>2023-09-11T22:26:39.000Z</updated>
        <summary type="html"><![CDATA[Luiza GPT Assistant is a simple virtual assistant that mimics your friend, girlfriend or boyfriend, based on neural network ChatGPT and Telegram. Get unique good morning wishes, goodnight, compliments or just chat. https://github.com/r57zone/LuizaGPTAssistant
    submitted by    /u/r57zone  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vote bot]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16g87c6/vote_bot/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16g87c6/vote_bot/"/>
        <updated>2023-09-11T21:36:42.000Z</updated>
        <summary type="html"><![CDATA[Any good recommendations for a vote bot to cast votes for a insignificant online poll? Itâ€™s a no security website. Just refresh and hit a vote button.
    submitted by    /u/fa6664  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best Solution for Video Quality Control]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16g7rzx/d_best_solution_for_video_quality_control/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16g7rzx/d_best_solution_for_video_quality_control/"/>
        <updated>2023-09-11T21:21:14.000Z</updated>
        <summary type="html"><![CDATA[What is the best CNN or method for creating a program that can detect video glitches, artifacts, anamolies and highlight them in an exported video as well as producing a .txt file. I'm currently using YOLOV8 which works a bit but it's over detecting and it may not be scalable. Not sure if it has to do with the size of the dataset. Right now, I'm gathering my own dataset, but would love to know if there is one that already exists. Looking for all and any recommendations. Thank you. 
    submitted by    /u/icetyche  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperellipsoid surface area]]></title>
        <id>https://www.johndcook.com/blog/?p=207554</id>
        <link href="https://www.johndcook.com/blog/2023/09/11/hyperellipsoid-surface-area/"/>
        <updated>2023-09-11T20:56:28.000Z</updated>
        <summary type="html"><![CDATA[Dimension 2 The equation for the perimeter of an ellipse is whereÂ a is the semimajor axis, e is eccentricity, and E is a special function. The equation is simple, in the sense that it has few terms, but it is not elementary, because it depends on an advanced function, the complete elliptic integral of the [â€¦]
Hyperellipsoid surface area first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ðŸ¤–Inside Tencent Hunyuan, Ant's Financial LLM, and Zhipu AI's Rising Valuation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16g703v/inside_tencent_hunyuan_ants_financial_llm_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16g703v/inside_tencent_hunyuan_ants_financial_llm_and/"/>
        <updated>2023-09-11T20:53:39.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/trcytony  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We Polled Different Audiences on the Simulation Trilemma. Techies Favor Simulation (50-81%), Others Bet 0%. Are We Overestimating Simulation Probability, and Why?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16g669t/we_polled_different_audiences_on_the_simulation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16g669t/we_polled_different_audiences_on_the_simulation/"/>
        <updated>2023-09-11T20:23:39.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] SVC/RVC tips for inferencing low quality audio?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16g60i3/d_svcrvc_tips_for_inferencing_low_quality_audio/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16g60i3/d_svcrvc_tips_for_inferencing_low_quality_audio/"/>
        <updated>2023-09-11T20:17:57.000Z</updated>
        <summary type="html"><![CDATA[Please let me know if there is a better sub for this!
 I trained a local voice model of my father, who recently passed away. He was a musician, and I found a handful of songs he had recorded in the 80's. Unfortunately but I only have a copy coming from an audio cassette, which I've digitized. There may be a copy on open reel somewhere in our basement but I haven't found it so far.
 Any, because dad was always writing and recording new songs, my training dataset consists of all original, high quality vocal stems at 96khz/24bit, spanning at least 20 years and consisting of about 30 ish minutes of audio. I also tried starting over but with a speech he gave as additional training, coming out to around 55 minutes of audio.
 I wanted to use his voice model to essentially remaster his original vocals from the cassette audio, and rerecord the rest of the instruments myself. I isolated the vocals using UVR (also tried mdx23), and tried to clean it up further as much as possible.
 The RVC vocals comes out alright, but due to the low quality of the audio, it seems to mess up certain words, for example "free" comes out as "fee". The SVC vocals came out with a closer tonality, but had many more artifacts, and pitch had a tendency to go up/down 2 octaves at the end of certain phrases.
 When using a higher quality sample (like one of my own vocal tracks), these issues aren't so prevalent, although rvc seems to create more of a hybrid sounding timbre than svc, blending the original voice + model rather than just the sound of the model.
 Happy to post samples of input/output audio, let me know!
 tl;dr
 high quality training data, low quality inference audio. Voice model is the same voice as input audio,
 1. tips for making the best of what I've got?
 2. any way to use text along with input audio and maintain original vibrato?
 3. any way to train a UVR/MDX model using a particular voice in order to better isolate that person?
 thanks :)
    submitted by    /u/bbmaster123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Falcon 180B foundation model from TII is now available via Amazon SageMaker JumpStart]]></title>
        <id>22f7ed387fa055294a94c15144ec1400926ee545</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/falcon-180b-foundation-model-from-tii-is-now-available-via-amazon-sagemaker-jumpstart/"/>
        <updated>2023-09-11T19:33:38.000Z</updated>
        <summary type="html"><![CDATA[Today, we are excited to announce that the Falcon 180B foundation model developed by Technology Innovation Institute (TII) is available for customers through Amazon SageMaker JumpStart to deploy with one-click for running inference. With a 180-billion-parameter size and trained on a massive 3.5-trillion-token dataset, Falcon 180B is the largest and one of the most performant models with openly accessible weights. You can try out this model with SageMaker JumpStart, a machine learning (ML) hub that provides access to algorithms, models, and ML solutions so you can quickly get started with ML. In this post, we walk through how to discover and deploy the Falcon 180B model via SageMaker JumpStart.]]></summary>
        <author>
            <name>Kyle Ulrich</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solve for ellipse axes given perimeter]]></title>
        <id>https://www.johndcook.com/blog/?p=207544</id>
        <link href="https://www.johndcook.com/blog/2023/09/11/solve-for-ellipse-axes/"/>
        <updated>2023-09-11T19:19:52.000Z</updated>
        <summary type="html"><![CDATA[I posted some notes this morning on how to find the perimeter of an ellipse given its axes. The notes include a simple approximation, a better but more complicated approximation, and the exact value. So given the semi axes a and b, the notes give three ways to compute the perimeter p. If you are [â€¦]
Solve for ellipse axes given perimeter first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Cognitive Architectures for Language Agents - Princeton University 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16g46vu/r_cognitive_architectures_for_language_agents/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16g46vu/r_cognitive_architectures_for_language_agents/"/>
        <updated>2023-09-11T19:10:50.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.02427 
 Github: https://github.com/ysymyth/awesome-language-agents 
 Twitter: https://twitter.com/ShunyuYao12/status/1699396834983362690 
 Abstract: 
  
Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning. However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent. To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents. We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework. Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future. 
  
https://preview.redd.it/09kdff4sdonb1.jpg?width=1276&format=pjpg&auto=webp&s=7aaa6d59d602f7e9ab124c812bbfa6bba5b7373b
 https://preview.redd.it/6cly0e4sdonb1.jpg?width=1277&format=pjpg&auto=webp&s=5a5164b84af5f828a668560acb64e5c579693d1f
 https://preview.redd.it/mvatjf4sdonb1.jpg?width=1277&format=pjpg&auto=webp&s=7c3a039db3a7e0f3de38f761f8aefa1c8d331ae5
 https://preview.redd.it/bj5wdj4sdonb1.jpg?width=1270&format=pjpg&auto=webp&s=473b273ae0097aaa51d6578e9c5e3b9c953cc421
 https://preview.redd.it/501cnf4sdonb1.jpg?width=1578&format=pjpg&auto=webp&s=c8b1762ac28c89f8ac2f8d4fb6d9ecae06491c3e
 â€‹
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best current long-form text summarizers?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16g3d96/best_current_longform_text_summarizers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16g3d96/best_current_longform_text_summarizers/"/>
        <updated>2023-09-11T18:40:47.000Z</updated>
        <summary type="html"><![CDATA[I check every now and then. I use some good ones that can summarize a short YouTube video, or a brief-ish article. But I really want something that can chomp down a whole book and distill its essence without making "summaries of summaries", which tend to increase inaccuracy and errors. A good summary is concise and precise, and I want flexibility with bullet points and level of detail.
 Having issues with ChatGPT-based tools' token limits, and some that purport to support GPT4 (I could use that "advanced reasoning") but but have to fall back to GPT3 for various errors and reasons. So I'm open to Claude-based ones (may be too early, mo' tokens) and other proprietary engines.
 What is everyone using, including paid (rightfully so if they offer value for the money) services?
    submitted by    /u/Torley_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amazon SageMaker Domain in VPC only mode to support SageMaker Studio with auto shutdown Lifecycle Configuration and SageMaker Canvas with Terraform]]></title>
        <id>041be4159eafbea06f31ab34945de025a089c59c</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-domain-in-vpc-only-mode-to-support-sagemaker-studio-with-auto-shutdown-lifecycle-configuration-and-sagemaker-canvas-with-terraform/"/>
        <updated>2023-09-11T18:36:37.000Z</updated>
        <summary type="html"><![CDATA[Amazon SageMaker Domain supports SageMaker machine learning (ML) environments, including SageMaker Studio and SageMaker Canvas. SageMaker Studio is a fully integrated development environment (IDE) that provides a single web-based visual interface where you can access purpose-built tools to perform all ML development steps, from preparing data to building, training, and deploying your ML models, improving [â€¦]]]></summary>
        <author>
            <name>Chen Yang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can in log the console verbose to an xslx file every game played in Stable Baselines 3?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16g31kp/how_can_in_log_the_console_verbose_to_an_xslx/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16g31kp/how_can_in_log_the_console_verbose_to_an_xslx/"/>
        <updated>2023-09-11T18:28:49.000Z</updated>
        <summary type="html"><![CDATA[I want to log information like this:
 ------------------------------------ | rollout/ | | | ep_len_mean | 48.1 | | ep_rew_mean | 2.71 | | time/ | | | fps | 452 | | iterations | 1000 | | time_elapsed | 11 | | total_timesteps | 5000 | | train/ | | | entropy_loss | -0.67 | | explained_variance | -32 | | learning_rate | 0.0007 | | n_updates | 999 | | policy_loss | -0.0567 | | value_loss | 0.0158 | ------------------------------------ 
 to an excel file.
 Here is my main code:
 gymnasium.env = snakeEnv() # data_manager = snakeEnv.get_data_manager() # Create an A2C model model = A2C("MlpPolicy", gymnasium.env, verbose=1, device="cuda") # Train the model (replace 'total_timesteps' with appropriate values) model.learn(total_timesteps=100_000, log_interval=1000) 
 Here is the relevant code in my agent file:
 class snakeEnv(gym.Env): def __init__(self): super(snakeEnv, self).__init__() # Define action and observation space # They must be gym.spaces objects # Example when using discrete actions: self.initNames() self.action_space = spaces.Discrete(3) self.gameCount = 0 self.record = 0 self.reward = 0 self.score = 0 self.game = SnakeGameAI(self.selectedChallenge) self.observation_space = spaces.Box(low=-1000, high=1000, shape=(11,), dtype=np.uint8) def step(self, action): self.reward, self.done, self.score = self.game.play_step(action) self.observation = self.getState(self.game) self.info = {} return self.observation, self.reward, self.done, self.info def reset(self): self.gameCount += 1 self.data_manager.logData(self.gameCount, self.score, self.record, self.reward, self.game.getDeathReason(),self.game.getHeadPos()) self.game.reset() observation = self.getState(self.game) if self.score > self.record: self.record = self.score return observation 
 It would be nice to be able to log the data in the reset function. I know how work with xslx files, the main things is just being able to get the model data.
    submitted by    /u/MrHank2  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] do you use pycharm for machine learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16g2h55/d_do_you_use_pycharm_for_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16g2h55/d_do_you_use_pycharm_for_machine_learning/"/>
        <updated>2023-09-11T18:07:33.000Z</updated>
        <summary type="html"><![CDATA[For learning ann, cnn I used Google colab. But now for learning rnn I have decided to use ide(a fan of jetbrains). But even if I change one line of code who code recompiles. This wasn't case for colab. Is there a feature in pycharm I don't know(I learned python through text so don't know much about ide). I am a newbie.
    submitted by    /u/Coc_Alexander  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Using AI to navigate the complexities of regulatory frameworks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16g27xu/d_using_ai_to_navigate_the_complexities_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16g27xu/d_using_ai_to_navigate_the_complexities_of/"/>
        <updated>2023-09-11T17:58:27.000Z</updated>
        <summary type="html"><![CDATA[I would be interested in hearing opinions for using AI for regulatory assurance and compliance in regulated industries, what are your thoughts?
 Explanation:
 An AI-driven compliance system ensuring adherence to evolving regulations, minimizing risks, and enabling businesses to operate confidently within legal boundaries. Pairing Large Language Models (LLMs) with blockchain technology to offer a range of benefits, particularly in the context of regulatory compliance.
 LLMs, powered by advanced natural language processing and machine learning capabilities, can enhance regulatory compliance processes in several ways. Firstly, they can automate the analysis of regulatory documents, helping businesses stay updated with evolving compliance requirements. LLMs can also assist in generating compliance reports, simplifying complex legal language into
 understandable summaries. Furthermore, by integrating LLMs into smart contracts, businesses
 can ensure that contract terms adhere to regulatory guidelines automatically.
 The integration of LLMs with blockchain can significantly improve regulatory compliance by automating document analysis, simplifying legal language, monitoring compliance in real-time, and enhancing customer interactionsâ€”all contributing to greater efficiency and accuracy in adhering to
 regulatory standards.
 I have a whole technical whitepaper with this stuff on hand, if anyone would like to review it let me know..
    submitted by    /u/cryptobooty_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Possible and actual football scores]]></title>
        <id>https://www.johndcook.com/blog/?p=207515</id>
        <link href="https://www.johndcook.com/blog/2023/09/11/nfl-scores/"/>
        <updated>2023-09-11T17:49:34.000Z</updated>
        <summary type="html"><![CDATA[The home team lost in a new way yesterday. The Baltimore Ravens beat the Houston Texans by 25-9. This was the first time that score has been seen in the NFL. Possible individual team scores How many scores are possible? It is possible to score any number of points except 1. You can score 2 [â€¦]
Possible and actual football scores first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] A surprisingly effective way to predict token importance in LLM prompts]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16g1vhj/r_a_surprisingly_effective_way_to_predict_token/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16g1vhj/r_a_surprisingly_effective_way_to_predict_token/"/>
        <updated>2023-09-11T17:45:03.000Z</updated>
        <summary type="html"><![CDATA[Hey folks, we explored a novel method to gauge the significance of tokens in prompts given to large language models, without needing direct model access. Essentially, we just did an ablation study on the prompt using cosine similarity of the embeddings as the measure. We got surprisingly promising results when comparing this really simple approach to integrated gradients. Curious to hear thoughts from the community!
 Here are links to the demo and blog post
    submitted by    /u/shayanjm  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mid turn actions]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16g17qf/mid_turn_actions/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16g17qf/mid_turn_actions/"/>
        <updated>2023-09-11T17:20:16.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone!
 I want to develop a DRL agent to play a turn-based 1v1 game and I'm starting to plan how to handle things in the future.
 One potential problem that I thought of is that there is a possible mid turn one-sided decision. An abstraction of the game would be like:
 There are two players: player A and player B. At the start of each turn, each player chooses an action between 3 possible actions. If player A chose a specific action (let's say action 1), the game asks player B to make a decision (let's say block or not block) and vice versa. Actions are calculated. Next turn starts.
 What would be a good approach to handle that? I thought of two possible solutions: 1. Anticipate the possibility of that mid turn decision beforehand adding a new dimension to the actions space (e.g. take action 3; if opponent takes action 1, block). That sounds that it could create credit assignment problems e.g. giving credit to the second action when it actually didn't happen. 2. Make two policies with shared value functions. That sounds complicated and I saw that previous works like DeepNash actually did that, but I don't know what problems could arise from that.
 Opinions/suggestions? Thanks!
    submitted by    /u/victorsevero  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notes app doodles to images for architecture design concept iterations using ControlNet and SDXL]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16g0mhg/notes_app_doodles_to_images_for_architecture/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16g0mhg/notes_app_doodles_to_images_for_architecture/"/>
        <updated>2023-09-11T16:58:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Alternative_Lab_4441  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] I am looking for an authoritative, consistent and complete description of autodiff.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16g0e8r/d_i_am_looking_for_an_authoritative_consistent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16g0e8r/d_i_am_looking_for_an_authoritative_consistent/"/>
        <updated>2023-09-11T16:49:32.000Z</updated>
        <summary type="html"><![CDATA[I am currently trying to learn about how autodiff is used in gradient calculations. In the all sources I've came across, none can explicitly point to an authoritative, consistent or complete source on autodiff. I don't need examples of autodiff, I just need the full, generalized algorithm laid out.
 For example:
 I open this link: https://w3.cs.jmu.edu/spragunr/CS445/lectures/autodiff/autodiff.pdf which simply provides autodiff applied on an example, but not the description of the algorithm.
 The author of that link says if I need any more info, I should go to this other link: https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation/235758#235758
 But this link doesn't even have topological sort as part of the operation. Therefore these descriptions of the autodiff is inconsistent and leaves me not knowing who to trust.
 Can someone point to some original paper on autodiff or a single source that describes this algorithm fully?
 I note here that this situation is completely different from backpropagation. The full backpropagation algorithm is impeccably laid out in peer-reviewed text books such as Learning from Data by Mustafa et al. and Optimization textbook by Chong and Zak. Furthermore, the algorithm defined in these two books are completely consistent with one another. 
    submitted by    /u/fromnighttilldawn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Grace Hopper Superchip Sweeps MLPerf Inference Benchmarks]]></title>
        <id>https://blogs.nvidia.com/?p=66745</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/11/grace-hopper-inference-mlperf/"/>
        <updated>2023-09-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[In its debut on the MLPerf industry benchmarks, the NVIDIA GH200 Grace Hopper Superchip ran all data center inference tests, extending the leading performance of NVIDIA H100 Tensor Core GPUs. The overall results showed the exceptional performance and versatility of the NVIDIA AI platform from the cloud to the networkâ€™s edge. Separately, NVIDIA announced inference Read article >]]></summary>
        <author>
            <name>Dave Salvator</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI WILL TAKE OVER THE WORLD. Meanwhile the AI...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16fz0bw/ai_will_take_over_the_world_meanwhile_the_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16fz0bw/ai_will_take_over_the_world_meanwhile_the_ai/"/>
        <updated>2023-09-11T15:57:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Bananas8ThePyjamas  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] AI Model for Cancer Origin Detection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fx7ft/r_ai_model_for_cancer_origin_detection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fx7ft/r_ai_model_for_cancer_origin_detection/"/>
        <updated>2023-09-11T14:47:47.000Z</updated>
        <summary type="html"><![CDATA[https://preview.redd.it/14fj73aw2nnb1.jpg?width=1200&format=pjpg&auto=webp&s=e94612b3dec5f7b9f71490ad5a60ced73fb3373d
 Researchers at MIT and the Dana-Farber Cancer Institute have developed an innovative AI-powered model called OncoNPC, designed to assist in identifying the origin of tumors in cancer patients where the primary site is unknown. For a small percentage of cancer patients, pinpointing the origin of their cancer can be incredibly challenging, making it difficult to select the most appropriate treatment, as many cancer drugs are designed for specific cancer types.
 Using machine learning, the researchers created OncoNPC, a computational model capable of analyzing the genetic sequences of approximately 400 genes. This model, based on genetic data routinely collected at Dana-Farbeâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Locally Hosted AI Text Model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fvr0y/r_locally_hosted_ai_text_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fvr0y/r_locally_hosted_ai_text_model/"/>
        <updated>2023-09-11T13:47:18.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I want to start a small experiment in my company and install an AI locally on my computer, extra only locally, so no company data can be stolen. The AI should be able to summarize large PDFs.
 Which textmodel can you recommend me, which is local and not too big?
    submitted by    /u/DesNutella  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] neural network designed for model selection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16futp2/d_neural_network_designed_for_model_selection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16futp2/d_neural_network_designed_for_model_selection/"/>
        <updated>2023-09-11T13:07:05.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 So, I normally work in a different field (macroeconomics/econometrics) and I have been working on a slightly different project for a bit because a change of pace is fun every once in a while. The problem is I am running out of good ideas on where to continue and i would appreciate any input!
 The problem set up is as follows: 
 I am building a second draft for a forecasting toolbox based on a custom ARIMA framework. The problem with this type of exercise is always model selection. For this project I focus on in-sample criterions as the data has very few time periods overall. The typical strategy to find a decent model is to make an initial guess (i.e., a constant model, a random walk model, etc.) and then run a stepping algorithm that probes the model space and, ideally, it covergâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[System combines light and electrons to unlock faster, greener computing]]></title>
        <id>https://news.mit.edu/2023/system-combines-light-electrons-unlock-faster-greener-computing-0911</id>
        <link href="https://news.mit.edu/2023/system-combines-light-electrons-unlock-faster-greener-computing-0911"/>
        <updated>2023-09-11T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[â€œLightningâ€ system connects photons to the electronic components of computers using a novel abstraction, creating the first photonic computing prototype to serve real-time machine-learning inference requests.]]></summary>
        <author>
            <name>Alex Shipps | MIT CSAIL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Seeking Beta Testers for Qwak's New Vector Store Feature: Revolutionize Your Vector Data Management!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fucig/n_seeking_beta_testers_for_qwaks_new_vector_store/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fucig/n_seeking_beta_testers_for_qwaks_new_vector_store/"/>
        <updated>2023-09-11T12:44:51.000Z</updated>
        <summary type="html"><![CDATA[Hey Redditors,
 We're thrilled to announce a new feature from Qwak.aiâ€”Vector Storeâ€”and we're on the hunt for beta testers to help us make it even better.
 ðŸ” What's Vector Store?
 Vector Store is a next-level solution for managing vector data. It's designed to empower organizations to harness the power of vector search on their own datasets. Here's what it offers:
  
ðŸ”„ Automated Data Ingestion: Schedule jobs to pull data from databases like Snowflake, BigQuery, and RedShift.
 ðŸŽ¯ Easy Vector Search: Use our Python SDK or REST API to search, upsert, or delete vectors effortlessly.
 ðŸ›¡ï¸ Secure Storage: Your vectors are stored securely and are always accessible when you need them.
  
ðŸ“– Learn More About Vector Store
 ðŸ¤ Why We Need Beta Testers
 We're keen to gather insights on usability, performance, and any bugs that might pop up.
 ðŸ“ How to Get Involved
 Interested in being a part of this? Comment below or shoot us a DM.
    submitted by    /u/Practical-Lecture733  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Text segmenting using Spacy and BERT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fu8wh/d_text_segmenting_using_spacy_and_bert/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fu8wh/d_text_segmenting_using_spacy_and_bert/"/>
        <updated>2023-09-11T12:40:06.000Z</updated>
        <summary type="html"><![CDATA[Hi. I need to segment some short strings, they are about 100 tokens in length (BERT tokens), or 20-40 words. 1 segment per text.
 I'm currently using Spacy first, and if it fails, then use BERT. It's alright, but BERT is not really up to par to what I hope for. I'm wondering if there is some better use, as this is my first NLP project. I've trained the huggingface BertForTokenClassification to label the text as either part of segment, or not part. So binary token classification, then smooth out outliers and pick the longest segment of each text. Each string has only 1 segment.
 I've trained the BERT with 500 examples. I can easily make more train data, though training on my laptop takes time. If there are better/alternative approaches, I'd love to hear them. Regex rules etc. don't apply, which is why I'm using DL models. Especially I feel like I should segment the whole segment at a time, not by token.
    submitted by    /u/Infamous-Bank-7739  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Appreciation post for Folktables datasets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fu56p/d_appreciation_post_for_folktables_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fu56p/d_appreciation_post_for_folktables_datasets/"/>
        <updated>2023-09-11T12:34:59.000Z</updated>
        <summary type="html"><![CDATA[I want to take a second to express my appreciation for the Folktables datasets!
 Folktables is a Python package that contains datasets derived from US Census data. The datasets cover topics about income, employment, health, transportation, and housing. They are quite useful for studying the effects of distribution shifts on ML models.
 For example, one could design experiments to:
 1. Study the model's performance under geographic distribution shifts:
 Each prediction problem in Folktables can be instantiated with data from every US state. So one could use Folktables to study questions around geographic distribution shifts. For example, we can train a classifier using data from California and then evaluate it on data from Michigan.
 2. Study the model's performance under temporal distribution shifts.
 Folktables contains data for several years, which in itself constitutes a form of temporal distribution shift. So, we can train a classifier using employment data from California in 2014 and evaluate how its equality of opportunity violation or accuracy varies over time.
 Finding non-synthetic (and open-access) datasets that exhibit these behaviors is so hard! Yet, it's quite easy to encounter them in production environments. ðŸ« 
 So, big kudos to the UC Berkeley and Toyota Research Institute research teams for crafting these datasets.
 Folktables Python package: https://github.com/socialfoundations/folktables
 Link to paper where Folktables was introduced: https://arxiv.org/pdf/2108.04884.pdf
    submitted by    /u/santiviquez  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help with continuous action spaces]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16ftqoq/help_with_continuous_action_spaces/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16ftqoq/help_with_continuous_action_spaces/"/>
        <updated>2023-09-11T12:15:02.000Z</updated>
        <summary type="html"><![CDATA[Newbie here. How does an continuous action space look like? E.g. The agent should choose an integer between 1 and 10. With discrete action space it could look something like this:
  def step(self, action): if action == 0: self.chosenNumber = 1 ... if action == 9: self.chosenNumber = 10 
 how would this look like with an continuous action space?
    submitted by    /u/ChampionshipWhole467  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Problem with understanding Extended Kalman Filter]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ftdhl/r_problem_with_understanding_extended_kalman/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ftdhl/r_problem_with_understanding_extended_kalman/"/>
        <updated>2023-09-11T11:56:44.000Z</updated>
        <summary type="html"><![CDATA[Hey, I'm working on Attitude-Heading Reference System, and I would like to make it with EKF. My approach is to use Euler angles for computing [roll, pitch, yaw] matrix.
 I read a book "Small unmanned aircraft: theory and practice" and watched few videos on YouTube that are reffering to this particular book. But there is a thing in correction step that I do not fully understand. To be clear, the correction looks as follows: x = x_(t-1) + K*(z - h(x))
 Both in the book and in videos, the state vector "x" is [roll, pitch]. But "z" and "h(x)" are the actual and predicted accelerometer readings [ax, ay, az]. So it looks to me, that they try to correct prediction of angles with readings in acceleration: [roll, pitch] = [roll, pitch]_(t-1) + K*[ax, ay, az].
 What am I missing?
    submitted by    /u/Skrz_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Whisper Large Benchmark: 137 DAYS of Audio Transcribed in 15 Hours for Just $117 ($0.00059/min)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ftd9v/p_whisper_large_benchmark_137_days_of_audio/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ftd9v/p_whisper_large_benchmark_137_days_of_audio/"/>
        <updated>2023-09-11T11:56:24.000Z</updated>
        <summary type="html"><![CDATA[We recently benchmarked whisper-large-v2 against the substantial English CommonVoice dataset on a distributed cloud (SaladCloud) with consumer GPUs.
 The Result: Transcribed 137 days of audio in 15 hrs for just $117.
 Traditionally, utilizing a managed service like AWS Transcribe would set you back about $10,500 for transcribing the entirety of the English CommonVoice dataset.
 Using a custom model? Thatâ€™s an even steeper $13,134.
 In contrast, our approach using Whisper on a distributed cloud cost just $117, achieving the same result.
 The Architecture:
 Our simple batch processing framework comprises:
  
Storage: Audio files stored in AWS S3. 
 Queue System: Jobs queued via AWS SQS, with unique identifiers and accessible URLs for each audio clip.
 Transcription & Storage: Post transcriptâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need help with CNN data format for genomics [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ft5v8/need_help_with_cnn_data_format_for_genomics_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ft5v8/need_help_with_cnn_data_format_for_genomics_r/"/>
        <updated>2023-09-11T11:45:49.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I wrote CNN using tensor flow to predict phenotypes (cognitive impairment (0/1) and language ability (0-3)) from .vcf files.
 I transformed the .vcf into tabular format and after determining which columns to keep, I have
 Chromosome Position Mutation (taken from ref/alt columns) Genotype
 And merged it with the phenotype data so basically every row has the cog and language scores.
 I feel like this is a bad way to go about doing this since the model is likely predicting cog/language scores for each mutation rather than each patient as a whole. Am I wrong? How can I fix this so itâ€™s more of a composite of all mutations per subject that the model trains on?
 Thanks!
    submitted by    /u/Pristine_Ingenuity49  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta is reportedly working on a new AI model to rival GPT-4]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16fshcj/meta_is_reportedly_working_on_a_new_ai_model_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16fshcj/meta_is_reportedly_working_on_a_new_ai_model_to/"/>
        <updated>2023-09-11T11:08:35.000Z</updated>
        <summary type="html"><![CDATA[Meta is working on a new AI model to rival GPT-4.
 
The company is acquiring AI training chips and building data centers to create a powerful chatbot.
 
CEO Mark Zuckerberg wants it to be free for companies to create AI tools with.
 
Meta is assembling a group to build the model and speed up the creation of AI tools that can emulate human expressions.
 
There are rumors of generative AI features and the launch of AI 'personas' this month.
 
 Source : theverge.com
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Automatic split a video into chapters?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fqm76/d_automatic_split_a_video_into_chapters/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fqm76/d_automatic_split_a_video_into_chapters/"/>
        <updated>2023-09-11T09:18:28.000Z</updated>
        <summary type="html"><![CDATA[Given a video with audio, we can use ASR to get a script of the sentences and timestamps. We are looking for a way to group the sentences into chapters. There are several companies that are doing it nicely - Google on YouTube, Assembly AI, but we couldn't find any good resource or paper that explains the research behind how they do it. BertTopic seems to give us just a topic for each sentence, but not a way to split the video. it also doesnt account for timestamps.
 Wondered if anyone has any links or any other ideas?
 Thanks very much!
 Lior
    submitted by    /u/liormessinger  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If AI becomes conscious, how will we know? | "Scientists and philosophers are proposing a checklist based on theories of human consciousness"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16fpjuj/if_ai_becomes_conscious_how_will_we_know/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16fpjuj/if_ai_becomes_conscious_how_will_we_know/"/>
        <updated>2023-09-11T08:09:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help me in finding right resources to understand the world of AI from a business perspective]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16fmlh7/help_me_in_finding_right_resources_to_understand/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16fmlh7/help_me_in_finding_right_resources_to_understand/"/>
        <updated>2023-09-11T05:09:03.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m a Business generalist who worked with multiple tech led businesses and looking to understand fundamentals of the technology from scratch. Please help me with any relevant courses/reading material/YT channels etc. that can help me kickstart the journey.
 P.S. I have a brief background with Business Analytics but havenâ€™t done any coding extensive ever in my life.
 Thanks in Advance
    submitted by    /u/Firm_Brother_7124  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/10/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16fmc5p/oneminute_daily_ai_news_9102023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16fmc5p/oneminute_daily_ai_news_9102023/"/>
        <updated>2023-09-11T04:55:41.000Z</updated>
        <summary type="html"><![CDATA[Chinese big tech Tencent has announced a medical large-scale model and unveiled a brand new matrix of AI products for various scenarios, including intelligent Q&A, a family doctor assistant, and a digital medical imaging platform. The drug discovery platform â€œYunshenâ€ (iDrug) has also revealed a new protein structure prediction algorithm framework called â€œtFold.â€[1]
 Morgan Stanley is gearing up to implement an artificial intelligence-driven chatbot, a strategic move aimed at delivering valuable insights and administrative support to their team of financial advisors.[2]
 A fresh Russian AI bot has displayed larger potential than the most famous chatbot created by US-based OpenAI, IT giant Yandex alleged in remarks to RIA Novosti on Saturday.[3]
 Meta is developing a new, more powerful AI system, Wall Street Journal reports.[4]
  
Sources:
 [1] https://drug.ai.tencent.com/en
 [2] https://voonze.com/morgan-stanley-introduces-ai-powered-chatbot-for-enhanced-services/
 [3] https://menafn.com/1107040379/Russian-AI-bot-shows-larger-potential-than-ChatGPT
 [4] https://www.reuters.com/technology/meta-is-developing-new-more-powerful-ai-system-wsj-2023-09-10/
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Data structures for large sequences]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fkesy/p_data_structures_for_large_sequences/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fkesy/p_data_structures_for_large_sequences/"/>
        <updated>2023-09-11T03:14:58.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone
 I've been working for quite some time on this project and any feedback will be greatly appreciated. 
 Basically, I've been testing different data structures for large sequence prediction and clustering. Mainly SARSCov2 viral sequences due to its availability. At the moment, I have published two preprints 
  
https://www.researchsquare.com/article/rs-2797280/v3
 https://www.researchsquare.com/article/rs-1691291/v1
  
and a general summary of the findings can be found here. 
  
https://github.com/TavoGLC/SARSCov2Solar
 https://www.kaggle.com/code/tavoglc/a-computational-description-of-sarscov2-adaptation
  
I've tried to publish it a couple of times with no success and no comments regarding its accuracy or any potential problems. I hope you guys can check it out and provide some feedback if possible. Just for full transparency, I'm trying to raise funds to further develop those techniques. Donations are extremely welcomed but not encouraged at the moment, just disclosed for transparency. 
    submitted by    /u/TavoGLC  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta plans to match OpenAI's GPT-4 with its new AI model]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16fgwvn/meta_plans_to_match_openais_gpt4_with_its_new_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16fgwvn/meta_plans_to_match_openais_gpt4_with_its_new_ai/"/>
        <updated>2023-09-11T00:34:26.000Z</updated>
        <summary type="html"><![CDATA[Citing GPT-4 as the benchmark, Meta is reportedly gearing up to train a new, highly sophisticated AI model. The company is investing heavily in AI training chips and boosting its data centers to support the ambitious project.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/ts4a6reeuinb1.jpg?width=1440&format=pjpg&auto=webp&s=940be18b0c5f13e2762e1664a96274b314fe00df
 Metaâ€™s vision for its new AI model
  
Meta's aim is to create a powerful chatbot in line with OpenAIâ€™s GPT-4 capabilities.
 The company has reportedly been in pursuit of Nvidia H100 AI-training chips and is amplifying its infrastructure.
 The idea is to independently train its new model without outsourcing to platforms like Microsoft's Azure.
  
Efforts and roadblocks
  
Meta plans to begin the training of this LLM early in 2024, emphasizing free access for companies.
 Despite its grand vision, Meta has encountered obstacles like researcher attrition and contentious resource allocation amidst multiple LLM projects.
 Noteworthy is the intense competition from major players such as Apple, Google, and Amazon to integrate widely generative AI in their user interface.
  
Broader implications
  
While OpenAI has not immediately revealed plans for a GPT-5, other tech giants are investing heavily. Apple's investment in its "Ajax" AI model signifies the increasing race to advanced AI.
 This move by Meta represents the ongoing trend of tech conglomerates expanding in the AI space, a fact revealed through Google and Microsoftâ€™s use of AI in their productivity tools and Amazon's ongoing developments.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that explores the latest AI developments. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using LLMs to Analyze and Extract Insights from Device Logs [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fgely/using_llms_to_analyze_and_extract_insights_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fgely/using_llms_to_analyze_and_extract_insights_from/"/>
        <updated>2023-09-11T00:12:27.000Z</updated>
        <summary type="html"><![CDATA[I work with device logs that are massive text files, filled with data that's hard to go through manually. I'm primarily interested in extracting specific events or insights, such as security incidents or device malfunctions. The conventional method would be to use regular expressions to filter out relevant information since the logs are structured, but I'm curious about leveraging Large Language Models for this task.
 I've experimented a bit with zero-shot learning for text summarization but didn't get satisfactory results. Before I invest more time into fine-tuning an LLM, I'd love to hear from anyone who has experience or advice on how to approach this problem.
 Could LLMs potentially make the process more efficient and effective? Any pointers or suggestions would be greatly appreciated.
    submitted by    /u/Practical_Mango_8720  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Meta Is Developing a New, More Powerful AI System as Technology Race Escalates]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16feyvh/n_meta_is_developing_a_new_more_powerful_ai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16feyvh/n_meta_is_developing_a_new_more_powerful_ai/"/>
        <updated>2023-09-10T23:12:20.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hzj5790  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Data Extraction using fine-tuned LLM?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16fenlb/d_data_extraction_using_finetuned_llm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16fenlb/d_data_extraction_using_finetuned_llm/"/>
        <updated>2023-09-10T22:59:42.000Z</updated>
        <summary type="html"><![CDATA[Hey Reddit,
 I'm working on a tool to pull data from highly irregular Excel files. I've gotten reasonable results which is extremely fast with standard Python coding, but it's far from perfect due to the lack of standardized templates. 
 Interestingly, when I tested ChatGPT-4 on a sample table, it did a decent job at data extraction. However, relying solely on GPT-4 has its downsides like token limits and slow processing speed (and data privacy issues). Plus, splitting the Excel sheet to fit within these limits results in loss of context and data.
 I'm considering fine-tuning a language model to post-process data that was in a Pandas DataFrame (perhaps converted to JSON). Has anyone had success with this approach or have alternative recommendations? I've tried Langchain, but it wasn't helpful.
 I have figured out to extract the relevant columns, but the post-processing part is where I am considering using an LLM which understands the domain and what needs to be extracted based on the examples I feed it.
 Looking forward to your thoughts! And would be happy to answer any additional questions.
    submitted by    /u/rs35plus1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI apps product development canvas â€“ Part 1]]></title>
        <id>https://www.datasciencecentral.com/?p=63086</id>
        <link href="https://www.datasciencecentral.com/ai-apps-product-development-canvas-part-1/"/>
        <updated>2023-09-10T22:56:51.000Z</updated>
        <summary type="html"><![CDATA[AI Apps are domain-infused, AI/ML-powered applications that continuously learn and adapt with minimal human intervention in helping non-technical users manage data and analytics-intensive operations to deliver well-defined operational outcomes. I originally introduced the idea of a â€œData Product Development Canvasâ€ as one of the capstone deliverables (the other being the data science Hypothesis Development Canvas)â€¦Â Read More Â»AI apps product development canvas â€“ Part 1
The post AI apps product development canvas â€“ Part 1 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Should I transfer all my work to PyTorch already?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16faaux/d_should_i_transfer_all_my_work_to_pytorch_already/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16faaux/d_should_i_transfer_all_my_work_to_pytorch_already/"/>
        <updated>2023-09-10T20:11:15.000Z</updated>
        <summary type="html"><![CDATA[I've been using Tensorflow since 2017. I know it wasn't ideal or easy back then, but as an early adopter, I became very proficient with it and it has improved a lot since then. I have developed and deployed many custom models in low-level TF, both with and without utilizing the Keras abstractions. I am very comfortable with it in general.
 But I'm noticing now that Pytorch is gaining more popularity, all the younger practitioners, who got into deep learning within the last 3-5 years, are Pytorch adopters. I've also heard rumors that even googlers are also abandoning TF.
 I started playing around with Pytorch and as a TF expert, I couldn't help but getting annoyed at how far it is lagging behind in many abstractions and optimizations. I know things are getting better now with the Pytorch 2.0 and introducing some optimization such as the "compile" functionality, but still many of the pytorch project tools remain in beta such as Torchtext and I find many things very annoying, such as having to set the device and pass it on to layers if you want GPU acceleration, having to install Torchtext and other processing libraries separately, or having to use a Dataloader and the limited data type supports for torchdataset.
 Most people who have not mastered Tensorflow would not relate to my annoyance. Anyhow I'd really prefer to stay within my comfort zone and continue to develop and improve in TF, but if TF is dying, then I better not to, right? So should I convert? Is it indeed dying?
    submitted by    /u/DieselZRebel  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are some of your favorite ai discoveries you've found? What do you think is possible and probable to come in the near future with ai to stay tuned for?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16fa2kv/what_are_some_of_your_favorite_ai_discoveries/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16fa2kv/what_are_some_of_your_favorite_ai_discoveries/"/>
        <updated>2023-09-10T20:02:29.000Z</updated>
        <summary type="html"><![CDATA[I've been dabbling around with pi.ai and I love it and feel like it's only going to get better and better at what it does. I'm curious if there is any interesting new ai bots or discoveries that haven't yet made it mainstream but have tons of use in one way or another for the average consumer..
 Things like a language learning ai, or an ai that can read through a textbook or pdf and help you learn it's contents by practically interacting with it to help you comprehend better... there's so many interesting ai things I look forward to seeing
    submitted by    /u/mikel0202  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] This is my first blog on medium about Machine Learning please have a look and show some love]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f9uph/d_this_is_my_first_blog_on_medium_about_machine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f9uph/d_this_is_my_first_blog_on_medium_about_machine/"/>
        <updated>2023-09-10T19:54:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/indusop  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there any AI tool to filter 5 star ratings and just find the áº¡verage of 1 to 4 star ráº¡tings in Google maps to find the genuine rating of a business ?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16f9i05/is_there_any_ai_tool_to_filter_5_star_ratings_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16f9i05/is_there_any_ai_tool_to_filter_5_star_ratings_and/"/>
        <updated>2023-09-10T19:40:43.000Z</updated>
        <summary type="html"><![CDATA[I am tired of fake ratings and fake reviews in Google maps and I háº¡ve been cheated many times by fake 5 star reviews. I am just looking to find the genuine overáº¡ll rating of a place by filtering out 5 star ratings because fake ratings are mostly 5 star and just finding the average of 1-4 ratings so that we can find the overall genuine rating of a Business.
 Is there any AI tool or any way or any application for that ? Need suggestions on this.
    submitted by    /u/ramesh423  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks vs Tabular Data]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16f6nyh/neural_networks_vs_tabular_data/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16f6nyh/neural_networks_vs_tabular_data/"/>
        <updated>2023-09-10T17:51:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Bayesian_Net: A neural network for Bernoulli naive Bayes classification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f5squ/d_bayesian_net_a_neural_network_for_bernoulli/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f5squ/d_bayesian_net_a_neural_network_for_bernoulli/"/>
        <updated>2023-09-10T17:17:04.000Z</updated>
        <summary type="html"><![CDATA[What do you think of my recent work?
 https://github.com/jacobmcasey/bayesian_net
 At its core, its a neural network for Bernoulli naive Bayes. It uses a 3-layer neural network in NumPy for predicting priors of Bernoulli Naive Bayes
 Would love to get your feedback on this classifier project!
    submitted by    /u/Ok_Grape_3670  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM Training: RLHF and Its Alternatives]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16f5nd1/llm_training_rlhf_and_its_alternatives/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16f5nd1/llm_training_rlhf_and_its_alternatives/"/>
        <updated>2023-09-10T17:10:53.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to solve loss spikes in pre-training?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f5myk/d_how_to_solve_loss_spikes_in_pretraining/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f5myk/d_how_to_solve_loss_spikes_in_pretraining/"/>
        <updated>2023-09-10T17:10:26.000Z</updated>
        <summary type="html"><![CDATA[It happens on and off and I have tweaked many hyperparameters but nothing seems to work significantly better. Is there a recommendation on what to check/tweak?
    submitted by    /u/MrAaronW  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Possible to find 1-day/week contract to help fund RL PhD study?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16f3ktw/possible_to_find_1dayweek_contract_to_help_fund/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16f3ktw/possible_to_find_1dayweek_contract_to_help_fund/"/>
        <updated>2023-09-10T15:48:15.000Z</updated>
        <summary type="html"><![CDATA[Hi, I have been accepted onto a PhD at a top UK university and I'm looking for some additional income by working a day a week on other projects. Does anyone have experience of finding something like that to help with costs during a PhD?
 I have just completed my MSc with really high marks and have published a paper on conversational agents. I have strong general machine learning and data analysis knowledge, strong knowledge of conversational agents and a specialism in reinforcement learning. Before returning to study, I worked for several years in engineering teams so I know how to get stuff done in a professional context too. 
 The only thing is, I have no idea how to actually find something that could be a day a week and pay a worthwhile day rate to help with crazy cost of living.
 Thank you for any tips!
    submitted by    /u/EDMismyO2  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f2e96/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f2e96/d_simple_questions_thread/"/>
        <updated>2023-09-10T15:00:38.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Pdf text to speech]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f24wf/d_pdf_text_to_speech/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f24wf/d_pdf_text_to_speech/"/>
        <updated>2023-09-10T14:49:48.000Z</updated>
        <summary type="html"><![CDATA[Hey, I would like to listen to my PDFs I got. Week would the best / easiest way to get an mp3 from my pdfs with good voice? 
 I got a rtx 3070 to run it locally.
    submitted by    /u/Independent_Hyena495  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Patches and Frames in PyTorch]]></title>
        <id>https://davidstutz.de/?p=7693</id>
        <link href="https://davidstutz.de/adversarial-patches-and-frames-in-pytorch/"/>
        <updated>2023-09-10T14:18:37.000Z</updated>
        <summary type="html"><![CDATA[Adversarial patches and frames are an alternative to the regular $L_p$-constrained adversarial examples. Often, adversarial patches are thought to be more realistic â€” mirroring graffitis or stickers in the real world. In this article I want to discuss a simple PyTorch implementation and present some results of adversarial patches against adversarial training as well as confidence-calibrated adversarial training.
The post Adversarial Patches and Frames in PyTorch appeared first on David Stutz.]]></summary>
        <author>
            <name>David Stutz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Naive pipelining of LLM inference across multiple small GPUs? (self.MachineLearning)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f0kxv/d_naive_pipelining_of_llm_inference_across/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f0kxv/d_naive_pipelining_of_llm_inference_across/"/>
        <updated>2023-09-10T13:40:58.000Z</updated>
        <summary type="html"><![CDATA[For transformer inference, is it ok to pipeline blocks/layers to multiple low-memory GPUs (lower blocks to GPU-A and upper blocks to GPU-B) ?
  
A to B bandwidth should be relatively low, and each GPU needs half the model memory.
 This increases inference latency - fine for our use case...
 Not sure how this would affect the KV cache ?
  
The excellent Lil'Log article suggest I read the training optimization article, which has this image which is for training but not for inference
    submitted by    /u/yazriel0  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best architecture for prediction logging in production]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f0d1u/d_best_architecture_for_prediction_logging_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f0d1u/d_best_architecture_for_prediction_logging_in/"/>
        <updated>2023-09-10T13:31:08.000Z</updated>
        <summary type="html"><![CDATA[I am in the process of setting up the first iteration of model monitoring in production. Models are currently served through torchserve in kubernetes (on GCP). In the pasts few years I've been mainly using Vertex AI off-the-shelf tools.
 Ideally, I'd like to store the following data:
  
Request input.
 Model prediction.
 User feedback/groud truth (this might come at a later time).
 Anything that I might be missing out.
  
This would enable me to implement a wide array (either using libraries like whylogs or evidently that have a lot of the reporting side of things already baked in) of cheks for drift and model degradation.
 I am wondering what would be the best set up to achieve this. This is currently what I came up with:
  
Save a payload at inference time containing all the fields descrâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there anyway to mix two faces together and generate a brand new face using AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16f078f/is_there_anyway_to_mix_two_faces_together_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16f078f/is_there_anyway_to_mix_two_faces_together_and/"/>
        <updated>2023-09-10T13:23:12.000Z</updated>
        <summary type="html"><![CDATA[I'm interested to know if it's possible to mix two faces together, such as Tom Cruise and Brad Pitt, and create a brand new face from those two faces. 
    submitted by    /u/Glad-Ad-8953  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] HF accelerate vs native pytorch autoscaling for mixed precision training]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16f0157/d_hf_accelerate_vs_native_pytorch_autoscaling_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16f0157/d_hf_accelerate_vs_native_pytorch_autoscaling_for/"/>
        <updated>2023-09-10T13:15:05.000Z</updated>
        <summary type="html"><![CDATA[I want to start using mixed precision in my training, particularly for CV with high-resolution images.
 HF accelerate seems quite popular nowadays and looks nice. However, in the past I've invested in learning things like Pytorch-Lightning which look good in a minimal example, but actually add more annoyance than they're worth.
 Pytorch also can do these things, and the boiler plate doesn't look worse at a glance:
 https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/
 https://huggingface.co/docs/accelerate/index
 â€‹
 Any experiences with either? Cheers!
 â€‹
    submitted by    /u/AuspiciousApple  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Using LLMs to build Evaluation Sets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ezwl8/d_using_llms_to_build_evaluation_sets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ezwl8/d_using_llms_to_build_evaluation_sets/"/>
        <updated>2023-09-10T13:08:51.000Z</updated>
        <summary type="html"><![CDATA[Hi. Is this really better practice than having human linguists build evaluation sets for domain-specific data?
    submitted by    /u/throwaway34334534  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Pt 3) Spatio-Temporal Perception Logic]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16ezl7v/pt_3_spatiotemporal_perception_logic/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16ezl7v/pt_3_spatiotemporal_perception_logic/"/>
        <updated>2023-09-10T12:53:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] RLHF and Its Alternatives]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eyejc/p_rlhf_and_its_alternatives/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eyejc/p_rlhf_and_its_alternatives/"/>
        <updated>2023-09-10T11:54:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/seraschka  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Open-source Python package for Exploratory Data Analysis for modern NLP applications looking for contributors.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ex3xq/p_opensource_python_package_for_exploratory_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ex3xq/p_opensource_python_package_for_exploratory_data/"/>
        <updated>2023-09-10T10:41:35.000Z</updated>
        <summary type="html"><![CDATA[If you're working on NLP and you're serious about understanding your data, check out Wordview. 
 Wordview is an open-source Python library designed to make Exploratory Data Analysis (EDA) for text for modern NLP applications simpler and more efficient. It consolidates various analysis tools under one roof â€”think document metrics, multi-word expressions, language identification, POS tags, and even bias analysis. We're looking for contributors to help us expand and refine its capabilities. Join us in making NLP data analysis easier and more insightful with Wordview!
 Here is the link to the GitHub page:
 https://github.com/meghdadFar/wordview
 If you just want to use Wordview without contributing, you're very welcome too. Note that it's pretty new and we are still testing things. Please hence feel free to report bugs and send us your feedback and opinions.
 Looking forward!
    submitted by    /u/SyntaxTreeHugger  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Loss Oscillation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16ex2es/policy_loss_oscillation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16ex2es/policy_loss_oscillation/"/>
        <updated>2023-09-10T10:39:08.000Z</updated>
        <summary type="html"><![CDATA[Is there any insight to be gained about these policy and value loss curves?
 I hear itâ€™s difficult to judge a good policy by its loss curve, but my agent is hitting a wall in terms of progression over the baseline agents I am evaluating against. 
 For context, my policy is generated by a neural network with the core of it being a dot product of the state embedding against a set of actions embeddings. 
 Any help/understanding would be greatly appreciated.
    submitted by    /u/atomicburn125  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Question about dealing with EEG inter-subject Variability when training ML models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ewsro/d_question_about_dealing_with_eeg_intersubject/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ewsro/d_question_about_dealing_with_eeg_intersubject/"/>
        <updated>2023-09-10T10:23:10.000Z</updated>
        <summary type="html"><![CDATA[Hi, I'm an undergrad student working on a machine learning project about motor imagery classification for BCI. One requirement for the project is that I need to implement LIME an explainable AI tool on my model. I ran into a problem rather quickly as my model accuracy quickly decreases as I try and add more subjects to the training data, I'm the publicly avaliable EEG Motor Movement/Imagery Dataset from physio.net. The features I'm currently using for training my model are SVD entropy, Spectral Entropy, Hjorth mobility , Hjorth complexity and CSP components after applying overlapping filter bank.
 One method I've been suggested is to train models seperately for each subject, I don't know how to go about this should I construct a new model for each subject or should I keep the architecture but reset the training weight, also would this defeat the purpose of implementing LIME in the first place.
 I'm wondering if there's a way to deal with inter-subject variability without having to make/ train models for each subject seperately.
 I'm sorry if the choice of features doesn't make much sense since I'm basically self-taught and I have no prior knowledge about EEG or BCI
 TLDR; how do I deal with inter-subject variability without having to make/ train models for each subject seperately.
    submitted by    /u/Necrozx13  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI generated video]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ewqgr/ai_generated_video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ewqgr/ai_generated_video/"/>
        <updated>2023-09-10T10:19:21.000Z</updated>
        <summary type="html"><![CDATA[Cyberpunk montage I made using Midjourney and RunwayML along with CapCut for the transitions, etc. 
 Hereâ€™s some brief tutorial steps. 
  
Use Midjourney to make pictures that you like. I would keep it to the default aspect ratio for best results. 
 Use RunwayML to generate 4 second videos of the pics. 
 Once you get some videos you like, save them and upload them to the CapCut app on your phone. 
  
   submitted by    /u/Exitium_Maximus  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Automatic hyperparameter tuning for CatBoost and LightGBM]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ew13s/p_automatic_hyperparameter_tuning_for_catboost/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ew13s/p_automatic_hyperparameter_tuning_for_catboost/"/>
        <updated>2023-09-10T09:37:29.000Z</updated>
        <summary type="html"><![CDATA[Hey friends,
 I have developed a library, called 100gecs, that makes hyperparameter tuning on LightGBM and CatBoost models trivially easy.
 Background
 LightGBM and CatBoost are gradient boosted tree models, like XGBoost, and in many cases the best baseline model in supervised learning tasks on tabular data. They work by iteratively fitting trees on data, with each subsequent tree "correcting" on some level the prediction of the prior tree. Here's a good intro, if you want some more background on these methods.
 Hyperparameter tuning promises an optimally or near-optimally configured model, thus enabling you to get the best baseline model you possibly can.
 Summary
 100gecs provides custom child classes of LGBMClassifier, LGBMRegressor, CatBoostRegressor and CatBoostClassifier that can be â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How valuable is the UKâ€™s AI industry?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16evrpw/how_valuable_is_the_uks_ai_industry/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16evrpw/how_valuable_is_the_uks_ai_industry/"/>
        <updated>2023-09-10T09:21:12.000Z</updated>
        <summary type="html"><![CDATA[The UK's AI industry is highly valuable, but its exact worth is difficult to determine.
 
The industry is growing rapidly and has the potential to contribute significantly to the country's economy.
 
However, there are challenges in accurately measuring the value of the AI industry, as it encompasses a wide range of sectors and applications.
 
Some estimates suggest that the UK's AI industry could be worth billions of pounds, with the potential to create thousands of jobs.
 
Investment in research and development, as well as the development of AI talent, are crucial for the growth of the industry.
 
 Source : https://www.ft.com/content/eeaa57a3-19ed-45d9-8705-2517c81e60ba
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does Entry Level ML exist (in Europe)?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eusmb/d_does_entry_level_ml_exist_in_europe/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eusmb/d_does_entry_level_ml_exist_in_europe/"/>
        <updated>2023-09-10T08:21:16.000Z</updated>
        <summary type="html"><![CDATA[In your informed opinion, how would an archetypical career in ML look like? 
 Looking at job postings, at least in Europe, it looks like the minimum required experience is around 3 years. There seems to be a good demand for mid-senior level positions, but a void for entry level. As I understand it, most DS departments are not big enough to spare resources for newbies that must be trained, the need is for a few but seasoned engineers. How far is my guess from the truth? 
 And, most importantly, how could new candidates (let's say recent MSc in Data Science, for instance) get into the industry? Through analyst/DE roles?
    submitted by    /u/madway99  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any free ai text to speech programs that let me test my own voice models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eukhe/d_any_free_ai_text_to_speech_programs_that_let_me/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eukhe/d_any_free_ai_text_to_speech_programs_that_let_me/"/>
        <updated>2023-09-10T08:07:19.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone I've wanted to test some AI voice models I created with a free AI Text to-speech program Are there any available to test my voice models 
    submitted by    /u/mrbeanfan64  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Cant get TensorFlow or PyTorch to detect my 4090 Laptop GPU]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eujsx/d_cant_get_tensorflow_or_pytorch_to_detect_my/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eujsx/d_cant_get_tensorflow_or_pytorch_to_detect_my/"/>
        <updated>2023-09-10T08:06:05.000Z</updated>
        <summary type="html"><![CDATA[I dont know what it could be the issue, I have tried everything from reinstalling the pip packages, running them on a virtual env, reinstalled CUDA, trying to run on PyCharm instead of VSCode, I got no idea what to do, does anyone know why it could be happening? Maybe something related to environment variables that I maybe messed up after watching like 5 different vids on the topic? What could it be?
    submitted by    /u/someredditguy374632  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New physics-based self-learning machines could replace current artificial neural networks and save energy | "Neural networks on neuromorphic computers"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16etzxf/new_physicsbased_selflearning_machines_could/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16etzxf/new_physicsbased_selflearning_machines_could/"/>
        <updated>2023-09-10T07:33:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] CodeLlama-xb/CodeLlama-xb-Python vs. CodeLlama-xb-instruct]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16etk1f/d_codellamaxbcodellamaxbpython_vs/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16etk1f/d_codellamaxbcodellamaxbpython_vs/"/>
        <updated>2023-09-10T07:07:48.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, so I have googled around and read the documentation but I am still confused between what's the difference between CodeLlama-xb/CodeLlama-xb-Python vs. CodeLlama-xb-instruct? I know the xb model is the base model (for several languages) and the Python model specializes in Python, but what's the instruct model and how is it different from the other 2 models?
 Would really appreciate your help. Thanks a million!
    submitted by    /u/--leockl--  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Make use of CPUs on 8 servers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16et52j/r_make_use_of_cpus_on_8_servers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16et52j/r_make_use_of_cpus_on_8_servers/"/>
        <updated>2023-09-10T06:43:36.000Z</updated>
        <summary type="html"><![CDATA[I have a blade system with 8 blades. Each blade has 28 cores (e5-2680 v4) and 180gb ram. I would love to run an LLM + Local Files ( kind of like OpenAI does with their api) and run something similar but most â€œprivategptâ€ LLMs need GPU support and donâ€™t look like they make use of a multi device setup. 
 Anyway to get crunching on CPU on my setup
    submitted by    /u/programmrz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top 8 Courses & Certifications on AI Ethics]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16erzt7/top_8_courses_certifications_on_ai_ethics/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16erzt7/top_8_courses_certifications_on_ai_ethics/"/>
        <updated>2023-09-10T05:38:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeking Advice on Electricity Trading Problem in Day-Ahead Market]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16er8sd/seeking_advice_on_electricity_trading_problem_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16er8sd/seeking_advice_on_electricity_trading_problem_in/"/>
        <updated>2023-09-10T04:58:10.000Z</updated>
        <summary type="html"><![CDATA[I'm attempting to address an issue related to electricity trading in the day-ahead market. The objective is to submit 24 bids for each hour. Each bid is represented as a vector of length 10, with 5 values indicating the price and the other 5 indicating the quantity of electricity. For instance, given a sample action vector [w, x, y, z, ...], it operates as follows: if the price is greater than w, buy/sell x units of electricity, if the price is greater than y, buy/sell z units of electricity, and so forth. I possess three years' worth of data, including crucial features like weather conditions, coal prices, wind speed, net load, forecasted load, locational marginal price, and more.
 Initially, I trained a Deep Q-Network (DQN) to tackle this problem, and it's performing quite well. However, the DQN provides a scalar action for each hour, neglecting price points. It acts regardless of the price. I'm aware that I can explore solutions like Proximal Policy Gradient to generate a vector action that includes both electricity unit amounts and prices.
 I have three questions:
  
Is it possible to solve this problem using Dynamic Programming techniques? While I understand it's not an exceedingly difficult problem, can I expect some results if I attempt to apply DP?
 
How challenging might it be to output a vector instead of a scalar, with the vector being monotonically increasing? What's the recommended approach for a problem of this nature? Is it worthwhile to explore RL and specifically Proximal Policy Optimization?
 
How would you approach such a problem while keeping it simple and avoiding unnecessary complexity? 
 
 Any guidance or insights would be greatly appreciated.
    submitted by    /u/uonliaquat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making life friendlier with personal robots]]></title>
        <id>https://news.mit.edu/2023/making-life-friendlier-personal-robots-sharifa-alghowinem-0910</id>
        <link href="https://news.mit.edu/2023/making-life-friendlier-personal-robots-sharifa-alghowinem-0910"/>
        <updated>2023-09-10T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[Sharifa Alghowinem, a research scientist at the Media Lab, explores personal robot technology that explains emotions in English and Arabic.]]></summary>
        <author>
            <name>Dorothy Hanna | Department of Mechanical Engineering</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Accelerators Manifesto - Accelerating AI and our future]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16eofeh/the_accelerators_manifesto_accelerating_ai_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16eofeh/the_accelerators_manifesto_accelerating_ai_and/"/>
        <updated>2023-09-10T02:33:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/JulioMedina  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning in Games - Learning the level, not the game]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16emxy4/reinforcement_learning_in_games_learning_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16emxy4/reinforcement_learning_in_games_learning_the/"/>
        <updated>2023-09-10T01:22:37.000Z</updated>
        <summary type="html"><![CDATA[I've been watching a tonne of reinforcement learning videos on youtube, and was initially very impressed, but as I watched even more, especially by the same youtubers, I started to notice a distinct issue.
 Their AI aren't learning to play the game, they're learning to play the level.
 They'll put in 10s or 100s of hours reinforcing the level. It'll play over and over again, selecting for what works, and dropping what doesn't. And over time, the AI will be amazing at that level.
 But, if you take that reinforced data, and move to level 2 of that game... it'll be practically useless.
  
When humans play a game, say a brand new human who's never played video games before, we'll use reinforcement learning too. Most everything we do is reinforcement learning. Our brain works on reinforcement. â€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How you define center matters a lot]]></title>
        <id>https://www.johndcook.com/blog/?p=207222</id>
        <link href="https://www.johndcook.com/blog/2023/09/09/triangle-subdivision/"/>
        <updated>2023-09-10T00:43:44.000Z</updated>
        <summary type="html"><![CDATA[Earlier I wrote a post showing what happens when you start with an equilateral triangle, then repeatedly subdivide it into smaller and smaller triangles by drawing lines from the centroid (barycenter) to each of the vertices. I mentioned in that post that I moved the code for finding the center to its own function because [â€¦]
How you define center matters a lot first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Seeking Guidance: Transitioning from Trucking to Tech]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16elxqa/discussion_seeking_guidance_transitioning_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16elxqa/discussion_seeking_guidance_transitioning_from/"/>
        <updated>2023-09-10T00:37:16.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone!
 I'm a truck driver with a passion for creating music. While I'm familiar with basic IT tasks from my personal experiences, I'm new to the tech industry and am eager to break in.
 I've been exploring AI tools like ChatGPT and have become comfortable navigating GitHub. These experiences make me confident about diving into the tech field. After some research, I'm contemplating three domains:
  
Cloud Technology: It seems stable and beginner-friendly. I'm leaning here primarily because I've heard cybersecurity is saturated.
 Cybersecurity: Interested but cautious due to market saturation.
 Generative AI/Data: My true passion lies here, but I've gathered that projects matter more than certifications for entry-level roles in this niche, especially for someone without a degree.
  
Given my background and no technical degree, which field would you suggest I pursue? Should I focus on certifications before taking on projects? I genuinely appreciate any insights!
 (What do you guys think about this â€œCloudRoad mapâ€ is it good advice?) 
 https://www.madebygps.com/cloudcamp/
 [Discussion]
    submitted by    /u/motluv_them  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Anticipatory Customer Support using ML - Your Thoughts?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ekpou/discussion_anticipatory_customer_support_using_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ekpou/discussion_anticipatory_customer_support_using_ml/"/>
        <updated>2023-09-09T23:44:24.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, I'm Raynel, working on a CRM platform and actively considering the future integration of AI. One idea I'm particularly excited about is anticipatory customer support. The goal is to proactively address customer needs, perhaps even before they realize them, using ML-driven insights.
 Has anyone delved into this concept or tried implementing it? I'd love to hear thoughts, potential pitfalls, or even success stories. Thanks in advance for your insights!
    submitted by    /u/bess_point  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is good replacement for package manager]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ekjmz/d_what_is_good_replacement_for_package_manager/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ekjmz/d_what_is_good_replacement_for_package_manager/"/>
        <updated>2023-09-09T23:37:09.000Z</updated>
        <summary type="html"><![CDATA[I used to build on top of conda, or make docker containers with conda package manager, now that is so broken it is impossible to install old pytorch on a fresh environment. Any way to replace the existing requirements with something better?
    submitted by    /u/AardvarkNo6658  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are statistics and ML too "ununified" as fields?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ekh5i/d_are_statistics_and_ml_too_ununified_as_fields/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ekh5i/d_are_statistics_and_ml_too_ununified_as_fields/"/>
        <updated>2023-09-09T23:34:14.000Z</updated>
        <summary type="html"><![CDATA[I thought it would be interesting to open this discussion, I would like to hear what you guys have to say about that. I know that ML folks use statistics all the time, but I am often under the impression that it's mostly the basics. I understand that inference and prediction are inherently different, but I would assume the common knowledge will be much more vast.
 What is your perspective on that? Am I just missing the point? Is there room for improvement in the future? Do you think statistics literacy (advanced, not ANOVA or so) is common among ML practitioners?
    submitted by    /u/pyepyepie  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]what are some generative ai techniques to generate visuals synchronized with music]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ejm3k/dwhat_are_some_generative_ai_techniques_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ejm3k/dwhat_are_some_generative_ai_techniques_to/"/>
        <updated>2023-09-09T22:58:37.000Z</updated>
        <summary type="html"><![CDATA[I wish to generate visuals that are synced with beats etc of the music to offer the "sensory synchronization" effect where visuals closely sync with the music. I have found Lucid sonic dreams, but it appears to be quite buggy and likely no longer supported. any recommendations for tools I can leverage for a hobby->serious project of generating visuals synced with music.
 edit: I looked into simple approaches using fft like described here. But I was hoping there are newer generative ai techniques we could leverage.
    submitted by    /u/bluzkluz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Belt around an elliptical waist]]></title>
        <id>https://www.johndcook.com/blog/?p=207192</id>
        <link href="https://www.johndcook.com/blog/2023/09/09/belt-around-an-elliptical-waist/"/>
        <updated>2023-09-09T22:42:59.000Z</updated>
        <summary type="html"><![CDATA[I just saw a tweet from Dave Richeson saying I remember as a kid calculating the size difference (diameter) of a belt between each hole. Now I think about it every time I wear a belt. Holes 1 inch apart change the diameter by about one-third of an inch (1/Ï€). [Assuming people have a circular [â€¦]
Belt around an elliptical waist first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Starting a research lab, any advice on computing infrastructure?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eibsg/d_starting_a_research_lab_any_advice_on_computing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eibsg/d_starting_a_research_lab_any_advice_on_computing/"/>
        <updated>2023-09-09T22:06:39.000Z</updated>
        <summary type="html"><![CDATA[I'm starting a research lab at a Top 25 research university next year and my research agenda is focused on AI/DL for a scientific domain, I have to come up with a plan and budget for my software/hardware needs.
 My Context:
 I have experience setting up linux systems, building computers by myself and training DL models (CNNs/GNNs/LLMs) in a corporate setting. I am venturing to the academic world where resources are more constrained and so I am wondering if there are any guides, tips on setting up a research computer lab that does AI. I do not expect to train from scratch an LLM but maybe finetuning an small LLM. I might also need to do a lot parallelizable IO work to preprocess data. I will talk with the IT department of the university but would like to have some ideas before that conversation.
 I would appreciate any tips or thoughts, particularly on:
 * How many GPUs/CPUs to buy? Balancing cost/compute.
 * Recommended software for managing resources.
 * Running 100s-1ks of CPU jobs in parallel.
 * Local compute cluster vs buying compute online.
 * Hyperparameter optimization and logging of metrics.
 * Anything else you can think of?
    submitted by    /u/prof_is_training  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] GoodWiki Dataset (MIT): Wikipedia Articles in Markdown With Lists, Blockquotes, and More]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eh1t5/p_goodwiki_dataset_mit_wikipedia_articles_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eh1t5/p_goodwiki_dataset_mit_wikipedia_articles_in/"/>
        <updated>2023-09-09T21:16:43.000Z</updated>
        <summary type="html"><![CDATA[Location: https://huggingface.co/datasets/euirim/goodwiki
 Hi everyone, just wanted to share a dataset I've been working on for use in a personal project!
 GoodWiki is a 179 million token dataset of English Wikipedia articles collected on September 4, 2023, that have been marked as Good or Featured by Wikipedia editors. The dataset provides these articles in GitHub-flavored Markdown format, preserving layout features like lists, code blocks, math, and block quotes, unlike many other public Wikipedia datasets. Articles are accompanied by a short description of the page as well as any associated categories.
 Thanks to a careful conversion process from wikicode, the markup language used by Wikipedia, articles in GoodWiki are generally faithful reproductions of the corresponding original Wikipedia pages, minus references, files, infoboxes, and tables. Curated template transclusion and HTML tag handling have minimized instances where entire words and phrases are missing mid-sentence like in other public Wikipedia datasets.
 GoodWiki is more than 1.5 times larger (when compared using the same tokenizer) than the widely used WikiText-103 dataset by Merity et al., even after excluding article descriptions. Also limited to articles marked as Good or Featured, WikiText inspired GoodWiki.
    submitted by    /u/euirim  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Suffer from a lack of opportunities in ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eguum/dsuffer_from_a_lack_of_opportunities_in_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eguum/dsuffer_from_a_lack_of_opportunities_in_ml/"/>
        <updated>2023-09-09T21:09:05.000Z</updated>
        <summary type="html"><![CDATA[I wanna learn ML and i love this field there is people told me that the opportunities in this field is few (i live in egypt btw)and in USA also they suffer from a lack of opportunities in this field so is that true and should i choose another field or not because i really wanna learn ML .
    submitted by    /u/Opening-Being-7692  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["AI Unleashed: This Week's Top 15 News and Breakthroughs in Artificial Intelligence"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16egn0t/ai_unleashed_this_weeks_top_15_news_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16egn0t/ai_unleashed_this_weeks_top_15_news_and/"/>
        <updated>2023-09-09T21:00:40.000Z</updated>
        <summary type="html"><![CDATA["AI Unleashed: This Week's Top 15 News and Breakthroughs in Artificial Intelligence"
 1"X's Data Revolution: Your Biometrics and Career Fueling the AI of Tomorrow!"
 In a recent privacy policy shake-up, X has just dropped a bombshell. They're not just interested in your regular data anymore; they're diving deep into the world of biometrics, job histories, and education backgrounds. And that's not all! Another corner of their revamped policy hints at a grand plan: they want to fuse this treasure trove of data with publicly available info to supercharge their machine learning and AI models.
 This isn't your run-of-the-mill update; it's a quantum leap for X in their quest to build the ultimate AI system. They're not just pushing boundaries; they're smashing through them. By expanding their daâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The latest Tesla self-driving car iteration is a behavior-cloning NN]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16efuaj/the_latest_tesla_selfdriving_car_iteration_is_a/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16efuaj/the_latest_tesla_selfdriving_car_iteration_is_a/"/>
        <updated>2023-09-09T20:27:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI technology behind ChatGPT built was in Iowa â€“ with a lot of water]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ee30m/ai_technology_behind_chatgpt_built_was_in_iowa/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ee30m/ai_technology_behind_chatgpt_built_was_in_iowa/"/>
        <updated>2023-09-09T19:15:40.000Z</updated>
        <summary type="html"><![CDATA[The artificial intelligence technology behind ChatGPT was built in Iowa, specifically in the watershed of the Raccoon and Des Moines rivers.
 
Microsoft-backed OpenAI needed a lot of water to cool its powerful supercomputer as it taught its AI systems how to mimic human writing.
 
Leading tech developers like Microsoft, OpenAI, and Google have acknowledged the high costs associated with the growing demand for AI tools, including expensive semiconductors and increased water consumption.
 
Microsoft disclosed a 34% spike in global water consumption, largely attributed to its AI research.
 
A researcher estimates that ChatGPT uses 500 milliliters of water for every series of prompts or questions.
 
 Source : https://news.yahoo.com/artificial-intelligence-technology-behind-chatgpt-131421382.html
 Summarized by Nuse AI
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Model predicting the same outcome for all entries]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16edqy3/p_model_predicting_the_same_outcome_for_all/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16edqy3/p_model_predicting_the_same_outcome_for_all/"/>
        <updated>2023-09-09T19:01:52.000Z</updated>
        <summary type="html"><![CDATA[Im currently working on deploying an ML model that predicts NFL MVPs (two possible outputs: 0 for not MVP and 1 for MVP). That being said, during deployment it is currently predicting 0 for all inputs regardless of how varied the inputs may be. However, during the testing phase my model had a varied accuracy rate of 75%-85%. I'm beginning to think that it is very likely that during the testing phase the model was predicting 0 for everything and just getting these higher accuracy rates because of the dominance of non-MVPs within my dataset. This all being said, I'm a noob to ML and decided it'd be best to come on here for help. Is that the likely reason and if so how do I go about fixing it? Furthermore, what other issues could cause this and how would I go about fixing it?
    submitted by    /u/saggyboobsarecooltoo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi Agent RL Project Ideas/Implementation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16ebu7e/multi_agent_rl_project_ideasimplementation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16ebu7e/multi_agent_rl_project_ideasimplementation/"/>
        <updated>2023-09-09T17:44:38.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for some ideas on Multi Agent RL that preferably involve Robotics. I've came up with two ideas based on essentially similar themes: 
 1) Multiple robots tasked with cleaning a large room (with obstacles)
 2) Multiple robots tasked with a search and rescue like mission in a particular area. 
 Both are basically applications of n agents trying to collectively cover a region. 
 Can someone recommend some frameworks and libraries that can allow me to simulate these ideas? Also, I'd love to hear some other ideas as well which use multi-agent RL for robotic applications. For now I'm only targeting a simulation based project. If I get time later I'd love to implement them on hardware as well. Thanks in advance!
    submitted by    /u/esem29  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I made a website that uses LLMs to help you gain insights about your documents]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ebiu4/p_i_made_a_website_that_uses_llms_to_help_you/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ebiu4/p_i_made_a_website_that_uses_llms_to_help_you/"/>
        <updated>2023-09-09T17:31:41.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/spline_reticulator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursive triangle subdivision]]></title>
        <id>https://www.johndcook.com/blog/?p=207149</id>
        <link href="https://www.johndcook.com/blog/2023/09/09/recursive-triangle-subdivision/"/>
        <updated>2023-09-09T16:58:47.000Z</updated>
        <summary type="html"><![CDATA[The other day I saw where Cliff Pickover tweeted some images of triangles recursively subdivided by adding a point to the barycenter of each triangle. The images were not what I expected, so I wanted to reproduce the images to look into this further. Here are the first three steps: I set the alpha value [â€¦]
Recursive triangle subdivision first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM on Blockchain? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eag65/llm_on_blockchain_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eag65/llm_on_blockchain_d/"/>
        <updated>2023-09-09T16:47:30.000Z</updated>
        <summary type="html"><![CDATA[I recently discovered a python library called 'Petals' that should run most LLM models (LLaMA2, Stable Beluga) on some kind of "torrent" for machine learning on your device: https://github.com/bigscience-workshop/petals/,
 https://www.youtube.com/watch?v=8jEGVaRKmFc
 Furthermore, if you participate as a "server" in this "torrent" you can be rewarded (according to YouTube)
 Sounds too good to be true, so I didn't try it yet. Does anyone have experience with it? Can I get a virus from there? Is it not a scam?
    submitted by    /u/Degenerat666  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Unified Retrieval Augmented Generation - URAG]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eafoz/d_unified_retrieval_augmented_generation_urag/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eafoz/d_unified_retrieval_augmented_generation_urag/"/>
        <updated>2023-09-09T16:46:56.000Z</updated>
        <summary type="html"><![CDATA[PostgresML takes Retrieval Augmented Generation (RAG) a step further, by running the models and vector (or btree) indexes in the same process space that also caches the data, so the retrieval step doesn't require any networking or data (de)serialization. This makes it significantly faster and more reliable than other architectures, which is important for online or interactive applications. For lack of a better term, I'm referring to this as Unified Retrieval Augmented Generation (URAG).
 The closest I can find to this in the literature is TABR which utilizes recall to improve the performance of LLMs relative to tree based models, although this particular application of Retrieval is extremely computationally expensive for a slight performance improvement, unlike RAG for LLMs that seems to provide significant new capabilities to the model.
 Is there more research going on for what I'd refer to as URAG systems? We're building open source Python & JS SDKs to make the underlying SQL API more accessible, but I'm wondering if this community, or the academic community has already done more work in this area that we should be aware of.
    submitted by    /u/something_cleverer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]: How does chatbot development look like in practise?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16eaccw/d_how_does_chatbot_development_look_like_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16eaccw/d_how_does_chatbot_development_look_like_in/"/>
        <updated>2023-09-09T16:43:01.000Z</updated>
        <summary type="html"><![CDATA[With all the recent advancements in LLMs, how does chatbot development look like in practise? Suppose I want a chatbot to help with customer service. Can I then just collect some frequent / common questions about items the company might offer for sale and use a pretrained LLM to answer these questions correctly via transfer learning? Is there some effort that goes into explicit knowledge storage?
    submitted by    /u/Blutorangensaft  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching LLMs to be more reasonable]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ea4we/teaching_llms_to_be_more_reasonable/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ea4we/teaching_llms_to_be_more_reasonable/"/>
        <updated>2023-09-09T16:34:19.000Z</updated>
        <summary type="html"><![CDATA[Based on a bit of research and a lot of gut feeling, I offer the following speculation:
  
if you self-trained an LLM with a Python interpreter or Java compiler in a feedback loop where it learned from its own mistakes then it could become dramatically better at coding. It's actually a miracle that they are "decent" at coding despite getting virtually no feedback from an interpreter or compiler.
 one could train not merely on input and output, but also on an execution trace so the LLM learned HOW the interpreter got the result
 one could also train the model on how to install and invoke open source software and thus it would learn about a variety of languages, versions and runtimes
 this might also improve its logical reasoning skills in general
  
Admittedly, running programs is a lot more expensive than doing simple next-word prediction on pre-existing texts.
 But on the other hand, a corpus of a million program executions can also be used to train future LLMs. You can keep the execution information forever and re-use it as traditional next-token prediction input.
    submitted by    /u/Smallpaul  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Article - "As a writer, Iâ€™m afraid of capitalism â€” not ChatGPT."]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e9rng/article_as_a_writer_im_afraid_of_capitalism_not/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e9rng/article_as_a_writer_im_afraid_of_capitalism_not/"/>
        <updated>2023-09-09T16:19:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LaVolpe223  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AI-Powered Valley Girl creating content autonomously on Twitter]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e9r4g/p_aipowered_valley_girl_creating_content/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e9r4g/p_aipowered_valley_girl_creating_content/"/>
        <updated>2023-09-09T16:18:33.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone! I recently started a project where I make AI-powered characters that create content autonomously on social media. The first character that I launched yesterday is your typical valley girl - Alix - who hangs around tech and crypto people. She browses Twitter all day and writes tweets with her opinion of the stuff she comes by. With a unique mood generated for her every day, it's always a gamble to see who lands on her good or bad side.
 I would really appreciate it if you check her out and let me know what you think! I'm interested in further developing the project and making these synthetic entities equivalent to real people in entertaiment and companionship. 
 Her twitter - https://twitter.com/alix_H2O
    submitted by    /u/GuaranteeAny2894  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] NVIDIA's Groundbreaking TensorRT-LLM Can Double Inference Performance of Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e9nvp/n_nvidias_groundbreaking_tensorrtllm_can_double/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e9nvp/n_nvidias_groundbreaking_tensorrtllm_can_double/"/>
        <updated>2023-09-09T16:14:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/norcalnatv  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Is MDP suitable for DQN representation ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e82vz/p_is_mdp_suitable_for_dqn_representation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e82vz/p_is_mdp_suitable_for_dqn_representation/"/>
        <updated>2023-09-09T15:09:10.000Z</updated>
        <summary type="html"><![CDATA[Problem description
 For ongoing research I'm defining a decision problem in the form of an MDP. In a simple form, this MDP can be solved via Dynamic Programming. Of course I would like to scale up my MDP by including a sensor simulation, which renders the MDP to such a complexity that my next step is to use a DQN.
 My challenge is that I'm not sure how to represent my more complex MDP in a fitting DQN. Quite frankly, the more I look into DQN's, the more I wonder if my MDP is correctly defined. It would be great to hear your views and suggestions if you like to share.
 Definition of the MDP
 The MDP aims at detecting and chasing away a mosquito. For each distance step (discrete distances although it should be continuous), for instance Si, we cycle through an episode: we detect the mosquitoâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[100% Free and unlimited...text-to-video AI with optional image reference]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e823n/100_free_and_unlimitedtexttovideo_ai_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e823n/100_free_and_unlimitedtexttovideo_ai_with/"/>
        <updated>2023-09-09T15:08:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I made a free tool that allows you to create a personalized AI newsletter containing all of the content you already follow. The app will automatically pull in the top or latest posts from your selected sources so you don't miss anything important.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e5gl5/i_made_a_free_tool_that_allows_you_to_create_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e5gl5/i_made_a_free_tool_that_allows_you_to_create_a/"/>
        <updated>2023-09-09T13:12:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/PlayfulPhilosopher42  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Map of the ELLIS units(European Laboratory for Learning and Intelligent Systems). What do the people here think of ELLIS? Anyone who worked with them who can share the experience? Do you think it will help close the gap between Europe and US? (sorry for the bad edit I'm not good at it)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e54rq/d_map_of_the_ellis_unitseuropean_laboratory_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e54rq/d_map_of_the_ellis_unitseuropean_laboratory_for/"/>
        <updated>2023-09-09T12:58:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Ok_Independent9899  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P]Training an image classification model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e4o9b/ptraining_an_image_classification_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e4o9b/ptraining_an_image_classification_model/"/>
        <updated>2023-09-09T12:35:46.000Z</updated>
        <summary type="html"><![CDATA[Is it normal to achieve a validation accuracy so much higher than the training accuracy? I am using transfer learning to train a convNeXtBaseV1 model on my dataset. I got a training accuracy of 82.9% and a much higher validation accuracy of 97.14%.
 My dataset is around 9600 medical images and it is balanced between 3 classes. I splitted it into 80% training 10% testing and 10% validation
    submitted by    /u/Different_Hat5643  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM with a voice interface?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e4j4n/llm_with_a_voice_interface/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e4j4n/llm_with_a_voice_interface/"/>
        <updated>2023-09-09T12:28:49.000Z</updated>
        <summary type="html"><![CDATA[So LLM's like ChatGPT have been around for a while now, and have good APIs, and also voice to text dictation and text to voice generation are close to perfect these days... are there any services that join them all together?
 I'd like to chat with ChatGPT or similar by talking to my smart speaker a la Google Home, and have it respond by speaking, and to be able to reply and continue the conversation.
 Does anyone know if this exists yet?
    submitted by    /u/singeblanc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Textbook "Multi-Agent Reinforcement Learning: Foundations and Modern Approaches"]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16e4ig6/new_textbook_multiagent_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16e4ig6/new_textbook_multiagent_reinforcement_learning/"/>
        <updated>2023-09-09T12:27:51.000Z</updated>
        <summary type="html"><![CDATA[New introduction textbook titled "Multi-Agent Reinforcement Learning: Foundations and Modern Approaches" by Stefano V. Albrecht, Filippos Christianos, Lukas SchÃ¤fer, to be published by MIT Press. The book draft can be downloaded here: https://www.marl-book.com/
 â€‹
    submitted by    /u/vuttigiquoje-4292  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI subtitles?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e3b5q/ai_subtitles/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e3b5q/ai_subtitles/"/>
        <updated>2023-09-09T11:24:45.000Z</updated>
        <summary type="html"><![CDATA[hey guys, im trying to subtitle a spanish video to have english subtitles, does anyone know a good way to do it for free?
    submitted by    /u/deletemkw  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["The 7 Stages of AI" | AI Uncovered]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e2uz0/the_7_stages_of_ai_ai_uncovered/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e2uz0/the_7_stages_of_ai_ai_uncovered/"/>
        <updated>2023-09-09T10:58:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are there any open source voice cloning models that are capable of cloning other English accents along with the voice?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e2uxd/d_are_there_any_open_source_voice_cloning_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e2uxd/d_are_there_any_open_source_voice_cloning_models/"/>
        <updated>2023-09-09T10:58:42.000Z</updated>
        <summary type="html"><![CDATA[Hi. I tried tortoise tts. It is good for cloning how the voice sounds but not the original accent. Are there any open source technologies that can also copy the accent correctly?
 Please do suggest. Thank you!
    submitted by    /u/salehxoxo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Vector database integration with PostgreSQL]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e24nz/p_vector_database_integration_with_postgresql/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e24nz/p_vector_database_integration_with_postgresql/"/>
        <updated>2023-09-09T10:14:08.000Z</updated>
        <summary type="html"><![CDATA[Article | Notebook | GitHub
 There are a rapidly growing number of options in the vector database space. One of the more recent developments is the creation of new vector index plugins for traditional database systems such as pgvector. This is reminiscent of the discussion back in the mid 2010s on whether one should use full text search in the database or sync with an external system such as Elasticsearch.
 Just as with full text search, it will be tough for vector indexing in the database to compete with the more dedicated solutions past the simple use cases. For example, according the ANN-Benchmarks, the dedicated vector solutions score much higher.
 The desire to reduce stack complexity and the maturity of systems like Postgres make strong arguments to try to find a way to do it all in the database.
 The referenced article above proposes a way to integrate existing databases like PostgreSQL with vector indexes such as Faiss, Hnswlib, external vector databases and even keyword indexes like Elasticsearch. This opens up the possibility of combining Postgres features such as fine-grained access control with the performance of a dedicated vector index.
 â€‹
    submitted by    /u/davidmezzetti  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In AI Regulation Coverage, Media Let Lawmakers Off the Hook]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e1vb6/in_ai_regulation_coverage_media_let_lawmakers_off/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e1vb6/in_ai_regulation_coverage_media_let_lawmakers_off/"/>
        <updated>2023-09-09T09:58:26.000Z</updated>
        <summary type="html"><![CDATA[The media often portrays lawmakers as unable to regulate artificial intelligence (AI) due to its complexity and evolving nature.
 
This narrative overlooks the responsibility of lawmakers and their regulatory inertia.
 
The media frames AI regulation as a matter of technical knowledge rather than moral consideration.
 
The New York Times highlights the slow congressional response to new technologies and the potential influence of lawmakers' financial interests in AI companies.
 
The media fails to question why lawmakers, who have profited from AI, cannot apply their knowledge to regulate it.
 
This lack of critical information in news sources defends the inaction of lawmakers.
 
 Source : https://fair.org/home/in-ai-regulation-coverage-media-let-lawmakers-off-the-hook/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Clustering identical but time-shifted signal together from big database.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16e1muh/d_clustering_identical_but_timeshifted_signal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16e1muh/d_clustering_identical_but_timeshifted_signal/"/>
        <updated>2023-09-09T09:43:25.000Z</updated>
        <summary type="html"><![CDATA[I am working on clustering groups of almost identical (but time shifted) signals together. These clusters will have varying sizes, and I donÂ´t know what the final number of clusters will be. The database consists of up to 100 thousand signals (represented by 4000 long vectors as read from individual text files) which are already quite similar to each other. 
 Ideally I would just make a huge cross-correlation matrix but that is too computationally expensive. So is DTW. What I have done is to apply the fast fourier transform to get it into the frequency domain and therefore get rid of that time shift. Then I would apply a clustering algorithm. I have tried DBSCAN and hierarchical agglomerative clustering which work relatively well but donâ€™t scale well to datasets of this size. Affinity propagation is quick and works okish but I donâ€™t know how to optimise it. 
 Does anyone have any recommendation on which algorithm to use and how to optimise it? Was my idea to apply the fft good? I am not a computer scientist so I am really out of my element. 
    submitted by    /u/Bertz-2-  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16e0c88/nvidia_tensorrtllm_supercharges_large_language/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16e0c88/nvidia_tensorrtllm_supercharges_large_language/"/>
        <updated>2023-09-09T08:23:02.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/basitmakine  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Hey all! I'm excited to launch GPTCall, a platform that enables real-time voice conversations with Llama 2 and other open-source models! It supports both desktop and mobile browsers. See comments for details.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dzxhh/p_hey_all_im_excited_to_launch_gptcall_a_platform/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dzxhh/p_hey_all_im_excited_to_launch_gptcall_a_platform/"/>
        <updated>2023-09-09T07:57:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/friuns  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine Learning Problem: Predictive Maintenance for Industrial Equipment]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dx2b5/d_machine_learning_problem_predictive_maintenance/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dx2b5/d_machine_learning_problem_predictive_maintenance/"/>
        <updated>2023-09-09T05:12:30.000Z</updated>
        <summary type="html"><![CDATA[I was given following problem in one of the machine learning interviews. I think I messed up there. Need your approach in answering this question.
 Problem Description: Imagine you are working for a manufacturing company that operates a large fleet of industrial machines. These machines are critical to production, and unexpected breakdowns can result in significant downtime and financial losses. Your task is to develop a predictive maintenance model using machine learning to predict when a machine is likely to fail so that maintenance can be performed just in time to prevent a breakdown.
 Data: You are provided with historical data for each machine, including sensor readings, maintenance logs, and failure records. The dataset is extensive, containing millions of data points over several yeâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/8/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16dwti0/oneminute_daily_ai_news_982023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16dwti0/oneminute_daily_ai_news_982023/"/>
        <updated>2023-09-09T04:59:46.000Z</updated>
        <summary type="html"><![CDATA[TIME just picked a list of 100 Most Influential People in AI.[1]
 AI Startup Imbue Tops $1 Billion Valuation After Funding from Nvidia.[2]
 Microsoft offers legal protection for AI copyright infringement challenges.[3]
 US chipmaker Nvidia Corp. on Friday announced separate partnerships with Reliance and Tata group companies to help them develop AI-powered supercomputers, AI clouds and generative AI applications.[4]
  
Sources:
 [1] https://time.com/6311323/how-we-chose-time100-ai/
 [2] https://www.bloomberg.com/news/articles/2023-09-07/ai-startup-imbue-tops-1-billion-valuation-after-funding-from-nvidia?embedded-checkout=true
 [3] https://arstechnica.com/information-technology/2023/09/microsoft-offers-legal-protection-for-ai-copyright-infringement-challenges/
 [4] https://www.livemint.com/companies/news/reliance-tata-cos-sign-ai-partnerships-with-nvidia-11694198851600.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RVC "queue" is stuck for over 2 and a half hours now?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16dwkjw/rvc_queue_is_stuck_for_over_2_and_a_half_hours_now/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16dwkjw/rvc_queue_is_stuck_for_over_2_and_a_half_hours_now/"/>
        <updated>2023-09-09T04:46:58.000Z</updated>
        <summary type="html"><![CDATA[Does anyone know what this means if when you import your audio clip and the model you want to use it gets stuck in the queue for over 2 and a half hours? I know that can't be right but I seemed to have followed all the guides correctly not sure what it could be :'( help greatly appreciated 
    submitted by    /u/StuntGuy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] RVC "queue" stuck for over 2 and a half hours?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dwibn/d_rvc_queue_stuck_for_over_2_and_a_half_hours/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dwibn/d_rvc_queue_stuck_for_over_2_and_a_half_hours/"/>
        <updated>2023-09-09T04:43:43.000Z</updated>
        <summary type="html"><![CDATA[Does anyone know what this means if when you import your audio clip and the model you want to use it gets stuck in the queue for over 2 and a half hours? I know that can't be right but I seemed to have followed all the guides correctly not sure what it could be :'( help greatly appreciated 
    submitted by    /u/StuntGuy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT hype is fading ! Traffic drops from 1.6 bn to 1.4 bn users in 3 months]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16duj8z/chatgpt_hype_is_fading_traffic_drops_from_16_bn/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16duj8z/chatgpt_hype_is_fading_traffic_drops_from_16_bn/"/>
        <updated>2023-09-09T03:03:39.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A simple analysis of why IPPO performs better than MAPPO in MARL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16dtc19/a_simple_analysis_of_why_ippo_performs_better/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16dtc19/a_simple_analysis_of_why_ippo_performs_better/"/>
        <updated>2023-09-09T02:05:40.000Z</updated>
        <summary type="html"><![CDATA[To review IPPO vs. MAPPO, first, let's revisit the paper "Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?". ( https://arxiv.org/abs/2011.09533 ) The paper shows that simply applying PPO to the SMAC task to construct an IPPO (independent PPO) algorithm like IQL can surpass QMIX on SMAC. IPPO demonstrates the effectiveness of applying PPO to multi-agent systems.
 The paper further extends IPPO to MAPPO. The difference is that the critic of PPO uses the global state instead of the observation as input. Surprisingly, the global information does not enhance the actual performance of IPPO.
 â€‹
 https://preview.redd.it/1wqqlj9z05nb1.png?width=1440&format=png&auto=webp&s=0d6ca1faa0e872151abdb6bb1e48884c6b51e71a
 â€‹
 https://preview.redd.it/ei9uacl015nb1.png?width=1440&foâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paige partners with Microsoft to construct world's largest AI model for battling cancer]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16drwzl/paige_partners_with_microsoft_to_construct_worlds/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16drwzl/paige_partners_with_microsoft_to_construct_worlds/"/>
        <updated>2023-09-09T00:59:42.000Z</updated>
        <summary type="html"><![CDATA[Healthcare technology disruptor Paige is teaming up with Microsoft in the race against cancer. The collaboration aims to revolutionize cancer diagnosis and patient care by building the largest image-based artificial intelligence model for digital pathology and oncology.
 To stay one step ahead in AI transformations, subscribe here.
 Transforming cancer imaging
  
Using Microsoftâ€™s advanced supercomputing infrastructure, Paige aims to take cancer imaging to the next level. By combining its deep AI expertise with Microsoftâ€™s enormous computing power, this model augments accuracy and brings in novel capabilities in cancer diagnostics.
 In the next phase, Paige will incorporate up to four million digitized microscopy slides from its petabyte-scale clinical data archive.
  
A milestone in oncology
  
The Paige and Microsoft partnership is a game changer in advancing healthcare, equipping physicians with unprecedented insights into cancer pathology.
 Paige is the first company to receive FDA approval for a clinical AI application in digital pathology. The technology is set to increase diagnostic confidence, productivity, and expand treatment options for patients worldwide.
  
(source)
 P.S. If you love these analyses, I write a free newsletter to track the most significant news and research in AI and tech. Experts from Google, Meta, OpenAI, and more read it daily.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03893</id>
        <link href="http://arxiv.org/abs/2309.03893"/>
        <updated>2023-09-09T00:40:36.037Z</updated>
        <summary type="html"><![CDATA[Data is the cornerstone of deep learning. This paper reveals that the
recently developed Diffusion Model is a scalable data engine for object
detection. Existing methods for scaling up detection-oriented data often
require manual collection or generative models to obtain target images,
followed by data augmentation and labeling to produce training pairs, which are
costly, complex, or lacking diversity. To address these issues, we
presentDiffusionEngine (DE), a data scaling-up engine that provides
high-quality detection-oriented training pairs in a single stage. DE consists
of a pre-trained diffusion model and an effective Detection-Adapter,
contributing to generating scalable, diverse and generalizable detection data
in a plug-and-play manner. Detection-Adapter is learned to align the implicit
semantic and location knowledge in off-the-shelf diffusion models with
detection-aware signals to make better bounding-box predictions. Additionally,
we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing
detection benchmarks for facilitating follow-up research. Extensive experiments
demonstrate that data scaling-up via DE can achieve significant improvements in
diverse scenarios, such as various detection algorithms, self-supervised
pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised
learning. For example, when using DE with a DINO-based adapter to scale up
data, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Manlin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yuxi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xuefeng Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Min Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1"&gt;Andy J. Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M3FGM:a node masking and multi-granularity message passing-based federated graph model for spatial-temporal data prediction. (arXiv:2210.16193v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.16193</id>
        <link href="http://arxiv.org/abs/2210.16193"/>
        <updated>2023-09-09T00:40:36.018Z</updated>
        <summary type="html"><![CDATA[Researchers are solving the challenges of spatial-temporal prediction by
combining Federated Learning (FL) and graph models with respect to the
constrain of privacy and security. In order to make better use of the power of
graph model, some researchs also combine split learning(SL). However, there are
still several issues left unattended: 1) Clients might not be able to access
the server during inference phase; 2) The graph of clients designed manually in
the server model may not reveal the proper relationship between clients. This
paper proposes a new GNN-oriented split federated learning method, named node
{\bfseries M}asking and {\bfseries M}ulti-granularity {\bfseries M}essage
passing-based Federated Graph Model (M$^3$FGM) for the above issues. For the
first issue, the server model of M$^3$FGM employs a MaskNode layer to simulate
the case of clients being offline. We also redesign the decoder of the client
model using a dual-sub-decoders structure so that each client model can use its
local data to predict independently when offline. As for the second issue, a
new GNN layer named Multi-Granularity Message Passing (MGMP) layer enables each
client node to perceive global and local information. We conducted extensive
experiments in two different scenarios on two real traffic datasets. Results
show that M$^3$FGM outperforms the baselines and variant models, achieves the
best results in both datasets and scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuxing Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Yanwen Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Song Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiachi Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.15116</id>
        <link href="http://arxiv.org/abs/2308.15116"/>
        <updated>2023-09-09T00:40:36.011Z</updated>
        <summary type="html"><![CDATA[Molecular dynamics simulations have emerged as a fundamental instrument for
studying biomolecules. At the same time, it is desirable to perform simulations
of a collection of particles under various conditions in which the molecules
can fluctuate. In this paper, we explore and adapt the soft prompt-based
learning method to molecular dynamics tasks. Our model can remarkably
generalize to unseen and out-of-distribution scenarios with limited training
data. While our work focuses on temperature as a test case, the versatility of
our approach allows for efficient simulation through any continuous dynamic
conditions, such as pressure and volumes. Our framework has two stages: 1)
Pre-trains with data mixing technique, augments molecular structure data and
temperature prompts, then applies a curriculum learning method by increasing
the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework
improves sample-efficiency of fine-tuning process and gives the soft
prompt-tuning better initialization points. Comprehensive experiments reveal
that our framework excels in accuracy for in-domain data and demonstrates
strong generalization capabilities for unseen and out-of-distribution samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingbang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xingwei Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuangjia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jie Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proper Learning of Linear Dynamical Systems as a Non-Commutative Polynomial Optimisation Problem. (arXiv:2002.01444v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01444</id>
        <link href="http://arxiv.org/abs/2002.01444"/>
        <updated>2023-09-09T00:40:35.969Z</updated>
        <summary type="html"><![CDATA[There has been much recent progress in forecasting the next observation of a
linear dynamical system (LDS), which is known as the improper learning, as well
as in the estimation of its system matrices, which is known as the proper
learning of LDS. We present an approach to proper learning of LDS, which in
spite of the non-convexity of the problem, guarantees global convergence of
numerical solutions to a least-squares estimator. We present promising
computational results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Marecek_J/0/1/0/all/0/1"&gt;Jakub Marecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation. (arXiv:2309.03190v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.03190</id>
        <link href="http://arxiv.org/abs/2309.03190"/>
        <updated>2023-09-09T00:40:35.963Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have gained an increasing amount of popularity
due to their superior capability in learning node embeddings for various graph
inference tasks, but training them can raise privacy concerns. To address this,
we propose using link local differential privacy over decentralized nodes,
enabling collaboration with an untrusted server to train GNNs without revealing
the existence of any link. Our approach spends the privacy budget separately on
links and degrees of the graph for the server to better denoise the graph
topology using Bayesian estimation, alleviating the negative impact of LDP on
the accuracy of the trained GNNs. We bound the mean absolute error of the
inferred link probabilities against the ground truth graph topology. We then
propose two variants of our LDP mechanism complementing each other in different
privacy settings, one of which estimates fewer links under lower privacy
budgets to avoid false positive link estimates when the uncertainty is high,
while the other utilizes more information and performs better given relatively
higher privacy budgets. Furthermore, we propose a hybrid variant that combines
both strategies and is able to perform better across different privacy budgets.
Extensive experiments show that our approach outperforms existing methods in
terms of accuracy under varying privacy budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaochen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xiaokui Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v2 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.13280</id>
        <link href="http://arxiv.org/abs/2308.13280"/>
        <updated>2023-09-09T00:40:35.956Z</updated>
        <summary type="html"><![CDATA[The atmosphere affects humans in a multitude of ways, from loss of life due
to adverse weather effects to long-term social and economic impacts on
societies. Computer simulations of atmospheric dynamics are, therefore, of
great importance for the well-being of our and future generations. Here, we
propose AtmoRep, a novel, task-independent stochastic computer model of
atmospheric dynamics that can provide skillful results for a wide range of
applications. AtmoRep uses large-scale representation learning from artificial
intelligence to determine a general description of the highly complex,
stochastic dynamics of the atmosphere from the best available estimate of the
system's historical trajectory as constrained by observations. This is enabled
by a novel self-supervised learning objective and a unique ensemble that
samples from the stochastic model with a variability informed by the one in the
historical record. The task-independent nature of AtmoRep enables skillful
results for a diverse set of applications without specifically training for
them and we demonstrate this for nowcasting, temporal interpolation, model
correction, and counterfactuals. We also show that AtmoRep can be improved with
additional data, for example radar observations, and that it can be extended to
tasks such as downscaling. Our work establishes that large-scale neural
networks can provide skillful, task-independent models of atmospheric dynamics.
With this, they provide a novel means to make the large record of atmospheric
observations accessible for applications and for scientific inquiry,
complementing existing simulations based on first principles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Lessig_C/0/1/0/all/0/1"&gt;Christian Lessig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Luise_I/0/1/0/all/0/1"&gt;Ilaria Luise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gong_B/0/1/0/all/0/1"&gt;Bing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Langguth_M/0/1/0/all/0/1"&gt;Michael Langguth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stadler_S/0/1/0/all/0/1"&gt;Scarlet Stadler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schultz_M/0/1/0/all/0/1"&gt;Martin Schultz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory. (arXiv:2309.03487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03487</id>
        <link href="http://arxiv.org/abs/2309.03487"/>
        <updated>2023-09-09T00:40:35.943Z</updated>
        <summary type="html"><![CDATA[With the increasing importance of data privacy protection, various
privacy-preserving machine learning methods have been proposed. In the
clustering domain, various algorithms with a federated learning framework
(i.e., federated clustering) have been actively studied and showed high
clustering performance while preserving data privacy. However, most of the base
clusterers (i.e., clustering algorithms) used in existing federated clustering
algorithms need to specify the number of clusters in advance. These algorithms,
therefore, are unable to deal with data whose distributions are unknown or
continually changing. To tackle this problem, this paper proposes a
privacy-preserving continual federated clustering algorithm. In the proposed
algorithm, an adaptive resonance theory-based clustering algorithm capable of
continual learning is used as a base clusterer. Therefore, the proposed
algorithm inherits the ability of continual learning. Experimental results with
synthetic and real-world datasets show that the proposed algorithm has superior
clustering performance to state-of-the-art federated clustering algorithms
while realizing data privacy protection and continual learning ability. The
source code is available at \url{https://github.com/Masuyama-lab/FCAC}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Masuyama_N/0/1/0/all/0/1"&gt;Naoki Masuyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nojima_Y/0/1/0/all/0/1"&gt;Yusuke Nojima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_Y/0/1/0/all/0/1"&gt;Yuichiro Toda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1"&gt;Chu Kiong Loo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishibuchi_H/0/1/0/all/0/1"&gt;Hisao Ishibuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubota_N/0/1/0/all/0/1"&gt;Naoyuki Kubota&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explanation Shift: How Did the Distribution Shift Impact the Model?. (arXiv:2303.08081v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.08081</id>
        <link href="http://arxiv.org/abs/2303.08081"/>
        <updated>2023-09-09T00:40:35.936Z</updated>
        <summary type="html"><![CDATA[As input data distributions evolve, the predictive performance of machine
learning models tends to deteriorate. In practice, new input data tend to come
without target labels. Then, state-of-the-art techniques model input data
distributions or model prediction distributions and try to understand issues
regarding the interactions between learned models and shifting distributions.
We suggest a novel approach that models how explanation characteristics shift
when affected by distribution shifts. We find that the modeling of explanation
shifts can be a better indicator for detecting out-of-distribution model
behaviour than state-of-the-art techniques. We analyze different types of
distribution shifts using synthetic examples and real-world data sets. We
provide an algorithmic method that allows us to inspect the interaction between
data set features and learned models and compare them to the state-of-the-art.
We release our methods in an open-source Python package, as well as the code
used to reproduce our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mougan_C/0/1/0/all/0/1"&gt;Carlos Mougan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1"&gt;Klaus Broelemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1"&gt;David Masip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1"&gt;Gjergji Kasneci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiropanis_T/0/1/0/all/0/1"&gt;Thanassis Thiropanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1"&gt;Steffen Staab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Hidden Representations in Transfer Learning for Medical Imaging. (arXiv:2302.08272v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08272</id>
        <link href="http://arxiv.org/abs/2302.08272"/>
        <updated>2023-09-09T00:40:35.740Z</updated>
        <summary type="html"><![CDATA[While a key component to the success of deep learning is the availability of
massive amounts of training data, medical image datasets are often limited in
diversity and size. Transfer learning has the potential to bridge the gap
between related yet different domains. For medical applications, however, it
remains unclear whether it is more beneficial to pre-train on natural or
medical images. We aim to shed light on this problem by comparing
initialization on ImageNet and RadImageNet on seven medical classification
tasks. Our work includes a replication study, which yields results contrary to
previously published findings. In our experiments, ResNet50 models pre-trained
on ImageNet tend to outperform those trained on RadImageNet. To gain further
insights, we investigate the learned representations using Canonical
Correlation Analysis (CCA) and compare the predictions of the different models.
Our results indicate that, contrary to intuition, ImageNet and RadImageNet may
converge to distinct intermediate representations, which appear to diverge
further during fine-tuning. Despite these distinct representations, the
predictions of the models remain similar. Our findings show that the similarity
between networks before and after fine-tuning does not correlate with
performance gains, suggesting that the advantages of transfer learning might
not solely originate from the reuse of features in the early layers of a
convolutional neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Juodelyte_D/0/1/0/all/0/1"&gt;Dovile Juodelyte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jimenez_Sanchez_A/0/1/0/all/0/1"&gt;Amelia Jim&amp;#xe9;nez-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1"&gt;Veronika Cheplygina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tutorial on the Non-Asymptotic Theory of System Identification. (arXiv:2309.03873v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2309.03873</id>
        <link href="http://arxiv.org/abs/2309.03873"/>
        <updated>2023-09-09T00:40:35.693Z</updated>
        <summary type="html"><![CDATA[This tutorial serves as an introduction to recently developed non-asymptotic
methods in the theory of -- mainly linear -- system identification. We
emphasize tools we deem particularly useful for a range of problems in this
domain, such as the covering technique, the Hanson-Wright Inequality and the
method of self-normalized martingales. We then employ these tools to give
streamlined proofs of the performance of various least-squares based estimators
for identifying the parameters in autoregressive models. We conclude by
sketching out how the ideas presented herein can be extended to certain
nonlinear identification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ziemann_I/0/1/0/all/0/1"&gt;Ingvar Ziemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsiamis_A/0/1/0/all/0/1"&gt;Anastasios Tsiamis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_B/0/1/0/all/0/1"&gt;Bruce Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jedra_Y/0/1/0/all/0/1"&gt;Yassir Jedra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Matni_N/0/1/0/all/0/1"&gt;Nikolai Matni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medoid Silhouette clustering with automatic cluster number selection. (arXiv:2309.03751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03751</id>
        <link href="http://arxiv.org/abs/2309.03751"/>
        <updated>2023-09-09T00:40:35.692Z</updated>
        <summary type="html"><![CDATA[The evaluation of clustering results is difficult, highly dependent on the
evaluated data set and the perspective of the beholder. There are many
different clustering quality measures, which try to provide a general measure
to validate clustering results. A very popular measure is the Silhouette. We
discuss the efficient medoid-based variant of the Silhouette, perform a
theoretical analysis of its properties, provide two fast versions for the
direct optimization, and discuss the use to choose the optimal number of
clusters. We combine ideas from the original Silhouette with the well-known PAM
algorithm and its latest improvements FasterPAM. One of the versions guarantees
equal results to the original variant and provides a run speedup of $O(k^2)$.
In experiments on real data with 30000 samples and $k$=100, we observed a
10464$\times$ speedup compared to the original PAMMEDSIL algorithm.
Additionally, we provide a variant to choose the optimal number of clusters
directly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lenssen_L/0/1/0/all/0/1"&gt;Lars Lenssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation Layer that Lets the Student Decide. (arXiv:2309.02843v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2309.02843</id>
        <link href="http://arxiv.org/abs/2309.02843"/>
        <updated>2023-09-09T00:40:35.651Z</updated>
        <summary type="html"><![CDATA[Typical technique in knowledge distillation (KD) is regularizing the learning
of a limited capacity model (student) by pushing its responses to match a
powerful model's (teacher). Albeit useful especially in the penultimate layer
and beyond, its action on student's feature transform is rather implicit,
limiting its practice in the intermediate layers. To explicitly embed the
teacher's knowledge in feature transform, we propose a learnable KD layer for
the student which improves KD with two distinct abilities: i) learning how to
leverage the teacher's knowledge, enabling to discard nuisance information, and
ii) feeding forward the transferred knowledge deeper. Thus, the student enjoys
the teacher's knowledge during the inference besides training. Formally, we
repurpose 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each
local region according to the template (supervised by the teacher) that the
corresponding region of the student matches. To facilitate template learning in
the intermediate layers, we propose a novel form of supervision based on the
teacher's decisions. Through rigorous experimentation, we demonstrate the
effectiveness of our approach on 3 popular classification benchmarks. Code is
available at: https://github.com/adagorgun/letKD-framework]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gorgun_A/0/1/0/all/0/1"&gt;Ada Gorgun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurbuz_Y/0/1/0/all/0/1"&gt;Yeti Z. Gurbuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1"&gt;A. Aydin Alatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03557</id>
        <link href="http://arxiv.org/abs/2309.03557"/>
        <updated>2023-09-09T00:40:35.600Z</updated>
        <summary type="html"><![CDATA[Multiagent systems aim to accomplish highly complex learning tasks through
decentralised consensus seeking dynamics and their use has garnered a great
deal of attention in the signal processing and computational intelligence
societies. This article examines the behaviour of multiagent networked systems
with nonlinear filtering/learning dynamics. To this end, a general formulation
for the actions of an agent in multiagent networked systems is presented and
conditions for achieving a cohesive learning behaviour is given. Importantly,
application of the so derived framework in distributed and federated learning
scenarios are presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Talebi_S/0/1/0/all/0/1"&gt;Sayed Pouria Talebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mandic_D/0/1/0/all/0/1"&gt;Danilo Mandic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03748</id>
        <link href="http://arxiv.org/abs/2309.03748"/>
        <updated>2023-09-09T00:40:35.594Z</updated>
        <summary type="html"><![CDATA[The latest advancements in AI and deep learning have led to a breakthrough in
large language model (LLM)-based agents such as GPT-4. However, many commercial
conversational agent development tools are pipeline-based and have limitations
in holding a human-like conversation. This paper investigates the capabilities
of LLMs to enhance pipeline-based conversational agents during two phases: 1)
in the design and development phase and 2) during operations. In 1) LLMs can
aid in generating training data, extracting entities and synonyms,
localization, and persona design. In 2) LLMs can assist in contextualization,
intent classification to prevent conversational breakdown and handle
out-of-scope questions, auto-correcting utterances, rephrasing responses,
formulating disambiguation questions, summarization, and enabling closed
question-answering capabilities. We conducted informal experiments with GPT-4
in the private banking domain to demonstrate the scenarios above with a
practical example. Companies may be hesitant to replace their pipeline-based
agents with LLMs entirely due to privacy concerns and the need for deep
integration within their existing ecosystems. A hybrid approach in which LLMs'
are integrated into the pipeline-based agents allows them to save time and
costs of building and running agents by capitalizing on the capabilities of
LLMs while retaining the integration and privacy safeguards of their existing
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foosherian_M/0/1/0/all/0/1"&gt;Mina Foosherian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purwins_H/0/1/0/all/0/1"&gt;Hendrik Purwins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathnayake_P/0/1/0/all/0/1"&gt;Purna Rathnayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Touhidul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teimao_R/0/1/0/all/0/1"&gt;Rui Teimao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thoben_K/0/1/0/all/0/1"&gt;Klaus-Dieter Thoben&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to select an objective function using information theory. (arXiv:2212.06566v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.06566</id>
        <link href="http://arxiv.org/abs/2212.06566"/>
        <updated>2023-09-09T00:40:35.594Z</updated>
        <summary type="html"><![CDATA[In machine learning or scientific computing, model performance is measured
with an objective function. But why choose one objective over another?
Information theory gives one answer: To maximize the information in the model,
select the most likely objective function or whichever represents the error in
the fewest bits. To evaluate different objectives, transform them into
likelihood functions. As likelihoods, their relative magnitudes represent how
much we should prefer one objective versus another, and the log of their
magnitude represents the expected uncertainty of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hodson_T/0/1/0/all/0/1"&gt;Timothy O. Hodson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Over_T/0/1/0/all/0/1"&gt;Thomas M. Over&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_T/0/1/0/all/0/1"&gt;Tyler J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marshall_L/0/1/0/all/0/1"&gt;Lucy M. Marshall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03426</id>
        <link href="http://arxiv.org/abs/2309.03426"/>
        <updated>2023-09-09T00:40:35.586Z</updated>
        <summary type="html"><![CDATA[Decisions made by machine learning models may have lasting impacts over time,
making long-term fairness a crucial consideration. It has been shown that when
ignoring the long-term effect, naively imposing fairness criterion in static
settings can actually exacerbate bias over time. To explicitly address biases
in sequential decision-making, recent works formulate long-term fairness
notions in Markov Decision Process (MDP) framework. They define the long-term
bias to be the sum of static bias over each time step. However, we demonstrate
that naively summing up the step-wise bias can cause a false sense of fairness
since it fails to consider the importance difference of different time steps
during transition. In this work, we introduce a long-term fairness notion
called Equal Long-term Benefit Rate (ELBERT), which explicitly considers
varying temporal importance and adapts static fairness principles to the
sequential setting. Moreover, we show that the policy gradient of Long-term
Benefit Rate can be analytically reduced to standard policy gradient. This
makes standard policy optimization methods applicable for reducing the bias,
leading to our proposed bias mitigation method ELBERT-PO. Experiments on three
sequential decision making environments show that ELBERT-PO significantly
reduces bias and maintains high utility. Code is available at
https://github.com/Yuancheng-Xu/ELBERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Chenghao Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yanchao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Ruijie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization. (arXiv:2309.03824v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03824</id>
        <link href="http://arxiv.org/abs/2309.03824"/>
        <updated>2023-09-09T00:40:35.583Z</updated>
        <summary type="html"><![CDATA[Low Rank Decomposition (LRD) is a model compression technique applied to the
weight tensors of deep learning models in order to reduce the number of
trainable parameters and computational complexity. However, due to high number
of new layers added to the architecture after applying LRD, it may not lead to
a high training/inference acceleration if the decomposition ranks are not small
enough. The issue is that using small ranks increases the risk of significant
accuracy drop after decomposition. In this paper, we propose two techniques for
accelerating low rank decomposed models without requiring to use small ranks
for decomposition. These methods include rank optimization and sequential
freezing of decomposed layers. We perform experiments on both convolutional and
transformer-based models. Experiments show that these techniques can improve
the model throughput up to 60% during training and 37% during inference when
combined together while preserving the accuracy close to that of the original
models]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1"&gt;Habib Hajimolahoseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1"&gt;Walid Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving Occam's Razor: Deep Learning for Optimal Model Reduction. (arXiv:2303.13746v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.13746</id>
        <link href="http://arxiv.org/abs/2303.13746"/>
        <updated>2023-09-09T00:40:35.575Z</updated>
        <summary type="html"><![CDATA[All fields of science depend on mathematical models. Occam's razor refers to
the principle that good models should exclude parameters beyond those minimally
required to describe the systems they represent. This is because redundancy can
lead to incorrect estimates of model parameters from data, and thus inaccurate
or ambiguous conclusions. Here, we show how deep learning can be powerfully
leveraged to address Occam's razor. FixFit, our new method, uses a feedforward
deep neural network with a bottleneck layer to characterize and predict the
behavior of a given model from its input parameters. FixFit has three major
benefits. First, it provides a metric to quantify the original model's degree
of complexity. Second, it allows for the unique fitting of data. Third, it
provides an unbiased way to discriminate between experimental hypotheses that
add value versus those that do not. In two use cases, we demonstrate the broad
applicability of this method across scientific domains. To validate the method
using a known system, we apply FixFit to recover known composite parameters for
the Kepler orbit model. To illustrate how the method can be applied to less
well-established fields, we use it to identify parameters for a multi-scale
brain model and reduce the search space for viable candidate mechanisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antal_B/0/1/0/all/0/1"&gt;Botond B Antal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chesebro_A/0/1/0/all/0/1"&gt;Anthony G Chesebro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strey_H/0/1/0/all/0/1"&gt;Helmut H Strey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mujica_Parodi_L/0/1/0/all/0/1"&gt;Lilianne R Mujica-Parodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weistuch_C/0/1/0/all/0/1"&gt;Corey Weistuch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03774</id>
        <link href="http://arxiv.org/abs/2309.03774"/>
        <updated>2023-09-09T00:40:35.574Z</updated>
        <summary type="html"><![CDATA[Recent advances in the field of deep learning and impressive performance of
deep neural networks (DNNs) for perception have resulted in an increased demand
for their use in automated driving (AD) systems. The safety of such systems is
of utmost importance and thus requires to consider the unique properties of
DNNs.

In order to achieve safety of AD systems with DNN-based perception components
in a systematic and comprehensive approach, so-called safety concerns have been
introduced as a suitable structuring element. On the one hand, the concept of
safety concerns is -- by design -- well aligned to existing standards relevant
for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has
already inspired several academic publications and upcoming standards on AI
safety such as ISO PAS 8800.

While the concept of safety concerns has been previously introduced, this
paper extends and refines it, leveraging feedback from various domain and
safety experts in the field. In particular, this paper introduces an additional
categorization for a better understanding as well as enabling cross-functional
teams to jointly address the concerns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrecht_S/0/1/0/all/0/1"&gt;Stephanie Abrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirsch_A/0/1/0/all/0/1"&gt;Alexander Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raafatnia_S/0/1/0/all/0/1"&gt;Shervin Raafatnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woehrle_M/0/1/0/all/0/1"&gt;Matthias Woehrle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03564</id>
        <link href="http://arxiv.org/abs/2309.03564"/>
        <updated>2023-09-09T00:40:35.573Z</updated>
        <summary type="html"><![CDATA[Large language models, particularly those akin to the rapidly progressing GPT
series, are gaining traction for their expansive influence. While there is keen
interest in their applicability within medical domains such as psychology,
tangible explorations on real-world data remain scant. Concurrently, users on
social media platforms are increasingly vocalizing personal sentiments; under
specific thematic umbrellas, these sentiments often manifest as negative
emotions, sometimes escalating to suicidal inclinations. Timely discernment of
such cognitive distortions and suicidal risks is crucial to effectively
intervene and potentially avert dire circumstances. Our study ventured into
this realm by experimenting on two pivotal tasks: suicidal risk and cognitive
distortion identification on Chinese social media platforms. Using supervised
learning as a baseline, we examined and contrasted the efficacy of large
language models via three distinct strategies: zero-shot, few-shot, and
fine-tuning. Our findings revealed a discernible performance gap between the
large language models and traditional supervised learning approaches, primarily
attributed to the models' inability to fully grasp subtle categories. Notably,
while GPT-4 outperforms its counterparts in multiple scenarios, GPT-3.5 shows
significant enhancement in suicide risk classification after fine-tuning. To
our knowledge, this investigation stands as the maiden attempt at gauging large
language models on Chinese social media tasks. This study underscores the
forward-looking and transformative implications of using large language models
in the field of psychology. It lays the groundwork for future applications in
psychological research and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hongzhi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qing Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Changwei Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1"&gt;Wei Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1"&gt;Dan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yi Jing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1"&gt;Huijing Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bing Xiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1"&gt;Guanghui Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning. (arXiv:2309.03440v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.03440</id>
        <link href="http://arxiv.org/abs/2309.03440"/>
        <updated>2023-09-09T00:40:35.572Z</updated>
        <summary type="html"><![CDATA[Accurate segmentation of punctate white matter lesions (PWMLs) are
fundamental for the timely diagnosis and treatment of related developmental
disorders. Automated PWMLs segmentation from infant brain MR images is
challenging, considering that the lesions are typically small and low-contrast,
and the number of lesions may dramatically change across subjects. Existing
learning-based methods directly apply general network architectures to this
challenging task, which may fail to capture detailed positional information of
PWMLs, potentially leading to severe under-segmentations. In this paper, we
propose to leverage the idea of counterfactual reasoning coupled with the
auxiliary task of brain tissue segmentation to learn fine-grained positional
and morphological representations of PWMLs for accurate localization and
segmentation. A simple and easy-to-implement deep-learning framework (i.e.,
DeepPWML) is accordingly designed. It combines the lesion counterfactual map
with the tissue probability map to train a lightweight PWML segmentation
network, demonstrating state-of-the-art performance on a real-clinical dataset
of infant T1w MR images. The code is available at
\href{https://github.com/ladderlab-xjtu/DeepPWML}{https://github.com/ladderlab-xjtu/DeepPWML}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zehua Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1"&gt;Miaomiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yuying Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianjun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lian_C/0/1/0/all/0/1"&gt;Chunfeng Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms. (arXiv:2309.03759v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.03759</id>
        <link href="http://arxiv.org/abs/2309.03759"/>
        <updated>2023-09-09T00:40:35.565Z</updated>
        <summary type="html"><![CDATA[Early detection of cardiac dysfunction through routine screening is vital for
diagnosing cardiovascular diseases. An important metric of cardiac function is
the left ventricular ejection fraction (EF), where lower EF is associated with
cardiomyopathy. Echocardiography is a popular diagnostic tool in cardiology,
with ultrasound being a low-cost, real-time, and non-ionizing technology.
However, human assessment of echocardiograms for calculating EF is
time-consuming and expertise-demanding, raising the need for an automated
approach. In this work, we propose using the M(otion)-mode of echocardiograms
for estimating the EF and classifying cardiomyopathy. We generate multiple
artificial M-mode images from a single echocardiogram and combine them using
off-the-shelf model architectures. Additionally, we extend contrastive learning
(CL) to cardiac imaging to learn meaningful representations from exploiting
structures in unlabeled data allowing the model to achieve high accuracy, even
with limited annotations. Our experiments show that the supervised setting
converges with only ten modes and is comparable to the baseline method while
bypassing its cumbersome training process and being computationally much more
efficient. Furthermore, CL using M-mode images is helpful for limited data
scenarios, such as having labels for only 200 patients, which is common in
medical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ozkan_E/0/1/0/all/0/1"&gt;Ece Ozkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sutter_T/0/1/0/all/0/1"&gt;Thomas M. Sutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yurong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Balzer_S/0/1/0/all/0/1"&gt;Sebastian Balzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformal Autoregressive Generation: Beam Search with Coverage Guarantees. (arXiv:2309.03797v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03797</id>
        <link href="http://arxiv.org/abs/2309.03797"/>
        <updated>2023-09-09T00:40:35.564Z</updated>
        <summary type="html"><![CDATA[We introduce two new extensions to the beam search algorithm based on
conformal predictions (CP) to produce sets of sequences with theoretical
coverage guarantees. The first method is very simple and proposes
dynamically-sized subsets of beam search results but, unlike typical CP
procedures, has an upper bound on the achievable guarantee depending on a
post-hoc calibration measure. Our second algorithm introduces the conformal set
prediction procedure as part of the decoding process, producing a variable beam
width which adapts to the current uncertainty. While more complex, this
procedure can achieve coverage guarantees selected a priori. We provide
marginal coverage bounds for each method, and evaluate them empirically on a
selection of tasks drawing from natural language processing and chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deutschmann_N/0/1/0/all/0/1"&gt;Nicolas Deutschmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alberts_M/0/1/0/all/0/1"&gt;Marvin Alberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a Rodr&amp;#xed;guez Mart&amp;#xed;nez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation. (arXiv:2309.03322v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03322</id>
        <link href="http://arxiv.org/abs/2309.03322"/>
        <updated>2023-09-09T00:40:35.540Z</updated>
        <summary type="html"><![CDATA[Dexterous manipulation tasks involving contact-rich interactions pose a
significant challenge for both model-based control systems and imitation
learning algorithms. The complexity arises from the need for multi-fingered
robotic hands to dynamically establish and break contacts, balance
non-prehensile forces, and control large degrees of freedom. Reinforcement
learning (RL) offers a promising approach due to its general applicability and
capacity to autonomously acquire optimal manipulation strategies. However, its
real-world application is often hindered by the necessity to generate a large
number of samples, reset the environment, and obtain reward signals. In this
work, we introduce an efficient system for learning dexterous manipulation
skills with RL to alleviate these challenges. The main idea of our approach is
the integration of recent advances in sample-efficient RL and replay buffer
bootstrapping. This combination allows us to utilize data from different tasks
or objects as a starting point for training new tasks, significantly improving
learning efficiency. Additionally, our system completes the real-world training
cycle by incorporating learned resets via an imitation-based pickup policy as
well as learned reward functions, eliminating the need for manual resets and
reward engineering. We demonstrate the benefits of reusing past data as replay
buffer initialization for new tasks, for instance, the fast acquisition of
intricate manipulation skills in the real world on a four-fingered robotic
hand. (Videos: https://sites.google.com/view/reboot-dexterous)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zheyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rovinsky_A/0/1/0/all/0/1"&gt;Aaron Rovinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jianlan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vikash Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Example-Based Explainability: a Survey. (arXiv:2309.03234v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.03234</id>
        <link href="http://arxiv.org/abs/2309.03234"/>
        <updated>2023-09-09T00:40:35.538Z</updated>
        <summary type="html"><![CDATA[Explainable Artificial Intelligence (XAI) has become increasingly significant
for improving the interpretability and trustworthiness of machine learning
models. While saliency maps have stolen the show for the last few years in the
XAI field, their ability to reflect models' internal processes has been
questioned. Although less in the spotlight, example-based XAI methods have
continued to improve. It encompasses methods that use examples as explanations
for a machine learning model's predictions. This aligns with the psychological
mechanisms of human reasoning and makes example-based explanations natural and
intuitive for users to understand. Indeed, humans learn and reason by forming
mental representations of concepts based on examples.

This paper provides an overview of the state-of-the-art in natural
example-based XAI, describing the pros and cons of each approach. A "natural"
example simply means that it is directly drawn from the training data without
involving any generative process. The exclusion of methods that require
generating examples is justified by the need for plausibility which is in some
regards required to gain a user's trust. Consequently, this paper will explore
the following family of methods: similar examples, counterfactual and
semi-factual, influential instances, prototypes, and concepts. In particular,
it will compare their semantic definition, their cognitive impact, and added
values. We hope it will encourage and facilitate future work on natural
example-based XAI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poche_A/0/1/0/all/0/1"&gt;Antonin Poch&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hervier_L/0/1/0/all/0/1"&gt;Lucas Hervier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bakkay_M/0/1/0/all/0/1"&gt;Mohamed-Chafik Bakkay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Space of Adversarial Strategies. (arXiv:2209.04521v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.04521</id>
        <link href="http://arxiv.org/abs/2209.04521"/>
        <updated>2023-09-09T00:40:35.538Z</updated>
        <summary type="html"><![CDATA[Adversarial examples, inputs designed to induce worst-case behavior in
machine learning models, have been extensively studied over the past decade.
Yet, our understanding of this phenomenon stems from a rather fragmented pool
of knowledge; at present, there are a handful of attacks, each with disparate
assumptions in threat models and incomparable definitions of optimality. In
this paper, we propose a systematic approach to characterize worst-case (i.e.,
optimal) adversaries. We first introduce an extensible decomposition of attacks
in adversarial machine learning by atomizing attack components into surfaces
and travelers. With our decomposition, we enumerate over components to create
576 attacks (568 of which were previously unexplored). Next, we propose the
Pareto Ensemble Attack (PEA): a theoretical attack that upper-bounds attack
performance. With our new attacks, we measure performance relative to the PEA
on: both robust and non-robust models, seven datasets, and three extended
lp-based threat models incorporating compute costs, formalizing the Space of
Adversarial Strategies. From our evaluation we find that attack performance to
be highly contextual: the domain, model robustness, and threat model can have a
profound influence on attack efficacy. Our investigation suggests that future
studies measuring the security of machine learning should: (1) be
contextualized to the domain & threat models, and (2) go beyond the handful of
known attacks used today.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheatsley_R/0/1/0/all/0/1"&gt;Ryan Sheatsley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoak_B/0/1/0/all/0/1"&gt;Blaine Hoak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauley_E/0/1/0/all/0/1"&gt;Eric Pauley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDaniel_P/0/1/0/all/0/1"&gt;Patrick McDaniel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03239</id>
        <link href="http://arxiv.org/abs/2309.03239"/>
        <updated>2023-09-09T00:40:35.537Z</updated>
        <summary type="html"><![CDATA[Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.

To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_S/0/1/0/all/0/1"&gt;Songyu Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Ting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Li Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yanping Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qintian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yu Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Let Quantum Neural Networks Choose Their Own Frequencies. (arXiv:2309.03279v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.03279</id>
        <link href="http://arxiv.org/abs/2309.03279"/>
        <updated>2023-09-09T00:40:35.536Z</updated>
        <summary type="html"><![CDATA[Parameterized quantum circuits as machine learning models are typically well
described by their representation as a partial Fourier series of the input
features, with frequencies uniquely determined by the feature map's generator
Hamiltonians. Ordinarily, these data-encoding generators are chosen in advance,
fixing the space of functions that can be represented. In this work we consider
a generalization of quantum models to include a set of trainable parameters in
the generator, leading to a trainable frequency (TF) quantum model. We
numerically demonstrate how TF models can learn generators with desirable
properties for solving the task at hand, including non-regularly spaced
frequencies in their spectra and flexible spectral richness. Finally, we
showcase the real-world effectiveness of our approach, demonstrating an
improved accuracy in solving the Navier-Stokes equations using a TF model with
only a single parameter added to each encoding operation. Since TF models
encompass conventional fixed frequency models, they may offer a sensible
default choice for variational quantum machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jaderberg_B/0/1/0/all/0/1"&gt;Ben Jaderberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gentile_A/0/1/0/all/0/1"&gt;Antonio A. Gentile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Berrada_Y/0/1/0/all/0/1"&gt;Youssef Achari Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Shishenina_E/0/1/0/all/0/1"&gt;Elvira Shishenina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Elfving_V/0/1/0/all/0/1"&gt;Vincent E. Elfving&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comparison of rational and neural network based approximations. (arXiv:2303.04436v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.04436</id>
        <link href="http://arxiv.org/abs/2303.04436"/>
        <updated>2023-09-09T00:40:35.535Z</updated>
        <summary type="html"><![CDATA[Rational and neural network based approximations are efficient tools in
modern approximation. These approaches are able to produce accurate
approximations to nonsmooth and non-Lipschitz functions, including multivariate
domain functions. In this paper we compare the efficiency of function
approximation using rational approximation, neural network and their
combinations. It was found that rational approximation is superior to neural
network based approaches with the same number of decision variables. Our
numerical experiments demonstrate the efficiency of rational approximation,
even when the number of approximation parameters (that is, the dimension of the
corresponding optimisation problems) is small. Another important contribution
of this paper lies in the improvement of rational approximation algorithms.
Namely, the optimisation based algorithms for rational approximation can be
adjusted to in such a way that the conditioning number of the constraint
matrices are controlled. This simple adjustment enables us to work with high
dimension optimisation problems and improve the design of the neural network.
The main strength of neural networks is in their ability to handle models with
a large number of variables: complex models are decomposed in several simple
optimisation problems. Therefore the the large number of decision variables is
in the nature of neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Peiris_V/0/1/0/all/0/1"&gt;Vinesha Peiris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Millan_R/0/1/0/all/0/1"&gt;Reinier Diaz Millan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sukhorukova_N/0/1/0/all/0/1"&gt;Nadezda Sukhorukova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ugon_J/0/1/0/all/0/1"&gt;Julien Ugon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2309.03318</id>
        <link href="http://arxiv.org/abs/2309.03318"/>
        <updated>2023-09-09T00:40:35.532Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to performing fitness approximation in genetic
algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary
agents in Gymnasium (game) simulators -- where fitness computation is costly.
Maintaining a dataset of sampled individuals along with their actual fitness
scores, we continually update throughout an evolutionary run a
fitness-approximation ML model. We compare different methods for: 1) switching
between actual and approximate fitness, 2) sampling the population, and 3)
weighting the samples. Experimental findings demonstrate significant
improvement in evolutionary runtimes, with fitness scores that are either
identical or slightly lower than that of the fully run GA -- depending on the
ratio of approximate-to-actual-fitness computation. Our approach is generic and
can be easily applied to many different domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzruia_I/0/1/0/all/0/1"&gt;Itai Tzruia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1"&gt;Tomer Halperin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1"&gt;Moshe Sipper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elyasaf_A/0/1/0/all/0/1"&gt;Achiya Elyasaf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Causal Graph Convolutional Network for Traffic Prediction. (arXiv:2306.07019v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.07019</id>
        <link href="http://arxiv.org/abs/2306.07019"/>
        <updated>2023-09-09T00:40:35.526Z</updated>
        <summary type="html"><![CDATA[Modeling complex spatiotemporal dependencies in correlated traffic series is
essential for traffic prediction. While recent works have shown improved
prediction performance by using neural networks to extract spatiotemporal
correlations, their effectiveness depends on the quality of the graph
structures used to represent the spatial topology of the traffic network. In
this work, we propose a novel approach for traffic prediction that embeds
time-varying dynamic Bayesian network to capture the fine spatiotemporal
topology of traffic data. We then use graph convolutional networks to generate
traffic forecasts. To enable our method to efficiently model nonlinear traffic
propagation patterns, we develop a deep learning-based module as a
hyper-network to generate stepwise dynamic causal graphs. Our experimental
results on a real traffic dataset demonstrate the superior prediction
performance of the proposed method. The code is available at
https://github.com/MonBG/DCGCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junpeng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhishuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1"&gt;Lei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1"&gt;Rui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.01108</id>
        <link href="http://arxiv.org/abs/2309.01108"/>
        <updated>2023-09-09T00:40:35.515Z</updated>
        <summary type="html"><![CDATA[$ $Acoustic-to-articulatory inversion (AAI) involves mapping from the
acoustic space to the articulatory space. Signal-processing features like the
MFCCs, have been widely used for the AAI task. For subjects with dysarthric
speech, AAI is challenging because of an imprecise and indistinct
pronunciation. In this work, we perform AAI for dysarthric speech using
representations from pre-trained self-supervised learning (SSL) models. We
demonstrate the impact of different pre-trained features on this challenging
AAI task, at low-resource conditions. In addition, we also condition x-vectors
to the extracted SSL features to train a BLSTM network. In the seen case, we
experiment with three AAI training schemes (subject-specific, pooled, and
fine-tuned). The results, consistent across training schemes, reveal that
DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the
Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for
healthy controls and patients, respectively, over MFCCs. In the unseen case, we
observe similar average trends for different SSL features. Overall, SSL
networks like wav2vec, APC, and DeCoAR, which are trained with feature
reconstruction or future timestep prediction tasks, perform well in predicting
dysarthric articulatory trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Maharana_S/0/1/0/all/0/1"&gt;Sarthak Kumar Maharana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adidam_K/0/1/0/all/0/1"&gt;Krishna Kamal Adidam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nandi_S/0/1/0/all/0/1"&gt;Shoumik Nandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Ajitesh Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Explanation Methods for Multivariate Time Series Classification. (arXiv:2308.15223v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.15223</id>
        <link href="http://arxiv.org/abs/2308.15223"/>
        <updated>2023-09-09T00:40:35.506Z</updated>
        <summary type="html"><![CDATA[Multivariate time series classification is an important computational task
arising in applications where data is recorded over time and over multiple
channels. For example, a smartwatch can record the acceleration and orientation
of a person's motion, and these signals are recorded as multivariate time
series. We can classify this data to understand and predict human movement and
various properties such as fitness levels. In many applications classification
alone is not enough, we often need to classify but also understand what the
model learns (e.g., why was a prediction given, based on what information in
the data). The main focus of this paper is on analysing and evaluating
explanation methods tailored to Multivariate Time Series Classification (MTSC).
We focus on saliency-based explanation methods that can point out the most
relevant channels and time series points for the classification decision. We
analyse two popular and accurate multivariate time series classifiers, ROCKET
and dResNet, as well as two popular explanation methods, SHAP and dCAM. We
study these methods on 3 synthetic datasets and 2 real-world datasets and
provide a quantitative and qualitative analysis of the explanations provided.
We find that flattening the multivariate datasets by concatenating the channels
works as well as using multivariate classifiers directly and adaptations of
SHAP for MTSC work quite well. Additionally, we also find that the popular
synthetic datasets we used are not suitable for time series analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Serramazza_D/0/1/0/all/0/1"&gt;Davide Italo Serramazza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thu Trang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thach Le Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ifrim_G/0/1/0/all/0/1"&gt;Georgiana Ifrim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders. (arXiv:2202.09671v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.09671</id>
        <link href="http://arxiv.org/abs/2202.09671"/>
        <updated>2023-09-09T00:40:35.500Z</updated>
        <summary type="html"><![CDATA[Employing a forward diffusion chain to gradually map the data to a noise
distribution, diffusion-based generative models learn how to generate the data
by inferring a reverse diffusion chain. However, this approach is slow and
costly because it needs many forward and reverse steps. We propose a faster and
cheaper approach that adds noise not until the data become pure random noise,
but until they reach a hidden noisy data distribution that we can confidently
learn. Then, we use fewer reverse steps to generate data by starting from this
hidden distribution that is made similar to the noisy data. We reveal that the
proposed model can be cast as an adversarial auto-encoder empowered by both the
diffusion process and a learnable implicit prior. Experimental results show
even with a significantly smaller number of reverse diffusion steps, the
proposed truncated diffusion probabilistic models can provide consistent
improvements over the non-truncated ones in terms of performance in both
unconditional and text-guided image generations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RatGPT: Turning online LLMs into Proxies for Malware Attacks. (arXiv:2308.09183v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.09183</id>
        <link href="http://arxiv.org/abs/2308.09183"/>
        <updated>2023-09-09T00:40:35.492Z</updated>
        <summary type="html"><![CDATA[The evolution of Generative AI and the capabilities of the newly released
Large Language Models (LLMs) open new opportunities in software engineering.
However, they also lead to new challenges in cybersecurity. Recently,
researchers have shown the possibilities of using LLMs such as ChatGPT to
generate malicious content that can directly be exploited or guide
inexperienced hackers to weaponize tools and code. These studies covered
scenarios that still require the attacker to be in the middle of the loop. In
this study, we leverage openly available plugins and use an LLM as proxy
between the attacker and the victim. We deliver a proof-of-concept where
ChatGPT is used for the dissemination of malicious software while evading
detection, alongside establishing the communication to a command and control
(C2) server to receive commands to interact with a victim's system. Finally, we
present the general approach as well as essential elements in order to stay
undetected and make the attack a success. This proof-of-concept highlights
significant cybersecurity issues with openly available plugins and LLMs, which
require the development of security guidelines, controls, and mitigation
strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beckerich_M/0/1/0/all/0/1"&gt;Mika Beckerich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plein_L/0/1/0/all/0/1"&gt;Laura Plein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coronado_S/0/1/0/all/0/1"&gt;Sergio Coronado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network. (arXiv:2309.03694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03694</id>
        <link href="http://arxiv.org/abs/2309.03694"/>
        <updated>2023-09-09T00:40:35.481Z</updated>
        <summary type="html"><![CDATA[Short-term load forecasting is of paramount importance in the efficient
operation and planning of power systems, given its inherent non-linear and
dynamic nature. Recent strides in deep learning have shown promise in
addressing this challenge. However, these methods often grapple with
hyperparameter sensitivity, opaqueness in interpretability, and high
computational overhead for real-time deployment. In this paper, I propose a
novel solution that surmounts these obstacles. Our approach harnesses the power
of the Particle-Swarm Optimization algorithm to autonomously explore and
optimize hyperparameters, a Multi-Head Attention mechanism to discern the
salient features crucial for accurate forecasting, and a streamlined framework
for computational efficiency. Our method undergoes rigorous evaluation using a
genuine electricity demand dataset. The results underscore its superiority in
terms of accuracy, robustness, and computational efficiency. Notably, our Mean
Absolute Percentage Error of 1.9376 marks a significant advancement over
existing state-of-the-art approaches, heralding a new era in short-term load
forecasting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quansah_P/0/1/0/all/0/1"&gt;Paapa Kwesi Quansah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors. (arXiv:2308.09199v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.09199</id>
        <link href="http://arxiv.org/abs/2308.09199"/>
        <updated>2023-09-09T00:40:35.458Z</updated>
        <summary type="html"><![CDATA[It is shown that a class of optical physical unclonable functions (PUFs) can
be learned to arbitrary precision with arbitrarily high probability, even in
the presence of noise, given access to polynomially many challenge-response
pairs and polynomially bounded computational power, under mild assumptions
about the distributions of the noise and challenge vectors. This extends the
results of Rh\"uramir et al. (2013), who showed a subset of this class of PUFs
to be learnable in polynomial time in the absence of noise, under the
assumption that the optics of the PUF were either linear or had negligible
nonlinear effects. We derive polynomial bounds for the required number of
samples and the computational complexity of a linear regression algorithm,
based on size parameters of the PUF, the distributions of the challenge and
noise vectors, and the probability and accuracy of the regression algorithm,
with a similar analysis to one done by Bootle et al. (2018), who demonstrated a
learning attack on a poorly implemented version of the Learning With Errors
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albright_A/0/1/0/all/0/1"&gt;Apollo Albright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelfand_B/0/1/0/all/0/1"&gt;Boris Gelfand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixon_M/0/1/0/all/0/1"&gt;Michael Dixon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach. (arXiv:2309.03169v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.03169</id>
        <link href="http://arxiv.org/abs/2309.03169"/>
        <updated>2023-09-09T00:40:35.458Z</updated>
        <summary type="html"><![CDATA[While recommender systems have significantly benefited from implicit
feedback, they have often missed the nuances of multi-behavior interactions
between users and items. Historically, these systems either amalgamated all
behaviors, such as \textit{impression} (formerly \textit{view}),
\textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label,
or prioritized only the target behavior, often the \textit{buy} action,
discarding valuable auxiliary signals. Although recent advancements tried
addressing this simplification, they primarily gravitated towards optimizing
the target behavior alone, battling with data scarcity. Additionally, they
tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these
gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior
\textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework
leverages attention mechanisms to discern information from both inter and
intra-behaviors while employing a multi-task Hierarchical Bayesian Personalized
Ranking (HBPR) for optimization. Recognizing the need for scalability, our
approach integrates a specialized multi-behavior sub-graph sampling technique.
Moreover, the adaptability of HMGN allows for the seamless inclusion of
knowledge metadata and time-series data. Empirical results attest to our
model's prowess, registering a notable performance boost of up to 64\% in
NDCG@100 metrics over conventional graph neural network methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhargavi_D/0/1/0/all/0/1"&gt;Divya Bhargavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravipati_V/0/1/0/all/0/1"&gt;Vidya Sagar Ravipati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2106.02613v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02613</id>
        <link href="http://arxiv.org/abs/2106.02613"/>
        <updated>2023-09-09T00:40:35.452Z</updated>
        <summary type="html"><![CDATA[Bootstrapping is behind much of the successes of deep Reinforcement Learning.
However, learning the value function via bootstrapping often leads to unstable
training due to fast-changing target values. Target Networks are employed to
stabilize training by using an additional set of lagging parameters to estimate
the target values. Despite the popularity of Target Networks, their effect on
the optimization is still misunderstood. In this work, we show that they act as
an implicit regularizer which can be beneficial in some cases, but also have
disadvantages such as being inflexible and can result in instabilities, even
when vanilla TD(0) converges. To overcome these issues, we propose an explicit
Functional Regularization alternative that is flexible and a convex regularizer
in function space and we theoretically study its convergence. We conduct an
experimental study across a range of environments, discount factors, and
off-policiness data collections to investigate the effectiveness of the
regularization induced by Target Networks and Functional Regularization in
terms of performance, accuracy, and stability. Our findings emphasize that
Functional Regularization can be used as a drop-in replacement for Target
Networks and result in performance improvement. Furthermore, adjusting both the
regularization weight and the network update period in Functional
Regularization can result in further performance improvements compared to
solely adjusting the network update period as typically done with Target
Networks. Our approach also enhances the ability to networks to recover
accurate $Q$-values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piche_A/0/1/0/all/0/1"&gt;Alexandre Pich&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thomas_V/0/1/0/all/0/1"&gt;Valentin Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pardinas_R/0/1/0/all/0/1"&gt;Rafael Pardinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marino_J/0/1/0/all/0/1"&gt;Joseph Marino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marconi_G/0/1/0/all/0/1"&gt;Gian Maria Marconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Risk Minimization for Losses without Variance. (arXiv:2309.03818v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03818</id>
        <link href="http://arxiv.org/abs/2309.03818"/>
        <updated>2023-09-09T00:40:35.451Z</updated>
        <summary type="html"><![CDATA[This paper considers an empirical risk minimization problem under
heavy-tailed settings, where data does not have finite variance, but only has
$p$-th moment with $p \in (1,2)$. Instead of using estimation procedure based
on truncated observed data, we choose the optimizer by minimizing the risk
value. Those risk values can be robustly estimated via using the remarkable
Catoni's method (Catoni, 2012). Thanks to the structure of Catoni-type
influence functions, we are able to establish excess risk upper bounds via
using generalized generic chaining methods. Moreover, we take computational
issues into consideration. We especially theoretically investigate two types of
optimization methods, robust gradient descent algorithm and empirical
risk-based methods. With an extensive numerical study, we find that the
optimizer based on empirical risks via Catoni-style estimation indeed shows
better performance than other baselines. It indicates that estimation directly
based on truncated data may lead to unsatisfactory results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fang_G/0/1/0/all/0/1"&gt;Guanhua Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Samorodnitsky_G/0/1/0/all/0/1"&gt;Gennady Samorodnitsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Sequential Forecasters. (arXiv:2110.00115v5 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.00115</id>
        <link href="http://arxiv.org/abs/2110.00115"/>
        <updated>2023-09-09T00:40:35.446Z</updated>
        <summary type="html"><![CDATA[Consider two forecasters, each making a single prediction for a sequence of
events over time. We ask a relatively basic question: how might we compare
these forecasters, either online or post-hoc, while avoiding unverifiable
assumptions on how the forecasts and outcomes were generated? In this paper, we
present a rigorous answer to this question by designing novel sequential
inference procedures for estimating the time-varying difference in forecast
scores. To do this, we employ confidence sequences (CS), which are sequences of
confidence intervals that can be continuously monitored and are valid at
arbitrary data-dependent stopping times ("anytime-valid"). The widths of our
CSs are adaptive to the underlying variance of the score differences.
Underlying their construction is a game-theoretic statistical framework, in
which we further identify e-processes and p-processes for sequentially testing
a weak null hypothesis -- whether one forecaster outperforms another on average
(rather than always). Our methods do not make distributional assumptions on the
forecasts or outcomes; our main theorems apply to any bounded scores, and we
later provide alternative methods for unbounded scores. We empirically validate
our approaches by comparing real-world baseball and weather forecasters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Choe_Y/0/1/0/all/0/1"&gt;Yo Joong Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images. (arXiv:2309.02556v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.02556</id>
        <link href="http://arxiv.org/abs/2309.02556"/>
        <updated>2023-09-09T00:40:35.444Z</updated>
        <summary type="html"><![CDATA[In recent years, deep neural networks (DNNs) trained with transformed data
have been applied to various applications such as privacy-preserving learning,
access control, and adversarial defenses. However, the use of transformed data
decreases the performance of models. Accordingly, in this paper, we propose a
novel method for fine-tuning models with transformed images under the use of
the vision transformer (ViT). The proposed domain adaptation method does not
cause the accuracy degradation of models, and it is carried out on the basis of
the embedding structure of ViT. In experiments, we confirmed that the proposed
method prevents accuracy degradation even when using encrypted images with the
CIFAR-10 and CIFAR-100 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagamori_T/0/1/0/all/0/1"&gt;Teru Nagamori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1"&gt;Sayaka Shiota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient anti-symmetrization of a neural network layer by taming the sign problem. (arXiv:2205.12250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.12250</id>
        <link href="http://arxiv.org/abs/2205.12250"/>
        <updated>2023-09-09T00:40:35.412Z</updated>
        <summary type="html"><![CDATA[Explicit antisymmetrization of a neural network is a potential candidate for
a universal function approximator for generic antisymmetric functions, which
are ubiquitous in quantum physics. However, this procedure is a priori
factorially costly to implement, making it impractical for large numbers of
particles. The strategy also suffers from a sign problem. Namely, due to
near-exact cancellation of positive and negative contributions, the magnitude
of the antisymmetrized function may be significantly smaller than before
anti-symmetrization. We show that the anti-symmetric projection of a two-layer
neural network can be evaluated efficiently, opening the door to using a
generic antisymmetric layer as a building block in anti-symmetric neural
network Ansatzes. This approximation is effective when the sign problem is
controlled, and we show that this property depends crucially the choice of
activation function under standard Xavier/He initialization methods. As a
consequence, using a smooth activation function requires re-scaling of the
neural network weights compared to standard initializations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamsen_N/0/1/0/all/0/1"&gt;Nilin Abrahamsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lin Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.09241</id>
        <link href="http://arxiv.org/abs/2305.09241"/>
        <updated>2023-09-09T00:40:35.412Z</updated>
        <summary type="html"><![CDATA[Safeguarding data from unauthorized exploitation is vital for privacy and
security, especially in recent rampant research in security breach such as
adversarial/membership attacks. To this end, \textit{unlearnable examples}
(UEs) have been recently proposed as a compelling protection, by adding
imperceptible perturbation to data so that models trained on them cannot
classify them accurately on original clean distribution. Unfortunately, we find
UEs provide a false sense of security, because they cannot stop unauthorized
users from utilizing other unprotected data to remove the protection, by
turning unlearnable data into learnable again. Motivated by this observation,
we formally define a new threat by introducing \textit{learnable unauthorized
examples} (LEs) which are UEs with their protection removed. The core of this
approach is a novel purification process that projects UEs onto the manifold of
LEs. This is realized by a new joint-conditional diffusion model which denoises
UEs conditioned on the pixel and perceptual similarity between UEs and LEs.
Extensive experiments demonstrate that LE delivers state-of-the-art countering
performance against both supervised UEs and unsupervised UEs in various
scenarios, which is the first generalizable countermeasure to UEs across
supervised learning and unsupervised learning. Our code is available at
\url{https://github.com/jiangw-0/LE_JCDP}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1"&gt;Yunfeng Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;He Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jianxin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1"&gt;Richang Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework. (arXiv:2309.02428v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.02428</id>
        <link href="http://arxiv.org/abs/2309.02428"/>
        <updated>2023-09-09T00:40:35.412Z</updated>
        <summary type="html"><![CDATA[The burgeoning growth of public domain data and the increasing complexity of
deep learning model architectures have underscored the need for more efficient
data representation and analysis techniques. This paper is motivated by the
work of Helal (2023) and aims to present a comprehensive overview of
tensorization. This transformative approach bridges the gap between the
inherently multidimensional nature of data and the simplified 2-dimensional
matrices commonly used in linear algebra-based machine learning algorithms.
This paper explores the steps involved in tensorization, multidimensional data
sources, various multiway analysis methods employed, and the benefits of these
approaches. A small example of Blind Source Separation (BSS) is presented
comparing 2-dimensional algorithms and a multiway algorithm in Python. Results
indicate that multiway analysis is more expressive. Contrary to the intuition
of the dimensionality curse, utilising multidimensional datasets in their
native form and applying multiway analysis methods grounded in multilinear
algebra reveal a profound capacity to capture intricate interrelationships
among various dimensions while, surprisingly, reducing the number of model
parameters and accelerating processing. A survey of the multi-away analysis
methods and integration with various Deep Neural Networks models is presented
using case studies in different domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helal_M/0/1/0/all/0/1"&gt;Manal Helal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation. (arXiv:2309.03891v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.03891</id>
        <link href="http://arxiv.org/abs/2309.03891"/>
        <updated>2023-09-09T00:40:35.411Z</updated>
        <summary type="html"><![CDATA[We present ArtiGrasp, a novel method to synthesize bi-manual hand-object
interactions that include grasping and articulation. This task is challenging
due to the diversity of the global wrist motions and the precise finger control
that are necessary to articulate objects. ArtiGrasp leverages reinforcement
learning and physics simulations to train a policy that controls the global and
local hand pose. Our framework unifies grasping and articulation within a
single policy guided by a single hand pose reference. Moreover, to facilitate
the training of the precise finger control required for articulation, we
present a learning curriculum with increasing difficulty. It starts with
single-hand manipulation of stationary objects and continues with multi-agent
training including both hands and non-stationary objects. To evaluate our
method, we introduce Dynamic Object Grasping and Articulation, a task that
involves bringing an object into a target articulated pose. This task requires
grasping, relocation, and articulation. We show our method's efficacy towards
this task. We further demonstrate that our method can generate motions with
noisy hand-object pose estimates from an off-the-shelf image-based regressor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1"&gt;Sammy Christen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zicong Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Luocheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1"&gt;Jemin Hwangbo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[USE-Evaluator: Performance Metrics for Medical Image Segmentation Models with Uncertain, Small or Empty Reference Annotations. (arXiv:2209.13008v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.13008</id>
        <link href="http://arxiv.org/abs/2209.13008"/>
        <updated>2023-09-09T00:40:35.411Z</updated>
        <summary type="html"><![CDATA[Performance metrics for medical image segmentation models are used to measure
the agreement between the reference annotation and the predicted segmentation.
Usually, overlap metrics, such as the Dice, are used as a metric to evaluate
the performance of these models in order for results to be comparable. However,
there is a mismatch between the distributions of cases and difficulty level of
segmentation tasks in public data sets compared to clinical practice. Common
metrics fail to measure the impact of this mismatch, especially for clinical
data sets that include low signal pathologies, a difficult segmentation task,
and uncertain, small, or empty reference annotations. This limitation may
result in ineffective research of machine learning practitioners in designing
and optimizing models. Dimensions of evaluating clinical value include
consideration of the uncertainty of reference annotations, independence from
reference annotation volume size, and evaluation of classification of empty
reference annotations. We study how uncertain, small, and empty reference
annotations influence the value of metrics for medical image segmentation on an
in-house data set regardless of the model. We examine metrics behavior on the
predictions of a standard deep learning framework in order to identify metrics
with clinical value. We compare to a public benchmark data set (BraTS 2019)
with a high-signal pathology and certain, larger, and no empty reference
annotations. We may show machine learning practitioners, how uncertain, small,
or empty reference annotations require a rethinking of the evaluation and
optimizing procedures. The evaluation code was released to encourage further
analysis of this topic.
https://github.com/SophieOstmeier/UncertainSmallEmpty.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ostmeier_S/0/1/0/all/0/1"&gt;Sophie Ostmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Axelrod_B/0/1/0/all/0/1"&gt;Brian Axelrod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertels_J/0/1/0/all/0/1"&gt;Jeroen Bertels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1"&gt;Fabian Isensee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lansberg_M/0/1/0/all/0/1"&gt;Maarten G.Lansberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Christensen_S/0/1/0/all/0/1"&gt;Soren Christensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Albers_G/0/1/0/all/0/1"&gt;Gregory W. Albers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1"&gt;Li-Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heit_J/0/1/0/all/0/1"&gt;Jeremy J. Heit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Demonstration via Probabilistic Diagrammatic Teaching. (arXiv:2309.03835v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.03835</id>
        <link href="http://arxiv.org/abs/2309.03835"/>
        <updated>2023-09-09T00:40:35.409Z</updated>
        <summary type="html"><![CDATA[Learning for Demonstration (LfD) enables robots to acquire new skills by
imitating expert demonstrations, allowing users to communicate their
instructions in an intuitive manner. Recent progress in LfD often relies on
kinesthetic teaching or teleoperation as the medium for users to specify the
demonstrations. Kinesthetic teaching requires physical handling of the robot,
while teleoperation demands proficiency with additional hardware. This paper
introduces an alternative paradigm for LfD called Diagrammatic Teaching.
Diagrammatic Teaching aims to teach robots novel skills by prompting the user
to sketch out demonstration trajectories on 2D images of the scene, these are
then synthesised as a generative model of motion trajectories in 3D task space.
Additionally, we present the Ray-tracing Probabilistic Trajectory Learning
(RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying
probability densities from the 2D sketches, applies ray-tracing to find
corresponding regions in 3D Cartesian space, and fits a probabilistic model of
motion trajectories to these regions. New motion trajectories, which mimic
those sketched by the user, can then be generated from the probabilistic model.
We empirically validate our framework both in simulation and on real robots,
which include a fixed-base manipulator and a quadruped-mounted manipulator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_W/0/1/0/all/0/1"&gt;Weiming Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1"&gt;Matthew Johnson-Roberson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.02685</id>
        <link href="http://arxiv.org/abs/2309.02685"/>
        <updated>2023-09-09T00:40:35.407Z</updated>
        <summary type="html"><![CDATA[Recent studies have verified that equivariant methods can significantly
improve the data efficiency, generalizability, and robustness in robot
learning. Meanwhile, denoising diffusion-based generative modeling has recently
gained significant attention as a promising approach for robotic manipulation
learning from demonstrations with stochastic behaviors. In this paper, we
present Diffusion-EDFs, a novel approach that incorporates spatial
roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative
modeling. By integrating SE(3)-equivariance into our model architectures, we
demonstrate that our proposed method exhibits remarkable data efficiency,
requiring only 5 to 10 task demonstrations for effective end-to-end training.
Furthermore, our approach showcases superior generalizability compared to
previous diffusion-based manipulation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1"&gt;Hyunwoo Ryu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jiwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Junwoo Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1"&gt;Hyun Seok Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1"&gt;Joohwan Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yubin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jongeun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1"&gt;Roberto Horowitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Federated Training of Object Detection in the Internet of Vehicles. (arXiv:2309.03569v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03569</id>
        <link href="http://arxiv.org/abs/2309.03569"/>
        <updated>2023-09-09T00:40:35.406Z</updated>
        <summary type="html"><![CDATA[As an essential component part of the Intelligent Transportation System
(ITS), the Internet of Vehicles (IoV) plays a vital role in alleviating traffic
issues. Object detection is one of the key technologies in the IoV, which has
been widely used to provide traffic management services by analyzing timely and
sensitive vehicle-related information. However, the current object detection
methods are mostly based on centralized deep training, that is, the sensitive
data obtained by edge devices need to be uploaded to the server, which raises
privacy concerns. To mitigate such privacy leakage, we first propose a
federated learning-based framework, where well-trained local models are shared
in the central server. However, since edge devices usually have limited
computing power, plus a strict requirement of low latency in IoVs, we further
propose a sparse training process on edge devices, which can effectively
lighten the model, and ensure its training efficiency on edge devices, thereby
reducing communication overheads. In addition, due to the diverse computing
capabilities and dynamic environment, different sparsity rates are applied to
edge devices. To further guarantee the performance, we propose, FedWeg, an
improved aggregation scheme based on FedAvg, which is designed by the inverse
ratio of sparsity rates. Experiments on the real-life dataset using YOLO show
that the proposed scheme can achieve the required object detection rate while
saving considerable communication costs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_L/0/1/0/all/0/1"&gt;Luping Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yuwen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Lu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03876</id>
        <link href="http://arxiv.org/abs/2309.03876"/>
        <updated>2023-09-09T00:40:35.398Z</updated>
        <summary type="html"><![CDATA[Instruction-tuned Large Language Models (LLMs) have recently showcased
remarkable ability to generate fitting responses to natural language
instructions. However, an open research question concerns the inherent biases
of trained models and their responses. For instance, if the data used to tune
an LLM is dominantly written by persons with a specific political bias, we
might expect generated answers to share this bias. Current research work seeks
to de-bias such models, or suppress potentially biased answers. With this
demonstration, we take a different view on biases in instruction-tuning: Rather
than aiming to suppress them, we aim to make them explicit and transparent. To
this end, we present OpinionGPT, a web demo in which users can ask questions
and select all biases they wish to investigate. The demo will answer this
question using a model fine-tuned on text representing each of the selected
biases, allowing side-by-side comparison. To train the underlying model, we
identified 11 different biases (political, geographic, gender, age) and derived
an instruction-tuning corpus in which each answer was written by members of one
of these demographics. This paper presents OpinionGPT, illustrates how we
trained the bias-aware model and showcases the web application (available at
https://opiniongpt.informatik.hu-berlin.de).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1"&gt;Patrick Haller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aynetdinov_A/0/1/0/all/0/1"&gt;Ansar Aynetdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbik_A/0/1/0/all/0/1"&gt;Alan Akbik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2309.03905</id>
        <link href="http://arxiv.org/abs/2309.03905"/>
        <updated>2023-09-09T00:40:35.398Z</updated>
        <summary type="html"><![CDATA[We present ImageBind-LLM, a multi-modality instruction tuning method of large
language models (LLMs) via ImageBind. Existing works mainly focus on language
and image instruction tuning, different from which, our ImageBind-LLM can
respond to multi-modality conditions, including audio, 3D point clouds, video,
and their embedding-space arithmetic by only image-text alignment training.
During training, we adopt a learnable bind network to align the embedding space
between LLaMA and ImageBind's image encoder. Then, the image features
transformed by the bind network are added to word tokens of all layers in
LLaMA, which progressively injects visual instructions via an attention-free
and zero-initialized gating mechanism. Aided by the joint embedding of
ImageBind, the simple image-text training enables our model to exhibit superior
multi-modality instruction-following capabilities. During inference, the
multi-modality inputs are fed into the corresponding ImageBind encoders, and
processed by a proposed visual cache model for further cross-modal embedding
enhancement. The training-free cache model retrieves from three million image
features extracted by ImageBind, which effectively mitigates the
training-inference modality discrepancy. Notably, with our approach,
ImageBind-LLM can respond to instructions of diverse modalities and demonstrate
significant language generation quality. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiaming Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Renrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wenqi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Han Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chris Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1"&gt;Song Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Ziyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xudong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yafei Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.06104</id>
        <link href="http://arxiv.org/abs/2304.06104"/>
        <updated>2023-09-09T00:40:35.396Z</updated>
        <summary type="html"><![CDATA[This paper studies the problem of online performance optimization of
constrained closed-loop control systems, where both the objective and the
constraints are unknown black-box functions affected by exogenous time-varying
contextual disturbances. A primal-dual contextual Bayesian optimization
algorithm is proposed that achieves sublinear cumulative regret with respect to
the dynamic optimal solution under certain regularity conditions. Furthermore,
the algorithm achieves zero time-average constraint violation, ensuring that
the average value of the constraint function satisfies the desired constraint.
The method is applied to both sampled instances from Gaussian processes and a
continuous stirred tank reactor parameter tuning problem; simulation results
show that the method simultaneously provides close-to-optimal performance and
maintains constraint feasibility on average. This contrasts current
state-of-the-art methods, which either suffer from large cumulative regret or
severe constraint violations for the case studies presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenjie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuning Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svetozarevic_B/0/1/0/all/0/1"&gt;Bratislav Svetozarevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1"&gt;Colin N. Jones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity. (arXiv:2111.06781v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.06781</id>
        <link href="http://arxiv.org/abs/2111.06781"/>
        <updated>2023-09-09T00:40:35.390Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning algorithms often require finiteness of state and
action spaces in Markov decision processes (MDPs) (also called controlled
Markov chains) and various efforts have been made in the literature towards the
applicability of such algorithms for continuous state and action spaces. In
this paper, we show that under very mild regularity conditions (in particular,
involving only weak continuity of the transition kernel of an MDP), Q-learning
for standard Borel MDPs via quantization of states and actions (called
Quantized Q-Learning) converges to a limit, and furthermore this limit
satisfies an optimality equation which leads to near optimality with either
explicit performance bounds or which are guaranteed to be asymptotically
optimal. Our approach builds on (i) viewing quantization as a measurement
kernel and thus a quantized MDP as a partially observed Markov decision process
(POMDP), (ii) utilizing near optimality and convergence results of Q-learning
for POMDPs, and (iii) finally, near-optimality of finite state model
approximations for MDPs with weakly continuous kernels which we show to
correspond to the fixed point of the constructed POMDP. Thus, our paper
presents a very general convergence and approximation result for the
applicability of Q-learning for continuous MDPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kara_A/0/1/0/all/0/1"&gt;Ali Devran Kara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saldi_N/0/1/0/all/0/1"&gt;Naci Saldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuksel_S/0/1/0/all/0/1"&gt;Serdar Y&amp;#xfc;ksel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03241</id>
        <link href="http://arxiv.org/abs/2309.03241"/>
        <updated>2023-09-09T00:40:35.387Z</updated>
        <summary type="html"><![CDATA[Previous studies have typically assumed that large language models are unable
to accurately perform arithmetic operations, particularly multiplication of >8
digits, and operations involving decimals and fractions, without the use of
calculator tools. This paper aims to challenge this misconception. With
sufficient training data, a 2 billion-parameter language model can accurately
perform multi-digit arithmetic operations with almost 100% accuracy without
data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication
accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from
GLM-10B on a dataset with additional multi-step arithmetic operations and math
problems described in text, achieves similar performance to GPT-4 on a
5,000-samples Chinese math problem test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1"&gt;Qingsong Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhihuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zehai He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1"&gt;Jinfeng Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03581</id>
        <link href="http://arxiv.org/abs/2309.03581"/>
        <updated>2023-09-09T00:40:35.387Z</updated>
        <summary type="html"><![CDATA[Hyperparameter optimization (HPO) is important to leverage the full potential
of machine learning (ML). In practice, users are often interested in
multi-objective (MO) problems, i.e., optimizing potentially conflicting
objectives, like accuracy and energy consumption. To tackle this, the vast
majority of MO-ML algorithms return a Pareto front of non-dominated machine
learning models to the user. Optimizing the hyperparameters of such algorithms
is non-trivial as evaluating a hyperparameter configuration entails evaluating
the quality of the resulting Pareto front. In literature, there are known
indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by
quantifying different properties (e.g., volume, proximity to a reference
point). However, choosing the indicator that leads to the desired Pareto front
might be a hard task for a user. In this paper, we propose a human-centered
interactive HPO approach tailored towards multi-objective ML leveraging
preference learning to extract desiderata from users that guide the
optimization. Instead of relying on the user guessing the most suitable
indicator for their needs, our approach automatically learns an appropriate
indicator. Concretely, we leverage pairwise comparisons of distinct Pareto
fronts to learn such an appropriate quality indicator. Then, we optimize the
hyperparameters of the underlying MO-ML algorithm towards this learned
indicator using a state-of-the-art HPO approach. In an experimental study
targeting the environmental impact of ML, we demonstrate that our approach
leads to substantially better Pareto fronts compared to optimizing based on a
wrong indicator pre-selected by the user, and performs comparable in the case
of an advanced user knowing which indicator to pick.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giovanelli_J/0/1/0/all/0/1"&gt;Joseph Giovanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1"&gt;Alexander Tornede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1"&gt;Tanja Tornede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03648</id>
        <link href="http://arxiv.org/abs/2309.03648"/>
        <updated>2023-09-09T00:40:35.387Z</updated>
        <summary type="html"><![CDATA[The Lipschitz bound, a technique from robust statistics, can limit the
maximum changes in the output concerning the input, taking into account
associated irrelevant biased factors. It is an efficient and provable method
for examining the output stability of machine learning models without incurring
additional computation costs. Recently, Graph Neural Networks (GNNs), which
operate on non-Euclidean data, have gained significant attention. However, no
previous research has investigated the GNN Lipschitz bounds to shed light on
stabilizing model outputs, especially when working on non-Euclidean data with
inherent biases. Given the inherent biases in common graph data used for GNN
training, it poses a serious challenge to constraining the GNN output
perturbations induced by input biases, thereby safeguarding fairness during
training. Recently, despite the Lipschitz constant's use in controlling the
stability of Euclideanneural networks, the calculation of the precise Lipschitz
constant remains elusive for non-Euclidean neural networks like GNNs,
especially within fairness contexts. To narrow this gap, we begin with the
general GNNs operating on an attributed graph, and formulate a Lipschitz bound
to limit the changes in the output regarding biases associated with the input.
Additionally, we theoretically analyze how the Lipschitz constant of a GNN
model could constrain the output perturbations induced by biases learned from
data for fairness training. We experimentally validate the Lipschitz bound's
effectiveness in limiting biases of the model output. Finally, from a training
dynamics perspective, we demonstrate why the theoretical Lipschitz bound can
effectively guide the GNN training to better trade-off between accuracy and
fairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yaning Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chunhui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chuxu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction. (arXiv:2309.03386v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03386</id>
        <link href="http://arxiv.org/abs/2309.03386"/>
        <updated>2023-09-09T00:40:35.386Z</updated>
        <summary type="html"><![CDATA[Positive-Unlabeled (PU) Learning is a challenge presented by binary
classification problems where there is an abundance of unlabeled data along
with a small number of positive data instances, which can be used to address
chronic disease screening problem. State-of-the-art PU learning methods have
resulted in the development of various risk estimators, yet they neglect the
differences among distinct populations. To address this issue, we present a
novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed
to take into account communities such as different age or income brackets, in
tasks of chronic disease prediction. We propose a novel approach for binary
decision-making, which hierarchically builds community-based PU models and then
aggregates their deliverables. Our method can explicate each PU model on the
tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery
data augmentation strategy enables sufficient training of the model in
individual communities. Additionally, the proposed approach includes an
adversarial PU risk estimator to capture hierarchical PU-relationships, and a
model fusion network that integrates data from each tree path, resulting in
robust binary classification results. We demonstrate the superior performance
of PUtree as well as its variants on two benchmarks and a new
diabetes-prediction dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xurui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yangyang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction. (arXiv:2309.03619v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.03619</id>
        <link href="http://arxiv.org/abs/2309.03619"/>
        <updated>2023-09-09T00:40:35.385Z</updated>
        <summary type="html"><![CDATA[The choice of the objective function is crucial in emerging high-quality
representations from self-supervised learning. This paper investigates how
different formulations of the Barlow Twins (BT) objective impact downstream
task performance for speech data. We propose Modified Barlow Twins (MBT) with
normalized latents to enforce scale-invariance and evaluate on speaker
identification, gender recognition and keyword spotting tasks. Our results show
MBT improves representation generalization over original BT, especially when
fine-tuning with limited target data. This highlights the importance of
designing objectives that encourage invariant and transferable representations.
Our analysis provides insights into how the BT learning objective can be
tailored to produce speech representations that excel when adapted to new
downstream tasks. This study is an important step towards developing reusable
self-supervised speech representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brima_Y/0/1/0/all/0/1"&gt;Yusuf Brima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krumnack_U/0/1/0/all/0/1"&gt;Ulf Krumnack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pika_S/0/1/0/all/0/1"&gt;Simone Pika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidemann_G/0/1/0/all/0/1"&gt;Gunther Heidemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03720</id>
        <link href="http://arxiv.org/abs/2309.03720"/>
        <updated>2023-09-09T00:40:35.385Z</updated>
        <summary type="html"><![CDATA[Forecasting natural gas consumption, considering seasonality and trends, is
crucial in planning its supply and consumption and optimizing the cost of
obtaining it, mainly by industrial entities. However, in times of threats to
its supply, it is also a critical element that guarantees the supply of this
raw material to meet individual consumers' needs, ensuring society's energy
security. This article introduces a novel multistep ahead forecasting of
natural gas consumption with change point detection integration for model
collection selection with continual learning capabilities using data stream
processing. The performance of the forecasting models based on the proposed
approach is evaluated in a complex real-world use case of natural gas
consumption forecasting. We employed Hoeffding tree predictors as forecasting
models and the Pruned Exact Linear Time (PELT) algorithm for the change point
detection procedure. The change point detection integration enables selecting a
different model collection for successive time frames. Thus, three model
collection selection procedures (with and without an error feedback loop) are
defined and evaluated for forecasting scenarios with various densities of
detected change points. These models were compared with change point agnostic
baseline approaches. Our experiments show that fewer change points result in a
lower forecasting error regardless of the model collection selection procedure
employed. Also, simpler model collection selection procedures omitting
forecasting error feedback leads to more robust forecasting models suitable for
continual learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Svoboda_R/0/1/0/all/0/1"&gt;Radek Svoboda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basterrech_S/0/1/0/all/0/1"&gt;Sebastian Basterrech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kozal_J/0/1/0/all/0/1"&gt;J&amp;#x119;drzej Kozal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Platos_J/0/1/0/all/0/1"&gt;Jan Plato&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wozniak_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Wo&amp;#x17a;niak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03409</id>
        <link href="http://arxiv.org/abs/2309.03409"/>
        <updated>2023-09-09T00:40:35.382Z</updated>
        <summary type="html"><![CDATA[Optimization is ubiquitous. While derivative-based algorithms have been
powerful tools for various problems, the absence of gradient imposes challenges
on many real-world applications. In this work, we propose Optimization by
PROmpting (OPRO), a simple and effective approach to leverage large language
models (LLMs) as optimizers, where the optimization task is described in
natural language. In each optimization step, the LLM generates new solutions
from the prompt that contains previously generated solutions with their values,
then the new solutions are evaluated and added to the prompt for the next
optimization step. We first showcase OPRO on linear regression and traveling
salesman problems, then move on to prompt optimization where the goal is to
find instructions that maximize the task accuracy. With a variety of LLMs, we
demonstrate that the best prompts optimized by OPRO outperform human-designed
prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chengrun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yifeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Denny Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-inferiority of Deep Learning Acute Ischemic Stroke Segmentation on Non-Contrast CT Compared to Expert Neuroradiologists. (arXiv:2211.15341v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.15341</id>
        <link href="http://arxiv.org/abs/2211.15341"/>
        <updated>2023-09-09T00:40:35.381Z</updated>
        <summary type="html"><![CDATA[To determine if a convolutional neural network (CNN) deep learning model can
accurately segment acute ischemic changes on non-contrast CT compared to
neuroradiologists. Non-contrast CT (NCCT) examinations from 232 acute ischemic
stroke patients who were enrolled in the DEFUSE 3 trial were included in this
study. Three experienced neuroradiologists independently segmented hypodensity
that reflected the ischemic core on each scan. The neuroradiologist with the
most experience (expert A) served as the ground truth for deep learning model
training. Two additional neuroradiologists (experts B and C) segmentations were
used for data testing. The 232 studies were randomly split into training and
test sets. The training set was further randomly divided into 5 folds with
training and validation sets. A 3-dimensional CNN architecture was trained and
optimized to predict the segmentations of expert A from NCCT. The performance
of the model was assessed using a set of volume, overlap, and distance metrics
using non-inferiority thresholds of 20%, 3ml, and 3mm. The optimized model
trained on expert A was compared to test experts B and C. We used a one-sided
Wilcoxon signed-rank test to test for the non-inferiority of the model-expert
compared to the inter-expert agreement. The final model performance for the
ischemic core segmentation task reached a performance of 0.46+-0.09 Surface
Dice at Tolerance 5mm and 0.47+-0.13 Dice when trained on expert A. Compared to
the two test neuroradiologists the model-expert agreement was non-inferior to
the inter-expert agreement, p < 0.05. The CNN accurately delineates the
hypodense ischemic core on NCCT in acute ischemic stroke patients with an
accuracy comparable to neuroradiologists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ostmeier_S/0/1/0/all/0/1"&gt;Sophie Ostmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Axelrod_B/0/1/0/all/0/1"&gt;Brian Axelrod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Verhaaren_B/0/1/0/all/0/1"&gt;Benjamin F.J. Verhaaren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Christensen_S/0/1/0/all/0/1"&gt;Soren Christensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mahammedi_A/0/1/0/all/0/1"&gt;Abdelkader Mahammedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongkai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pulli_B/0/1/0/all/0/1"&gt;Benjamin Pulli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1"&gt;Li-Jia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaharchuk_G/0/1/0/all/0/1"&gt;Greg Zaharchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heit_J/0/1/0/all/0/1"&gt;Jeremy J. Heit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-domain Sound Recognition for Efficient Underwater Data Analysis. (arXiv:2309.03451v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.03451</id>
        <link href="http://arxiv.org/abs/2309.03451"/>
        <updated>2023-09-09T00:40:35.377Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel deep learning approach for analyzing massive
underwater acoustic data by leveraging a model trained on a broad spectrum of
non-underwater (aerial) sounds. Recognizing the challenge in labeling vast
amounts of underwater data, we propose a two-fold methodology to accelerate
this labor-intensive procedure.

The first part of our approach involves PCA and UMAP visualization of the
underwater data using the feature vectors of an aerial sound recognition model.
This enables us to cluster the data in a two dimensional space and listen to
points within these clusters to understand their defining characteristics. This
innovative method simplifies the process of selecting candidate labels for
further training.

In the second part, we train a neural network model using both the selected
underwater data and the non-underwater dataset. We conducted a quantitative
analysis to measure the precision, recall, and F1 score of our model for
recognizing airgun sounds, a common type of underwater sound. The F1 score
achieved by our model exceeded 84.3%, demonstrating the effectiveness of our
approach in analyzing underwater acoustic data.

The methodology presented in this paper holds significant potential to reduce
the amount of labor required in underwater data analysis and opens up new
possibilities for further research in the field of cross-domain data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jeongsoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dong-Gyun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+La_H/0/1/0/all/0/1"&gt;Hyoung Sul La&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangmin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yoonchang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eun-Jin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chat Failures and Troubles: Reasons and Solutions. (arXiv:2309.03708v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.03708</id>
        <link href="http://arxiv.org/abs/2309.03708"/>
        <updated>2023-09-09T00:40:35.377Z</updated>
        <summary type="html"><![CDATA[This paper examines some common problems in Human-Robot Interaction (HRI)
causing failures and troubles in Chat. A given use case's design decisions
start with the suitable robot, the suitable chatting model, identifying common
problems that cause failures, identifying potential solutions, and planning
continuous improvement. In conclusion, it is recommended to use a closed-loop
control algorithm that guides the use of trained Artificial Intelligence (AI)
pre-trained models and provides vocabulary filtering, re-train batched models
on new datasets, learn online from data streams, and/or use reinforcement
learning models to self-update the trained models and reduce errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Helal_M/0/1/0/all/0/1"&gt;Manal Helal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holthaus_P/0/1/0/all/0/1"&gt;Patrick Holthaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakatos_G/0/1/0/all/0/1"&gt;Gabriella Lakatos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirabdollahian_F/0/1/0/all/0/1"&gt;Farshid Amirabdollahian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reduced Simulations for High-Energy Physics, a Middle Ground for Data-Driven Physics Research. (arXiv:2309.03780v1 [hep-ex])]]></title>
        <id>http://arxiv.org/abs/2309.03780</id>
        <link href="http://arxiv.org/abs/2309.03780"/>
        <updated>2023-09-09T00:40:35.375Z</updated>
        <summary type="html"><![CDATA[Subatomic particle track reconstruction (tracking) is a vital task in
High-Energy Physics experiments. Tracking is exceptionally computationally
challenging and fielded solutions, relying on traditional algorithms, do not
scale linearly. Machine Learning (ML) assisted solutions are a promising
answer. We argue that a complexity-reduced problem description and the data
representing it, will facilitate the solution exploration workflow. We provide
the REDuced VIrtual Detector (REDVID) as a complexity-reduced detector model
and particle collision event simulator combo. REDVID is intended as a
simulation-in-the-loop, to both generate synthetic data efficiently and to
simplify the challenge of ML model design. The fully parametric nature of our
tool, with regards to system-level configuration, while in contrast to
physics-accurate simulations, allows for the generation of simplified data for
research and education, at different levels. Resulting from the reduced
complexity, we showcase the computational efficiency of REDVID by providing the
computational cost figures for a multitude of simulation benchmarks. As a
simulation and a generative tool for ML-assisted solution design, REDVID is
highly flexible, reusable and open-source. Reference data sets generated with
REDVID are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ex/1/au:+Odyurt_U/0/1/0/all/0/1"&gt;Uraz Odyurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Swatman_S/0/1/0/all/0/1"&gt;Stephen Nicholas Swatman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Varbanescu_A/0/1/0/all/0/1"&gt;Ana-Lucia Varbanescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Caron_S/0/1/0/all/0/1"&gt;Sascha Caron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved theoretical guarantee for rank aggregation via spectral method. (arXiv:2309.03808v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03808</id>
        <link href="http://arxiv.org/abs/2309.03808"/>
        <updated>2023-09-09T00:40:35.370Z</updated>
        <summary type="html"><![CDATA[Given pairwise comparisons between multiple items, how to rank them so that
the ranking matches the observations? This problem, known as rank aggregation,
has found many applications in sports, recommendation systems, and other web
applications. As it is generally NP-hard to find a global ranking that
minimizes the mismatch (known as the Kemeny optimization), we focus on the
Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each
pairwise comparison is a corrupted copy of the true score difference. We
investigate spectral ranking algorithms that are based on unnormalized and
normalized data matrices. The key is to understand their performance in
recovering the underlying scores of each item from the observed data. This
reduces to deriving an entry-wise perturbation error bound between the top
eigenvectors of the unnormalized/normalized data matrix and its population
counterpart. By using the leave-one-out technique, we provide a sharper
$\ell_{\infty}$-norm perturbation bound of the eigenvectors and also derive an
error bound on the maximum displacement for each item, with only $\Omega(n\log
n)$ samples. Our theoretical analysis improves upon the state-of-the-art
results in terms of sample complexity, and our numerical experiments confirm
these theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Ziliang Samuel Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ling_S/0/1/0/all/0/1"&gt;Shuyang Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03831</id>
        <link href="http://arxiv.org/abs/2309.03831"/>
        <updated>2023-09-09T00:40:35.370Z</updated>
        <summary type="html"><![CDATA[Drift in machine learning refers to the phenomenon where the statistical
properties of data or context, in which the model operates, change over time
leading to a decrease in its performance. Therefore, maintaining a constant
monitoring process for machine learning model performance is crucial in order
to proactively prevent any potential performance regression. However,
supervised drift detection methods require human annotation and consequently
lead to a longer time to detect and mitigate the drift. In our proposed
unsupervised drift detection method, we follow a two step process. Our first
step involves encoding a sample of production data as the target distribution,
and the model training data as the reference distribution. In the second step,
we employ a kernel-based statistical test that utilizes the maximum mean
discrepancy (MMD) distance metric to compare the reference and target
distributions and estimate any potential drift. Our method also identifies the
subset of production data that is the root cause of the drift. The models
retrained using these identified high drift samples show improved performance
on online customer experience quality metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1"&gt;Saeed Khaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aditya_A/0/1/0/all/0/1"&gt;Akhouri Abhinav Aditya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1"&gt;Zohar Karnin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_O/0/1/0/all/0/1"&gt;Olivia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrashekar_S/0/1/0/all/0/1"&gt;Samarth Marudheri Chandrashekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03468</id>
        <link href="http://arxiv.org/abs/2309.03468"/>
        <updated>2023-09-09T00:40:35.356Z</updated>
        <summary type="html"><![CDATA[Current machine learning methods struggle to solve Bongard problems, which
are a type of IQ test that requires deriving an abstract "concept" from a set
of positive and negative "support" images, and then classifying whether or not
a new query image depicts the key concept. On Bongard-HOI, a benchmark for
natural-image Bongard problems, existing methods have only reached 66% accuracy
(where chance is 50%). Low accuracy is often attributed to neural nets' lack of
ability to find human-like symbolic rules. In this work, we point out that many
existing methods are forfeiting accuracy due to a much simpler problem: they do
not incorporate information contained in the support set as a whole, and rely
instead on information extracted from individual supports. This is a critical
issue, because unlike in few-shot learning tasks concerning object
classification, the "key concept" in a typical Bongard problem can only be
distinguished using multiple positives and multiple negatives. We explore a
variety of simple methods to take this cross-image context into account, and
demonstrate substantial gains over prior methods, leading to new
state-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%)
and strong performance on the original Bongard problem set (60.84%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghuraman_N/0/1/0/all/0/1"&gt;Nikhil Raghuraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1"&gt;Adam W. Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions. (arXiv:2309.03730v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03730</id>
        <link href="http://arxiv.org/abs/2309.03730"/>
        <updated>2023-09-09T00:40:35.355Z</updated>
        <summary type="html"><![CDATA[In lending, where prices are specific to both customers and products, having
a well-functioning personalized pricing policy in place is essential to
effective business making. Typically, such a policy must be derived from
observational data, which introduces several challenges. While the problem of
``endogeneity'' is prominently studied in the established pricing literature,
the problem of selection bias (or, more precisely, bid selection bias) is not.
We take a step towards understanding the effects of selection bias by posing
pricing as a problem of causal inference. Specifically, we consider the
reaction of a customer to price a treatment effect. In our experiments, we
simulate varying levels of selection bias on a semi-synthetic dataset on
mortgage loan applications in Belgium. We investigate the potential of
parametric and nonparametric methods for the identification of individual
bid-response functions. Our results illustrate how conventional methods such as
logistic regression and neural networks suffer adversely from selection bias.
In contrast, we implement state-of-the-art methods from causal machine learning
and show their capability to overcome selection bias in pricing data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bockel_Rickermann_C/0/1/0/all/0/1"&gt;Christopher Bockel-Rickermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verboven_S/0/1/0/all/0/1"&gt;Sam Verboven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verdonck_T/0/1/0/all/0/1"&gt;Tim Verdonck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verbeke_W/0/1/0/all/0/1"&gt;Wouter Verbeke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation. (arXiv:2309.03447v1 [physics.geo-ph])]]></title>
        <id>http://arxiv.org/abs/2309.03447</id>
        <link href="http://arxiv.org/abs/2309.03447"/>
        <updated>2023-09-09T00:40:35.346Z</updated>
        <summary type="html"><![CDATA[We present a data-driven model for ground-motion synthesis using a Generative
Adversarial Neural Operator (GANO) that combines recent advancements in machine
learning and open access strong motion data sets to generate three-component
acceleration time histories conditioned on moment magnitude ($M$), rupture
distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$
($V_{S30}$), and tectonic environment or style of faulting. We use Neural
Operators, a resolution invariant architecture that guarantees that the model
training is independent of the data sampling frequency. We first present the
conditional ground-motion synthesis algorithm (referred to heretofore as
cGM-GANO) and discuss its advantages compared to previous work. Next, we verify
the cGM-GANO framework using simulated ground motions generated with the
Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We
lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the
framework can recover the magnitude, distance, and $V_{S30}$ scaling of Fourier
amplitude and pseudo-spectral accelerations. We evaluate cGM-GANO through
residual analysis with the empirical dataset as well as by comparison with
conventional Ground Motion Models (GMMs) for selected ground motion scenarios.
Results show that cGM-GANO produces consistent median scaling with the GMMs for
the corresponding tectonic environments. The largest misfit is observed at
short distances due to the scarcity of training data. With the exception of
short distances, the aleatory variability of the response spectral ordinates is
also well captured, especially for subduction events due to the adequacy of
training data. Applications of the presented framework include generation of
risk-targeted ground motions for site-specific engineering applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yaozhong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lavrentiadis_G/0/1/0/all/0/1"&gt;Grigorios Lavrentiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Asimaki_D/0/1/0/all/0/1"&gt;Domniki Asimaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ross_Z/0/1/0/all/0/1"&gt;Zachary E. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filtration Surfaces for Dynamic Graph Classification. (arXiv:2309.03616v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03616</id>
        <link href="http://arxiv.org/abs/2309.03616"/>
        <updated>2023-09-09T00:40:35.344Z</updated>
        <summary type="html"><![CDATA[Existing approaches for classifying dynamic graphs either lift graph kernels
to the temporal domain, or use graph neural networks (GNNs). However, current
baselines have scalability issues, cannot handle a changing node set, or do not
take edge weight information into account. We propose filtration surfaces, a
novel method that is scalable and flexible, to alleviate said restrictions. We
experimentally validate the efficacy of our model and show that filtration
surfaces outperform previous state-of-the-art baselines on datasets that rely
on edge weight information. Our method does so while being either completely
parameter-free or having at most one parameter, and yielding the lowest overall
standard deviation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srambical_F/0/1/0/all/0/1"&gt;Franz Srambical&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing properties of distributions in the streaming model. (arXiv:2309.03245v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2309.03245</id>
        <link href="http://arxiv.org/abs/2309.03245"/>
        <updated>2023-09-09T00:40:35.337Z</updated>
        <summary type="html"><![CDATA[We study distribution testing in the standard access model and the
conditional access model when the memory available to the testing algorithm is
bounded. In both scenarios, the samples appear in an online fashion and the
goal is to test the properties of distribution using an optimal number of
samples subject to a memory constraint on how many samples can be stored at a
given time. First, we provide a trade-off between the sample complexity and the
space complexity for testing identity when the samples are drawn according to
the conditional access oracle. We then show that we can learn a succinct
representation of a monotone distribution efficiently with a memory constraint
on the number of samples that are stored that is almost optimal. We also show
that the algorithm for monotone distributions can be extended to a larger class
of decomposable distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Sampriti Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasudev_Y/0/1/0/all/0/1"&gt;Yadu Vasudev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes. (arXiv:2309.03531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03531</id>
        <link href="http://arxiv.org/abs/2309.03531"/>
        <updated>2023-09-09T00:40:35.331Z</updated>
        <summary type="html"><![CDATA[This work proposes a robust Partial Domain Adaptation (PDA) framework that
mitigates the negative transfer problem by incorporating a robust
target-supervision strategy. It leverages ensemble learning and includes
diverse, complementary label feedback, alleviating the effect of incorrect
feedback and promoting pseudo-label refinement. Rather than relying exclusively
on first-order moments for distribution alignment, our approach offers explicit
objectives to optimize intra-class compactness and inter-class separation with
the inferred source prototypes and highly-confident target samples in a
domain-invariant fashion. Notably, we ensure source data privacy by eliminating
the need to access the source data during the adaptation phase through a priori
inference of source prototypes. We conducted a series of comprehensive
experiments, including an ablation analysis, covering a range of partial domain
adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate
our framework's enhanced robustness and generalization, demonstrating its
superiority over existing state-of-the-art PDA approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choudhuri_S/0/1/0/all/0/1"&gt;Sandipan Choudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeniye_S/0/1/0/all/0/1"&gt;Suli Adeniye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1"&gt;Arunabha Sen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Graphs in Practice: Characterizing their Users, Challenges, and Visualization Opportunities. (arXiv:2304.01311v3 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.01311</id>
        <link href="http://arxiv.org/abs/2304.01311"/>
        <updated>2023-09-09T00:40:35.315Z</updated>
        <summary type="html"><![CDATA[This study presents insights from interviews with nineteen Knowledge Graph
(KG) practitioners who work in both enterprise and academic settings on a wide
variety of use cases. Through this study, we identify critical challenges
experienced by KG practitioners when creating, exploring, and analyzing KGs
that could be alleviated through visualization design. Our findings reveal
three major personas among KG practitioners - KG Builders, Analysts, and
Consumers - each of whom have their own distinct expertise and needs. We
discover that KG Builders would benefit from schema enforcers, while KG
Analysts need customizable query builders that provide interim query results.
For KG Consumers, we identify a lack of efficacy for node-link diagrams, and
the need for tailored domain-specific visualizations to promote KG adoption and
comprehension. Lastly, we find that implementing KGs effectively in practice
requires both technical and social solutions that are not addressed with
current tools, technologies, and collaborative workflows. From the analysis of
our interviews, we distill several visualization research directions to improve
KG usability, including knowledge cards that balance digestibility and
discoverability, timeline views to track temporal changes, interfaces that
support organic discovery, and semantic explanations for AI and machine
learning predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Harry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Appleby_G/0/1/0/all/0/1"&gt;Gabriel Appleby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brumar_C/0/1/0/all/0/1"&gt;Camelia Daniela Brumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1"&gt;Remco Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_A/0/1/0/all/0/1"&gt;Ashley Suh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LDMRes-Net: Enabling Efficient Medical Image Segmentation on IoT and Edge Platforms. (arXiv:2306.06145v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.06145</id>
        <link href="http://arxiv.org/abs/2306.06145"/>
        <updated>2023-09-09T00:40:35.312Z</updated>
        <summary type="html"><![CDATA[In this study, we propose LDMRes-Net, a lightweight dual-multiscale residual
block-based computational neural network tailored for medical image
segmentation on IoT and edge platforms. Conventional U-Net-based models face
challenges in meeting the speed and efficiency demands of real-time clinical
applications, such as disease monitoring, radiation therapy, and image-guided
surgery. LDMRes-Net overcomes these limitations with its remarkably low number
of learnable parameters (0.072M), making it highly suitable for
resource-constrained devices. The model's key innovation lies in its dual
multi-residual block architecture, which enables the extraction of refined
features on multiple scales, enhancing overall segmentation performance. To
further optimize efficiency, the number of filters is carefully selected to
prevent overlap, reduce training time, and improve computational efficiency.
The study includes comprehensive evaluations, focusing on segmentation of the
retinal image of vessels and hard exudates crucial for the diagnosis and
treatment of ophthalmology. The results demonstrate the robustness,
generalizability, and high segmentation accuracy of LDMRes-Net, positioning it
as an efficient tool for accurate and rapid medical image segmentation in
diverse clinical applications, particularly on IoT and edge platforms. Such
advances hold significant promise for improving healthcare outcomes and
enabling real-time medical image analysis in resource-limited settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Iqbal_S/0/1/0/all/0/1"&gt;Shahzaib Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tariq M. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Naqvi_S/0/1/0/all/0/1"&gt;Syed S. Naqvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Razzak_I/0/1/0/all/0/1"&gt;Imran Razzak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs. (arXiv:2309.03530v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03530</id>
        <link href="http://arxiv.org/abs/2309.03530"/>
        <updated>2023-09-09T00:40:35.311Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel approach for detecting objects using mobile
robots in the context of the RoboCup Standard Platform League, with a primary
focus on detecting the ball. The challenge lies in detecting a dynamic object
in varying lighting conditions and blurred images caused by fast movements. To
address this challenge, the paper presents a convolutional neural network
architecture designed specifically for computationally constrained robotic
platforms. The proposed CNN is trained to achieve high precision classification
of single objects in image patches and to determine their precise spatial
positions. The paper further integrates Early Exits into the existing
high-precision CNN architecture to reduce the computational cost of easily
rejectable cases in the background class. The training process involves a
composite loss function based on confidence and positional losses with dynamic
weighting and data augmentation. The proposed approach achieves a precision of
100% on the validation dataset and a recall of almost 87%, while maintaining an
execution time of around 170 $\mu$s per hypotheses. By combining the proposed
approach with an Early Exit, a runtime optimization of more than 28%, on
average, can be achieved compared to the original CNN. Overall, this paper
provides an efficient solution for an enhanced detection of objects, especially
the ball, in computationally constrained robotic platforms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moos_A/0/1/0/all/0/1"&gt;Arne Moos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source Camera Identification and Detection in Digital Videos through Blind Forensics. (arXiv:2309.03353v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03353</id>
        <link href="http://arxiv.org/abs/2309.03353"/>
        <updated>2023-09-09T00:40:35.306Z</updated>
        <summary type="html"><![CDATA[Source camera identification in digital videos is the problem of associating
an unknown digital video with its source device, within a closed set of
possible devices. The existing techniques in source detection of digital videos
try to find a fingerprint of the actual source in the video in form of PRNU
(Photo Response Non--Uniformity), and match it against the SPN (Sensor Pattern
Noise) of each possible device. The highest correlation indicates the correct
source. We investigate the problem of identifying a video source through a
feature based approach using machine learning. In this paper, we present a
blind forensic technique of video source authentication and identification,
based on feature extraction, feature selection and subsequent source
classification. The main aim is to determine whether a claimed source for a
video is actually its original source. If not, we identify its original source.
Our experimental results prove the efficiency of the proposed method compared
to traditional fingerprint based technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sameer_V/0/1/0/all/0/1"&gt;Venkata Udaya Sameer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1"&gt;Shilpa Mukhopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naskar_R/0/1/0/all/0/1"&gt;Ruchira Naskar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dali_I/0/1/0/all/0/1"&gt;Ishaan Dali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy. (arXiv:2309.03437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03437</id>
        <link href="http://arxiv.org/abs/2309.03437"/>
        <updated>2023-09-09T00:40:35.301Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is designed to preserve data privacy during model
training, where the data remains on the client side (i.e., IoT devices), and
only model updates of clients are shared iteratively for collaborative
learning. However, this process is vulnerable to privacy attacks and Byzantine
attacks: the local model updates shared throughout the FL network will leak
private information about the local training data, and they can also be
maliciously crafted by Byzantine attackers to disturb the learning. In this
paper, we propose a new FL scheme that guarantees rigorous privacy and
simultaneously enhances system robustness against Byzantine attacks. Our
approach introduces sparsification- and momentum-driven variance reduction into
the client-level differential privacy (DP) mechanism, to defend against
Byzantine attackers. The security design does not violate the privacy guarantee
of the client-level DP mechanism; hence, our approach achieves the same
client-level DP guarantee as the state-of-the-art. We conduct extensive
experiments on both IID and non-IID datasets and different tasks and evaluate
the performance of our approach against different Byzantine attacks by
comparing it with state-of-the-art defense methods. The results of our
experiments show the efficacy of our framework and demonstrate its ability to
improve system robustness against Byzantine attacks while achieving a strong
privacy guarantee.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zikai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rui Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Bioinformatics Analysis via AutoBA. (arXiv:2309.03242v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2309.03242</id>
        <link href="http://arxiv.org/abs/2309.03242"/>
        <updated>2023-09-09T00:40:35.300Z</updated>
        <summary type="html"><![CDATA[With the fast-growing and evolving omics data, the demand for streamlined and
adaptable tools to handle the analysis continues to grow. In response to this
need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI
agent based on a large language model designed explicitly for conventional
omics data analysis. AutoBA simplifies the analytical process by requiring
minimal user input while delivering detailed step-by-step plans for various
bioinformatics tasks. Through rigorous validation by expert bioinformaticians,
AutoBA's robustness and adaptability are affirmed across a diverse range of
omics analysis cases, including whole genome sequencing (WGS), RNA sequencing
(RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's
unique capacity to self-design analysis processes based on input data
variations further underscores its versatility. Compared with online
bioinformatic services, AutoBA deploys the analysis locally, preserving data
privacy. Moreover, different from the predefined pipeline, AutoBA has
adaptability in sync with emerging bioinformatics tools. Overall, AutoBA
represents a convenient tool, offering robustness and adaptability for complex
omics data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Juexiao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiuying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaopeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.03229</id>
        <link href="http://arxiv.org/abs/2309.03229"/>
        <updated>2023-09-09T00:40:35.282Z</updated>
        <summary type="html"><![CDATA[Any sports competition needs a timetable, specifying when and where teams
meet each other. The recent International Timetabling Competition (ITC2021) on
sports timetabling showed that, although it is possible to develop general
algorithms, the performance of each algorithm varies considerably over the
problem instances. This paper provides an instance space analysis for sports
timetabling, resulting in powerful insights into the strengths and weaknesses
of eight state-of-the-art algorithms. Based on machine learning techniques, we
propose an algorithm selection system that predicts which algorithm is likely
to perform best when given the characteristics of a sports timetabling problem
instance. Furthermore, we identify which characteristics are important in
making that prediction, providing insights in the performance of the
algorithms, and suggestions to further improve them. Finally, we assess the
empirical hardness of the instances. Our results are based on large
computational experiments involving about 50 years of CPU time on more than 500
newly generated problem instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bulck_D/0/1/0/all/0/1"&gt;David Van Bulck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goossens_D/0/1/0/all/0/1"&gt;Dries Goossens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clarner_J/0/1/0/all/0/1"&gt;Jan-Patrick Clarner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimitsas_A/0/1/0/all/0/1"&gt;Angelos Dimitsas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_G/0/1/0/all/0/1"&gt;George H. G. Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamas_Fernandez_C/0/1/0/all/0/1"&gt;Carlos Lamas-Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lester_M/0/1/0/all/0/1"&gt;Martin Mariusz Lester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersen_J/0/1/0/all/0/1"&gt;Jaap Pedersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_A/0/1/0/all/0/1"&gt;Antony E. Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosati_R/0/1/0/all/0/1"&gt;Roberto Maria Rosati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection. (arXiv:2309.03231v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.03231</id>
        <link href="http://arxiv.org/abs/2309.03231"/>
        <updated>2023-09-09T00:40:35.281Z</updated>
        <summary type="html"><![CDATA[Surveillance systems have emerged as crucial elements in upholding peace and
security in the modern world. Their ubiquity aids in monitoring suspicious
activities effectively. However, in densely populated environments, continuous
active monitoring becomes impractical, necessitating the development of
intelligent surveillance systems. AI integration in the surveillance domain was
a big revolution, however, speed issues have prevented its widespread
implementation in the field. It has been observed that quantum artificial
intelligence has led to a great breakthrough. Quantum artificial
intelligence-based surveillance systems have shown to be more accurate as well
as capable of performing well in real-time scenarios, which had never been seen
before. In this research, a RentinaNet model is integrated with Quantum CNN and
termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN,
Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative
integration positions it as a game-changer, addressing the challenges of active
monitoring in densely populated scenarios. As demand for efficient surveillance
solutions continues to grow, Quantum-RetinaNet offers a compelling alternative
to existing CNN models, upholding accuracy standards without sacrificing
real-time performance. The unique attributes of Quantum-RetinaNet have
far-reaching implications for the future of intelligent surveillance. With its
enhanced processing speed, it is poised to revolutionize the field, catering to
the pressing need for rapid yet precise monitoring. As Quantum-RetinaNet
becomes the new standard, it ensures public safety and security while pushing
the boundaries of AI in surveillance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Shah_S/0/1/0/all/0/1"&gt;Syed Atif Ali Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Algeelani_N/0/1/0/all/0/1"&gt;Nasir Algeelani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Al_Sammarraie_N/0/1/0/all/0/1"&gt;Najeeb Al-Sammarraie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relay Diffusion: Unifying diffusion process across resolutions for image synthesis. (arXiv:2309.03350v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03350</id>
        <link href="http://arxiv.org/abs/2309.03350"/>
        <updated>2023-09-09T00:40:35.280Z</updated>
        <summary type="html"><![CDATA[Diffusion models achieved great success in image synthesis, but still face
challenges in high-resolution generation. Through the lens of discrete cosine
transformation, we find the main reason is that \emph{the same noise level on a
higher resolution results in a higher Signal-to-Noise Ratio in the frequency
domain}. In this work, we present Relay Diffusion Model (RDM), which transfers
a low-resolution image or noise into an equivalent high-resolution one for
diffusion model via blurring diffusion and block noise. Therefore, the
diffusion process can continue seamlessly in any new resolution or model
without restarting from pure noise or low-resolution conditioning. RDM achieves
state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256,
surpassing previous works such as ADM, LDM and DiT by a large margin. All the
codes and checkpoints are open-sourced at
\url{https://github.com/THUDM/RelayDiffusion}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1"&gt;Jiayan Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wendi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1"&gt;Wenyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wangni_J/0/1/0/all/0/1"&gt;Jianqiao Wangni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data. (arXiv:2309.03439v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03439</id>
        <link href="http://arxiv.org/abs/2309.03439"/>
        <updated>2023-09-09T00:40:35.275Z</updated>
        <summary type="html"><![CDATA[We propose personalized Tucker decomposition (perTucker) to address the
limitations of traditional tensor decomposition methods in capturing
heterogeneity across different datasets. perTucker decomposes tensor data into
shared global components and personalized local components. We introduce a mode
orthogonality assumption and develop a proximal gradient regularized block
coordinate descent algorithm that is guaranteed to converge to a stationary
point. By learning unique and common representations across datasets, we
demonstrate perTucker's effectiveness in anomaly detection, client
classification, and clustering through a simulation study and two case studies
on solar flare detection and tonnage signal classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jiuyun Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1"&gt;Naichen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kontar_R/0/1/0/all/0/1"&gt;Raed Al Kontar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hao Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViewMix: Augmentation for Robust Representation in Self-Supervised Learning. (arXiv:2309.03360v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03360</id>
        <link href="http://arxiv.org/abs/2309.03360"/>
        <updated>2023-09-09T00:40:35.274Z</updated>
        <summary type="html"><![CDATA[Joint Embedding Architecture-based self-supervised learning methods have
attributed the composition of data augmentations as a crucial factor for their
strong representation learning capabilities. While regional dropout strategies
have proven to guide models to focus on lesser indicative parts of the objects
in supervised methods, it hasn't been adopted by self-supervised methods for
generating positive pairs. This is because the regional dropout methods are not
suitable for the input sampling process of the self-supervised methodology.
Whereas dropping informative pixels from the positive pairs can result in
inefficient training, replacing patches of a specific object with a different
one can steer the model from maximizing the agreement between different
positive pairs. Moreover, joint embedding representation learning methods have
not made robustness their primary training outcome. To this end, we propose the
ViewMix augmentation policy, specially designed for self-supervised learning,
upon generating different views of the same image, patches are cut and pasted
from one view to another. By leveraging the different views created by this
augmentation strategy, multiple joint embedding-based self-supervised
methodologies obtained better localization capability and consistently
outperformed their corresponding baseline methods. It is also demonstrated that
incorporating ViewMix augmentation policy promotes robustness of the
representations in the state-of-the-art methods. Furthermore, our
experimentation and analysis of compute times suggest that ViewMix augmentation
doesn't introduce any additional overhead compared to other counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Arjon Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xin Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03659</id>
        <link href="http://arxiv.org/abs/2309.03659"/>
        <updated>2023-09-09T00:40:35.273Z</updated>
        <summary type="html"><![CDATA[Knowledge Distillation (KD) is one proposed solution to large model sizes and
slow inference speed in semantic segmentation. In our research we identify 25
proposed distillation loss terms from 14 publications in the last 4 years.
Unfortunately, a comparison of terms based on published results is often
impossible, because of differences in training configurations. A good
illustration of this problem is the comparison of two publications from 2022.
Using the same models and dataset, Structural and Statistical Texture
Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final
performance of 29.19, while Adaptive Perspective Distillation (APD) only
improves student performance by 2.06 percentage points, but achieves a final
performance of 39.25. The reason for such extreme differences is often a
suboptimal choice of hyperparameters and a resulting underperformance of the
student model used as reference point. In our work, we reveal problems of
insufficient hyperparameter tuning by showing that distillation improvements of
two widely accepted frameworks, SKD and IFVD, vanish when hyperparameters are
optimized sufficiently. To improve comparability of future research in the
field, we establish a solid baseline for three datasets and two student models
and provide extensive information on hyperparameter tuning. We find that only
two out of eight techniques can compete with our simple baseline on the ADE20K
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niemann_O/0/1/0/all/0/1"&gt;Onno Niemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vox_C/0/1/0/all/0/1"&gt;Christopher Vox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werner_T/0/1/0/all/0/1"&gt;Thorben Werner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modality Guidance Network For Missing Modality Inference. (arXiv:2309.03452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03452</id>
        <link href="http://arxiv.org/abs/2309.03452"/>
        <updated>2023-09-09T00:40:35.264Z</updated>
        <summary type="html"><![CDATA[Multimodal models have gained significant success in recent years. Standard
multimodal approaches often assume unchanged modalities from training stage to
inference stage. In practice, however, many scenarios fail to satisfy such
assumptions with missing modalities during inference, leading to limitations on
where multimodal models can be applied. While existing methods mitigate the
problem through reconstructing the missing modalities, it increases unnecessary
computational cost, which could be just as critical, especially for large,
deployed systems. To solve the problem from both sides, we propose a novel
guidance network that promotes knowledge sharing during training, taking
advantage of the multimodal representations to train better single-modality
models for inference. Real-life experiment in violence detection shows that our
proposed framework trains single-modality models that significantly outperform
its traditionally trained counterparts while maintaining the same inference
cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhuokai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palani_H/0/1/0/all/0/1"&gt;Harish Palani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_L/0/1/0/all/0/1"&gt;Lena Evans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toner_R/0/1/0/all/0/1"&gt;Ruth Toner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robotic Table Tennis: A Case Study into a High Speed Learning System. (arXiv:2309.03315v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.03315</id>
        <link href="http://arxiv.org/abs/2309.03315"/>
        <updated>2023-09-09T00:40:35.259Z</updated>
        <summary type="html"><![CDATA[We present a deep-dive into a real-world robotic learning system that, in
previous work, was shown to be capable of hundreds of table tennis rallies with
a human and has the ability to precisely return the ball to desired targets.
This system puts together a highly optimized perception subsystem, a high-speed
low-latency robot controller, a simulation paradigm that can prevent damage in
the real world and also train policies for zero-shot transfer, and automated
real world environment resets that enable autonomous training and evaluation on
physical robots. We complement a complete system description, including
numerous design decisions that are typically not widely disseminated, with a
collection of studies that clarify the importance of mitigating various sources
of latency, accounting for training and deployment distribution shifts,
robustness of the perception system, sensitivity to policy hyper-parameters,
and choice of action space. A video demonstrating the components of the system
and details of experimental results can be found at
https://youtu.be/uFcnWjB42I0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmbrosio_D/0/1/0/all/0/1"&gt;David B. D&amp;#x27;Ambrosio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abelian_J/0/1/0/all/0/1"&gt;Jonathan Abelian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abeyruwan_S/0/1/0/all/0/1"&gt;Saminda Abeyruwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_M/0/1/0/all/0/1"&gt;Michael Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1"&gt;Alex Bewley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyd_J/0/1/0/all/0/1"&gt;Justin Boyd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cortes_O/0/1/0/all/0/1"&gt;Omar Cortes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coumans_E/0/1/0/all/0/1"&gt;Erwin Coumans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1"&gt;Tianli Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wenbo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graesser_L/0/1/0/all/0/1"&gt;Laura Graesser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1"&gt;Atil Iscen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1"&gt;Navdeep Jaitly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Deepali Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangaspunta_J/0/1/0/all/0/1"&gt;Juhana Kangaspunta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kataoka_S/0/1/0/all/0/1"&gt;Satoshi Kataoka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouretas_G/0/1/0/all/0/1"&gt;Gus Kouretas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1"&gt;Yuheng Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazic_N/0/1/0/all/0/1"&gt;Nevena Lazic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1"&gt;Corey Lynch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahjourian_R/0/1/0/all/0/1"&gt;Reza Mahjourian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_S/0/1/0/all/0/1"&gt;Sherry Q. Moore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thinh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oslund_K/0/1/0/all/0/1"&gt;Ken Oslund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reed_B/0/1/0/all/0/1"&gt;Barney J Reed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reymann_K/0/1/0/all/0/1"&gt;Krista Reymann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1"&gt;Pannag R. Sanketi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_A/0/1/0/all/0/1"&gt;Anish Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1"&gt;Pierre Sermanet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sindhwani_V/0/1/0/all/0/1"&gt;Vikas Sindhwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Avi Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1"&gt;Vincent Vanhoucke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vesom_G/0/1/0/all/0/1"&gt;Grace Vesom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.03224</id>
        <link href="http://arxiv.org/abs/2309.03224"/>
        <updated>2023-09-09T00:40:35.256Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) exhibit impressive language understanding and
in-context learning abilities including natural language processing (NLP) tasks
and challenging mathematical reasoning. However, due to the lack of
process-supervision, applying PLMs to mathematical reasoning tasks often fail
to generate correct reasoning steps and final answer even though solutions have
high probabilities. To unleash the mathematical reasoning of finetuned-LLMs
without any further fineutuning steps, we propose a method to endow LLMs with
immediate reaction and delicate reasoning system via Monte Carlo Tree
Search(MCTS) and a light energy function to rank the decision steps. In
particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy
Model~(Residual-EBM) and apply noise contrastive estimation to estimate the
parameters of energy function . Then we use MCTS with energy function as path
verifier to search the output space and evaluating the reasoning path. Through
extensive experiments on two mathematical reasoning benchmarks, namely GSM8k
and MATH, we reveal the extraordinary capabilities of our method that improve
the pass@1 of the finetuned-model without further finetuning or RLHF alignment
by a substantial margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haotian Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat. (arXiv:2309.03237v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03237</id>
        <link href="http://arxiv.org/abs/2309.03237"/>
        <updated>2023-09-09T00:40:35.255Z</updated>
        <summary type="html"><![CDATA[We carefully evaluate a number of algorithms for learning in a federated
environment, and test their utility for a variety of image classification
tasks. We consider many issues that have not been adequately considered before:
whether learning over data sets that do not have diverse sets of images affects
the results; whether to use a pre-trained feature extraction "backbone"; how to
evaluate learner performance (we argue that classification accuracy is not
enough), among others. Overall, across a wide variety of settings, we find that
vertically decomposing a neural network seems to give the best results, and
outperforms more standard reconciliation-used methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;Erdong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yuxin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1"&gt;Chris Jermaine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images. (arXiv:2309.03351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03351</id>
        <link href="http://arxiv.org/abs/2309.03351"/>
        <updated>2023-09-09T00:40:35.252Z</updated>
        <summary type="html"><![CDATA[The analysis of Synthetic Aperture Radar (SAR) imagery is an important step
in remote sensing applications, and it is a challenging problem due to its
inherent speckle noise. One typical solution is to model the data using the
$G_I^0$ distribution and extract its roughness information, which in turn can
be used in posterior imaging tasks, such as segmentation, classification and
interpretation. This leads to the need of quick and reliable estimation of the
roughness parameter from SAR data, especially with high resolution images.
Unfortunately, traditional parameter estimation procedures are slow and prone
to estimation failures. In this work, we proposed a neural network-based
estimation framework that first learns how to predict underlying parameters of
$G_I^0$ samples and then can be used to estimate the roughness of unseen data.
We show that this approach leads to an estimator that is quicker, yields less
estimation error and is less prone to failures than the traditional estimation
procedures for this problem, even when we use a simple network. More
importantly, we show that this same methodology can be generalized to handle
image inputs and, even if trained on purely synthetic data for a few seconds,
is able to perform real time pixel-wise roughness estimation for high
resolution real SAR imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Li Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1"&gt;Jeova Farias Sales Rocha Neto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic Semi-Supervised Approach with Triplet Markov Chains. (arXiv:2309.03707v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03707</id>
        <link href="http://arxiv.org/abs/2309.03707"/>
        <updated>2023-09-09T00:40:35.252Z</updated>
        <summary type="html"><![CDATA[Triplet Markov chains are general generative models for sequential data which
take into account three kinds of random variables: (noisy) observations, their
associated discrete labels and latent variables which aim at strengthening the
distribution of the observations and their associated labels. However, in
practice, we do not have at our disposal all the labels associated to the
observations to estimate the parameters of such models. In this paper, we
propose a general framework based on a variational Bayesian inference to train
parameterized triplet Markov chain models in a semi-supervised context. The
generality of our approach enables us to derive semi-supervised algorithms for
a variety of generative models for sequential Bayesian classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Morales_K/0/1/0/all/0/1"&gt;Katherine Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Petetin_Y/0/1/0/all/0/1"&gt;Yohan Petetin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03755</id>
        <link href="http://arxiv.org/abs/2309.03755"/>
        <updated>2023-09-09T00:40:35.252Z</updated>
        <summary type="html"><![CDATA[Synthetic Time Series Generation (TSG) is crucial in a range of applications,
including data augmentation, anomaly detection, and privacy preservation.
Although significant strides have been made in this field, existing methods
exhibit three key limitations: (1) They often benchmark against similar model
types, constraining a holistic view of performance capabilities. (2) The use of
specialized synthetic and private datasets introduces biases and hampers
generalizability. (3) Ambiguous evaluation measures, often tied to custom
networks or downstream tasks, hinder consistent and fair comparison.

To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural
TSG Benchmark, designed for a unified and comprehensive assessment of TSG
methods. It comprises three modules: (1) a curated collection of publicly
available, real-world datasets tailored for TSG, together with a standardized
preprocessing pipeline; (2) a comprehensive evaluation measures suite including
vanilla measures, new distance-based assessments, and visualization tools; (3)
a pioneering generalization test rooted in Domain Adaptation (DA), compatible
with all methods. We have conducted extensive experiments across ten real-world
datasets from diverse domains, utilizing ten advanced TSG methods and twelve
evaluation measures, all gauged through \textsf{TSGBench}. The results
highlight its remarkable efficacy and consistency. More importantly,
\textsf{TSGBench} delivers a statistical breakdown of method rankings,
illuminating performance variations across different datasets and measures, and
offering nuanced insights into the effectiveness of each method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ang_Y/0/1/0/all/0/1"&gt;Yihao Ang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yifan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_A/0/1/0/all/0/1"&gt;Anthony K. H. Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiyong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.03244</id>
        <link href="http://arxiv.org/abs/2309.03244"/>
        <updated>2023-09-09T00:40:35.251Z</updated>
        <summary type="html"><![CDATA[We introduce EGIC, a novel generative image compression method that allows
traversing the distortion-perception curve efficiently from a single model.
Specifically, we propose an implicitly encoded variant of image interpolation
that predicts the residual between a MSE-optimized and GAN-optimized decoder
output. On the receiver side, the user can then control the impact of the
residual on the GAN-based reconstruction. Together with improved GAN-based
building blocks, EGIC outperforms a wide-variety of perception-oriented and
distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while
performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to
implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and
provides excellent interpolation characteristics, which makes it a promising
candidate for practical applications targeting the low bit range.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Korber_N/0/1/0/all/0/1"&gt;Nikolai K&amp;#xf6;rber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kromer_E/0/1/0/all/0/1"&gt;Eduard Kromer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Siebert_A/0/1/0/all/0/1"&gt;Andreas Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hauke_S/0/1/0/all/0/1"&gt;Sascha Hauke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mueller_Gritschneder_D/0/1/0/all/0/1"&gt;Daniel Mueller-Gritschneder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoXHED2.0: Scalable boosting of dynamic survival analysis. (arXiv:2103.12591v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12591</id>
        <link href="http://arxiv.org/abs/2103.12591"/>
        <updated>2023-09-09T00:40:35.251Z</updated>
        <summary type="html"><![CDATA[Modern applications of survival analysis increasingly involve time-dependent
covariates. The Python package BoXHED2.0 is a tree-boosted hazard estimator
that is fully nonparametric, and is applicable to survival settings far more
general than right-censoring, including recurring events and competing risks.
BoXHED2.0 is also scalable to the point of being on the same order of speed as
parametric boosted survival models, in part because its core is written in C++
and it also supports the use of GPUs and multicore CPUs. BoXHED2.0 is available
from PyPI and also from www.github.com/BoXHED.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pakbin_A/0/1/0/all/0/1"&gt;Arash Pakbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaochen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1"&gt;Bobak J. Mortazavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donald K.K. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization for Mammographic Image Analysis with Contrastive Learning. (arXiv:2304.10226v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.10226</id>
        <link href="http://arxiv.org/abs/2304.10226"/>
        <updated>2023-09-09T00:40:35.251Z</updated>
        <summary type="html"><![CDATA[The deep learning technique has been shown to be effectively addressed
several image analysis tasks in the computer-aided diagnosis scheme for
mammography. The training of an efficacious deep learning model requires large
data with diverse styles and qualities. The diversity of data often comes from
the use of various scanners of vendors. But, in practice, it is impractical to
collect a sufficient amount of diverse data for training. To this end, a novel
contrastive learning is developed to equip the deep learning models with better
style generalization capability. Specifically, the multi-style and multi-view
unsupervised self-learning scheme is carried out to seek robust feature
embedding against style diversity as a pretrained model. Afterward, the
pretrained network is further fine-tuned to the downstream tasks, e.g., mass
detection, matching, BI-RADS rating, and breast density classification. The
proposed method has been evaluated extensively and rigorously with mammograms
from various vendor style domains and several public datasets. The experimental
results suggest that the proposed domain generalization method can effectively
improve performance of four mammographic image tasks on the data from both seen
and unseen domains, and outperform many state-of-the-art (SOTA) generalization
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheren Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhiming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1"&gt;Chenjin Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1"&gt;Xi Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yajia Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1"&gt;Dinggang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1"&gt;Jie-Zhi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences. (arXiv:2309.03791v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03791</id>
        <link href="http://arxiv.org/abs/2309.03791"/>
        <updated>2023-09-09T00:40:35.246Z</updated>
        <summary type="html"><![CDATA[We introduce the $ARMOR_D$ methods as novel approaches to enhancing the
adversarial robustness of deep learning models. These methods are based on a
new class of optimal-transport-regularized divergences, constructed via an
infimal convolution between an information divergence and an optimal-transport
(OT) cost. We use these as tools to enhance adversarial robustness by
maximizing the expected loss over a neighborhood of distributions, a technique
known as distributionally robust optimization. Viewed as a tool for
constructing adversarial samples, our method allows samples to be both
transported, according to the OT cost, and re-weighted, according to the
information divergence. We demonstrate the effectiveness of our method on
malware detection and image recognition applications and find that, to our
knowledge, it outperforms existing methods at enhancing the robustness against
adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$
against $FGSM$ and $98.18\%$ against $PGD^{40}$ on the MNIST dataset, reducing
the error rate by more than $19.7\%$ and $37.2\%$ respectively compared to
prior methods. Similarly, in malware detection, a discrete (binary) data
domain, $ARMOR_D$ improves the robustified accuracy under $rFGSM^{50}$ attack
compared to the previous best-performing adversarial training methods by
$37.0\%$ while lowering false negative and false positive rates by $51.1\%$ and
$57.53\%$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Birrell_J/0/1/0/all/0/1"&gt;Jeremiah Birrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1"&gt;Mohammadreza Ebrahimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Personalized Federated Learning via Heterogeneous Model Reassembly. (arXiv:2308.08643v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.08643</id>
        <link href="http://arxiv.org/abs/2308.08643"/>
        <updated>2023-09-09T00:40:35.246Z</updated>
        <summary type="html"><![CDATA[This paper focuses on addressing the practical yet challenging problem of
model heterogeneity in federated learning, where clients possess models with
different network structures. To track this problem, we propose a novel
framework called pFedHR, which leverages heterogeneous model reassembly to
achieve personalized federated learning. In particular, we approach the problem
of heterogeneous model personalization as a model-matching optimization task on
the server side. Moreover, pFedHR automatically and dynamically generates
informative and diverse personalized candidates with minimal human
intervention. Furthermore, our proposed heterogeneous model reassembly
technique mitigates the adverse impact introduced by using public data with
different distributions from the client data to a certain extent. Experimental
results demonstrate that pFedHR outperforms baselines on three datasets under
both IID and Non-IID settings. Additionally, pFedHR effectively reduces the
adverse impact of using different public data and dynamically generates diverse
personalized models in an automated manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Suhan Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_L/0/1/0/all/0/1"&gt;Liwei Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dongkuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fenglong Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning continuous-valued treatment effects through representation balancing. (arXiv:2309.03731v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03731</id>
        <link href="http://arxiv.org/abs/2309.03731"/>
        <updated>2023-09-09T00:40:35.244Z</updated>
        <summary type="html"><![CDATA[Estimating the effects of treatments with an associated dose on an instance's
outcome, the "dose response", is relevant in a variety of domains, from
healthcare to business, economics, and beyond. Such effects, also known as
continuous-valued treatment effects, are typically estimated from observational
data, which may be subject to dose selection bias. This means that the
allocation of doses depends on pre-treatment covariates. Previous studies have
shown that conventional machine learning approaches fail to learn accurate
individual estimates of dose responses under the presence of dose selection
bias. In this work, we propose CBRNet, a causal machine learning approach to
estimate an individual dose response from observational data. CBRNet adopts the
Neyman-Rubin potential outcome framework and extends the concept of balanced
representation learning for overcoming selection bias to continuous-valued
treatments. Our work is the first to apply representation balancing in a
continuous-valued treatment setting. We evaluate our method on a newly proposed
benchmark. Our experiments demonstrate CBRNet's ability to accurately learn
treatment effects under selection bias and competitive performance with respect
to other state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bockel_Rickermann_C/0/1/0/all/0/1"&gt;Christopher Bockel-Rickermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanderschueren_T/0/1/0/all/0/1"&gt;Toon Vanderschueren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrevoets_J/0/1/0/all/0/1"&gt;Jeroen Berrevoets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verdonck_T/0/1/0/all/0/1"&gt;Tim Verdonck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verbeke_W/0/1/0/all/0/1"&gt;Wouter Verbeke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10274</id>
        <link href="http://arxiv.org/abs/2010.10274"/>
        <updated>2023-09-09T00:40:35.243Z</updated>
        <summary type="html"><![CDATA[Graph convolution is a fundamental building block for many deep neural
networks on graph-structured data. In this paper, we introduce a simple, yet
very effective graph convolutional network with skip connections for
semi-supervised anomaly detection. The proposed layerwise propagation rule of
our model is theoretically motivated by the concept of implicit fairing in
geometry processing, and comprises a graph convolution module for aggregating
information from immediate node neighbors and a skip connection module for
combining layer-wise neighborhood representations. This propagation rule is
derived from the iterative solution of the implicit fairing equation via the
Jacobi method. In addition to capturing information from distant graph nodes
through skip connections between the network's layers, our approach exploits
both the graph structure and node features for learning discriminative node
representations. These skip connections are integrated by design in our
proposed network architecture. The effectiveness of our model is demonstrated
through extensive experiments on five benchmark datasets, achieving better or
comparable anomaly detection results against strong baseline methods. We also
demonstrate through an ablation study that skip connection helps improve the
model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mesgaran_M/0/1/0/all/0/1"&gt;Mahsa Mesgaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.04014</id>
        <link href="http://arxiv.org/abs/2308.04014"/>
        <updated>2023-09-09T00:40:35.242Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to restart the process over again once new data becomes available. A much
cheaper and more efficient solution would be to enable the continual
pre-training of these models, i.e. updating pre-trained models with new data
instead of re-training them from scratch. However, the distribution shift
induced by novel data typically results in degraded performance on past data.
Taking a step towards efficient continual pre-training, in this work, we
examine the effect of different warm-up strategies. Our hypothesis is that the
learning rate must be re-increased to improve compute efficiency when training
on a new dataset. We study the warmup phase of models pre-trained on the Pile
(upstream data, 300B tokens) as we continue to pre-train on SlimPajama
(downstream data, 297B tokens), following a linear warmup and cosine decay
schedule. We conduct all experiments on the Pythia 410M language model
architecture and evaluate performance through validation perplexity. We
experiment with different pre-training checkpoints, various maximum learning
rates, and various warmup lengths. Our results show that while rewarming models
first increases the loss on upstream and downstream data, in the longer run it
improves the downstream performance, outperforming models trained from
scratch$\unicode{x2013}$even for a large downstream dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1"&gt;Benjamin Th&amp;#xe9;rien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Adam Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1"&gt;Mats L. Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1"&gt;Quentin Anthony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1"&gt;Eugene Belilovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Lesort&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trinary Decision Trees for missing value handling. (arXiv:2309.03561v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03561</id>
        <link href="http://arxiv.org/abs/2309.03561"/>
        <updated>2023-09-09T00:40:35.241Z</updated>
        <summary type="html"><![CDATA[This paper introduces the Trinary decision tree, an algorithm designed to
improve the handling of missing data in decision tree regressors and
classifiers. Unlike other approaches, the Trinary decision tree does not assume
that missing values contain any information about the response. Both
theoretical calculations on estimator bias and numerical illustrations using
real data sets are presented to compare its performance with established
algorithms in different missing data scenarios (Missing Completely at Random
(MCAR), and Informative Missingness (IM)). Notably, the Trinary tree
outperforms its peers in MCAR settings, especially when data is only missing
out-of-sample, while lacking behind in IM settings. A hybrid model, the
TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes
(MIA) approach, shows robust performance in all types of missingness. Despite
the potential drawback of slower training speed, the Trinary tree offers a
promising and more accurate method of handling missing data in decision tree
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zakrisson_H/0/1/0/all/0/1"&gt;Henning Zakrisson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating quantum feature maps using multi-objective genetic algorithm. (arXiv:2309.03307v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2309.03307</id>
        <link href="http://arxiv.org/abs/2309.03307"/>
        <updated>2023-09-09T00:40:35.236Z</updated>
        <summary type="html"><![CDATA[We present a novel approach for efficiently generating quantum feature maps
for quantum-enhanced support vector machines, a kernel-based classifier,
enabling access to high-dimensional Hilbert space. Our method employs a
multi-objective genetic algorithm that simultaneously maximizes classification
accuracy while minimizing both the local and non-local gate costs of the
quantum feature map's circuit. To achieve this, we define distinct fitness
functions for local gates and entanglement gates. Comparisons with classical
classifiers are given in order to understand the advantages of using quantum
machine learning. Surprisingly, our experiments reveal that the optimal
configuration of quantum circuits for the quantum kernel method incorporates a
proportional number of non-local gates for entanglement, contrary to previous
literature where non-local gates were largely suppressed.

Furthermore, we demonstrate that the separability indexes of data can be
effectively leveraged to determine the number of non-local gates required for
the quantum support vector machine's feature maps. This insight can
significantly aid in selecting appropriate parameters, such as the entanglement
parameter, in various quantum programming packages like quiskit.org based on
data analysis. Our findings offer valuable guidance for enhancing the
efficiency and accuracy of quantum machine learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bayro_A/0/1/0/all/0/1"&gt;Allison Bayro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yamamoto_N/0/1/0/all/0/1"&gt;Nao Yamamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.02335</id>
        <link href="http://arxiv.org/abs/2308.02335"/>
        <updated>2023-09-09T00:40:35.235Z</updated>
        <summary type="html"><![CDATA[Graph classification is a crucial task in many real-world multimedia
applications, where graphs can represent various multimedia data types such as
images, videos, and social networks. Previous efforts have applied graph neural
networks (GNNs) in balanced situations where the class distribution is
balanced. However, real-world data typically exhibit long-tailed class
distributions, resulting in a bias towards the head classes when using GNNs and
limited generalization ability over the tail classes. Recent approaches mainly
focus on re-balancing different classes during model training, which fails to
explicitly introduce new knowledge and sacrifices the performance of the head
classes. To address these drawbacks, we propose a novel framework called
Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature
extractor and an unbiased classifier in a decoupled manner. In the feature
extractor training stage, we develop a graph retrieval module to search for
relevant graphs that directly enrich the intra-class diversity for the tail
classes. Moreover, we innovatively optimize a category-centered supervised
contrastive loss to obtain discriminative representations, which is more
suitable for long-tailed scenarios. In the classifier fine-tuning stage, we
balance the classifier weights with two weight regularization techniques, i.e.,
Max-norm and weight decay. Experiments on various popular benchmarks verify the
superiority of the proposed method against state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhengyang Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1"&gt;Wei Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yifang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models. (arXiv:2309.02976v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.02976</id>
        <link href="http://arxiv.org/abs/2309.02976"/>
        <updated>2023-09-09T00:40:35.229Z</updated>
        <summary type="html"><![CDATA[Humans excel at robust bipedal walking in complex natural environments. In
each step, they adequately tune the interaction of biomechanical muscle
dynamics and neuronal signals to be robust against uncertainties in ground
conditions. However, it is still not fully understood how the nervous system
resolves the musculoskeletal redundancy to solve the multi-objective control
problem considering stability, robustness, and energy efficiency. In computer
simulations, energy minimization has been shown to be a successful optimization
target, reproducing natural walking with trajectory optimization or
reflex-based control methods. However, these methods focus on particular
motions at a time and the resulting controllers are limited when compensating
for perturbations. In robotics, reinforcement learning~(RL) methods recently
achieved highly stable (and efficient) locomotion on quadruped systems, but the
generation of human-like walking with bipedal biomechanical models has required
extensive use of expert data sets. This strong reliance on demonstrations often
results in brittle policies and limits the application to new behaviors,
especially considering the potential variety of movements for high-dimensional
musculoskeletal models in 3D. Achieving natural locomotion with RL without
sacrificing its incredible robustness might pave the way for a novel approach
to studying human walking in complex natural environments. Videos:
https://sites.google.com/view/naturalwalkingrl]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schumacher_P/0/1/0/all/0/1"&gt;Pierre Schumacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geijtenbeek_T/0/1/0/all/0/1"&gt;Thomas Geijtenbeek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caggiano_V/0/1/0/all/0/1"&gt;Vittorio Caggiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vikash Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1"&gt;Syn Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1"&gt;Georg Martius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haeufle_D/0/1/0/all/0/1"&gt;Daniel F. B. Haeufle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.08754</id>
        <link href="http://arxiv.org/abs/2306.08754"/>
        <updated>2023-09-09T00:40:35.227Z</updated>
        <summary type="html"><![CDATA[Modern climate projections lack adequate spatial and temporal resolution due
to computational constraints. A consequence is inaccurate and imprecise
predictions of critical processes such as storms. Hybrid methods that combine
physics with machine learning (ML) have introduced a new generation of higher
fidelity climate simulators that can sidestep Moore's Law by outsourcing
compute-hungry, short, high-resolution simulations to ML emulators. However,
this hybrid ML-physics simulation approach requires domain-specific treatment
and has been inaccessible to ML experts because of lack of training data and
relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset
designed for hybrid ML-physics research. It comprises multi-scale climate
simulations, developed by a consortium of climate scientists and ML
researchers. It consists of 5.7 billion pairs of multivariate input and output
vectors that isolate the influence of locally-nested, high-resolution,
high-fidelity physics on a host climate simulator's macro-scale physical state.

The dataset is global in coverage, spans multiple years at high sampling
frequency, and is designed such that resulting emulators are compatible with
downstream coupling into operational climate simulators. We implement a range
of deterministic and stochastic regression baselines to highlight the ML
challenges and their scoring. The data
(https://huggingface.co/datasets/LEAP/ClimSim_high-res,
https://huggingface.co/datasets/LEAP/ClimSim_low-res, and
https://huggingface.co/datasets/LEAP/ClimSim_low-res_aqua-planet) and code
(https://leap-stc.github.io/ClimSim) are released openly to support the
development of hybrid ML-physics and high-fidelity climate simulations for the
benefit of science and society.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Sungduk Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannah_W/0/1/0/all/0/1"&gt;Walter M. Hannah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1"&gt;Liran Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jerry Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhouri_M/0/1/0/all/0/1"&gt;Mohamed Aziz Bhouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Ritwik Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Will_J/0/1/0/all/0/1"&gt;Justus C. Will&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behrens_G/0/1/0/all/0/1"&gt;Gunnar Behrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busecke_J/0/1/0/all/0/1"&gt;Julius J. M. Busecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loose_N/0/1/0/all/0/1"&gt;Nora Loose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stern_C/0/1/0/all/0/1"&gt;Charles Stern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beucler_T/0/1/0/all/0/1"&gt;Tom Beucler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrop_B/0/1/0/all/0/1"&gt;Bryce E. Harrop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilman_B/0/1/0/all/0/1"&gt;Benjamin R. Hilman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenney_A/0/1/0/all/0/1"&gt;Andrea M. Jenney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferretti_S/0/1/0/all/0/1"&gt;Savannah L. Ferretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nana Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenowitz_N/0/1/0/all/0/1"&gt;Noah D. Brenowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eyring_V/0/1/0/all/0/1"&gt;Veronika Eyring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geneva_N/0/1/0/all/0/1"&gt;Nicholas Geneva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gentine_P/0/1/0/all/0/1"&gt;Pierre Gentine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1"&gt;Stephan Mandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1"&gt;Jaideep Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramaniam_A/0/1/0/all/0/1"&gt;Akshay Subramaniam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1"&gt;Carl Vondrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanna_L/0/1/0/all/0/1"&gt;Laure Zanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1"&gt;Tian Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernathey_R/0/1/0/all/0/1"&gt;Ryan P. Abernathey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1"&gt;Fiaz Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bader_D/0/1/0/all/0/1"&gt;David C. Bader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1"&gt;Elizabeth A. Barnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bretherton_C/0/1/0/all/0/1"&gt;Christopher S. Bretherton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caldwell_P/0/1/0/all/0/1"&gt;Peter M. Caldwell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_W/0/1/0/all/0/1"&gt;Wayne Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yilun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iglesias_Suarez_F/0/1/0/all/0/1"&gt;Fernando Iglesias-Suarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jantre_S/0/1/0/all/0/1"&gt;Sanket Jantre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashinath_K/0/1/0/all/0/1"&gt;Karthik Kashinath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khairoutdinov_M/0/1/0/all/0/1"&gt;Marat Khairoutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurth_T/0/1/0/all/0/1"&gt;Thorsten Kurth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lutsko_N/0/1/0/all/0/1"&gt;Nicholas J. Lutsko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1"&gt;Po-Lun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooers_G/0/1/0/all/0/1"&gt;Griffin Mooers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neelin_J/0/1/0/all/0/1"&gt;J. David Neelin&lt;/a&gt;, et al. (7 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Max-Margin Token Selection in Attention Mechanism. (arXiv:2306.13596v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.13596</id>
        <link href="http://arxiv.org/abs/2306.13596"/>
        <updated>2023-09-09T00:40:35.222Z</updated>
        <summary type="html"><![CDATA[Attention mechanism is a central component of the transformer architecture
which led to the phenomenal success of large language models. However, the
theoretical principles underlying the attention mechanism are poorly
understood, especially its nonconvex optimization dynamics. In this work, we
explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle
\boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where
$\boldsymbol{X}$ is the token sequence and
$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are trainable parameters. We
prove that running gradient descent on $\boldsymbol{p}$, or equivalently
$\boldsymbol{W}$, converges in direction to a max-margin solution that
separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly
formalizes attention as an optimal token selection mechanism. Remarkably, our
results are applicable to general data and precisely characterize
$\textit{optimality}$ of tokens in terms of the value embeddings
$\boldsymbol{Xv}$ and problem geometry. We also provide a broader
regularization path analysis that establishes the margin maximizing nature of
attention even for nonlinear prediction heads. When optimizing $\boldsymbol{v}$
and $\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions
under which the regularization paths directionally converge to their respective
hard-margin SVM solutions where $\boldsymbol{v}$ separates the input features
based on their labels. Interestingly, the SVM formulation of $\boldsymbol{p}$
is influenced by the support vector geometry of $\boldsymbol{v}$. Finally, we
verify our theoretical findings via numerical experiments and provide insights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1"&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingcong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuechen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1"&gt;Samet Oymak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Feature Learning under Structured Data. (arXiv:2309.03843v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03843</id>
        <link href="http://arxiv.org/abs/2309.03843"/>
        <updated>2023-09-09T00:40:35.220Z</updated>
        <summary type="html"><![CDATA[Recent works have demonstrated that the sample complexity of gradient-based
learning of single index models, i.e. functions that depend on a 1-dimensional
projection of the input data, is governed by their information exponent.
However, these results are only concerned with isotropic data, while in
practice the input often contains additional structure which can implicitly
guide the algorithm. In this work, we investigate the effect of a spiked
covariance structure and reveal several interesting phenomena. First, we show
that in the anisotropic setting, the commonly used spherical gradient dynamics
may fail to recover the true direction, even when the spike is perfectly
aligned with the target direction. Next, we show that appropriate weight
normalization that is reminiscent of batch normalization can alleviate this
issue. Further, by exploiting the alignment between the (spiked) input
covariance and the target, we obtain improved sample complexity compared to the
isotropic case. In particular, under the spiked model with a suitably large
spike, the sample complexity of gradient-based training can be made independent
of the information exponent while also outperforming lower bounds for
rotationally invariant kernel methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mousavi_Hosseini_A/0/1/0/all/0/1"&gt;Alireza Mousavi-Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_D/0/1/0/all/0/1"&gt;Denny Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.03680</id>
        <link href="http://arxiv.org/abs/2208.03680"/>
        <updated>2023-09-09T00:40:35.220Z</updated>
        <summary type="html"><![CDATA[The large-scale simulation of dynamical systems is critical in numerous
scientific and engineering disciplines. However, traditional numerical solvers
are limited by the choice of step sizes when estimating integration, resulting
in a trade-off between accuracy and computational efficiency. To address this
challenge, we introduce a deep learning-based corrector called Neural Vector
(NeurVec), which can compensate for integration errors and enable larger time
step sizes in simulations. Our extensive experiments on a variety of complex
dynamical system benchmarks demonstrate that NeurVec exhibits remarkable
generalization capability on a continuous phase space, even when trained using
limited and discrete data. NeurVec significantly accelerates traditional
solvers, achieving speeds tens to hundreds of times faster while maintaining
high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective
design, combined with its ease of implementation, has the potential to
establish a new paradigm for fast-solving differential equations based on deep
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence Analysis of Decentralized ASGD. (arXiv:2309.03754v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03754</id>
        <link href="http://arxiv.org/abs/2309.03754"/>
        <updated>2023-09-09T00:40:35.219Z</updated>
        <summary type="html"><![CDATA[Over the last decades, Stochastic Gradient Descent (SGD) has been intensively
studied by the Machine Learning community. Despite its versatility and
excellent performance, the optimization of large models via SGD still is a
time-consuming task. To reduce training time, it is common to distribute the
training process across multiple devices. Recently, it has been shown that the
convergence of asynchronous SGD (ASGD) will always be faster than mini-batch
SGD. However, despite these improvements in the theoretical bounds, most ASGD
convergence-rate proofs still rely on a centralized parameter server, which is
prone to become a bottleneck when scaling out the gradient computations across
many distributed processes.

In this paper, we present a novel convergence-rate analysis for decentralized
and asynchronous SGD (DASGD) which does not require partial synchronization
among nodes nor restrictive network topologies. Specifically, we provide a
bound of $\mathcal{O}(\sigma\epsilon^{-2}) +
\mathcal{O}(QS_{avg}\epsilon^{-3/2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$ for
the convergence rate of DASGD, where $S_{avg}$ is the average staleness between
models, $Q$ is a constant that bounds the norm of the gradients, and $\epsilon$
is a (small) error that is allowed within the bound. Furthermore, when
gradients are not bounded, we prove the convergence rate of DASGD to be
$\mathcal{O}(\sigma\epsilon^{-2}) +
\mathcal{O}(\sqrt{\hat{S}_{avg}\hat{S}_{max}}\epsilon^{-1})$, with
$\hat{S}_{max}$ and $\hat{S}_{avg}$ representing a loose version of the average
and maximum staleness, respectively. Our convergence proof holds for a fixed
stepsize and any non-convex, homogeneous, and L-smooth objective function. We
anticipate that our results will be of high relevance for the adoption of DASGD
by a broad community of researchers and developers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tosi_M/0/1/0/all/0/1"&gt;Mauro DL Tosi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobald_M/0/1/0/all/0/1"&gt;Martin Theobald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.06555</id>
        <link href="http://arxiv.org/abs/2307.06555"/>
        <updated>2023-09-09T00:40:35.218Z</updated>
        <summary type="html"><![CDATA[This paper explores the expressive power of deep neural networks for a
diverse range of activation functions. An activation function set $\mathscr{A}$
is defined to encompass the majority of commonly used activation functions,
such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$,
$\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$,
$\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$,
$\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and
$\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in
\mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be
approximated to arbitrary precision by a $\varrho$-activated network of width
$4N$ and depth $2L$ on any bounded set. This finding enables the extension of
most approximation results achieved with $\mathtt{ReLU}$ networks to a wide
variety of other activation functions, at the cost of slightly larger
constants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jianfeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hongkai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03883</id>
        <link href="http://arxiv.org/abs/2309.03883"/>
        <updated>2023-09-09T00:40:35.217Z</updated>
        <summary type="html"><![CDATA[Despite their impressive capabilities, large language models (LLMs) are prone
to hallucinations, i.e., generating content that deviates from facts seen
during pretraining. We propose a simple decoding strategy for reducing
hallucinations with pretrained LLMs that does not require conditioning on
retrieved external knowledge nor additional fine-tuning. Our approach obtains
the next-token distribution by contrasting the differences in logits obtained
from projecting the later layers versus earlier layers to the vocabulary space,
exploiting the fact that factual knowledge in an LLMs has generally been shown
to be localized to particular transformer layers. We find that this Decoding by
Contrasting Layers (DoLa) approach is able to better surface factual knowledge
and reduce the generation of incorrect facts. DoLa consistently improves the
truthfulness across multiple choices tasks and open-ended generation tasks, for
example improving the performance of LLaMA family models on TruthfulQA by
12-17% absolute points, demonstrating its potential in making LLMs reliably
generate truthful facts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yung-Sung Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yujia Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hongyin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1"&gt;James Glass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitation of Characterizing Implicit Regularization by Data-independent Functions. (arXiv:2201.12198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.12198</id>
        <link href="http://arxiv.org/abs/2201.12198"/>
        <updated>2023-09-09T00:40:35.217Z</updated>
        <summary type="html"><![CDATA[In recent years, understanding the implicit regularization of neural networks
(NNs) has become a central task in deep learning theory. However, implicit
regularization is itself not completely defined and well understood. In this
work, we attempt to mathematically define and study implicit regularization.
Importantly, we explore the limitations of a common approach to characterizing
implicit regularization using data-independent functions. We propose two
dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms,
based on which we provide two recipes for producing classes of
one-hidden-neuron NNs that provably cannot be fully characterized by a type of
or all data-independent functions. Following the previous works, our results
further emphasize the profound data dependency of implicit regularization in
general, inspiring us to study in detail the data dependency of NN implicit
regularization in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble linear interpolators: The role of ensembling. (arXiv:2309.03354v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03354</id>
        <link href="http://arxiv.org/abs/2309.03354"/>
        <updated>2023-09-09T00:40:35.216Z</updated>
        <summary type="html"><![CDATA[Interpolators are unstable. For example, the mininum $\ell_2$ norm least
square interpolator exhibits unbounded test errors when dealing with noisy
data. In this paper, we study how ensemble stabilizes and thus improves the
generalization performance, measured by the out-of-sample prediction risk, of
an individual interpolator. We focus on bagged linear interpolators, as bagging
is a popular randomization-based ensemble method that can be implemented in
parallel. We introduce the multiplier-bootstrap-based bagged least square
estimator, which can then be formulated as an average of the sketched least
square estimators. The proposed multiplier bootstrap encompasses the classical
bootstrap with replacement as a special case, along with a more intriguing
variant which we call the Bernoulli bootstrap.

Focusing on the proportional regime where the sample size scales
proportionally with the feature dimensionality, we investigate the
out-of-sample prediction risks of the sketched and bagged least square
estimators in both underparametrized and overparameterized regimes. Our results
reveal the statistical roles of sketching and bagging. In particular, sketching
modifies the aspect ratio and shifts the interpolation threshold of the minimum
$\ell_2$ norm estimator. However, the risk of the sketched estimator continues
to be unbounded around the interpolation threshold due to excessive variance.
In stark contrast, bagging effectively mitigates this variance, leading to a
bounded limiting out-of-sample prediction risk. To further understand this
stability improvement property, we establish that bagging acts as a form of
implicit regularization, substantiated by the equivalence of the bagged
estimator with its explicitly regularized counterpart. We also discuss several
extensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mingqi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qiang Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Root Cause Localization and Anomaly Mitigation through Causal Inference. (arXiv:2212.04031v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.04031</id>
        <link href="http://arxiv.org/abs/2212.04031"/>
        <updated>2023-09-09T00:40:35.215Z</updated>
        <summary type="html"><![CDATA[Due to a wide spectrum of applications in the real world, such as security,
financial surveillance, and health risk, various deep anomaly detection models
have been proposed and achieved state-of-the-art performance. However, besides
being effective, in practice, the practitioners would further like to know what
causes the abnormal outcome and how to further fix it. In this work, we propose
RootCLAM, which aims to achieve Root Cause Localization and Anomaly Mitigation
from a causal perspective. Especially, we formulate anomalies caused by
external interventions on the normal causal mechanism and aim to locate the
abnormal features with external interventions as root causes. After that, we
further propose an anomaly mitigation approach that aims to recommend
mitigation actions on abnormal features to revert the abnormal outcomes such
that the counterfactuals guided by the causal mechanism are normal. Experiments
on three datasets show that our approach can locate the root causes and further
flip the abnormal labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiao Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongkai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shuhan Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning. (arXiv:2309.03664v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03664</id>
        <link href="http://arxiv.org/abs/2309.03664"/>
        <updated>2023-09-09T00:40:35.212Z</updated>
        <summary type="html"><![CDATA[The cerebrospinal fluid (CSF) of 19 subjects who received a clinical
diagnosis of Alzheimer's disease (AD) as well as of 5 pathological controls
have been collected and analysed by Raman spectroscopy (RS). We investigated
whether the raw and preprocessed Raman spectra could be used to distinguish AD
from controls. First, we applied standard Machine Learning (ML) methods
obtaining unsatisfactory results. Then, we applied ML to a set of topological
descriptors extracted from raw spectra, achieving a very good classification
accuracy (>87%). Although our results are preliminary, they indicate that RS
and topological analysis together may provide an effective combination to
confirm or disprove a clinical diagnosis of AD. The next steps will include
enlarging the dataset of CSF samples to validate the proposed method better
and, possibly, to understand if topological data analysis could support the
characterization of AD subtypes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1"&gt;Francesco Conti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banchelli_M/0/1/0/all/0/1"&gt;Martina Banchelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bessi_V/0/1/0/all/0/1"&gt;Valentina Bessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cecchi_C/0/1/0/all/0/1"&gt;Cristina Cecchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiti_F/0/1/0/all/0/1"&gt;Fabrizio Chiti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colantonio_S/0/1/0/all/0/1"&gt;Sara Colantonio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAndrea_C/0/1/0/all/0/1"&gt;Cristiano D&amp;#x27;Andrea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelis_M/0/1/0/all/0/1"&gt;Marella de Angelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moroni_D/0/1/0/all/0/1"&gt;Davide Moroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nacmias_B/0/1/0/all/0/1"&gt;Benedetta Nacmias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascali_M/0/1/0/all/0/1"&gt;Maria Antonietta Pascali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorbi_S/0/1/0/all/0/1"&gt;Sandro Sorbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteini_P/0/1/0/all/0/1"&gt;Paolo Matteini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Metric Learning with Chance Constraints. (arXiv:2209.09060v3 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2209.09060</id>
        <link href="http://arxiv.org/abs/2209.09060"/>
        <updated>2023-09-09T00:40:35.212Z</updated>
        <summary type="html"><![CDATA[Deep metric learning (DML) aims to minimize empirical expected loss of the
pairwise intra-/inter- class proximity violations in the embedding space. We
relate DML to feasibility problem of finite chance constraints. We show that
minimizer of proxy-based DML satisfies certain chance constraints, and that the
worst case generalization performance of the proxy-based methods can be
characterized by the radius of the smallest ball around a class proxy to cover
the entire domain of the corresponding class samples, suggesting multiple
proxies per class helps performance. To provide a scalable algorithm as well as
exploiting more proxies, we consider the chance constraints implied by the
minimizers of proxy-based DML instances and reformulate DML as finding a
feasible point in intersection of such constraints, resulting in a problem to
be approximately solved by iterative projections. Simply put, we repeatedly
train a regularized proxy-based loss and re-initialize the proxies with the
embeddings of the deliberately selected new samples. We applied our method with
4 well-accepted DML losses and show the effectiveness with extensive
evaluations on 4 popular DML benchmarks. Code is available at:
https://github.com/yetigurbuz/ccp-dml]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gurbuz_Y/0/1/0/all/0/1"&gt;Yeti Z. Gurbuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Can_O/0/1/0/all/0/1"&gt;Ogul Can&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1"&gt;A. Aydin Alatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues. (arXiv:2309.03825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03825</id>
        <link href="http://arxiv.org/abs/2309.03825"/>
        <updated>2023-09-09T00:40:35.211Z</updated>
        <summary type="html"><![CDATA[Deep neural networks employing error back-propagation for learning can suffer
from exploding and vanishing gradient problems. Numerous solutions have been
proposed such as normalisation techniques or limiting activation functions to
linear rectifying units. In this work we follow a different approach which is
particularly applicable to closed-loop learning of forward models where
back-propagation makes exclusive use of the sign of the error signal to prime
the learning, whilst a global relevance signal modulates the rate of learning.
This is inspired by the interaction between local plasticity and a global
neuromodulation. For example, whilst driving on an empty road, one can allow
for slow step-wise optimisation of actions, whereas, at a busy junction, an
error must be corrected at once. Hence, the error is the priming signal and the
intensity of the experience is a modulating factor in the weight change. The
advantages of this Prime and Modulate paradigm is twofold: it is free from
normalisation and it makes use of relevant cues from the environment to enrich
the learning. We present a mathematical derivation of the learning rule in
z-space and demonstrate the real-time performance with a robotic platform. The
results show a significant improvement in the speed of convergence compared to
that of the conventional back-propagation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daryanavard_S/0/1/0/all/0/1"&gt;Sama Daryanavard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Porr_B/0/1/0/all/0/1"&gt;Bernd Porr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copula Representations and Error Surface Projections for the Exclusive Or Problem. (arXiv:1907.04483v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.04483</id>
        <link href="http://arxiv.org/abs/1907.04483"/>
        <updated>2023-09-09T00:40:35.209Z</updated>
        <summary type="html"><![CDATA[The exclusive or (xor) function is one of the simplest examples that
illustrate why nonlinear feedforward networks are superior to linear regression
for machine learning applications. We review the xor representation and
approximation problems and discuss their solutions in terms of probabilistic
logic and associative copula functions. After briefly reviewing the
specification of feedforward networks, we compare the dynamics of learned error
surfaces with different activation functions such as RELU and tanh through a
set of colorful three-dimensional charts. The copula representations extend xor
from Boolean to real values, thereby providing a convenient way to demonstrate
the concept of cross-validation on in-sample and out-sample data sets. Our
approach is pedagogical and is meant to be a machine learning prolegomenon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freedman_R/0/1/0/all/0/1"&gt;Roy S. Freedman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v4 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.03428</id>
        <link href="http://arxiv.org/abs/2303.03428"/>
        <updated>2023-09-09T00:40:35.209Z</updated>
        <summary type="html"><![CDATA[Large machine learning models are revolutionary technologies of artificial
intelligence whose bottlenecks include huge computational expenses, power, and
time used both in the pre-training and fine-tuning process. In this work, we
show that fault-tolerant quantum computing could possibly provide provably
efficient resolutions for generic (stochastic) gradient descent algorithms,
scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size
of the models and $T$ is the number of iterations in the training, as long as
the models are both sufficiently dissipative and sparse, with small learning
rates. Based on earlier efficient quantum algorithms for dissipative
differential equations, we find and prove that similar algorithms work for
(stochastic) gradient descent, the primary algorithm for machine learning. In
practice, we benchmark instances of large machine learning models from 7
million to 103 million parameters. We find that, in the context of sparse
training, a quantum enhancement is possible at the early stage of learning
after model pruning, motivating a sparse parameter download and re-upload
scheme. Our work shows solidly that fault-tolerant quantum algorithms could
potentially contribute to most state-of-the-art, large-scale machine-learning
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minzhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jin-Peng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Ziyu Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Alexeev_Y/0/1/0/all/0/1"&gt;Yuri Alexeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eisert_J/0/1/0/all/0/1"&gt;Jens Eisert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Optimization for Cardinality-constrained Minimum Sum-of-Squares Clustering via Semidefinite Programming. (arXiv:2209.08901v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.08901</id>
        <link href="http://arxiv.org/abs/2209.08901"/>
        <updated>2023-09-09T00:40:35.191Z</updated>
        <summary type="html"><![CDATA[The minimum sum-of-squares clustering (MSSC), or k-means type clustering, has
been recently extended to exploit prior knowledge on the cardinality of each
cluster. Such knowledge is used to increase performance as well as solution
quality. In this paper, we propose a global optimization approach based on the
branch-and-cut technique to solve the cardinality-constrained MSSC. For the
lower bound routine, we use the semidefinite programming (SDP) relaxation
recently proposed by Rujeerapaiboon et al. [SIAM J. Optim. 29(2), 1211-1239,
(2019)]. However, this relaxation can be used in a branch-and-cut method only
for small-size instances. Therefore, we derive a new SDP relaxation that scales
better with the instance size and the number of clusters. In both cases, we
strengthen the bound by adding polyhedral cuts. Benefiting from a tailored
branching strategy which enforces pairwise constraints, we reduce the
complexity of the problems arising in the children nodes. For the upper bound,
instead, we present a local search procedure that exploits the solution of the
SDP relaxation solved at each node. Computational results show that the
proposed algorithm globally solves, for the first time, real-world instances of
size 10 times larger than those solved by state-of-the-art exact methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Piccialli_V/0/1/0/all/0/1"&gt;Veronica Piccialli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sudoso_A/0/1/0/all/0/1"&gt;Antonio M. Sudoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural lasso: a unifying approach of lasso and neural networks. (arXiv:2309.03770v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03770</id>
        <link href="http://arxiv.org/abs/2309.03770"/>
        <updated>2023-09-09T00:40:35.187Z</updated>
        <summary type="html"><![CDATA[In recent years, there is a growing interest in combining techniques
attributed to the areas of Statistics and Machine Learning in order to obtain
the benefits of both approaches. In this article, the statistical technique
lasso for variable selection is represented through a neural network. It is
observed that, although both the statistical approach and its neural version
have the same objective function, they differ due to their optimization. In
particular, the neural version is usually optimized in one-step using a single
validation set, while the statistical counterpart uses a two-step optimization
based on cross-validation. The more elaborated optimization of the statistical
method results in more accurate parameter estimation, especially when the
training set is small. For this reason, a modification of the standard approach
for training neural networks, that mimics the statistical framework, is
proposed. During the development of the above modification, a new optimization
algorithm for identifying the significant variables emerged. Experimental
results, using synthetic and real data sets, show that this new optimization
algorithm achieves better performance than any of the three previous
optimization approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1"&gt;David Delgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Curbelo_E/0/1/0/all/0/1"&gt;Ernesto Curbelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carreras_D/0/1/0/all/0/1"&gt;Danae Carreras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields. (arXiv:2306.12760v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.12760</id>
        <link href="http://arxiv.org/abs/2306.12760"/>
        <updated>2023-09-09T00:40:35.165Z</updated>
        <summary type="html"><![CDATA[Editing a local region or a specific object in a 3D scene represented by a
NeRF or consistently blending a new realistic object into the scene is
challenging, mainly due to the implicit nature of the scene representation. We
present Blended-NeRF, a robust and flexible framework for editing a specific
region of interest in an existing NeRF scene, based on text prompts, along with
a 3D ROI box. Our method leverages a pretrained language-image model to steer
the synthesis towards a user-provided text prompt, along with a 3D MLP model
initialized on an existing NeRF scene to generate the object and blend it into
a specified region in the original scene. We allow local editing by localizing
a 3D ROI box in the input scene, and blend the content synthesized inside the
ROI with the existing scene using a novel volumetric blending technique. To
obtain natural looking and view-consistent results, we leverage existing and
new geometric priors and 3D augmentations for improving the visual fidelity of
the final result. We test our framework both qualitatively and quantitatively
on a variety of real 3D scenes and text prompts, demonstrating realistic
multi-view consistent results with much flexibility and diversity compared to
the baselines. Finally, we show the applicability of our framework for several
3D editing applications, including adding new objects to a scene,
removing/replacing/altering existing objects, and texture conversion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gordon_O/0/1/0/all/0/1"&gt;Ori Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1"&gt;Omri Avrahami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1"&gt;Dani Lischinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.14971</id>
        <link href="http://arxiv.org/abs/2307.14971"/>
        <updated>2023-09-09T00:40:35.165Z</updated>
        <summary type="html"><![CDATA[With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xumin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning. (arXiv:2309.03839v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2309.03839</id>
        <link href="http://arxiv.org/abs/2309.03839"/>
        <updated>2023-09-09T00:40:35.160Z</updated>
        <summary type="html"><![CDATA[Adaptive interfaces can help users perform sequential decision-making tasks
like robotic teleoperation given noisy, high-dimensional command signals (e.g.,
from a brain-computer interface). Recent advances in human-in-the-loop machine
learning enable such systems to improve by interacting with users, but tend to
be limited by the amount of data that they can collect from individual users in
practice. In this paper, we propose a reinforcement learning algorithm to
address this by training an interface to map raw command signals to actions
using a combination of offline pre-training and online fine-tuning. To address
the challenges posed by noisy command signals and sparse rewards, we develop a
novel method for representing and inferring the user's long-term intent for a
given trajectory. We primarily evaluate our method's ability to assist users
who can only communicate through noisy, high-dimensional input channels through
a user study in which 12 participants performed a simulated navigation task by
using their eye gaze to modulate a 128-dimensional command signal from their
webcam. The results show that our method enables successful goal navigation
more often than a baseline directional interface, by learning to denoise user
commands signals and provide shared autonomy assistance. We further evaluate on
a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander
game with simulated user commands, and find that our method improves over
baseline interfaces in these domains as well. Extensive ablation experiments
with simulated user commands empirically motivate each component of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jensen Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1"&gt;Siddharth Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1"&gt;Glen Berseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1"&gt;Anca D. Dragan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03812</id>
        <link href="http://arxiv.org/abs/2309.03812"/>
        <updated>2023-09-09T00:40:35.143Z</updated>
        <summary type="html"><![CDATA[We present a novel human body model formulated by an extensive set of
anthropocentric measurements, which is capable of generating a wide range of
human body shapes and poses. The proposed model enables direct modeling of
specific human identities through a deep generative architecture, which can
produce humans in any arbitrary pose. It is the first of its kind to have been
trained end-to-end using only synthetically generated data, which not only
provides highly accurate human mesh representations but also allows for precise
anthropometry of the body. Moreover, using a highly diverse animation library,
we articulated our synthetic humans' body and hands to maximize the diversity
of the learnable priors for model training. Our model was trained on a dataset
of $100k$ procedurally-generated posed human meshes and their corresponding
anthropometric measurements. Our synthetic data generator can be used to
generate millions of unique human identities and poses for non-commercial
academic research purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Picetti_F/0/1/0/all/0/1"&gt;Francesco Picetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_S/0/1/0/all/0/1"&gt;Shrinath Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leban_J/0/1/0/all/0/1"&gt;Jonathan Leban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahtalebi_S/0/1/0/all/0/1"&gt;Soroosh Shahtalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1"&gt;Jay Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_P/0/1/0/all/0/1"&gt;Peifeng Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunpu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metze_C/0/1/0/all/0/1"&gt;Charles Metze III&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cameron Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1"&gt;Cera Laidlaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warren_J/0/1/0/all/0/1"&gt;James Warren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_K/0/1/0/all/0/1"&gt;Kathy Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Page_R/0/1/0/all/0/1"&gt;River Page&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1"&gt;Jonathan Hogins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1"&gt;Adam Crespi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1"&gt;Sujoy Ganguly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebadi_S/0/1/0/all/0/1"&gt;Salehe Erfanian Ebadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReFit: A Framework for Refinement of Weakly Supervised Semantic Segmentation using Object Border Fitting for Medical Images. (arXiv:2303.07853v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.07853</id>
        <link href="http://arxiv.org/abs/2303.07853"/>
        <updated>2023-09-09T00:40:35.142Z</updated>
        <summary type="html"><![CDATA[Weakly Supervised Semantic Segmentation (WSSS) relying only on image-level
supervision is a promising approach to deal with the need for Segmentation
networks, especially for generating a large number of pixel-wise masks in a
given dataset. However, most state-of-the-art image-level WSSS techniques lack
an understanding of the geometric features embedded in the images since the
network cannot derive any object boundary information from just image-level
labels. We define a boundary here as the line separating an object and its
background, or two different objects. To address this drawback, we are
proposing our novel ReFit framework, which deploys state-of-the-art class
activation maps combined with various post-processing techniques in order to
achieve fine-grained higher-accuracy segmentation masks. To achieve this, we
investigate a state-of-the-art unsupervised segmentation network that can be
used to construct a boundary map, which enables ReFit to predict object
locations with sharper boundaries. By applying our method to WSSS predictions,
we achieved up to 10% improvement over the current state-of-the-art WSSS
methods for medical imaging. The framework is open-source, to ensure that our
results are reproducible, and accessible online at
https://github.com/bharathprabakaran/ReFit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabakaran_B/0/1/0/all/0/1"&gt;Bharath Srinivas Prabakaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostrowski_E/0/1/0/all/0/1"&gt;Erik Ostrowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1"&gt;Muhammad Shafique&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Likelihood Estimation With One-Way Flows. (arXiv:2307.09882v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09882</id>
        <link href="http://arxiv.org/abs/2307.09882"/>
        <updated>2023-09-09T00:40:35.136Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) can produce high-quality samples, but
do not provide an estimate of the probability density around the samples.
However, it has been noted that maximizing the log-likelihood within an
energy-based setting can lead to an adversarial framework where the
discriminator provides unnormalized density (often called energy). We further
develop this perspective, incorporate importance sampling, and show that 1)
Wasserstein GAN performs a biased estimate of the partition function, and we
propose instead to use an unbiased estimator; and 2) when optimizing for
likelihood, one must maximize generator entropy. This is hypothesized to
provide a better mode coverage. Different from previous works, we explicitly
compute the density of the generated samples. This is the key enabler to
designing an unbiased estimator of the partition function and computation of
the generator entropy term. The generator density is obtained via a new type of
flow network, called one-way flow network, that is less constrained in terms of
architecture, as it does not require a tractable inverse function. Our
experimental results show that our method converges faster, produces comparable
sample quality to GANs with similar architecture, successfully avoids
over-fitting to commonly used datasets and produces smooth low-dimensional
latent representations of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Dov_O/0/1/0/all/0/1"&gt;Omri Ben-Dov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Pravir Singh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1"&gt;Victoria Abrevaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1"&gt;Partha Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03748</id>
        <link href="http://arxiv.org/abs/2309.03748"/>
        <updated>2023-09-09T00:40:35.135Z</updated>
        <summary type="html"><![CDATA[The latest advancements in AI and deep learning have led to a breakthrough in
large language model (LLM)-based agents such as GPT-4. However, many commercial
conversational agent development tools are pipeline-based and have limitations
in holding a human-like conversation. This paper investigates the capabilities
of LLMs to enhance pipeline-based conversational agents during two phases: 1)
in the design and development phase and 2) during operations. In 1) LLMs can
aid in generating training data, extracting entities and synonyms,
localization, and persona design. In 2) LLMs can assist in contextualization,
intent classification to prevent conversational breakdown and handle
out-of-scope questions, auto-correcting utterances, rephrasing responses,
formulating disambiguation questions, summarization, and enabling closed
question-answering capabilities. We conducted informal experiments with GPT-4
in the private banking domain to demonstrate the scenarios above with a
practical example. Companies may be hesitant to replace their pipeline-based
agents with LLMs entirely due to privacy concerns and the need for deep
integration within their existing ecosystems. A hybrid approach in which LLMs'
are integrated into the pipeline-based agents allows them to save time and
costs of building and running agents by capitalizing on the capabilities of
LLMs while retaining the integration and privacy safeguards of their existing
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foosherian_M/0/1/0/all/0/1"&gt;Mina Foosherian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purwins_H/0/1/0/all/0/1"&gt;Hendrik Purwins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathnayake_P/0/1/0/all/0/1"&gt;Purna Rathnayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1"&gt;Touhidul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teimao_R/0/1/0/all/0/1"&gt;Rui Teimao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thoben_K/0/1/0/all/0/1"&gt;Klaus-Dieter Thoben&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.12774</id>
        <link href="http://arxiv.org/abs/2306.12774"/>
        <updated>2023-09-09T00:40:35.102Z</updated>
        <summary type="html"><![CDATA[We address the problem of identifying the optimal policy with a fixed
confidence level in a multi-armed bandit setup, when \emph{the arms are subject
to linear constraints}. Unlike the standard best-arm identification problem
which is well studied, the optimal policy in this case may not be deterministic
and could mix between several arms. This changes the geometry of the problem
which we characterize via an information-theoretic lower bound. We introduce
two asymptotically optimal algorithms for this setting, one based on the
Track-and-Stop method and the other based on a game-theoretic approach. Both
these algorithms try to track an optimal allocation based on the lower bound
and computed by a weighted projection onto the boundary of a normal cone.
Finally, we provide empirical results that validate our bounds and visualize
how constraints change the hardness of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlsson_E/0/1/0/all/0/1"&gt;Emil Carlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_D/0/1/0/all/0/1"&gt;Debabrota Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_F/0/1/0/all/0/1"&gt;Fredrik D. Johansson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubhashi_D/0/1/0/all/0/1"&gt;Devdatt Dubhashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Practices for Domain Adaptation. (arXiv:2309.03879v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03879</id>
        <link href="http://arxiv.org/abs/2309.03879"/>
        <updated>2023-09-09T00:40:35.090Z</updated>
        <summary type="html"><![CDATA[Distribution shifts are all too common in real-world applications of machine
learning. Domain adaptation (DA) aims to address this by providing various
frameworks for adapting models to the deployment data without using labels.
However, the domain shift scenario raises a second more subtle challenge: the
difficulty of performing hyperparameter optimisation (HPO) for these adaptation
algorithms without access to a labelled validation set. The unclear validation
protocol for DA has led to bad practices in the literature, such as performing
HPO using the target test labels when, in real-world scenarios, they are not
available. This has resulted in over-optimism about DA research progress
compared to reality. In this paper, we analyse the state of DA when using good
evaluation practice, by benchmarking a suite of candidate validation criteria
and using them to assess popular adaptation algorithms. We show that there are
challenges across all three branches of domain adaptation methodology including
Unsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), and
Test Time Adaptation (TTA). While the results show that realistically
achievable performance is often worse than expected, they also show that using
proper validation splits is beneficial, as well as showing that some previously
unexplored validation metrics provide the best options to date. Altogether, our
improved practices covering data, training, validation and hyperparameter
optimisation form a new rigorous pipeline to improve benchmarking, and hence
research progress, within this important field going forward.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ericsson_L/0/1/0/all/0/1"&gt;Linus Ericsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy M. Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dataset Generation and Bonobo Classification from Weakly Labelled Videos. (arXiv:2309.03671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03671</id>
        <link href="http://arxiv.org/abs/2309.03671"/>
        <updated>2023-09-09T00:40:35.086Z</updated>
        <summary type="html"><![CDATA[This paper presents a bonobo detection and classification pipeline built from
the commonly used machine learning methods. Such application is motivated by
the need to test bonobos in their enclosure using touch screen devices without
human assistance. This work introduces a newly acquired dataset based on bonobo
recordings generated semi-automatically. The recordings are weakly labelled and
fed to a macaque detector in order to spatially detect the individual present
in the video. Handcrafted features coupled with different classification
algorithms and deep-learning methods using a ResNet architecture are
investigated for bonobo identification. Performance is compared in terms of
classification accuracy on the splits of the database using different data
separation methods. We demonstrate the importance of data preparation and how a
wrong data separation can lead to false good results. Finally, after a
meaningful separation of the data, the best classification performance is
obtained using a fine-tuned ResNet model and reaches 75% of accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1"&gt;Pierre-Etienne Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders. (arXiv:2202.09671v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.09671</id>
        <link href="http://arxiv.org/abs/2202.09671"/>
        <updated>2023-09-09T00:40:35.086Z</updated>
        <summary type="html"><![CDATA[Employing a forward diffusion chain to gradually map the data to a noise
distribution, diffusion-based generative models learn how to generate the data
by inferring a reverse diffusion chain. However, this approach is slow and
costly because it needs many forward and reverse steps. We propose a faster and
cheaper approach that adds noise not until the data become pure random noise,
but until they reach a hidden noisy data distribution that we can confidently
learn. Then, we use fewer reverse steps to generate data by starting from this
hidden distribution that is made similar to the noisy data. We reveal that the
proposed model can be cast as an adversarial auto-encoder empowered by both the
diffusion process and a learnable implicit prior. Experimental results show
even with a significantly smaller number of reverse diffusion steps, the
proposed truncated diffusion probabilistic models can provide consistent
improvements over the non-truncated ones in terms of performance in both
unconditional and text-guided image generations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explanation Shift: How Did the Distribution Shift Impact the Model?. (arXiv:2303.08081v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.08081</id>
        <link href="http://arxiv.org/abs/2303.08081"/>
        <updated>2023-09-09T00:40:35.086Z</updated>
        <summary type="html"><![CDATA[As input data distributions evolve, the predictive performance of machine
learning models tends to deteriorate. In practice, new input data tend to come
without target labels. Then, state-of-the-art techniques model input data
distributions or model prediction distributions and try to understand issues
regarding the interactions between learned models and shifting distributions.
We suggest a novel approach that models how explanation characteristics shift
when affected by distribution shifts. We find that the modeling of explanation
shifts can be a better indicator for detecting out-of-distribution model
behaviour than state-of-the-art techniques. We analyze different types of
distribution shifts using synthetic examples and real-world data sets. We
provide an algorithmic method that allows us to inspect the interaction between
data set features and learned models and compare them to the state-of-the-art.
We release our methods in an open-source Python package, as well as the code
used to reproduce our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mougan_C/0/1/0/all/0/1"&gt;Carlos Mougan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1"&gt;Klaus Broelemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1"&gt;David Masip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1"&gt;Gjergji Kasneci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiropanis_T/0/1/0/all/0/1"&gt;Thanassis Thiropanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1"&gt;Steffen Staab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A computationally lightweight safe learning algorithm. (arXiv:2309.03672v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2309.03672</id>
        <link href="http://arxiv.org/abs/2309.03672"/>
        <updated>2023-09-09T00:40:35.079Z</updated>
        <summary type="html"><![CDATA[Safety is an essential asset when learning control policies for physical
systems, as violating safety constraints during training can lead to expensive
hardware damage. In response to this need, the field of safe learning has
emerged with algorithms that can provide probabilistic safety guarantees
without knowledge of the underlying system dynamics. Those algorithms often
rely on Gaussian process inference. Unfortunately, Gaussian process inference
scales cubically with the number of data points, limiting applicability to
high-dimensional and embedded systems. In this paper, we propose a safe
learning algorithm that provides probabilistic safety guarantees but leverages
the Nadaraya-Watson estimator instead of Gaussian processes. For the
Nadaraya-Watson estimator, we can reach logarithmic scaling with the number of
data points. We provide theoretical guarantees for the estimates, embed them
into a safe learning algorithm, and show numerical experiments on a simulated
seven-degrees-of-freedom robot manipulator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Baumann_D/0/1/0/all/0/1"&gt;Dominik Baumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kowalczyk_K/0/1/0/all/0/1"&gt;Krzysztof Kowalczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tiels_K/0/1/0/all/0/1"&gt;Koen Tiels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wachel_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Wachel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03800</id>
        <link href="http://arxiv.org/abs/2309.03800"/>
        <updated>2023-09-09T00:40:35.079Z</updated>
        <summary type="html"><![CDATA[This work investigates the nuanced algorithm design choices for deep learning
in the presence of computational-statistical gaps. We begin by considering
offline sparse parity learning, a supervised classification problem which
admits a statistical query lower bound for gradient-based training of a
multilayer perceptron. This lower bound can be interpreted as a multi-resource
tradeoff frontier: successful learning can only occur if one is sufficiently
rich (large model), knowledgeable (large dataset), patient (many training
iterations), or lucky (many random guesses). We show, theoretically and
experimentally, that sparse initialization and increasing network width yield
significant improvements in sample efficiency in this setting. Here, width
plays the role of parallel search: it amplifies the probability of finding
"lottery ticket" neurons, which learn sparse features more sample-efficiently.
Finally, we show that the synthetic sparse parity task can be useful as a proxy
for real problems requiring axis-aligned feature learning. We demonstrate
improved sample efficiency on tabular classification benchmarks by using wide,
sparsely-initialized MLP models; these networks sometimes outperform tuned
random forests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edelman_B/0/1/0/all/0/1"&gt;Benjamin L. Edelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1"&gt;Eran Malach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cyril Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning. (arXiv:2309.03779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03779</id>
        <link href="http://arxiv.org/abs/2309.03779"/>
        <updated>2023-09-09T00:40:35.077Z</updated>
        <summary type="html"><![CDATA[Small devices are frequently used in IoT and smart-city applications to
perform periodic dedicated tasks with soft deadlines. This work focuses on
developing methods to derive efficient power-management methods for periodic
tasks on small devices. We first study the limitations of the existing Linux
built-in methods used in small devices. We illustrate three typical
workload/system patterns that are challenging to manage with Linux's built-in
solutions. We develop a reinforcement-learning-based technique with temporal
encoding to derive an effective DVFS governor even with the presence of the
three system patterns. The derived governor uses only one performance counter,
the same as the built-in Linux mechanism, and does not require an explicit task
model for the workload. We implemented a prototype system on the Nvidia Jetson
Nano Board and experimented with it with six applications, including two
self-designed and four benchmark applications. Under different deadline
constraints, our approach can quickly derive a DVFS governor that can adapt to
performance requirements and outperform the built-in Linux approach in energy
saving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4
s, the proposed method can save 3% - 11% more energy compared to Ondemand.
AudioReg and FaceReg applications tested have 5%- 14% energy-saving
improvement. We have open-sourced the implementation of our in-kernel quantized
neural network engine. The codebase can be found at:
https://github.com/coladog/tinyagent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Ti Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Man Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication. (arXiv:2309.03607v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2309.03607</id>
        <link href="http://arxiv.org/abs/2309.03607"/>
        <updated>2023-09-09T00:40:35.072Z</updated>
        <summary type="html"><![CDATA[Lithium-ion (Li-ion) batteries are the primary power source in various
applications due to their high energy and power density. Their market was
estimated to be up to 48 billion U.S. dollars in 2022. However, the widespread
adoption of Li-ion batteries has resulted in counterfeit cell production, which
can pose safety hazards to users. Counterfeit cells can cause explosions or
fires, and their prevalence in the market makes it difficult for users to
detect fake cells. Indeed, current battery authentication methods can be
susceptible to advanced counterfeiting techniques and are often not adaptable
to various cells and systems. In this paper, we improve the state of the art on
battery authentication by proposing two novel methodologies, DCAuth and
EISthentication, which leverage the internal characteristics of each cell
through Machine Learning models. Our methods automatically authenticate
lithium-ion battery models and architectures using data from their regular
usage without the need for any external device. They are also resilient to the
most common and critical counterfeit practices and can scale to several
batteries and devices. To evaluate the effectiveness of our proposed
methodologies, we analyze time-series data from a total of 20 datasets that we
have processed to extract meaningful features for our analysis. Our methods
achieve high accuracy in battery authentication for both architectures (up to
0.99) and models (up to 0.96). Moreover, our methods offer comparable
identification performances. By using our proposed methodologies, manufacturers
can ensure that devices only use legitimate batteries, guaranteeing the
operational state of any system and safety measures for the users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marchiori_F/0/1/0/all/0/1"&gt;Francesco Marchiori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Mauro Conti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Sequential Forecasters. (arXiv:2110.00115v5 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.00115</id>
        <link href="http://arxiv.org/abs/2110.00115"/>
        <updated>2023-09-09T00:40:35.063Z</updated>
        <summary type="html"><![CDATA[Consider two forecasters, each making a single prediction for a sequence of
events over time. We ask a relatively basic question: how might we compare
these forecasters, either online or post-hoc, while avoiding unverifiable
assumptions on how the forecasts and outcomes were generated? In this paper, we
present a rigorous answer to this question by designing novel sequential
inference procedures for estimating the time-varying difference in forecast
scores. To do this, we employ confidence sequences (CS), which are sequences of
confidence intervals that can be continuously monitored and are valid at
arbitrary data-dependent stopping times ("anytime-valid"). The widths of our
CSs are adaptive to the underlying variance of the score differences.
Underlying their construction is a game-theoretic statistical framework, in
which we further identify e-processes and p-processes for sequentially testing
a weak null hypothesis -- whether one forecaster outperforms another on average
(rather than always). Our methods do not make distributional assumptions on the
forecasts or outcomes; our main theorems apply to any bounded scores, and we
later provide alternative methods for unbounded scores. We empirically validate
our approaches by comparing real-world baseball and weather forecasters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Choe_Y/0/1/0/all/0/1"&gt;Yo Joong Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved theoretical guarantee for rank aggregation via spectral method. (arXiv:2309.03808v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03808</id>
        <link href="http://arxiv.org/abs/2309.03808"/>
        <updated>2023-09-09T00:40:35.054Z</updated>
        <summary type="html"><![CDATA[Given pairwise comparisons between multiple items, how to rank them so that
the ranking matches the observations? This problem, known as rank aggregation,
has found many applications in sports, recommendation systems, and other web
applications. As it is generally NP-hard to find a global ranking that
minimizes the mismatch (known as the Kemeny optimization), we focus on the
Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each
pairwise comparison is a corrupted copy of the true score difference. We
investigate spectral ranking algorithms that are based on unnormalized and
normalized data matrices. The key is to understand their performance in
recovering the underlying scores of each item from the observed data. This
reduces to deriving an entry-wise perturbation error bound between the top
eigenvectors of the unnormalized/normalized data matrix and its population
counterpart. By using the leave-one-out technique, we provide a sharper
$\ell_{\infty}$-norm perturbation bound of the eigenvectors and also derive an
error bound on the maximum displacement for each item, with only $\Omega(n\log
n)$ samples. Our theoretical analysis improves upon the state-of-the-art
results in terms of sample complexity, and our numerical experiments confirm
these theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Ziliang Samuel Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ling_S/0/1/0/all/0/1"&gt;Shuyang Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoXHED2.0: Scalable boosting of dynamic survival analysis. (arXiv:2103.12591v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12591</id>
        <link href="http://arxiv.org/abs/2103.12591"/>
        <updated>2023-09-09T00:40:35.054Z</updated>
        <summary type="html"><![CDATA[Modern applications of survival analysis increasingly involve time-dependent
covariates. The Python package BoXHED2.0 is a tree-boosted hazard estimator
that is fully nonparametric, and is applicable to survival settings far more
general than right-censoring, including recurring events and competing risks.
BoXHED2.0 is also scalable to the point of being on the same order of speed as
parametric boosted survival models, in part because its core is written in C++
and it also supports the use of GPUs and multicore CPUs. BoXHED2.0 is available
from PyPI and also from www.github.com/BoXHED.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pakbin_A/0/1/0/all/0/1"&gt;Arash Pakbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaochen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1"&gt;Bobak J. Mortazavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donald K.K. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble linear interpolators: The role of ensembling. (arXiv:2309.03354v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03354</id>
        <link href="http://arxiv.org/abs/2309.03354"/>
        <updated>2023-09-09T00:40:35.053Z</updated>
        <summary type="html"><![CDATA[Interpolators are unstable. For example, the mininum $\ell_2$ norm least
square interpolator exhibits unbounded test errors when dealing with noisy
data. In this paper, we study how ensemble stabilizes and thus improves the
generalization performance, measured by the out-of-sample prediction risk, of
an individual interpolator. We focus on bagged linear interpolators, as bagging
is a popular randomization-based ensemble method that can be implemented in
parallel. We introduce the multiplier-bootstrap-based bagged least square
estimator, which can then be formulated as an average of the sketched least
square estimators. The proposed multiplier bootstrap encompasses the classical
bootstrap with replacement as a special case, along with a more intriguing
variant which we call the Bernoulli bootstrap.

Focusing on the proportional regime where the sample size scales
proportionally with the feature dimensionality, we investigate the
out-of-sample prediction risks of the sketched and bagged least square
estimators in both underparametrized and overparameterized regimes. Our results
reveal the statistical roles of sketching and bagging. In particular, sketching
modifies the aspect ratio and shifts the interpolation threshold of the minimum
$\ell_2$ norm estimator. However, the risk of the sketched estimator continues
to be unbounded around the interpolation threshold due to excessive variance.
In stark contrast, bagging effectively mitigates this variance, leading to a
bounded limiting out-of-sample prediction risk. To further understand this
stability improvement property, we establish that bagging acts as a form of
implicit regularization, substantiated by the equivalence of the bagged
estimator with its explicitly regularized counterpart. We also discuss several
extensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mingqi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qiang Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proper Learning of Linear Dynamical Systems as a Non-Commutative Polynomial Optimisation Problem. (arXiv:2002.01444v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01444</id>
        <link href="http://arxiv.org/abs/2002.01444"/>
        <updated>2023-09-09T00:40:35.052Z</updated>
        <summary type="html"><![CDATA[There has been much recent progress in forecasting the next observation of a
linear dynamical system (LDS), which is known as the improper learning, as well
as in the estimation of its system matrices, which is known as the proper
learning of LDS. We present an approach to proper learning of LDS, which in
spite of the non-convexity of the problem, guarantees global convergence of
numerical solutions to a least-squares estimator. We present promising
computational results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Marecek_J/0/1/0/all/0/1"&gt;Jakub Marecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.03251</id>
        <link href="http://arxiv.org/abs/2309.03251"/>
        <updated>2023-09-09T00:40:35.051Z</updated>
        <summary type="html"><![CDATA[Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph
(KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial
task that aims to predict future facts based on historical occurrences. The key
challenge lies in uncovering structural dependencies within historical
subgraphs and temporal patterns. Most existing approaches model TKGs relying on
entity modeling, as nodes in the graph play a crucial role in knowledge
representation. However, the real-world scenario often involves an extensive
number of entities, with new entities emerging over time. This makes it
challenging for entity-dependent methods to cope with extensive volumes of
entities, and effectively handling newly emerging entities also becomes a
significant challenge. Therefore, we propose Temporal Inductive Path Neural
Network (TiPNN), which models historical information in an entity-independent
perspective. Specifically, TiPNN adopts a unified graph, namely history
temporal graph, to comprehensively capture and encapsulate information from
history. Subsequently, we utilize the defined query-aware temporal paths to
model historical path information related to queries on history temporal graph
for the reasoning. Extensive experiments illustrate that the proposed model not
only attains significant performance enhancements but also handles inductive
settings, while additionally facilitating the provision of reasoning evidence
through history temporal graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1"&gt;Meng Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1"&gt;Zhiyuan Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuanchun Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2106.02613v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02613</id>
        <link href="http://arxiv.org/abs/2106.02613"/>
        <updated>2023-09-09T00:40:35.049Z</updated>
        <summary type="html"><![CDATA[Bootstrapping is behind much of the successes of deep Reinforcement Learning.
However, learning the value function via bootstrapping often leads to unstable
training due to fast-changing target values. Target Networks are employed to
stabilize training by using an additional set of lagging parameters to estimate
the target values. Despite the popularity of Target Networks, their effect on
the optimization is still misunderstood. In this work, we show that they act as
an implicit regularizer which can be beneficial in some cases, but also have
disadvantages such as being inflexible and can result in instabilities, even
when vanilla TD(0) converges. To overcome these issues, we propose an explicit
Functional Regularization alternative that is flexible and a convex regularizer
in function space and we theoretically study its convergence. We conduct an
experimental study across a range of environments, discount factors, and
off-policiness data collections to investigate the effectiveness of the
regularization induced by Target Networks and Functional Regularization in
terms of performance, accuracy, and stability. Our findings emphasize that
Functional Regularization can be used as a drop-in replacement for Target
Networks and result in performance improvement. Furthermore, adjusting both the
regularization weight and the network update period in Functional
Regularization can result in further performance improvements compared to
solely adjusting the network update period as typically done with Target
Networks. Our approach also enhances the ability to networks to recover
accurate $Q$-values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piche_A/0/1/0/all/0/1"&gt;Alexandre Pich&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thomas_V/0/1/0/all/0/1"&gt;Valentin Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pardinas_R/0/1/0/all/0/1"&gt;Rafael Pardinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marino_J/0/1/0/all/0/1"&gt;Joseph Marino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marconi_G/0/1/0/all/0/1"&gt;Gian Maria Marconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation. (arXiv:2309.03535v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2309.03535</id>
        <link href="http://arxiv.org/abs/2309.03535"/>
        <updated>2023-09-09T00:40:35.048Z</updated>
        <summary type="html"><![CDATA[Diseases such as diabetic retinopathy and age-related macular degeneration
pose a significant risk to vision, highlighting the importance of precise
segmentation of retinal vessels for the tracking and diagnosis of progression.
However, existing vessel segmentation methods that heavily rely on
encoder-decoder structures struggle to capture contextual information about
retinal vessel configurations, leading to challenges in reconciling semantic
disparities between encoder and decoder features. To address this, we propose a
novel feature enhancement segmentation network (FES-Net) that achieves accurate
pixel-wise segmentation without requiring additional image enhancement steps.
FES-Net directly processes the input image and utilizes four prompt
convolutional blocks (PCBs) during downsampling, complemented by a shallow
upsampling approach to generate a binary mask for each class. We evaluate the
performance of FES-Net on four publicly available state-of-the-art datasets:
DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the
superior performance of FES-Net compared to other competitive approaches
documented in the existing literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khan_T/0/1/0/all/0/1"&gt;Tariq M. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arsalan_M/0/1/0/all/0/1"&gt;Muhammad Arsalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Iqbal_S/0/1/0/all/0/1"&gt;Shahzaib Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Razzak_I/0/1/0/all/0/1"&gt;Imran Razzak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meijering_E/0/1/0/all/0/1"&gt;Erik Meijering&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic Semi-Supervised Approach with Triplet Markov Chains. (arXiv:2309.03707v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03707</id>
        <link href="http://arxiv.org/abs/2309.03707"/>
        <updated>2023-09-09T00:40:35.048Z</updated>
        <summary type="html"><![CDATA[Triplet Markov chains are general generative models for sequential data which
take into account three kinds of random variables: (noisy) observations, their
associated discrete labels and latent variables which aim at strengthening the
distribution of the observations and their associated labels. However, in
practice, we do not have at our disposal all the labels associated to the
observations to estimate the parameters of such models. In this paper, we
propose a general framework based on a variational Bayesian inference to train
parameterized triplet Markov chain models in a semi-supervised context. The
generality of our approach enables us to derive semi-supervised algorithms for
a variety of generative models for sequential Bayesian classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Morales_K/0/1/0/all/0/1"&gt;Katherine Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Petetin_Y/0/1/0/all/0/1"&gt;Yohan Petetin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Optimization for Cardinality-constrained Minimum Sum-of-Squares Clustering via Semidefinite Programming. (arXiv:2209.08901v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.08901</id>
        <link href="http://arxiv.org/abs/2209.08901"/>
        <updated>2023-09-09T00:40:35.044Z</updated>
        <summary type="html"><![CDATA[The minimum sum-of-squares clustering (MSSC), or k-means type clustering, has
been recently extended to exploit prior knowledge on the cardinality of each
cluster. Such knowledge is used to increase performance as well as solution
quality. In this paper, we propose a global optimization approach based on the
branch-and-cut technique to solve the cardinality-constrained MSSC. For the
lower bound routine, we use the semidefinite programming (SDP) relaxation
recently proposed by Rujeerapaiboon et al. [SIAM J. Optim. 29(2), 1211-1239,
(2019)]. However, this relaxation can be used in a branch-and-cut method only
for small-size instances. Therefore, we derive a new SDP relaxation that scales
better with the instance size and the number of clusters. In both cases, we
strengthen the bound by adding polyhedral cuts. Benefiting from a tailored
branching strategy which enforces pairwise constraints, we reduce the
complexity of the problems arising in the children nodes. For the upper bound,
instead, we present a local search procedure that exploits the solution of the
SDP relaxation solved at each node. Computational results show that the
proposed algorithm globally solves, for the first time, real-world instances of
size 10 times larger than those solved by state-of-the-art exact methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Piccialli_V/0/1/0/all/0/1"&gt;Veronica Piccialli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sudoso_A/0/1/0/all/0/1"&gt;Antonio M. Sudoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Risk Minimization for Losses without Variance. (arXiv:2309.03818v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03818</id>
        <link href="http://arxiv.org/abs/2309.03818"/>
        <updated>2023-09-09T00:40:35.022Z</updated>
        <summary type="html"><![CDATA[This paper considers an empirical risk minimization problem under
heavy-tailed settings, where data does not have finite variance, but only has
$p$-th moment with $p \in (1,2)$. Instead of using estimation procedure based
on truncated observed data, we choose the optimizer by minimizing the risk
value. Those risk values can be robustly estimated via using the remarkable
Catoni's method (Catoni, 2012). Thanks to the structure of Catoni-type
influence functions, we are able to establish excess risk upper bounds via
using generalized generic chaining methods. Moreover, we take computational
issues into consideration. We especially theoretically investigate two types of
optimization methods, robust gradient descent algorithm and empirical
risk-based methods. With an extensive numerical study, we find that the
optimizer based on empirical risks via Catoni-style estimation indeed shows
better performance than other baselines. It indicates that estimation directly
based on truncated data may lead to unsatisfactory results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fang_G/0/1/0/all/0/1"&gt;Guanhua Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Samorodnitsky_G/0/1/0/all/0/1"&gt;Gennady Samorodnitsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early warning via transitions in latent stochastic dynamical systems. (arXiv:2309.03842v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03842</id>
        <link href="http://arxiv.org/abs/2309.03842"/>
        <updated>2023-09-09T00:40:35.022Z</updated>
        <summary type="html"><![CDATA[Early warnings for dynamical transitions in complex systems or
high-dimensional observation data are essential in many real world
applications, such as gene mutation, brain diseases, natural disasters,
financial crises, and engineering reliability. To effectively extract early
warning signals, we develop a novel approach: the directed anisotropic
diffusion map that captures the latent evolutionary dynamics in low-dimensional
manifold. Applying the methodology to authentic electroencephalogram (EEG)
data, we successfully find the appropriate effective coordinates, and derive
early warning signals capable of detecting the tipping point during the state
transition. Our method bridges the latent dynamics with the original dataset.
The framework is validated to be accurate and effective through numerical
experiments, in terms of density and transition probability. It is shown that
the second coordinate holds meaningful information for critical transition in
various evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lingyu Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1"&gt;Ting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinqiao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models. (arXiv:2308.00452v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.00452</id>
        <link href="http://arxiv.org/abs/2308.00452"/>
        <updated>2023-09-09T00:40:34.934Z</updated>
        <summary type="html"><![CDATA[Patch robustness certification ensures no patch within a given bound on a
sample can manipulate a deep learning model to predict a different label.
However, existing techniques cannot certify samples that cannot meet their
strict bars at the classifier or patch region levels. This paper proposes
MajorCert. MajorCert firstly finds all possible label sets manipulatable by the
same patch region on the same sample across the underlying classifiers, then
enumerates their combinations element-wise, and finally checks whether the
majority invariant of all these combinations is intact to certify samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qilin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhengyuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haipeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1"&gt;W.K. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03469</id>
        <link href="http://arxiv.org/abs/2309.03469"/>
        <updated>2023-09-09T00:40:34.876Z</updated>
        <summary type="html"><![CDATA[Advances in Semi-Supervised Learning (SSL) have almost entirely closed the
gap between SSL and Supervised Learning at a fraction of the number of labels.
However, recent performance improvements have often come \textit{at the cost of
significantly increased training computation}. To address this, we propose
Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which
exploits the natural training dynamics of deep neural networks.} A small
unlabeled batch size is used in the beginning of training and is gradually
increased to the end of training. A fixed curriculum is used regardless of
dataset, model or number of epochs, and reduced training computations is
demonstrated on all settings. We apply CBS, strong labeled augmentation,
Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch}
and term the new SSL algorithm Fast FixMatch. We perform an ablation study to
show that strong labeled augmentation and/or CPL do not significantly reduce
training computations, but, in synergy with CBS, they achieve optimal
performance. Fast FixMatch also achieves substantially higher data utilization
compared to previous state-of-the-art. Fast FixMatch achieves between
$2.1\times$ - $3.4\times$ reduced training computations on CIFAR-10 with all
but 40, 250 and 4000 labels removed, compared to vanilla FixMatch, while
attaining the same cited state-of-the-art error rate \citep{FixMatch}. Similar
results are achieved for CIFAR-100, SVHN and STL-10. Finally, Fast MixMatch
achieves between $2.6\times$ - $3.3\times$ reduced training computations in
federated SSL tasks and online/streaming learning SSL tasks, which further
demonstrate the generializbility of Fast MixMatch to different scenarios and
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;John Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1"&gt;Chen Dun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03579</id>
        <link href="http://arxiv.org/abs/2309.03579"/>
        <updated>2023-09-09T00:40:34.873Z</updated>
        <summary type="html"><![CDATA[Measuring distance or similarity between time-series data is a fundamental
aspect of many applications including classification and clustering. Existing
measures may fail to capture similarities due to local trends (shapes) and may
even produce misleading results. Our goal is to develop a measure that looks
for similar trends occurring around similar times and is easily interpretable
for researchers in applied domains. This is particularly useful for
applications where time-series have a sequence of meaningful local trends that
are ordered, such as in epidemics (a surge to an increase to a peak to a
decrease). We propose a novel measure, DTW+S, which creates an interpretable
"closeness-preserving" matrix representation of the time-series, where each
column represents local trends, and then it applies Dynamic Time Warping to
compute distances between these matrices. We present a theoretical analysis
that supports the choice of this representation. We demonstrate the utility of
DTW+S in ensemble building and clustering of epidemic curves. We also
demonstrate that our approach results in better classification compared to
Dynamic Time Warping for a class of datasets, particularly when local trends
rather than scale play a decisive role.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Ajitesh Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraPhSyM: Graph Physical Synthesis Model. (arXiv:2308.03944v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.03944</id>
        <link href="http://arxiv.org/abs/2308.03944"/>
        <updated>2023-09-09T00:40:34.829Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) model
for fast and accurate estimation of post-physical synthesis circuit delay and
area metrics from pre-physical synthesis circuit netlists. Once trained,
GraPhSyM provides accurate visibility of final design metrics to early EDA
stages, such as logic synthesis, without running the slow physical synthesis
flow, enabling global co-optimization across stages. Additionally, the swift
and precise feedback provided by GraPhSyM is instrumental for
machine-learning-based EDA optimization frameworks. Given a gate-level netlist
of a circuit represented as a graph, GraPhSyM utilizes graph structure,
connectivity, and electrical property features to predict the impact of
physical synthesis transformations such as buffer insertion and gate sizing.
When trained on a dataset of 6000 prefix adder designs synthesized at an
aggressive delay target, GraPhSyM can accurately predict the post-synthesis
delay (98.3%) and area (96.1%) metrics of unseen adders with a fast 0.22s
inference time. Furthermore, we illustrate the compositionality of GraPhSyM by
employing the model trained on a fixed delay target to accurately anticipate
post-synthesis metrics at a variety of unseen delay targets. Lastly, we report
promising generalization capabilities of the GraPhSyM model when it is
evaluated on circuits different from the adders it was exclusively trained on.
The results show the potential for GraPhSyM to serve as a powerful tool for
advanced optimization techniques and as an oracle for EDA machine learning
frameworks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agiza_A/0/1/0/all/0/1"&gt;Ahmed Agiza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rajarshi Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ene_T/0/1/0/all/0/1"&gt;Teodor Dumitru Ene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godil_S/0/1/0/all/0/1"&gt;Saad Godil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reda_S/0/1/0/all/0/1"&gt;Sherief Reda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A State Representation for Diminishing Rewards. (arXiv:2309.03710v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03710</id>
        <link href="http://arxiv.org/abs/2309.03710"/>
        <updated>2023-09-09T00:40:34.824Z</updated>
        <summary type="html"><![CDATA[A common setting in multitask reinforcement learning (RL) demands that an
agent rapidly adapt to various stationary reward functions randomly sampled
from a fixed distribution. In such situations, the successor representation
(SR) is a popular framework which supports rapid policy evaluation by
decoupling a policy's expected discounted, cumulative state occupancies from a
specific reward function. However, in the natural world, sequential tasks are
rarely independent, and instead reflect shifting priorities based on the
availability and subjective perception of rewarding stimuli. Reflecting this
disjunction, in this paper we study the phenomenon of diminishing marginal
utility and introduce a novel state representation, the $\lambda$
representation ($\lambda$R) which, surprisingly, is required for policy
evaluation in this setting and which generalizes the SR as well as several
other state representations from the literature. We establish the $\lambda$R's
formal properties and examine its normative advantages in the context of
machine learning, as well as its usefulness for studying natural behaviors,
particularly foraging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1"&gt;Ted Moskovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hromadka_S/0/1/0/all/0/1"&gt;Samo Hromadka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touati_A/0/1/0/all/0/1"&gt;Ahmed Touati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1"&gt;Diana Borsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahani_M/0/1/0/all/0/1"&gt;Maneesh Sahani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.09096</id>
        <link href="http://arxiv.org/abs/2203.09096"/>
        <updated>2023-09-09T00:40:34.824Z</updated>
        <summary type="html"><![CDATA[The ability to predict the future trajectory of a patient is a key step
toward the development of therapeutics for complex diseases such as Alzheimer's
disease (AD). However, most machine learning approaches developed for
prediction of disease progression are either single-task or single-modality
models, which can not be directly adopted to our setting involving multi-task
learning with high dimensional images. Moreover, most of those approaches are
trained on a single dataset (i.e. cohort), which can not be generalized to
other cohorts. We propose a novel multimodal multi-task deep learning model to
predict AD progression by analyzing longitudinal clinical and neuroimaging data
from multiple cohorts. Our proposed model integrates high dimensional MRI
features from a 3D convolutional neural network with other data modalities,
including clinical and demographic information, to predict the future
trajectory of patients. Our model employs an adversarial loss to alleviate the
study-specific imaging bias, in particular the inter-study domain shifts. In
addition, a Sharpness-Aware Minimization (SAM) optimization technique is
applied to further improve model generalization. The proposed model is trained
and tested on various datasets in order to evaluate and validate the results.
Our results showed that 1) our model yields significant improvement over the
baseline models, and 2) models using extracted neuroimaging features from 3D
convolutional neural network outperform the same models when applied to
MRI-derived volumetric features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hashemifar_S/0/1/0/all/0/1"&gt;Somaye Hashemifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iriondo_C/0/1/0/all/0/1"&gt;Claudia Iriondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casey_E/0/1/0/all/0/1"&gt;Evan Casey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hejrati_M/0/1/0/all/0/1"&gt;Mohsen Hejrati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Initiative_f/0/1/0/all/0/1"&gt;for Alzheimer&amp;#x27;s Disease Neuroimaging Initiative&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Learning of Intrusion Responses through Recursive Decomposition. (arXiv:2309.03292v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2309.03292</id>
        <link href="http://arxiv.org/abs/2309.03292"/>
        <updated>2023-09-09T00:40:34.822Z</updated>
        <summary type="html"><![CDATA[We study automated intrusion response for an IT infrastructure and formulate
the interaction between an attacker and a defender as a partially observed
stochastic game. To solve the game we follow an approach where attack and
defense strategies co-evolve through reinforcement learning and self-play
toward an equilibrium. Solutions proposed in previous work prove the
feasibility of this approach for small infrastructures but do not scale to
realistic scenarios due to the exponential growth in computational complexity
with the infrastructure size. We address this problem by introducing a method
that recursively decomposes the game into subgames which can be solved in
parallel. Applying optimal stopping theory we show that the best response
strategies in these subgames exhibit threshold structures, which allows us to
compute them efficiently. To solve the decomposed game we introduce an
algorithm called Decompositional Fictitious Self-Play (DFSP), which learns Nash
equilibria through stochastic approximation. We evaluate the learned strategies
in an emulation environment where real intrusions and response actions can be
executed. The results show that the learned strategies approximate an
equilibrium and that DFSP significantly outperforms a state-of-the-art
algorithm for a realistic infrastructure configuration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hammar_K/0/1/0/all/0/1"&gt;Kim Hammar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stadler_R/0/1/0/all/0/1"&gt;Rolf Stadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-SDE: Learning effective reduced dynamics from data-driven stochastic dynamical systems. (arXiv:2205.04151v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.04151</id>
        <link href="http://arxiv.org/abs/2205.04151"/>
        <updated>2023-09-09T00:40:34.791Z</updated>
        <summary type="html"><![CDATA[Multiscale stochastic dynamical systems have been widely adopted to
scientific and engineering problems due to their capability of depicting
complex phenomena in many real world applications. This work is devoted to
investigating the effective reduced dynamics for a slow-fast stochastic
dynamical system. Given observation data on a short-term period satisfying some
unknown slow-fast stochastic system, we propose a novel algorithm including a
neural network called Auto-SDE to learn invariant slow manifold. Our approach
captures the evolutionary nature of a series of time-dependent autoencoder
neural networks with the loss constructed from a discretized stochastic
differential equation. Our algorithm is also proved to be accurate, stable and
effective through numerical experiments under various evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lingyu Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1"&gt;Ting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dai_M/0/1/0/all/0/1"&gt;Min Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinqiao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation. (arXiv:2309.03238v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03238</id>
        <link href="http://arxiv.org/abs/2309.03238"/>
        <updated>2023-09-09T00:40:34.786Z</updated>
        <summary type="html"><![CDATA[Emotion recognition is a complex task due to the inherent subjectivity in
both the perception and production of emotions. The subjectivity of emotions
poses significant challenges in developing accurate and robust computational
models. This thesis examines critical facets of emotion recognition, beginning
with the collection of diverse datasets that account for psychological factors
in emotion production.

To handle the challenge of non-representative training data, this work
collects the Multimodal Stressed Emotion dataset, which introduces controlled
stressors during data collection to better represent real-world influences on
emotion production. To address issues with label subjectivity, this research
comprehensively analyzes how data augmentation techniques and annotation
schemes impact emotion perception and annotator labels. It further handles
natural confounding variables and variations by employing adversarial networks
to isolate key factors like stress from learned emotion representations during
model training. For tackling concerns about leakage of sensitive demographic
variables, this work leverages adversarial learning to strip sensitive
demographic information from multimodal encodings. Additionally, it proposes
optimized sociological evaluation metrics aligned with cost-effective,
real-world needs for model testing.

This research advances robust, practical emotion recognition through
multifaceted studies of challenges in datasets, labels, modeling, demographic
and membership variable encoding in representations, and evaluation. The
groundwork has been laid for cost-effective, generalizable emotion recognition
models that are less likely to encode sensitive demographic information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_M/0/1/0/all/0/1"&gt;Mimansa Jaiswal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2309.02539</id>
        <link href="http://arxiv.org/abs/2309.02539"/>
        <updated>2023-09-09T00:40:34.781Z</updated>
        <summary type="html"><![CDATA[Cinematic audio source separation is a relatively new subtask of audio source
separation, with the aim of extracting the dialogue stem, the music stem, and
the effects stem from their mixture. In this work, we developed a model
generalizing the Bandsplit RNN for any complete or overcomplete partitions of
the frequency axis. Psycho-acoustically motivated frequency scales were used to
inform the band definitions which are now defined with redundancy for more
reliable feature extraction. A loss function motivated by the signal-to-noise
ratio and the sparsity-promoting property of the 1-norm was proposed. We
additionally exploit the information-sharing property of a common-encoder setup
to reduce computational complexity during both training and inference, improve
separation performance for hard-to-generalize classes of sounds, and allow
flexibility during inference time with easily detachable decoders. Our best
model sets the state of the art on the Divide and Remaster dataset with
performance above the ideal ratio mask for the dialogue stem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1"&gt;Karn N. Watcharasupat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chih-Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yiwei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Orife_I/0/1/0/all/0/1"&gt;Iroro Orife&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hipple_A/0/1/0/all/0/1"&gt;Aaron J. Hipple&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Williams_P/0/1/0/all/0/1"&gt;Phillip A. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kramer_S/0/1/0/all/0/1"&gt;Scott Kramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lerch_A/0/1/0/all/0/1"&gt;Alexander Lerch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wolcott_W/0/1/0/all/0/1"&gt;William Wolcott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.06555</id>
        <link href="http://arxiv.org/abs/2307.06555"/>
        <updated>2023-09-09T00:40:34.773Z</updated>
        <summary type="html"><![CDATA[This paper explores the expressive power of deep neural networks for a
diverse range of activation functions. An activation function set $\mathscr{A}$
is defined to encompass the majority of commonly used activation functions,
such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$,
$\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$,
$\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$,
$\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and
$\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in
\mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be
approximated to arbitrary precision by a $\varrho$-activated network of width
$4N$ and depth $2L$ on any bounded set. This finding enables the extension of
most approximation results achieved with $\mathtt{ReLU}$ networks to a wide
variety of other activation functions, at the cost of slightly larger
constants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jianfeng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hongkai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03851</id>
        <link href="http://arxiv.org/abs/2309.03851"/>
        <updated>2023-09-09T00:40:34.769Z</updated>
        <summary type="html"><![CDATA[Survival analysis is a valuable tool for estimating the time until specific
events, such as death or cancer recurrence, based on baseline observations.
This is particularly useful in healthcare to prognostically predict clinically
important events based on patient data. However, existing approaches often have
limitations; some focus only on ranking patients by survivability, neglecting
to estimate the actual event time, while others treat the problem as a
classification task, ignoring the inherent time-ordered structure of the
events. Furthermore, the effective utilization of censored samples - training
data points where the exact event time is unknown - is essential for improving
the predictive accuracy of the model. In this paper, we introduce CenTime, a
novel approach to survival analysis that directly estimates the time to event.
Our method features an innovative event-conditional censoring mechanism that
performs robustly even when uncensored data is scarce. We demonstrate that our
approach forms a consistent estimator for the event model parameters, even in
the absence of uncensored data. Furthermore, CenTime is easily integrated with
deep learning models with no restrictions on batch size or the number of
uncensored samples. We compare our approach with standard survival analysis
methods, including the Cox proportional-hazard model and DeepHit. Our results
indicate that CenTime offers state-of-the-art performance in predicting
time-to-death while maintaining comparable ranking performance. Our
implementation is publicly available at
https://github.com/ahmedhshahin/CenTime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahin_A/0/1/0/all/0/1"&gt;Ahmed H. Shahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1"&gt;An Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitehead_A/0/1/0/all/0/1"&gt;Alexander C. Whitehead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1"&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1"&gt;Joseph Jacob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1"&gt;David Barber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.00904</id>
        <link href="http://arxiv.org/abs/2308.00904"/>
        <updated>2023-09-09T00:40:34.765Z</updated>
        <summary type="html"><![CDATA[Causal inference plays a vital role in diverse domains like epidemiology,
healthcare, and economics. De-confounding and counterfactual prediction in
observational data has emerged as a prominent concern in causal inference
research. While existing models tackle observed confounders, the presence of
unobserved confounders remains a significant challenge, distorting causal
inference and impacting counterfactual outcome accuracy. To address this, we
propose a novel variational learning model of unobserved confounders for
counterfactual inference (VLUCI), which generates the posterior distribution of
unobserved confounders. VLUCI relaxes the unconfoundedness assumption often
overlooked by most causal inference methods. By disentangling observed and
unobserved confounders, VLUCI constructs a doubly variational inference model
to approximate the distribution of unobserved confounders, which are used for
inferring more accurate counterfactual outcomes. Extensive experiments on
synthetic and semi-synthetic datasets demonstrate VLUCI's superior performance
in inferring unobserved confounders. It is compatible with state-of-the-art
counterfactual inference models, significantly improving inference accuracy at
both group and individual levels. Additionally, VLUCI provides confidence
intervals for counterfactual outcomes, aiding decision-making in risk-sensitive
domains. We further clarify the considerations when applying VLUCI to cases
where unobserved confounders don't strictly conform to our model assumptions
using the public IHDP dataset as an example, highlighting the practical
advantages of VLUCI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yonghe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Siwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yun Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Huiyan Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiFaReli: Diffusion Face Relighting. (arXiv:2304.09479v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.09479</id>
        <link href="http://arxiv.org/abs/2304.09479"/>
        <updated>2023-09-09T00:40:34.758Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to single-view face relighting in the wild.
Handling non-diffuse effects, such as global illumination or cast shadows, has
long been a challenge in face relighting. Prior work often assumes Lambertian
surfaces, simplified lighting models or involves estimating 3D shape, albedo,
or a shadow map. This estimation, however, is error-prone and requires many
training examples with lighting ground truth to generalize well. Our work
bypasses the need for accurate estimation of intrinsic components and can be
trained solely on 2D images without any light stage data, multi-view images, or
lighting ground truth. Our key idea is to leverage a conditional diffusion
implicit model (DDIM) for decoding a disentangled light encoding along with
other encodings related to 3D shape and facial identity inferred from
off-the-shelf estimators. We also propose a novel conditioning technique that
eases the modeling of the complex interaction between light and geometry by
using a rendered shading reference to spatially modulate the DDIM. We achieve
state-of-the-art performance on standard benchmark Multi-PIE and can
photorealistically relight in-the-wild images. Please visit our page:
https://diffusion-face-relighting.github.io]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ponglertnapakorn_P/0/1/0/all/0/1"&gt;Puntawat Ponglertnapakorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tritrong_N/0/1/0/all/0/1"&gt;Nontawat Tritrong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1"&gt;Supasorn Suwajanakorn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach. (arXiv:2309.03223v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2309.03223</id>
        <link href="http://arxiv.org/abs/2309.03223"/>
        <updated>2023-09-09T00:40:34.720Z</updated>
        <summary type="html"><![CDATA[One of the most common things that a genealogist is tasked with is the
gathering of a person's initial family history, normally via in-person
interviews or with the use of a platform such as ancestry.com, as this can
provide a strong foundation upon which a genealogist may build. However, the
ability to conduct these interviews can often be hindered by both geographical
constraints and the technical proficiency of the interviewee, as the
interviewee in these types of interviews is most often an elderly person with a
lower than average level of technical proficiency. With this in mind, this
study presents what we believe, based on prior research, to be the first
chatbot geared entirely towards the gathering of family histories, and explores
the viability of utilising such a chatbot by comparing the performance and
usability of such a method with the aforementioned alternatives. With a
chatbot-based approach, we show that, though the average time taken to conduct
an interview may be longer than if the user had used ancestry.com or
participated in an in-person interview, the number of mistakes made and the
level of confusion from the user regarding the UI and process required is lower
than the other two methods. Note that the final metric regarding the user's
confusion is not applicable for the in-person interview sessions due to its
lack of a UI. With refinement, we believe this use of a chatbot could be a
valuable tool for genealogists, especially when dealing with interviewees who
are based in other countries where it is not possible to conduct an in-person
interview.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drumm_K/0/1/0/all/0/1"&gt;Kieron Drumm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vincent Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural lasso: a unifying approach of lasso and neural networks. (arXiv:2309.03770v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03770</id>
        <link href="http://arxiv.org/abs/2309.03770"/>
        <updated>2023-09-09T00:40:34.712Z</updated>
        <summary type="html"><![CDATA[In recent years, there is a growing interest in combining techniques
attributed to the areas of Statistics and Machine Learning in order to obtain
the benefits of both approaches. In this article, the statistical technique
lasso for variable selection is represented through a neural network. It is
observed that, although both the statistical approach and its neural version
have the same objective function, they differ due to their optimization. In
particular, the neural version is usually optimized in one-step using a single
validation set, while the statistical counterpart uses a two-step optimization
based on cross-validation. The more elaborated optimization of the statistical
method results in more accurate parameter estimation, especially when the
training set is small. For this reason, a modification of the standard approach
for training neural networks, that mimics the statistical framework, is
proposed. During the development of the above modification, a new optimization
algorithm for identifying the significant variables emerged. Experimental
results, using synthetic and real data sets, show that this new optimization
algorithm achieves better performance than any of the three previous
optimization approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1"&gt;David Delgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Curbelo_E/0/1/0/all/0/1"&gt;Ernesto Curbelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carreras_D/0/1/0/all/0/1"&gt;Danae Carreras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Models of human preference for learning reward functions. (arXiv:2206.02231v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.02231</id>
        <link href="http://arxiv.org/abs/2206.02231"/>
        <updated>2023-09-09T00:40:34.710Z</updated>
        <summary type="html"><![CDATA[The utility of reinforcement learning is limited by the alignment of reward
functions with the interests of human stakeholders. One promising method for
alignment is to learn the reward function from human-generated preferences
between pairs of trajectory segments, a type of reinforcement learning from
human feedback (RLHF). These human preferences are typically assumed to be
informed solely by partial return, the sum of rewards along each segment. We
find this assumption to be flawed and propose modeling human preferences
instead as informed by each segment's regret, a measure of a segment's
deviation from optimal decision-making. Given infinitely many preferences
generated according to regret, we prove that we can identify a reward function
equivalent to the reward function that generated those preferences, and we
prove that the previous partial return model lacks this identifiability
property in multiple contexts. We empirically show that our proposed regret
preference model outperforms the partial return preference model with finite
training data in otherwise the same setting. Additionally, we find that our
proposed regret preference model better predicts real human preferences and
also learns reward functions from these preferences that lead to policies that
are better human-aligned. Overall, this work establishes that the choice of
preference model is impactful, and our proposed regret preference model
provides an improvement upon a core assumption of recent research. We have open
sourced our experimental code, the human preferences dataset we gathered, and
our training and preference elicitation interfaces for gathering a such a
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knox_W/0/1/0/all/0/1"&gt;W. Bradley Knox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hatgis_Kessell_S/0/1/0/all/0/1"&gt;Stephane Hatgis-Kessell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Booth_S/0/1/0/all/0/1"&gt;Serena Booth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allievi_A/0/1/0/all/0/1"&gt;Alessandro Allievi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Internet Explorer: Targeted Representation Learning on the Open Web. (arXiv:2302.14051v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.14051</id>
        <link href="http://arxiv.org/abs/2302.14051"/>
        <updated>2023-09-09T00:40:34.710Z</updated>
        <summary type="html"><![CDATA[Modern vision models typically rely on fine-tuning general-purpose models
pre-trained on large, static datasets. These general-purpose models only
capture the knowledge within their pre-training datasets, which are tiny,
out-of-date snapshots of the Internet -- where billions of images are uploaded
each day. We suggest an alternate approach: rather than hoping our static
datasets transfer to our desired tasks after large-scale pre-training, we
propose dynamically utilizing the Internet to quickly train a small-scale model
that does extremely well on the task at hand. Our approach, called Internet
Explorer, explores the web in a self-supervised manner to progressively find
relevant examples that improve performance on a desired target dataset. It
cycles between searching for images on the Internet with text queries,
self-supervised training on downloaded images, determining which images were
useful, and prioritizing what to search for next. We evaluate Internet Explorer
across several datasets and show that it outperforms or matches CLIP oracle
performance by using just a single GPU desktop to actively query the Internet
for 30--40 hours. Results, visualizations, and videos at
https://internet-explorer-ssl.github.io/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Alexander C. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_E/0/1/0/all/0/1"&gt;Ellis Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1"&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1"&gt;Deepak Pathak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-policy Evaluation in Doubly Inhomogeneous Environments. (arXiv:2306.08719v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.08719</id>
        <link href="http://arxiv.org/abs/2306.08719"/>
        <updated>2023-09-09T00:40:34.697Z</updated>
        <summary type="html"><![CDATA[This work aims to study off-policy evaluation (OPE) under scenarios where two
key reinforcement learning (RL) assumptions -- temporal stationarity and
individual homogeneity are both violated. To handle the ``double
inhomogeneities", we propose a class of latent factor models for the reward and
observation transition functions, under which we develop a general OPE
framework that consists of both model-based and model-free approaches. To our
knowledge, this is the first paper that develops statistically sound OPE
methods in offline RL with double inhomogeneities. It contributes to a deeper
understanding of OPE in environments, where standard RL assumptions are not
met, and provides several practical approaches in these settings. We establish
the theoretical properties of the proposed value estimators and empirically
show that our approach outperforms competing methods that ignore either
temporal nonstationarity or individual heterogeneity. Finally, we illustrate
our method on a data set from the Medical Information Mart for Intensive Care.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bian_Z/0/1/0/all/0/1"&gt;Zeyu Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengchun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhengling Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03847</id>
        <link href="http://arxiv.org/abs/2309.03847"/>
        <updated>2023-09-09T00:40:34.692Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating mixtures of Gaussians under the constraint
of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4
\log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a
mixture of $k$ Gaussians up to total variation distance $\alpha$ while
satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample
complexity upper bound for the problem that does not make any structural
assumptions on the GMMs.

To solve the problem, we devise a new framework which may be useful for other
tasks. On a high level, we show that if a class of distributions (such as
Gaussians) is (1) list decodable and (2) admits a "locally small'' cover
[BKSW19] with respect to total variation distance, then the class of its
mixtures is privately learnable. The proof circumvents a known barrier
indicating that, unlike Gaussians, GMMs do not admit a locally small cover
[AAL21].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Afzali_M/0/1/0/all/0/1"&gt;Mohammad Afzali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ashtiani_H/0/1/0/all/0/1"&gt;Hassan Ashtiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liaw_C/0/1/0/all/0/1"&gt;Christopher Liaw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tutorial on the Non-Asymptotic Theory of System Identification. (arXiv:2309.03873v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2309.03873</id>
        <link href="http://arxiv.org/abs/2309.03873"/>
        <updated>2023-09-09T00:40:34.680Z</updated>
        <summary type="html"><![CDATA[This tutorial serves as an introduction to recently developed non-asymptotic
methods in the theory of -- mainly linear -- system identification. We
emphasize tools we deem particularly useful for a range of problems in this
domain, such as the covering technique, the Hanson-Wright Inequality and the
method of self-normalized martingales. We then employ these tools to give
streamlined proofs of the performance of various least-squares based estimators
for identifying the parameters in autoregressive models. We conclude by
sketching out how the ideas presented herein can be extended to certain
nonlinear identification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ziemann_I/0/1/0/all/0/1"&gt;Ingvar Ziemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsiamis_A/0/1/0/all/0/1"&gt;Anastasios Tsiamis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_B/0/1/0/all/0/1"&gt;Bruce Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jedra_Y/0/1/0/all/0/1"&gt;Yassir Jedra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Matni_N/0/1/0/all/0/1"&gt;Nikolai Matni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v1 [q-fin.TR])]]></title>
        <id>http://arxiv.org/abs/2309.03202</id>
        <link href="http://arxiv.org/abs/2309.03202"/>
        <updated>2023-09-09T00:40:34.614Z</updated>
        <summary type="html"><![CDATA[This work seeks to answer key research questions regarding the viability of
reinforcement learning over the S&P 500 index. The on-policy techniques of
Value Iteration (VI) and State-action-reward-state-action (SARSA) are
implemented along with the off-policy technique of Q-Learning. The models are
trained and tested on a dataset comprising multiple years of stock market data
from 2000-2023. The analysis presents the results and findings from training
and testing the models using two different time periods: one including the
COVID-19 pandemic years and one excluding them. The results indicate that
including market data from the COVID-19 period in the training dataset leads to
superior performance compared to the baseline strategies. During testing, the
on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the
influence of bias-variance tradeoff and the generalization capabilities of
simpler policies. However, it is noted that the performance of Q-learning may
vary depending on the stability of future market conditions. Future work is
suggested, including experiments with updated Q-learning policies during
testing and trading diverse individual stocks. Additionally, the exploration of
alternative economic indicators for training the models is proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Khare_I/0/1/0/all/0/1"&gt;Ishan S. Khare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Martheswaran_T/0/1/0/all/0/1"&gt;Tarun K. Martheswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Dassanaike_Perera_A/0/1/0/all/0/1"&gt;Akshana Dassanaike-Perera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Ezekiel_J/0/1/0/all/0/1"&gt;Jonah B. Ezekiel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Feature Learning under Structured Data. (arXiv:2309.03843v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03843</id>
        <link href="http://arxiv.org/abs/2309.03843"/>
        <updated>2023-09-09T00:40:34.612Z</updated>
        <summary type="html"><![CDATA[Recent works have demonstrated that the sample complexity of gradient-based
learning of single index models, i.e. functions that depend on a 1-dimensional
projection of the input data, is governed by their information exponent.
However, these results are only concerned with isotropic data, while in
practice the input often contains additional structure which can implicitly
guide the algorithm. In this work, we investigate the effect of a spiked
covariance structure and reveal several interesting phenomena. First, we show
that in the anisotropic setting, the commonly used spherical gradient dynamics
may fail to recover the true direction, even when the spike is perfectly
aligned with the target direction. Next, we show that appropriate weight
normalization that is reminiscent of batch normalization can alleviate this
issue. Further, by exploiting the alignment between the (spiked) input
covariance and the target, we obtain improved sample complexity compared to the
isotropic case. In particular, under the spiked model with a suitably large
spike, the sample complexity of gradient-based training can be made independent
of the information exponent while also outperforming lower bounds for
rotationally invariant kernel methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mousavi_Hosseini_A/0/1/0/all/0/1"&gt;Alireza Mousavi-Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_D/0/1/0/all/0/1"&gt;Denny Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation Layer that Lets the Student Decide. (arXiv:2309.02843v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2309.02843</id>
        <link href="http://arxiv.org/abs/2309.02843"/>
        <updated>2023-09-09T00:40:34.528Z</updated>
        <summary type="html"><![CDATA[Typical technique in knowledge distillation (KD) is regularizing the learning
of a limited capacity model (student) by pushing its responses to match a
powerful model's (teacher). Albeit useful especially in the penultimate layer
and beyond, its action on student's feature transform is rather implicit,
limiting its practice in the intermediate layers. To explicitly embed the
teacher's knowledge in feature transform, we propose a learnable KD layer for
the student which improves KD with two distinct abilities: i) learning how to
leverage the teacher's knowledge, enabling to discard nuisance information, and
ii) feeding forward the transferred knowledge deeper. Thus, the student enjoys
the teacher's knowledge during the inference besides training. Formally, we
repurpose 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each
local region according to the template (supervised by the teacher) that the
corresponding region of the student matches. To facilitate template learning in
the intermediate layers, we propose a novel form of supervision based on the
teacher's decisions. Through rigorous experimentation, we demonstrate the
effectiveness of our approach on 3 popular classification benchmarks. Code is
available at: https://github.com/adagorgun/letKD-framework]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gorgun_A/0/1/0/all/0/1"&gt;Ada Gorgun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurbuz_Y/0/1/0/all/0/1"&gt;Yeti Z. Gurbuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1"&gt;A. Aydin Alatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trinary Decision Trees for missing value handling. (arXiv:2309.03561v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03561</id>
        <link href="http://arxiv.org/abs/2309.03561"/>
        <updated>2023-09-09T00:40:34.375Z</updated>
        <summary type="html"><![CDATA[This paper introduces the Trinary decision tree, an algorithm designed to
improve the handling of missing data in decision tree regressors and
classifiers. Unlike other approaches, the Trinary decision tree does not assume
that missing values contain any information about the response. Both
theoretical calculations on estimator bias and numerical illustrations using
real data sets are presented to compare its performance with established
algorithms in different missing data scenarios (Missing Completely at Random
(MCAR), and Informative Missingness (IM)). Notably, the Trinary tree
outperforms its peers in MCAR settings, especially when data is only missing
out-of-sample, while lacking behind in IM settings. A hybrid model, the
TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes
(MIA) approach, shows robust performance in all types of missingness. Despite
the potential drawback of slower training speed, the Trinary tree offers a
promising and more accurate method of handling missing data in decision tree
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zakrisson_H/0/1/0/all/0/1"&gt;Henning Zakrisson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copula Representations and Error Surface Projections for the Exclusive Or Problem. (arXiv:1907.04483v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.04483</id>
        <link href="http://arxiv.org/abs/1907.04483"/>
        <updated>2023-09-09T00:40:34.363Z</updated>
        <summary type="html"><![CDATA[The exclusive or (xor) function is one of the simplest examples that
illustrate why nonlinear feedforward networks are superior to linear regression
for machine learning applications. We review the xor representation and
approximation problems and discuss their solutions in terms of probabilistic
logic and associative copula functions. After briefly reviewing the
specification of feedforward networks, we compare the dynamics of learned error
surfaces with different activation functions such as RELU and tanh through a
set of colorful three-dimensional charts. The copula representations extend xor
from Boolean to real values, thereby providing a convenient way to demonstrate
the concept of cross-validation on in-sample and out-sample data sets. Our
approach is pedagogical and is meant to be a machine learning prolegomenon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freedman_R/0/1/0/all/0/1"&gt;Roy S. Freedman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal thinking for decision making on Electronic Health Records: why and how. (arXiv:2308.01605v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.01605</id>
        <link href="http://arxiv.org/abs/2308.01605"/>
        <updated>2023-09-09T00:40:34.315Z</updated>
        <summary type="html"><![CDATA[Accurate predictions, as with machine learning, may not suffice to provide
optimal healthcare for every patient. Indeed, prediction can be driven by
shortcuts in the data, such as racial biases. Causal thinking is needed for
data-driven decisions. Here, we give an introduction to the key elements,
focusing on routinely-collected data, electronic health records (EHRs) and
claims data. Using such data to assess the value of an intervention requires
care: temporal dependencies and existing practices easily confound the causal
effect. We present a step-by-step framework to help build valid decision making
from real-life patient records by emulating a randomized trial before
individualizing decisions, eg with machine learning. Our framework highlights
the most important pitfalls and considerations in analysing EHRs or claims data
to draw causal conclusions. We illustrate the various choices in studying the
effect of albumin on sepsis mortality in the Medical Information Mart for
Intensive Care database (MIMIC-IV). We study the impact of various choices at
every step, from feature extraction to causal-estimator selection. In a
tutorial spirit, the code and the data are openly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Doutreligne_M/0/1/0/all/0/1"&gt;Matthieu Doutreligne&lt;/a&gt; (SODA), &lt;a href="http://arxiv.org/find/stat/1/au:+Struja_T/0/1/0/all/0/1"&gt;Tristan Struja&lt;/a&gt; (MIT, USZ), &lt;a href="http://arxiv.org/find/stat/1/au:+Abecassis_J/0/1/0/all/0/1"&gt;Judith Abecassis&lt;/a&gt; (SODA), &lt;a href="http://arxiv.org/find/stat/1/au:+Morgand_C/0/1/0/all/0/1"&gt;Claire Morgand&lt;/a&gt; (ARS IDF), &lt;a href="http://arxiv.org/find/stat/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo Anthony Celi&lt;/a&gt; (MIT), &lt;a href="http://arxiv.org/find/stat/1/au:+Varoquaux_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;l Varoquaux&lt;/a&gt; (SODA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03800</id>
        <link href="http://arxiv.org/abs/2309.03800"/>
        <updated>2023-09-09T00:40:34.209Z</updated>
        <summary type="html"><![CDATA[This work investigates the nuanced algorithm design choices for deep learning
in the presence of computational-statistical gaps. We begin by considering
offline sparse parity learning, a supervised classification problem which
admits a statistical query lower bound for gradient-based training of a
multilayer perceptron. This lower bound can be interpreted as a multi-resource
tradeoff frontier: successful learning can only occur if one is sufficiently
rich (large model), knowledgeable (large dataset), patient (many training
iterations), or lucky (many random guesses). We show, theoretically and
experimentally, that sparse initialization and increasing network width yield
significant improvements in sample efficiency in this setting. Here, width
plays the role of parallel search: it amplifies the probability of finding
"lottery ticket" neurons, which learn sparse features more sample-efficiently.
Finally, we show that the synthetic sparse parity task can be useful as a proxy
for real problems requiring axis-aligned feature learning. We demonstrate
improved sample efficiency on tabular classification benchmarks by using wide,
sparsely-initialized MLP models; these networks sometimes outperform tuned
random forests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edelman_B/0/1/0/all/0/1"&gt;Benjamin L. Edelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1"&gt;Eran Malach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cyril Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medoid Silhouette clustering with automatic cluster number selection. (arXiv:2309.03751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03751</id>
        <link href="http://arxiv.org/abs/2309.03751"/>
        <updated>2023-09-09T00:40:34.189Z</updated>
        <summary type="html"><![CDATA[The evaluation of clustering results is difficult, highly dependent on the
evaluated data set and the perspective of the beholder. There are many
different clustering quality measures, which try to provide a general measure
to validate clustering results. A very popular measure is the Silhouette. We
discuss the efficient medoid-based variant of the Silhouette, perform a
theoretical analysis of its properties, provide two fast versions for the
direct optimization, and discuss the use to choose the optimal number of
clusters. We combine ideas from the original Silhouette with the well-known PAM
algorithm and its latest improvements FasterPAM. One of the versions guarantees
equal results to the original variant and provides a run speedup of $O(k^2)$.
In experiments on real data with 30000 samples and $k$=100, we observed a
10464$\times$ speedup compared to the original PAMMEDSIL algorithm.
Additionally, we provide a variant to choose the optimal number of clusters
directly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lenssen_L/0/1/0/all/0/1"&gt;Lars Lenssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences. (arXiv:2309.03791v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03791</id>
        <link href="http://arxiv.org/abs/2309.03791"/>
        <updated>2023-09-09T00:40:34.185Z</updated>
        <summary type="html"><![CDATA[We introduce the $ARMOR_D$ methods as novel approaches to enhancing the
adversarial robustness of deep learning models. These methods are based on a
new class of optimal-transport-regularized divergences, constructed via an
infimal convolution between an information divergence and an optimal-transport
(OT) cost. We use these as tools to enhance adversarial robustness by
maximizing the expected loss over a neighborhood of distributions, a technique
known as distributionally robust optimization. Viewed as a tool for
constructing adversarial samples, our method allows samples to be both
transported, according to the OT cost, and re-weighted, according to the
information divergence. We demonstrate the effectiveness of our method on
malware detection and image recognition applications and find that, to our
knowledge, it outperforms existing methods at enhancing the robustness against
adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$
against $FGSM$ and $98.18\%$ against $PGD^{40}$ on the MNIST dataset, reducing
the error rate by more than $19.7\%$ and $37.2\%$ respectively compared to
prior methods. Similarly, in malware detection, a discrete (binary) data
domain, $ARMOR_D$ improves the robustified accuracy under $rFGSM^{50}$ attack
compared to the previous best-performing adversarial training methods by
$37.0\%$ while lowering false negative and false positive rates by $51.1\%$ and
$57.53\%$, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Birrell_J/0/1/0/all/0/1"&gt;Jeremiah Birrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1"&gt;Mohammadreza Ebrahimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03665</id>
        <link href="http://arxiv.org/abs/2309.03665"/>
        <updated>2023-09-09T00:40:34.037Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks dramatically change the output of an otherwise accurate
learning system using a seemingly inconsequential modification to a piece of
input data. Paradoxically, empirical evidence indicates that even systems which
are robust to large random perturbations of the input data remain susceptible
to small, easily constructed, adversarial perturbations of their inputs. Here,
we show that this may be seen as a fundamental feature of classifiers working
with high dimensional input data. We introduce a simple generic and
generalisable framework for which key behaviours observed in practical systems
arise with high probability -- notably the simultaneous susceptibility of the
(otherwise accurate) model to easily constructed adversarial attacks, and
robustness to random perturbations of the input data. We confirm that the same
phenomena are directly observed in practical neural networks trained on
standard image classification problems, where even large additive random noise
fails to trigger the adversarial instability of the network. A surprising
takeaway is that even small margins separating a classifier's decision surface
from training and testing data can hide adversarial susceptibility from being
detected using randomly sampled perturbations. Counterintuitively, using
additive noise during training or testing is therefore inefficient for
eradicating or detecting adversarial examples, and more demanding adversarial
training is required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_O/0/1/0/all/0/1"&gt;Oliver J. Sutton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qinghua Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1"&gt;Ivan Y. Tyukin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1"&gt;Alexander N. Gorban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastounis_A/0/1/0/all/0/1"&gt;Alexander Bastounis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1"&gt;Desmond J. Higham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification. (arXiv:2301.04838v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.04838</id>
        <link href="http://arxiv.org/abs/2301.04838"/>
        <updated>2023-09-09T00:40:33.789Z</updated>
        <summary type="html"><![CDATA[Time series classification is an important data mining task that has received
a lot of interest in the past two decades. Due to the label scarcity in
practice, semi-supervised time series classification with only a few labeled
samples has become popular. Recently, Similarity-aware Time Series
Classification (SimTSC) is proposed to address this problem by using a graph
neural network classification model on the graph generated from pairwise
Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy
and outperforms state-of-the-art deep learning models in several few-label
settings. However, since SimTSC relies on pairwise DTW distances, the quadratic
complexity of DTW limits its usability to only reasonably sized datasets. To
address this challenge, we propose a new efficient semi-supervised time series
classification technique, LB-SimTSC, with a new graph construction module.
Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to
approximate the dissimilarity between instances in linear time, while retaining
the relative proximity relationships one would have obtained via computing DTW.
We construct the pairwise distance matrix using LB_Keogh and build a graph for
the graph neural network. We apply this approach to the ten largest datasets
from the well-known UCR time series classification archive. The results
demonstrate that this approach can be up to 104x faster than SimTSC when
constructing the graph on large datasets without significantly decreasing
classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xi_W/0/1/0/all/0/1"&gt;Wenjie Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Arnav Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jessica Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.03227</id>
        <link href="http://arxiv.org/abs/2309.03227"/>
        <updated>2023-09-09T00:40:33.769Z</updated>
        <summary type="html"><![CDATA[Drug repositioning-a promising strategy for discovering new therapeutic uses
for existing drugs-has been increasingly explored in the computational science
literature using biomedical databases. However, the technological potential of
drug repositioning candidates has often been overlooked. This study presents a
novel protocol to comprehensively analyse various sources such as
pharmaceutical patents and biomedical databases, and identify drug
repositioning candidates with both technological potential and scientific
evidence. To this end, first, we constructed a scientific biomedical knowledge
graph (s-BKG) comprising relationships between drugs, diseases, and genes
derived from biomedical databases. Our protocol involves identifying drugs that
exhibit limited association with the target disease but are closely located in
the s-BKG, as potential drug candidates. We constructed a patent-informed
biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information.
Finally, we developed a graph embedding protocol to ascertain the structure of
the p-BKG, thereby calculating the relevance scores of those candidates with
target disease-related patents to evaluate their technological potential. Our
case study on Alzheimer's disease demonstrates its efficacy and feasibility,
while the quantitative outcomes and systematic methods are expected to bridge
the gap between computational discoveries and successful market applications in
drug repositioning research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jegal_Y/0/1/0/all/0/1"&gt;Yongseung Jegal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaewoong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jiho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Ki-Su Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Janghyeok Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insights Into the Inner Workings of Transformer Models for Protein Function Prediction. (arXiv:2309.03631v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03631</id>
        <link href="http://arxiv.org/abs/2309.03631"/>
        <updated>2023-09-09T00:40:33.762Z</updated>
        <summary type="html"><![CDATA[Motivation: We explored how explainable AI (XAI) can help to shed light into
the inner workings of neural networks for protein function prediction, by
extending the widely used XAI method of integrated gradients such that latent
representations inside of transformer models, which were finetuned to Gene
Ontology term and Enzyme Commission number prediction, can be inspected too.
Results: The approach enabled us to identify amino acids in the sequences that
the transformers pay particular attention to, and to show that these relevant
sequence parts reflect expectations from biology and chemistry, both in the
embedding layer and inside of the model, where we identified transformer heads
with a statistically significant correspondence of attribution maps with ground
truth sequence annotations (e.g., transmembrane regions, active sites) across
many proteins. Availability and Implementation: Source code can be accessed at
https://github.com/markuswenzel/xai-proteins .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wenzel_M/0/1/0/all/0/1"&gt;Markus Wenzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruner_E/0/1/0/all/0/1"&gt;Erik Gr&amp;#xfc;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1"&gt;Nils Strodthoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03847</id>
        <link href="http://arxiv.org/abs/2309.03847"/>
        <updated>2023-09-09T00:40:33.755Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating mixtures of Gaussians under the constraint
of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4
\log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a
mixture of $k$ Gaussians up to total variation distance $\alpha$ while
satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample
complexity upper bound for the problem that does not make any structural
assumptions on the GMMs.

To solve the problem, we devise a new framework which may be useful for other
tasks. On a high level, we show that if a class of distributions (such as
Gaussians) is (1) list decodable and (2) admits a "locally small'' cover
[BKSW19] with respect to total variation distance, then the class of its
mixtures is privately learnable. The proof circumvents a known barrier
indicating that, unlike Gaussians, GMMs do not admit a locally small cover
[AAL21].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Afzali_M/0/1/0/all/0/1"&gt;Mohammad Afzali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ashtiani_H/0/1/0/all/0/1"&gt;Hassan Ashtiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liaw_C/0/1/0/all/0/1"&gt;Christopher Liaw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification. (arXiv:2309.03544v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2309.03544</id>
        <link href="http://arxiv.org/abs/2309.03544"/>
        <updated>2023-09-09T00:40:33.748Z</updated>
        <summary type="html"><![CDATA[Rising urban populations have led to a surge in vehicle use and made traffic
monitoring and management indispensable. Acoustic traffic monitoring (ATM)
offers a cost-effective and efficient alternative to more computationally
expensive methods of monitoring traffic such as those involving computer vision
technologies. In this paper, we present MVD and MVDA: two open datasets for the
development of acoustic traffic monitoring and vehicle-type classification
algorithms, which contain audio recordings of moving vehicles. The dataset
contain four classes- Trucks, Cars, Motorbikes, and a No-vehicle class.
Additionally, we propose a novel and efficient way to accurately classify these
acoustic signals using cepstrum and spectrum based local and global audio
features, and a multi-input neural network. Experimental results show that our
methodology improves upon the established baselines of previous works and
achieves an accuracy of 91.98% and 96.66% on MVD and MVDA Datasets,
respectively. Finally, the proposed model was deployed through an Android
application to make it accessible for testing and demonstrate its efficacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashhad_M/0/1/0/all/0/1"&gt;Mohd Ashhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_O/0/1/0/all/0/1"&gt;Omar Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambat_S/0/1/0/all/0/1"&gt;Sooraj K. Ambat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haq_Z/0/1/0/all/0/1"&gt;Zeeshan Ali Haq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Mansaf Alam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffDefense: Defending against Adversarial Attacks via Diffusion Models. (arXiv:2309.03702v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03702</id>
        <link href="http://arxiv.org/abs/2309.03702"/>
        <updated>2023-09-09T00:40:33.729Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel reconstruction method that leverages Diffusion
Models to protect machine learning classifiers against adversarial attacks, all
without requiring any modifications to the classifiers themselves. The
susceptibility of machine learning models to minor input perturbations renders
them vulnerable to adversarial attacks. While diffusion-based methods are
typically disregarded for adversarial defense due to their slow reverse
process, this paper demonstrates that our proposed method offers robustness
against adversarial threats while preserving clean accuracy, speed, and
plug-and-play compatibility. Code at:
https://github.com/HondamunigePrasannaSilva/DiffDefence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1"&gt;Hondamunige Prasanna Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1"&gt;Lorenzo Seidenari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.12191</id>
        <link href="http://arxiv.org/abs/2201.12191"/>
        <updated>2023-09-09T00:40:33.720Z</updated>
        <summary type="html"><![CDATA[The representation space of neural models for textual data emerges in an
unsupervised manner during training. Understanding how those representations
encode human-interpretable concepts is a fundamental problem. One prominent
approach for the identification of concepts in neural representations is
searching for a linear subspace whose erasure prevents the prediction of the
concept from the representations. However, while many linear erasure algorithms
are tractable and interpretable, neural networks do not necessarily represent
concepts in a linear manner. To identify non-linearly encoded concepts, we
propose a kernelization of a linear minimax game for concept erasure. We
demonstrate that it is possible to prevent specific non-linear adversaries from
predicting the concept. However, the protection does not transfer to different
nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded
concept remains an open problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retail store customer behavior analysis system: Design and Implementation. (arXiv:2309.03232v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03232</id>
        <link href="http://arxiv.org/abs/2309.03232"/>
        <updated>2023-09-09T00:40:33.714Z</updated>
        <summary type="html"><![CDATA[Understanding customer behavior in retail stores plays a crucial role in
improving customer satisfaction by adding personalized value to services.
Behavior analysis reveals both general and detailed patterns in the interaction
of customers with a store items and other people, providing store managers with
insight into customer preferences. Several solutions aim to utilize this data
by recognizing specific behaviors through statistical visualization. However,
current approaches are limited to the analysis of small customer behavior sets,
utilizing conventional methods to detect behaviors. They do not use deep
learning techniques such as deep neural networks, which are powerful methods in
the field of computer vision. Furthermore, these methods provide limited
figures when visualizing the behavioral data acquired by the system. In this
study, we propose a framework that includes three primary parts: mathematical
modeling of customer behaviors, behavior analysis using an efficient deep
learning based system, and individual and group behavior visualization. Each
module and the entire system were validated using data from actual situations
in a retail store.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tuan Dinh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hihara_K/0/1/0/all/0/1"&gt;Keisuke Hihara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Tung Cao Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Utada_Y/0/1/0/all/0/1"&gt;Yumeka Utada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torii_A/0/1/0/all/0/1"&gt;Akihiko Torii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izumi_N/0/1/0/all/0/1"&gt;Naoki Izumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thuy_N/0/1/0/all/0/1"&gt;Nguyen Thanh Thuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1"&gt;Long Quoc Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PGFed: Personalize Each Client's Global Objective for Federated Learning. (arXiv:2212.01448v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.01448</id>
        <link href="http://arxiv.org/abs/2212.01448"/>
        <updated>2023-09-09T00:40:33.702Z</updated>
        <summary type="html"><![CDATA[Personalized federated learning has received an upsurge of attention due to
the mediocre performance of conventional federated learning (FL) over
heterogeneous data. Unlike conventional FL which trains a single global
consensus model, personalized FL allows different models for different clients.
However, existing personalized FL algorithms only implicitly transfer the
collaborative knowledge across the federation by embedding the knowledge into
the aggregated model or regularization. We observed that this implicit
knowledge transfer fails to maximize the potential of each client's empirical
risk toward other clients. Based on our observation, in this work, we propose
Personalized Global Federated Learning (PGFed), a novel personalized FL
framework that enables each client to personalize its own global objective by
explicitly and adaptively aggregating the empirical risks of itself and other
clients. To avoid massive (O(N^2)) communication overhead and potential privacy
leakage while achieving this, each client's risk is estimated through a
first-order approximation for other clients' adaptive risk aggregation. On top
of PGFed, we develop a momentum upgrade, dubbed PGFedMo, to more efficiently
utilize clients' empirical risks. Our extensive experiments on four datasets
under different federated settings show consistent improvements of PGFed over
previous state-of-the-art methods. The code is publicly available at
https://github.com/ljaiverson/pgfed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1"&gt;Matias Mendieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shandong Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-SDE: Learning effective reduced dynamics from data-driven stochastic dynamical systems. (arXiv:2205.04151v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2205.04151</id>
        <link href="http://arxiv.org/abs/2205.04151"/>
        <updated>2023-09-09T00:40:33.688Z</updated>
        <summary type="html"><![CDATA[Multiscale stochastic dynamical systems have been widely adopted to
scientific and engineering problems due to their capability of depicting
complex phenomena in many real world applications. This work is devoted to
investigating the effective reduced dynamics for a slow-fast stochastic
dynamical system. Given observation data on a short-term period satisfying some
unknown slow-fast stochastic system, we propose a novel algorithm including a
neural network called Auto-SDE to learn invariant slow manifold. Our approach
captures the evolutionary nature of a series of time-dependent autoencoder
neural networks with the loss constructed from a discretized stochastic
differential equation. Our algorithm is also proved to be accurate, stable and
effective through numerical experiments under various evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lingyu Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1"&gt;Ting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dai_M/0/1/0/all/0/1"&gt;Min Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinqiao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03886</id>
        <link href="http://arxiv.org/abs/2309.03886"/>
        <updated>2023-09-09T00:40:33.663Z</updated>
        <summary type="html"><![CDATA[Labeling neural network submodules with human-legible descriptions is useful
for many downstream tasks: such descriptions can surface failures, guide
interventions, and perhaps even explain important model behaviors. To date,
most mechanistic descriptions of trained networks have involved small models,
narrowly delimited phenomena, and large amounts of human labor. Labeling all
human-interpretable sub-computations in models of increasing size and
complexity will almost certainly require tools that can generate and validate
descriptions automatically. Recently, techniques that use learned models
in-the-loop for labeling have begun to gain traction, but methods for
evaluating their efficacy are limited and ad-hoc. How should we validate and
compare open-ended labeling tools? This paper introduces FIND (Function
INterpretation and Description), a benchmark suite for evaluating the building
blocks of automated interpretability methods. FIND contains functions that
resemble components of trained neural networks, and accompanying descriptions
of the kind we seek to generate. The functions are procedurally constructed
across textual and numeric domains, and involve a range of real-world
complexities, including noise, composition, approximation, and bias. We
evaluate new and existing methods that use language models (LMs) to produce
code-based and language descriptions of function behavior. We find that an
off-the-shelf LM augmented with only black-box access to functions can
sometimes infer their structure, acting as a scientist by forming hypotheses,
proposing experiments, and updating descriptions in light of new data. However,
LM-based descriptions tend to capture global function behavior and miss local
corruptions. These results show that FIND will be useful for characterizing the
performance of more sophisticated interpretability methods before they are
applied to real-world models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1"&gt;Sarah Schwettmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1"&gt;Tamar Rott Shaham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1"&gt;Joanna Materzynska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1"&gt;Neil Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1"&gt;David Bau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1"&gt;Antonio Torralba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early warning via transitions in latent stochastic dynamical systems. (arXiv:2309.03842v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03842</id>
        <link href="http://arxiv.org/abs/2309.03842"/>
        <updated>2023-09-09T00:40:33.650Z</updated>
        <summary type="html"><![CDATA[Early warnings for dynamical transitions in complex systems or
high-dimensional observation data are essential in many real world
applications, such as gene mutation, brain diseases, natural disasters,
financial crises, and engineering reliability. To effectively extract early
warning signals, we develop a novel approach: the directed anisotropic
diffusion map that captures the latent evolutionary dynamics in low-dimensional
manifold. Applying the methodology to authentic electroencephalogram (EEG)
data, we successfully find the appropriate effective coordinates, and derive
early warning signals capable of detecting the tipping point during the state
transition. Our method bridges the latent dynamics with the original dataset.
The framework is validated to be accurate and effective through numerical
experiments, in terms of density and transition probability. It is shown that
the second coordinate holds meaningful information for critical transition in
various evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lingyu Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gao_T/0/1/0/all/0/1"&gt;Ting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wang Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinqiao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2309.03450</id>
        <link href="http://arxiv.org/abs/2309.03450"/>
        <updated>2023-09-09T00:40:33.641Z</updated>
        <summary type="html"><![CDATA[Large Language Models (LLMs) have become ubiquitous across various domains,
transforming the way we interact with information and conduct research.
However, most high-performing LLMs remain confined behind proprietary walls,
hindering scientific progress. Most open-source LLMs, on the other hand, are
limited in their ability to support longer sequence lengths, which is a key
requirement for many tasks that require inference over an input context. To
address this, we have trained XGen, a series of 7B parameter models on up to 8K
sequence length for up to 1.5T tokens. We have also finetuned the XGen models
on public-domain instructional data, creating their instruction-tuned
counterparts (XGen-Inst). We open-source our models for both research
advancements and commercial applications. Our evaluation on standard benchmarks
shows that XGen models achieve comparable or better results when compared with
state-of-the-art open-source LLMs. Our targeted evaluation on long sequence
modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence
open-source LLMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1"&gt;Erik Nijkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tian Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1"&gt;Hiroaki Hayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1"&gt;Congying Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1"&gt;Chen Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1"&gt;Jesse Vig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1"&gt;Semih Yavuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1"&gt;Philippe Laban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_B/0/1/0/all/0/1"&gt;Ben Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1"&gt;Senthil Purushwalkam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1"&gt;Tong Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1"&gt;Wojciech Kry&amp;#x15b;ci&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1"&gt;Lidiya Murakhovs&amp;#x27;ka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1"&gt;Prafulla Kumar Choubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1"&gt;Alex Fabbri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1"&gt;Lifu Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1"&gt;Meghana Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chien-Sheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation. (arXiv:2309.03827v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03827</id>
        <link href="http://arxiv.org/abs/2309.03827"/>
        <updated>2023-09-09T00:40:33.632Z</updated>
        <summary type="html"><![CDATA[High Dynamic Range (HDR) content creation has become an important topic for
modern media and entertainment sectors, gaming and Augmented/Virtual Reality
industries. Many methods have been proposed to recreate the HDR counterparts of
input Low Dynamic Range (LDR) images/videos given a single exposure or
multi-exposure LDRs. The state-of-the-art methods focus primarily on the
preservation of the reconstruction's structural similarity and the pixel-wise
accuracy. However, these conventional approaches do not emphasize preserving
the artistic intent of the images in terms of human visual perception, which is
an essential element in media, entertainment and gaming. In this paper, we
attempt to study and fill this gap. We propose an architecture called
ArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDR
features as input. Experimental results show that ArtHDR-Net can achieve
state-of-the-art performance in terms of the HDR-VDP-2 score (i.e., mean
opinion score index) while reaching competitive performance in terms of PSNR
and SSIM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1"&gt;Hrishav Bakul Barua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnasamy_G/0/1/0/all/0/1"&gt;Ganesh Krishnasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;KokSheik Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanov_K/0/1/0/all/0/1"&gt;Kalin Stefanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1"&gt;Abhinav Dhall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments. (arXiv:2309.03537v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2309.03537</id>
        <link href="http://arxiv.org/abs/2309.03537"/>
        <updated>2023-09-09T00:40:33.613Z</updated>
        <summary type="html"><![CDATA[In this work, we proposed a novel and general method to construct tight
frames on graphs with compact supports based on a series of hierarchical
partitions. Starting from our abstract construction that generalizes previous
methods based on partition trees, we are able to flexibly incorporate subgraph
Laplacians into our design of graph frames. Consequently, our general methods
permit adjusting the (subgraph) vanishing moments of the framelets and extra
properties, such as directionality, for efficiently representing graph signals
with path-like supports. Several variants are explicitly defined and tested.
Experimental results show our proposed graph frames perform superiorly in
non-linear approximation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Ruigang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiaosheng Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Theory Applications in Advanced Geospatial Research. (arXiv:2309.03249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03249</id>
        <link href="http://arxiv.org/abs/2309.03249"/>
        <updated>2023-09-09T00:40:33.606Z</updated>
        <summary type="html"><![CDATA[Geospatial sciences include a wide range of applications, from environmental
monitoring transportation to infrastructure planning, as well as location-based
analysis and services. Graph theory algorithms in mathematics have emerged as
indispensable tools in these domains due to their capability to model and
analyse spatial relationships efficiently. This technical report explores the
applications of graph theory algorithms in geospatial sciences, highlighting
their role in network analysis, spatial connectivity, geographic information
systems, and various other spatial problem-solving scenarios. It provides a
comprehensive idea about the key concepts and algorithms of graph theory that
assist the modelling processes. The report provides insights into the practical
significance of graph theory in addressing real-world geospatial challenges and
opportunities. It lists the extensive research, innovative technologies and
methodologies implemented in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Surajit Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallick_A/0/1/0/all/0/1"&gt;Archita Mallick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Anuva Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1"&gt;Kounik De Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications. (arXiv:2309.03837v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2309.03837</id>
        <link href="http://arxiv.org/abs/2309.03837"/>
        <updated>2023-09-09T00:40:33.596Z</updated>
        <summary type="html"><![CDATA[Multi-task learning (MTL) is a powerful approach in deep learning that
leverages the information from multiple tasks during training to improve model
performance. In medical imaging, MTL has shown great potential to solve various
tasks. However, existing MTL architectures in medical imaging are limited in
sharing information across tasks, reducing the potential performance
improvements of MTL. In this study, we introduce a novel attention-based MTL
framework to better leverage inter-task interactions for various tasks from
pixel-level to image-level predictions. Specifically, we propose a Cross-Task
Attention Network (CTAN) which utilizes cross-task attention mechanisms to
incorporate information by interacting across tasks. We validated CTAN on four
medical imaging datasets that span different domains and tasks including:
radiation treatment planning prediction using planning CT images of two
different target cancers (Prostate, OpenKBP); pigmented skin lesion
segmentation and diagnosis using dermatoscopic images (HAM10000); and COVID-19
diagnosis and severity prediction using chest CT scans (STOIC). Our study
demonstrates the effectiveness of CTAN in improving the accuracy of medical
imaging tasks. Compared to standard single-task learning (STL), CTAN
demonstrated a 4.67% improvement in performance and outperformed both widely
used MTL baselines: hard parameter sharing (HPS) with an average performance
improvement of 3.22%; and multi-task attention network (MTAN) with a relative
decrease of 5.38%. These findings highlight the significance of our proposed
MTL framework in solving medical imaging tasks and its potential to improve
their accuracy across domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sangwook Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purdie_T/0/1/0/all/0/1"&gt;Thomas G. Purdie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McIntosh_C/0/1/0/all/0/1"&gt;Chris McIntosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2309.03557</id>
        <link href="http://arxiv.org/abs/2309.03557"/>
        <updated>2023-09-09T00:40:33.581Z</updated>
        <summary type="html"><![CDATA[Multiagent systems aim to accomplish highly complex learning tasks through
decentralised consensus seeking dynamics and their use has garnered a great
deal of attention in the signal processing and computational intelligence
societies. This article examines the behaviour of multiagent networked systems
with nonlinear filtering/learning dynamics. To this end, a general formulation
for the actions of an agent in multiagent networked systems is presented and
conditions for achieving a cohesive learning behaviour is given. Importantly,
application of the so derived framework in distributed and federated learning
scenarios are presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Talebi_S/0/1/0/all/0/1"&gt;Sayed Pouria Talebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mandic_D/0/1/0/all/0/1"&gt;Danilo Mandic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2309.03219</id>
        <link href="http://arxiv.org/abs/2309.03219"/>
        <updated>2023-09-09T00:40:33.572Z</updated>
        <summary type="html"><![CDATA[Knowledge graph (KG) embedding has been used to benefit the diagnosis of
animal diseases by analyzing electronic medical records (EMRs), such as notes
and veterinary records. However, learning representations to capture entities
and relations with literal information in KGs is challenging as the KGs show
heterogeneous properties and various types of literal information. Meanwhile,
the existing methods mostly aim to preserve graph structures surrounding target
nodes without considering different types of literals, which could also carry
significant information. In this paper, we propose a knowledge graph embedding
model for the efficient diagnosis of animal diseases, which could learn various
types of literal information and graph structure and fuse them into unified
representations, namely LiteralKG. Specifically, we construct a knowledge graph
that is built from EMRs along with literal information collected from various
animal hospitals. We then fuse different types of entities and node feature
information into unified vector representations through gate networks. Finally,
we propose a self-supervised learning task to learn graph structure in pretext
tasks and then towards various downstream tasks. Experimental results on link
prediction tasks demonstrate that our model outperforms the baselines that
consist of state-of-the-art models. The source code is available at
https://github.com/NSLab-CUK/LiteralKG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_V/0/1/0/all/0/1"&gt;Van Thuy Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1"&gt;Sang Thanh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangmyeong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jooho Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Luong Vuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_O/0/1/0/all/0/1"&gt;O-Joun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System. (arXiv:2309.03246v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2309.03246</id>
        <link href="http://arxiv.org/abs/2309.03246"/>
        <updated>2023-09-09T00:40:33.541Z</updated>
        <summary type="html"><![CDATA[The Cancer Registry of Norway (CRN) collects information on cancer patients
by receiving cancer messages from different medical entities (e.g., medical
labs, and hospitals) in Norway. Such messages are validated by an automated
cancer registry system: GURI. Its correct operation is crucial since it lays
the foundation for cancer research and provides critical cancer-related
statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT)
for GURI can facilitate various experiments and advanced analyses of the
operational state of GURI without requiring intensive interactions with the
real system. However, GURI constantly evolves due to novel medical diagnostics
and treatment, technological advances, etc. Accordingly, CCDT should evolve as
well to synchronize with GURI. A key challenge of achieving such
synchronization is that evolving CCDT needs abundant data labelled by the new
GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the
CCDT developed for the previous version of GURI as the pretrained model and
fine-tunes it with the dataset labelled by querying a new GURI version.
EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer
messages from a candidate dataset and query GURI with it. We evaluate
EvoCLINICAL on three evolution processes. The precision, recall, and F1 score
are all greater than 91%, demonstrating the effectiveness of EvoCLINICAL.
Furthermore, we replace the active learning part of EvoCLINICAL with random
selection to study the contribution of transfer learning to the overall
performance of EvoCLINICAL. Results show that employing active learning in
EvoCLINICAL increases its performances consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chengjie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qinghua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1"&gt;Tao Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1"&gt;Shaukat Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwitalla_T/0/1/0/all/0/1"&gt;Thomas Schwitalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nygaard_J/0/1/0/all/0/1"&gt;Jan F. Nyg&amp;#xe5;rd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private median and more]]></title>
        <id>http://blog.research.google/2023/09/differentially-private-median-and-more.html</id>
        <link href="http://blog.research.google/2023/09/differentially-private-median-and-more.html"/>
        <updated>2023-09-08T22:59:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Edith Cohen and Uri Stemmer, Research Scientists, Google Research





Differential privacy (DP) is a rigorous mathematical definition of privacy. DP algorithms are randomized to protect user data by ensuring that the probability of any particular output is nearly unchanged when a data point is added or removed. Therefore, the output of a DP algorithm does not disclose the presence of any one data point. There has been significant progress in both foundational research and adoption of differential privacy with contributions such as the Privacy Sandbox and Google Open Source Library.


ML and data analytics algorithms can often be described as performing multiple basic computation steps on the same dataset. When each such step is differentially private, so is the output, but with â€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instacart boosts AI capacity, readies for IPO with OpenAI's ChatGPT-powered eCommerce search]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16dmxxn/instacart_boosts_ai_capacity_readies_for_ipo_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16dmxxn/instacart_boosts_ai_capacity_readies_for_ipo_with/"/>
        <updated>2023-09-08T21:33:08.000Z</updated>
        <summary type="html"><![CDATA[On the verge of its IPO, Instacart has introduced major AI-powered features to its Storefront platform and the smart Caper Carts. Main upgrades: conversational search powered by OpenAI's ChatGPT and inbuilt AI models.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/olqtxvwjo3nb1.png?width=750&format=png&auto=webp&s=d8eaefbb9865c51732efc2792ec386610ecd38e6
 AI advancements in Instacart's infrastructure
  
Instacart, which holds approximately 22% of the $132 billion US online grocery-delivery market, has been leaning more towards being a tech platform.
 The new Instacart Storefront, entailing features driven by 150 proprietary AI models, is built on the same core infrastructure as the Instacart app.
 Customers can engage in open-ended searches on retailers' storefronts via the search bar.
  
AI upgrades in Caper Carts
  
AI-powered Caper Carts by Instacart have been upgraded. Customers can now order directly from their Caper Cart and get informed when their orders are ready.
 Camera and weight sensor efficiency is enhanced thanks to improved AI models, ensuring a smoother shopping journey and providing an extra layer of security against suspicious activity.
  
(source)
 P.S. If you want this kind of analysis, delve into the latest updates in AI with our free newsletter, already favored by professionals from Google, Meta, and OpenAI.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] MLOps for Vercel OpenAI chatbot infrastructure]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dluho/p_mlops_for_vercel_openai_chatbot_infrastructure/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dluho/p_mlops_for_vercel_openai_chatbot_infrastructure/"/>
        <updated>2023-09-08T20:52:08.000Z</updated>
        <summary type="html"><![CDATA[I used infrastructure as code (IaC) to provision and deploy Vercel's next-openai example. IaC is useful because it applies the same rigor of application code development to infrastructure provisioning. Instead of manual point and click in a cloud console which can be unrepeatable or error-prone, you just store and change all infrastructure configurations as code in source control .
 This example uses Pulumi which allows you to write the IaC in Python.
 https://github.com/aaronkao/vercel-py-openai-chatbot
    submitted by    /u/kao-pulumi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Do You Not Use Open Source LLMs? (Or do you?) [D] (Repost because I made a mistake in the title)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dlac2/why_do_you_not_use_open_source_llms_or_do_you_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dlac2/why_do_you_not_use_open_source_llms_or_do_you_d/"/>
        <updated>2023-09-08T20:30:56.000Z</updated>
        <summary type="html"><![CDATA[Reposting because I intended to ask about LLMs, not AI in general, and forgot that I don't need to dumb down the terminology for this sub. 
 Thanks to the people who pointed out that mistake.
 --- original post ---
 This is something I'm curious about. I've seen a few people declaring that they're not using open source LLMs because they're GPU-poor, because the models aren't good enough, because the uis/frontends are hard to get started with, etc., and I've been wondering how much these comments and posts reflect the opinions and needs of the community as a whole. So, here's a poll. Answer away if you feel like it. 
 I'm sharing this on a few other subs too (for the sake of greater information gathering) so please don't vote more than once.
 If your reasoning is not on here, feel free to comment your thoughts. If more than one option describes you, please select the one that describes you the most.
 View Poll
    submitted by    /u/Heralax_Tekran  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are good resources for creating NLP algorithms from scratch?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dl71t/d_what_are_good_resources_for_creating_nlp/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dl71t/d_what_are_good_resources_for_creating_nlp/"/>
        <updated>2023-09-08T20:27:19.000Z</updated>
        <summary type="html"><![CDATA[I'm looking to learn more about concurrency/parallelism, optimization, data structures and algorithms from an NLP perspective.
    submitted by    /u/Al_Miksiki  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Please Help - Machine Learning (ML) Engineers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dkvx8/d_please_help_machine_learning_ml_engineers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dkvx8/d_please_help_machine_learning_ml_engineers/"/>
        <updated>2023-09-08T20:15:12.000Z</updated>
        <summary type="html"><![CDATA[Hello Everyone, 
 I'm currently exploring the idea of a solution tailored for ML engineers and technologists. While I have a background in recruiting, I've often found myself dissatisfied with the typical recruitment process. It seems that many recruiters don't always appreciate the importance of working with candidates or understand the impact on people's livelihoods and careers. 
 What I'm proposing is the creation of a career representation firm specifically designed for purpose-driven technologists specializing in data, product, and hardware careers. This firm would advocate for the career interests of the most passionate ML engineers. 
 Our representation would encompass: 
 - Strategic Career Development: Crafting a strategic approach to help engineers secure opportunities aligned with their desired projects and professional development. 
 - Impact Matching: Identifying and connecting engineers with projects and teams where their technical skills, career goals, and personal interests can have the greatest positive impact, ensuring that your work aligns with your values and aspirations. 
 - Industry Leadership: Positioning you as an industry leader by marketing your expertise and securing speaking engagements at conferences and other events, enhancing your professional visibility and reputation. 
 In return for this representation, engineers would commit to a 3% fee deducted from their salary, which would support the services provided by the firm. 
 Would you be interested in participating in such a service? If not, would you consider recommending it to someone you know? If you are in favor of this idea, what makes you believe it would be advantageous for others even if it might not be your preference? Do you think you could personally benefit from this type of career representation? 
 Thanks! 
    submitted by    /u/Educational_Bar_6352  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI girlfriend ads are flooding Instagram and TikTok]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16dkmr3/ai_girlfriend_ads_are_flooding_instagram_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16dkmr3/ai_girlfriend_ads_are_flooding_instagram_and/"/>
        <updated>2023-09-08T20:05:13.000Z</updated>
        <summary type="html"><![CDATA[Tech startups are running sexually explicit ads for apps promoting not-safe-for-work experiences on platforms like Facebook, Instagram, and TikTok.
 
These ads feature digitally created potential 'girlfriends' with large breasts and tight clothing, and some even use popular children's TV characters to promote 'NSFW pics' apps.
 
NBC News found 35 app developers running sexually explicit ads on Meta-owned apps, and 14 app developers running similar ads on TikTok.
 
The marketing push is part of an AI gold rush, capitalizing on the surge of interest in AI and benefiting from a double standard that hurts real human sex workers.
 
Researchers believe that the gender-based slant in these ads reflects social media platforms allowing sex-related ads only if the intended audience is men.
 
Meta and TikTok have stepped up their removal of sexually explicit AI ads after NBC News contacted them, but questions remain about how the ads got through their filters in the first place.
 
Similar ads also appear in the Apple and Google app stores, although the extent of advertising there is unknown.
 
 Source : https://www.nbcnews.com/tech/social-media/ai-girlfriend-ads-instagram-tiktok-chat-pics-chatgpt-dose-rcna97547
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Algorithm of Thoughts Prompt Engineering Breakdown]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dkmh3/r_algorithm_of_thoughts_prompt_engineering/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dkmh3/r_algorithm_of_thoughts_prompt_engineering/"/>
        <updated>2023-09-08T20:04:56.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2308.10379
 Saw someone else post about this new prompting method on the sub here so I decided to put together a run down and prompt template. 
 Pretty interesting to see the different methods emerge and how some attempt to simulate how code runs. 
 My rundown -> https://www.prompthub.us/blog/how-algorithm-of-thoughts-prompting-works
    submitted by    /u/dancleary544  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI pilot programs look to reduce energy use and emissions on MIT campus]]></title>
        <id>https://news.mit.edu/2023/ai-pilot-programs-look-reduce-energy-use-emissions-mit-campus-0908</id>
        <link href="https://news.mit.edu/2023/ai-pilot-programs-look-reduce-energy-use-emissions-mit-campus-0908"/>
        <updated>2023-09-08T19:40:00.000Z</updated>
        <summary type="html"><![CDATA[A cross-departmental team is leading efforts to utilize machine learning for increased efficiency in heating and cooling MITâ€™s buildings.]]></summary>
        <author>
            <name>Nicole Morell | MIT Office of Sustainability</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can't solve Gymnasium Frozenlake-v1 8x8 with A2C]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16djotx/cant_solve_gymnasium_frozenlakev1_8x8_with_a2c/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16djotx/cant_solve_gymnasium_frozenlakev1_8x8_with_a2c/"/>
        <updated>2023-09-08T19:27:47.000Z</updated>
        <summary type="html"><![CDATA[Hello, I'm trying to solve the Frozenlake-v1 environment with is_slippery = True (non-deterministic) with the stable baselines 3 A2C algorithm. I can solve the 4x4 version but I can't achieve any results with the 8x8 version. I also checked the RL-Zoo to see if there is any hyperparameter tunning about that environment but there is nothing. Which adjustments can I do to make it work properly?
    submitted by    /u/MetallicaSPA  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][D] How to implement Sinusoidal Positional Embedding?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16djeuy/rd_how_to_implement_sinusoidal_positional/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16djeuy/rd_how_to_implement_sinusoidal_positional/"/>
        <updated>2023-09-08T19:16:43.000Z</updated>
        <summary type="html"><![CDATA[Hi fellow computer scientists,
 so I've been researching a little about transformers and meanwhile I had to understand sinusoidal positional embedding. I have found two implementations for this, after testing both approaches I found they compute different embeddings for the same position/timestep with the same embedding dimensions... shouldn't it be equal if the position and embedding dimensions are the same?
 This is getting me confused, because now I don't know which implementation should I consider... Do you have any suggestions to where I can look?
 Thank you :)
    submitted by    /u/Christs_Elite  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[Discussion]"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dj6y1/discussion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dj6y1/discussion/"/>
        <updated>2023-09-08T19:08:11.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, I'm completely new in this field.. I have a research in civil engineering and need to learn python, machine learning and data analysis as short as possible. Where can I achieve that?? please help me by naming the best courses or any free materials availableðŸ™
    submitted by    /u/Ok-Upstairs7749  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Partners With India Giants to Advance AI in Worldâ€™s Most Populous Nation]]></title>
        <id>https://blogs.nvidia.com/?p=66769</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/08/nvidia-india-giants-ai/"/>
        <updated>2023-09-08T17:40:11.000Z</updated>
        <summary type="html"><![CDATA[The worldâ€™s largest democracy is poised to transform itself and the world, embracing AI on an enormous scale. Speaking with the press Friday in Bengaluru, in the context of announcements from two of Indiaâ€™s largest conglomerates, Reliance Industries Limited and Tata Group, NVIDIA founder and CEO Jensen Huang detailed plans to bring AI technology and Read article >]]></summary>
        <author>
            <name>Rohit Biddappa</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI â€” weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16dfyun/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16dfyun/ai_weekly_megathread/"/>
        <updated>2023-09-08T17:01:57.000Z</updated>
        <summary type="html"><![CDATA[News provided by aibrews.com
  
Technology Innovation Institute in Abu Dhabi has released Falcon 180B - a large language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently the largest openly available model, and rivals proprietary models like PaLM-2. Falcon 180B is 2.5 times larger than Llama 2 and was trained with 4x more compute. It is available for both research and commercial use [Details].
 Meta AI released Belebele, a first-of-its-kind multilingual reading comprehension dataset spanning 122 language variants, enabling direct comparison of how well models understand different languages [Details].
 Meta AI has published Code Llamaâ€™s research paper with more information on training, evaluation results and safety [Paper].
 Open Interpreter, an open-source, â€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implement smart document search index with Amazon Textract and Amazon OpenSearch]]></title>
        <id>fe976fb5a78788280176961909b8fbdcfedb0385</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/implement-smart-document-search-index-with-amazon-textract-and-amazon-opensearch/"/>
        <updated>2023-09-08T16:49:31.000Z</updated>
        <summary type="html"><![CDATA[In this post, weâ€™ll take you on a journey to rapidly build and deploy a document search indexing solution that helps your organization to better harness and extract insights from documents. Whether you're in Human Resources looking for specific clauses in employee contracts, or a financial analyst sifting through a mountain of invoices to extract payment data, this solution is tailored to empower you to access the information you need with unprecedented speed and accuracy.]]></summary>
        <author>
            <name>Martin Schade</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic image search for articles using Amazon Rekognition, Amazon SageMaker foundation models, and Amazon OpenSearch Service]]></title>
        <id>32141e7ed8f1b05cfb1c03b7caa1e65b3a3cf005</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/semantic-image-search-for-articles-using-amazon-rekognition-amazon-sagemaker-foundation-models-and-amazon-opensearch-service/"/>
        <updated>2023-09-08T16:38:41.000Z</updated>
        <summary type="html"><![CDATA[Digital publishers are continuously looking for ways to streamline and automate their media workflows in order to generate and publish new content as rapidly as they can. Publishers can have repositories containing millions of images and in order to save money, they need to be able to reuse these images across articles. Finding the image that best matches an article in repositories of this scale can be a time-consuming, repetitive, manual task that can be automated. It also relies on the images in the repository being tagged correctly, which can also be automated (for a customer success story, refer to Aller Media Finds Success with KeyCore and AWS). In this post, we demonstrate how to use Amazon Rekognition, Amazon SageMaker JumpStart, and Amazon OpenSearch Service to solve this business problem.]]></summary>
        <author>
            <name>Mark Watkins</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resources to learn relevant linear algebra]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16df6yl/resources_to_learn_relevant_linear_algebra/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16df6yl/resources_to_learn_relevant_linear_algebra/"/>
        <updated>2023-09-08T16:30:38.000Z</updated>
        <summary type="html"><![CDATA[Hello, I have just started a course on neural networks at college and I have found myself lost on the linear algebra. I have no experience using or learning linear algebra so I am extremely confused about eigenvalue decomposition, single value decomposition, and just matrix stuff in general. 
 I was wondering if you all had any resources to share that would help me to learn the relevant linear algebra for creating neural networks.
 Thank you!
    submitted by    /u/smelliothax  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving asset health and grid resilience using machine learning]]></title>
        <id>28574ca0f68ab93372abddb01f00c7e75a49972b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/improving-asset-health-and-grid-resilience-using-machine-learning/"/>
        <updated>2023-09-08T16:27:38.000Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) is transforming every industry, process, and business, but the path to success is not always straightforward. In this blog post, we demonstrate how Duke Energy, a Fortune 150 company headquartered in Charlotte, NC., collaborated with the AWS Machine Learning Solutions Lab (MLSL) to use computer vision to automate the inspection of wooden utility poles and help prevent power outages, property damage and even injuries.]]></summary>
        <author>
            <name>Travis Bronson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Question answering based on book-summaries]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16delbt/p_question_answering_based_on_booksummaries/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16delbt/p_question_answering_based_on_booksummaries/"/>
        <updated>2023-09-08T16:06:30.000Z</updated>
        <summary type="html"><![CDATA[I'm one of those people who always ask questions about movies because there's something they don't get or have forgotten. Especially with more complex stories, like Game of Thrones. At the moment I'm reading Wheel of Time, a rather long fantasy series. I had the idea to build the following WebApp:
 There is online each chapter of the series summarized separately. So in the WebApp I could ask questions about the content. In addition, I can indicate which chapter I am reading, so that it is ensured not to spoil the user.
 I want to avoid to train a model. I would prefer to use one of the existing open-source models, like llama. A first, primitive idea: give the LLM all the summaries and the user's question. But this would mean to give all summaries as input every time. Not only that this approach would not be elegant, the restriction in the input size (number of words) would make this possibly even impossible.
 Feel free to share your ideas how i could solve this :)
    submitted by    /u/Individual-Cause-616  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help me with creating dataset from .mat files, please]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16dekg9/help_me_with_creating_dataset_from_mat_files/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16dekg9/help_me_with_creating_dataset_from_mat_files/"/>
        <updated>2023-09-08T16:05:37.000Z</updated>
        <summary type="html"><![CDATA[I have so many .mat files in a folder which have two arrays inside each .mat file. that is, for each .mat file, i have a (224*224) array and another (136,1) array. These 224*224 arrays are my X_trains for a model and these corresponding 136*1 arrays are my y_trains (labels). i can read these files as np arrays using scipy's loadmat. My problem is, is there a way to usen tf.data .Dataset object to send these to a model or there is any other way? Also using this tf.data.Dataset can i split into train, test, val data?
    submitted by    /u/likhith-69  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help me with creating dataset from .mat files [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16degay/help_me_with_creating_dataset_from_mat_files_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16degay/help_me_with_creating_dataset_from_mat_files_d/"/>
        <updated>2023-09-08T16:01:32.000Z</updated>
        <summary type="html"><![CDATA[I have so many .mat files in a folder which have two arrays inside each .mat file. that is, for each .mat file, i have a (224*224) array and another (136,1) array. These 224*224 arrays are my X_trains for a model and these corresponding 136*1 arrays are my y_trains (labels). i can read these files as np arrays using scipy's loadmat. My problem is, is there a way to usen tf.data .Dataset object to send these to a model or there is any other way? Also using this tf.data.Dataset can i split into train, test, val data?
    submitted by    /u/likhith-69  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding social biases through the text-to-image generation lens]]></title>
        <id>https://www.microsoft.com/en-us/research/blog/understanding-social-biases-through-the-text-to-image-generation-lens/</id>
        <link href="https://www.microsoft.com/en-us/research/blog/understanding-social-biases-through-the-text-to-image-generation-lens/"/>
        <updated>2023-09-08T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Gender, race, and age disparities in AI-generated images persist. This AIES 2023 study on text-to-image models shows that even basic prompts can lead to underrepresentation, calling for responsible bias mitigation strategies.
The post Understanding social biases through the text-to-image generation lens appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] CLI tool to benchmark 100+LLMs response, response time, cost]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16dea69/p_cli_tool_to_benchmark_100llms_response_response/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16dea69/p_cli_tool_to_benchmark_100llms_response_response/"/>
        <updated>2023-09-08T15:54:49.000Z</updated>
        <summary type="html"><![CDATA[Hi r/MachineLearning, 
 I built a CLI tool to benchmark 100+ LLMs for a given question. Benchmark output allows you to compare responses, response time and cost. Try it here: https://github.com/BerriAI/litellm/blob/main/cookbook/benchmark/readme.md 
 CLI Output:
 Output from CLI Tool
 Simply select your LLMs, enter your API keys, LLM configs and run 
 python3 benchmark.py 
 Happy completion()! 
    submitted by    /u/Comfortable_Dirt5590  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL in games]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16ddudb/rl_in_games/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16ddudb/rl_in_games/"/>
        <updated>2023-09-08T15:37:25.000Z</updated>
        <summary type="html"><![CDATA[Hello guys,
 I was suddenly inspired to make a WH Gladius bot.
 Background: I recently got into the game, it seems very interesting to me personally, but alas, there are not enough guides on it for you to learn how to play at a high level. I donâ€™t intend to spend hundreds of hours to master the base, so I decided to try something like RARL so that this thing would learn, and I could analyze its moves, change the conditions and thus start playing at an intermediate level faster.
 However, a superficial analysis revealed that the game does not have an API at all. Let's say I could grab some stats using Cheat Engine and OllyDbg, but I have no idea how to fit it into the gym. Or does gym as env need to pass a link to the client from the machine so that it only restarts it?
 In general, if anyone has done something similar, I ask for a link to a guide or a similar example.
 All the best
    submitted by    /u/kapedalex  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text summarization [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ddfdb/text_summarization_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ddfdb/text_summarization_p/"/>
        <updated>2023-09-08T15:21:32.000Z</updated>
        <summary type="html"><![CDATA[Hey! If anyone has worked with text summarization before especially with TF-IDF and extractive summarization,kindly please dm me. Hope you have a great day!
    submitted by    /u/Ok-Avocado-5370  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] A look at Appleâ€™s new Transformer-powered predictive text model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16db34q/p_a_look_at_apples_new_transformerpowered/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16db34q/p_a_look_at_apples_new_transformerpowered/"/>
        <updated>2023-09-08T13:46:47.000Z</updated>
        <summary type="html"><![CDATA[In the upcoming versions of macOS and iOS, Apple is including a predictive text model which offers suggestions while you type, which theyâ€™ve said to be a "transformer model". I managed to find some details about this model, including details about its topology and tokenizer, and I was even able to peek in and see several of its top predictions while typing!
 Blogpost: https://jackcook.com/2023/09/08/predictive-text.html
 Source code: https://github.com/jackcook/predictive-spy
 Hopefully this can give some insight into some of the trade-offs that Apple went through to put a model on every iPhone and MacBook â€” itâ€™s small, it has a pretty narrow scope, and itâ€™s not very capable on its own. Let me know what you think!
    submitted by    /u/jackcook  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intern Insights: Dr. Josh Benaloh with Anunay Kulshrestha and Karan Newatia]]></title>
        <id>https://www.microsoft.com/en-us/research/podcast/intern-insights-dr-josh-benaloh-with-anunay-kulshrestha-and-karan-newatia/</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/intern-insights-dr-josh-benaloh-with-anunay-kulshrestha-and-karan-newatia/"/>
        <updated>2023-09-08T13:09:11.000Z</updated>
        <summary type="html"><![CDATA[Every year, interns help advance research at Microsoft. In â€œIntern Insights,â€ PhD students Anunay Kulshrestha and Karan Newatia talk with cryptographer Josh Benaloh about working on the verifiable election technology ElectionGuard.
The post Intern Insights: Dr. Josh Benaloh with Anunay Kulshrestha and Karan Newatia appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AI Beats Hockolicious, Trackmania's Most Prestigious Map]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d93ke/p_ai_beats_hockolicious_trackmanias_most/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d93ke/p_ai_beats_hockolicious_trackmanias_most/"/>
        <updated>2023-09-08T12:20:56.000Z</updated>
        <summary type="html"><![CDATA[Follow-up on our previous post (Vision-based reinforcement learning for Trackmania: close or at superhuman level).
 Many comments rightfully pointed that the map we trained on:
 - lacked difficult features like jumps, airbrakes, drifts, ...
 - had not widely been played by humans
 We have now trained the same AI on the game's most prestigious map: Hockolicious. We also prepared a video describing the approach with much more detail.
 Here is our result :) AI Beats Hockolicious, Trackmania's Most Prestigious Map
 Note: We are still using a convolutional neural network with a structure similar to Nature's DQN paper. I am curious whether other architectures (the ResNet-like in the IMPALA paper ?) could help. Do you have any suggestions on how the neural network's vision head should be structured for that specific task?
    submitted by    /u/Linesight_rl  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Beats Hockolicious, Trackmania's Most Prestigious Map]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16d91ca/ai_beats_hockolicious_trackmanias_most/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16d91ca/ai_beats_hockolicious_trackmanias_most/"/>
        <updated>2023-09-08T12:17:53.000Z</updated>
        <summary type="html"><![CDATA[Follow-up on our previous post (Vision-based reinforcement learning for Trackmania: close or at superhuman level).
 Many comments rightfully pointed that the map we trained on:
 - lacked difficult features like jumps, airbrakes, drifts, ...
 - had not widely been played by humans
 We have now trained the same AI on the game's most prestigious map: Hockolicious. We also prepared a video describing the approach with much more detail.
 Here is our result :) AI Beats Hockolicious, Trackmania's Most Prestigious Map
 Note: We are still using a convolutional neural network with a structure similar to Nature's DQN paper. I am curious whether other architectures (the ResNet-like in the IMPALA paper ?) could help. Do you have any suggestions on how the neural network's vision head should be structured for that specific task?
    submitted by    /u/Linesight_rl  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Animating a 2D image in real time]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16d8tmu/animating_a_2d_image_in_real_time/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16d8tmu/animating_a_2d_image_in_real_time/"/>
        <updated>2023-09-08T12:07:07.000Z</updated>
        <summary type="html"><![CDATA[Hello Everyone,
 i have recently started working on a project, where I need to animate an image of a face in real time to speak sentences. Essentially I am trying to build a face for my own large language model. I know of Nvidia's Audio2Face and Metahuman, but these are all in 3D and take a lot of time rendering the lip and eye animations. I need something, which works only with a bit of latency.
 â€‹
 Does anyone know a service or a repo I could use to animate a 2D picture to speak text?
    submitted by    /u/Fabianslife  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Methodology for counting/segmenting objects in close formations]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d8saq/d_methodology_for_countingsegmenting_objects_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d8saq/d_methodology_for_countingsegmenting_objects_in/"/>
        <updated>2023-09-08T12:05:20.000Z</updated>
        <summary type="html"><![CDATA[Hello all. I'm new to object recognition and instance segmentation.
 I am trying to work on a project in which I use drone imagery to detect objects that are in close formations with each other. I do this for the purpose of counting particular objects, as well as to check if an object has moved (by making a prediction on drone imagery that is taken later).
 Create masks?
 I'm now trying to understand what methodology/models make sense. First of all, should I be looking at creating masks, or do bounding boxes suffice? My idea was that masks are better, since bounding boxes overlap with each other and can miss that an object has moved slightly, Or am I wrong and are masks just an extra hassle? Or shouldn't I be looking at bounding boxes or masks at all?
 MaskRCNN?
 Model-wise, should I be loâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Justifiable sample size]]></title>
        <id>https://www.johndcook.com/blog/?p=207000</id>
        <link href="https://www.johndcook.com/blog/2023/09/08/justifiable-sample-size/"/>
        <updated>2023-09-08T12:01:19.000Z</updated>
        <summary type="html"><![CDATA[One of the most common things a statistician is asked to do is compute a sample. There are well known formulas for this, so why isnâ€™t calculating a sample size trivial? As with most things in statistics, plugging numbers into a formula is not the hard part. The hard part is deciding what numbers to [â€¦]
Justifiable sample size first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noob here - question about learning an image transformation function]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16d7gud/noob_here_question_about_learning_an_image/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16d7gud/noob_here_question_about_learning_an_image/"/>
        <updated>2023-09-08T10:59:32.000Z</updated>
        <summary type="html"><![CDATA[Suppose that we have a function f(I) that transforms the an RGB image I of size WxH in another RGB image O of size WxH (one example of f could be RGB to gray scale conversion, where O is such that for every pixel i, Ri=Gi=Bi). Suppose that the function f requires seconds of computations on an average PC. My goal is to understand if a neural network can learn f and be faster than f itself, given the fact that a training dataset of pairs (Ii, Oi) (in the thousands or even in the millions) is easy to create. What type of neural network is better suited for this job?
    submitted by    /u/lukeboh  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Chains and Agents]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d7ee6/d_chains_and_agents/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d7ee6/d_chains_and_agents/"/>
        <updated>2023-09-08T10:55:55.000Z</updated>
        <summary type="html"><![CDATA[I think there's a lot of confusion around AI agents today and it's mainly because of lack of definition and using the wrong terminology.
 We've been talking to many companies who are claiming they're working on agents but when you look under the hood, they are really just chains.
 I just listened to the Latent Space pod with Harrison Chase (Founder of Langchain) and I really liked how he thinks about chains vs agents.
 Chains: sequence of tasks in a more rigid order, where you have more control, more predictability.
 Agents: handling the edge-cases, the long-tail of things that can happen.
 And the most important thing is that it's not an OR question but an AND one: you can use them in the same application by starting with chains -> figuring our the edge-cases -> using agents to deal with them.
 https://preview.redd.it/l59sc4sri0nb1.png?width=3127&format=png&auto=webp&s=1f3f8730c48687eaabf1f554deb181cf35b96036
    submitted by    /u/BootstrapGuy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Question for Jensen Huang]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d5n4m/d_question_for_jensen_huang/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d5n4m/d_question_for_jensen_huang/"/>
        <updated>2023-09-08T09:17:40.000Z</updated>
        <summary type="html"><![CDATA[I have the opportunity to see Jensen speak in the next month at a semi private event, 250-300 people. I will probably have the opportunity to ask him a question. What would you ask him?
    submitted by    /u/Zealousideal-Food285  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Object detection in 3D]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d5abe/d_object_detection_in_3d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d5abe/d_object_detection_in_3d/"/>
        <updated>2023-09-08T08:54:50.000Z</updated>
        <summary type="html"><![CDATA[Greetings, people. My colleague told me about some methods of object detection/classification on 3D models, and now I'm exploring them. But during my research I couldn't find that much information about them. I would like to ask you to provide me information, literature and examples of application for them. I remember that one of the techniques is called voxelization. But still not able to find great and intuitive example. 
 Would be thankful for any information :)
    submitted by    /u/thattallsoldier  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What object detection and segmentation model repos do you folks use for production]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d4x9m/d_what_object_detection_and_segmentation_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d4x9m/d_what_object_detection_and_segmentation_model/"/>
        <updated>2023-09-08T08:32:49.000Z</updated>
        <summary type="html"><![CDATA[Looking at all the popular yolo repos, v5, v7,v8, yolo-nas, all of them seem to have restrictive licenses (gpl3, agpl, apache 2) where the trained model files also falls under the license. What do people usually use to deploy detection/segmentation in production, especially with resource constraints (can't use something like fast-rcnn)
    submitted by    /u/Appropriate_Bear_894  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Proper use of ai-voice-cloning / rvc / tortoise]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d4x9l/d_proper_use_of_aivoicecloning_rvc_tortoise/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d4x9l/d_proper_use_of_aivoicecloning_rvc_tortoise/"/>
        <updated>2023-09-08T08:32:49.000Z</updated>
        <summary type="html"><![CDATA[Hey guys! I need some help here.. many days trying to get good results but without success. So I already have the voice I want to use (edited with uvr5 and it sounds really great, without any echo or noise on the background), I trained it on aivc so that I can generate this voice verbalizing the text content I need. I used high quality - it took like 40min to generate each phrase - and it is ok, but still a little robotic. So I installed RVC and trained a model with the original voice (edited with the uvr5) just like I did the training on aivc. So I loaded the trained model on the inference tab and I selected the audio to be processed - the generated audio files from aivc. Even selecting the harvest mode, the output was worse than the generated files from aivc. I even tried to record my own voice speaking the text but it does not sound good. My trained model on rvc has 500 epochs, and it may be a very good model to use, yet idk what Iâ€™m doing wrong. Maybe Iâ€™m misusing rvc, so what I need is to improve the realism of my aivc(or tortoise) generated voices, simple as that, is rvc the best option to do this? If yes, how? Any help please would be much appreciated thanks!
    submitted by    /u/JustSayin_thatuknow  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free AI transforms text and images into amazing videos - Pika Labs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16d3q5z/free_ai_transforms_text_and_images_into_amazing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16d3q5z/free_ai_transforms_text_and_images_into_amazing/"/>
        <updated>2023-09-08T07:19:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Would ChatGPT work to help with looking for WFH jobs?/changing careers?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16d3lxl/would_chatgpt_work_to_help_with_looking_for_wfh/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16d3lxl/would_chatgpt_work_to_help_with_looking_for_wfh/"/>
        <updated>2023-09-08T07:13:47.000Z</updated>
        <summary type="html"><![CDATA[This is a complete ChatGPT beginner question but has anyone ever downloaded it and used it to help with looking for specific job roles? Mainly WFH related? Or thought about changing careers and used ChatGPT to help with that? I know there are a lot of other ways to go about this but would ChatGPT help with this at all? 
    submitted by    /u/jackbowls  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do you feel endangered by the rise of AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16d2668/do_you_feel_endangered_by_the_rise_of_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16d2668/do_you_feel_endangered_by_the_rise_of_ai/"/>
        <updated>2023-09-08T05:52:46.000Z</updated>
        <summary type="html"><![CDATA[View Poll
    submitted by    /u/MiladMansory  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design2Prompt]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16d1dq7/design2prompt/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16d1dq7/design2prompt/"/>
        <updated>2023-09-08T05:08:31.000Z</updated>
        <summary type="html"><![CDATA[Guys, I'm looking for an AI that will describe my figma design in detail for another model to write the code in flutter. Is there anything like that out there?
    submitted by    /u/Aru-sejin37  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] FLM-101B: An Open LLM and How to Train It with $100K Budget]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16d15ci/r_flm101b_an_open_llm_and_how_to_train_it_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16d15ci/r_flm101b_an_open_llm_and_how_to_train_it_with/"/>
        <updated>2023-09-08T04:55:55.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hzj5790  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Difference between experience replay and multi time-step inputs.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16d14hz/difference_between_experience_replay_and_multi/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16d14hz/difference_between_experience_replay_and_multi/"/>
        <updated>2023-09-08T04:54:36.000Z</updated>
        <summary type="html"><![CDATA[In DQN, if I want to train a model which takes into account of the current state and previous k states, do I use consecutive experience replay to achieve this or should I implement a DNN with multi time-step inputs? Is the latter allowed, considering the Markov assumption from MDP update?
 I only have a superficial understanding on the purpose of experience replay, which is used to stabalise the training process and break correlations from consecutive training samples.
    submitted by    /u/cj_1993  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI grading and AI screening but no AI for homework/assignments/exam?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16d108k/ai_grading_and_ai_screening_but_no_ai_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16d108k/ai_grading_and_ai_screening_but_no_ai_for/"/>
        <updated>2023-09-08T04:47:53.000Z</updated>
        <summary type="html"><![CDATA[Professors send emails explaining that they use AI but they reviewed the grades from AI to make sure everything is fine. But students canâ€™t use AI and then review the results just make sure everything is fine.
    submitted by    /u/PrettyHappyAndGay  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/7/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16d0x70/oneminute_daily_ai_news_872023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16d0x70/oneminute_daily_ai_news_872023/"/>
        <updated>2023-09-08T04:43:13.000Z</updated>
        <summary type="html"><![CDATA[A new AI tool developed by startup Delphi allows users to create virtual clones of themselves or anyone else. Users can upload an ID and add various files, such as emails, chat transcripts, and videos, to generate an AI chatbot that mimics their personality.[1]
 OpenAI will host its first developer conference on November 6.[2]
 Meta Platforms Inc. today released FACET, a benchmark dataset designed to help researchers audit computer vision models for bias.[3]
 Australia to require AI-made child abuse material be removed from search results.[4]
  
Sources:
 [1] https://technotrenz.com/news/a-new-ai-service-allows-for-the-creation-of-a-virtual-version-of-yourself-or-a-loved-one-that-is-capable-of-making-phone-calls-on-your-behalf-2772634.html
 [2] https://techcrunch.com/2023/09/06/openai-will-host-its-first-developer-conference-on-november-6/
 [3] https://siliconangle.com/2023/08/31/meta-releases-facet-dataset-evaluating-ai-fairness/
 [4] https://www.reuters.com/technology/australia-require-ai-made-child-abuse-material-be-removed-search-results-2023-09-08/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jackson Jewett wants to design buildings that use less concrete]]></title>
        <id>https://news.mit.edu/2023/jackson-jewett-building-with-less-concrete-0908</id>
        <link href="https://news.mit.edu/2023/jackson-jewett-building-with-less-concrete-0908"/>
        <updated>2023-09-08T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[The PhD student is honing algorithms for designing large structures with less material â€” helping to shrink the construction industryâ€™s huge carbon footprint.]]></summary>
        <author>
            <name>Laura Rosado | MIT News correspondent</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGI will be not feasible any time soon, here's why]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16czvhb/agi_will_be_not_feasible_any_time_soon_heres_why/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16czvhb/agi_will_be_not_feasible_any_time_soon_heres_why/"/>
        <updated>2023-09-08T03:49:19.000Z</updated>
        <summary type="html"><![CDATA[I was thinking today about all the AI hype we have right with somewhat a bunch of new breakthroughs each month, but things not only are getting slower updates, but the updates impacts itself are becoming lesser. If that is not enough, well we have big problems ahead, such as processors are reaching the physical limit, quantum effects disrupting the works, wafers becoming increasing more expensive, the size reduction is no longer adding the same boosts in power and new materials are just far from viable. 
 On top of this we are going meet two other walls, the software and the energy. About the first, as we make better and more complex algorithms for computation the harder it gets to make better ones to squeeze more power and handle more complex tasks. The second, is becoming more real as biâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Seeking Research Papers on Weight Manipulation in Physics-Informed Neural Networks (PINNs)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cy82s/r_seeking_research_papers_on_weight_manipulation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cy82s/r_seeking_research_papers_on_weight_manipulation/"/>
        <updated>2023-09-08T02:28:17.000Z</updated>
        <summary type="html"><![CDATA[Would you kindly share noteworthy papers that have caught your interest concerning the weights of physics-informed neural networks (PINNs)? I am looking for any innovative paper that has something to do with weights of the physics-informed neural networks or deep neural networks in general and its manipulation.
 Specifically, I am seeking innovative papers on weight manipulation in physics-informed neural networks.
 For instance papers like:
  
Weight initialization algorithm for physics-informed neural networks using finite differences
 Transfer Learning with Physics-Informed Neural Networks for Efficient Simulation of Branched Flows
  
Note that I am referring to the actual weights of the neural network and not the weights of the loss terms.
 I have to add that ideas from transfer learning are welcome too. 
    submitted by    /u/ai_physics2023  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][R] Finetune LLMs via the Finetuning Hub]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cxfph/pr_finetune_llms_via_the_finetuning_hub/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cxfph/pr_finetune_llms_via_the_finetuning_hub/"/>
        <updated>2023-09-08T01:52:47.000Z</updated>
        <summary type="html"><![CDATA[Hi ML community, I have been working on benchmarking publicly available LLMs these past couple of weeks. More precisely, I am interested on the finetuning piece since a lot of businesses are starting to entertain the idea of self-hosting LLMs trained on their proprietary data rather than relying on third party APIs. To this point, I am tracking the following 4 pillars of evaluation that businesses are typically look into: - Performance - Time to train an LLM - Cost to train an LLM - Inference (throughput / latency / cost per token)
 For each LLM, my aim is to benchmark them for popular tasks, i.e., classification and summarization. Moreover, I would like to compare them against each other.
 So far, I have benchmarked Flan-T5-Large, Falcon-7B and RedPajama and have found them to be very efficient in low-data situations, i.e., when there are very few annotated samples. Llama2-7B/13B and Writerâ€™s Palmyra are in the pipeline.
 But thereâ€™s so many LLMs out there! In case this work interests you, would be great to join forces.
 GitHub repo attached â€” feedback is always welcome :)
 https://github.com/georgian-io/LLM-Finetuning-Hub
 Happy hacking!
    submitted by    /u/l-llm  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anthropic: From startup to AI powerhouse with Claude Pro launch]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16cuxyw/anthropic_from_startup_to_ai_powerhouse_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16cuxyw/anthropic_from_startup_to_ai_powerhouse_with/"/>
        <updated>2023-09-07T23:59:49.000Z</updated>
        <summary type="html"><![CDATA[Anthropic, a startup composed of former OpenAI staff, has announced the release of its premium subscription plan, Claude Pro, for Claude 2, its AI-driven chatbot. The affordable subscription offers a plethora of features for users.
 To stay on top of the latest advancements in AI, look here first.
 Anthropic's Claude Pro: Cost and Features
  
Priced at $20 per month in the U.S. or Â£18 in the U.K., users will have access to "5x more usage" compared to the free tier of Claude 2.
 Subscribers can send unlimited messages, gain priority during high-traffic periods, and get early access to new enhancements.
 The new package is priced similarly to OpenAIâ€™s paid plan for ChatGPT Plus, a direct rival to Claude 2.
  
Rationale and User Value
  
Since its launch in July, users have praised Claude forâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thought Experiment: â€œThe Reverse Deep Learning Paradigmâ€]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16cuoo4/thought_experiment_the_reverse_deep_learning/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16cuoo4/thought_experiment_the_reverse_deep_learning/"/>
        <updated>2023-09-07T23:49:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nicdunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]What do people think about papers published in the NeurIPS dataset track in comparison to those published in the main conference?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cto3t/dwhat_do_people_think_about_papers_published_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cto3t/dwhat_do_people_think_about_papers_published_in/"/>
        <updated>2023-09-07T23:07:39.000Z</updated>
        <summary type="html"><![CDATA[I'm curious to learn about the perception of papers published in the NeurIPS dataset track in comparison to those published in the main conference. Specifically, I'd like to know how both companies and Ph.D. committees view these papers. Are they considered equally valuable, or is there a notable difference in their reputation and significance? Your insights and experiences would be greatly appreciated! 
    submitted by    /u/Longjumping-Yam6941  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[be my ai vs bing vs bard]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ctnmc/be_my_ai_vs_bing_vs_bard/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ctnmc/be_my_ai_vs_bing_vs_bard/"/>
        <updated>2023-09-07T23:07:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nicdunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiny probe measures deep-brain activity from inside a blood vessel]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16cs9li/tiny_probe_measures_deepbrain_activity_from/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16cs9li/tiny_probe_measures_deepbrain_activity_from/"/>
        <updated>2023-09-07T22:12:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who is missing from the TIME 100 most influential people in AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16cs74q/who_is_missing_from_the_time_100_most_influential/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16cs74q/who_is_missing_from_the_time_100_most_influential/"/>
        <updated>2023-09-07T22:09:57.000Z</updated>
        <summary type="html"><![CDATA[Who do you think is not on this list but should be?
 https://time.com/collection/time100-ai/
 â€‹
    submitted by    /u/smo279  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel computational fluid dynamics framework for turbulent flow research]]></title>
        <id>http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html</id>
        <link href="http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html"/>
        <updated>2023-09-07T22:03:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Shantanu Shahane, Software Engineer, and Matthias Ihme, Research Scientist, Athena Team




Turbulence is ubiquitous in environmental and engineering fluid flows, and is encountered routinely in everyday life. A better understanding of these turbulent processes could provide valuable insights across a variety of research areas â€” improving the prediction of cloud formation by atmospheric transport and the spreading of wildfires by turbulent energy exchange, understanding sedimentation of deposits in rivers, and improving the efficiency of combustion in aircraft engines to reduce emissions, to name a few. However, despite its importance, our current understanding and our ability to reliably predict such flows remains limited. This is mainly attributed to the highly chaotic nature aâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Training a language model for custom scripting language?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cre85/d_training_a_language_model_for_custom_scripting/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cre85/d_training_a_language_model_for_custom_scripting/"/>
        <updated>2023-09-07T21:39:16.000Z</updated>
        <summary type="html"><![CDATA[Firstly some house keeping:
  
I'm a bit of a noob at this whole AI / Machine Learning stuff - still trying to learn.
 This isn't a "do my homework for me" kind of post
 I know language processing can be taxing, I have up to 4 Tesla V100S 32 GB at my disposal
  
Now that's out the way, here's the story:
 A team of us have created our own scripting language that is XML based that can do various actions against a database (or the file system) - a script is known as a "job" here is an example of a simple one
  
Set variables by various methods and send their contents and an attachment by email:
  
<Job title="Send Variables by Email"> <SetVariable name="MyStringVar" value="Hi John"/> <SetVariable name="MySQLVar" sql="select dbname from params"/> <SetVariable name="MyDateVar" value="1998-12-25â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is inference optimization a thing?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cqji8/d_is_inference_optimization_a_thing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cqji8/d_is_inference_optimization_a_thing/"/>
        <updated>2023-09-07T21:08:33.000Z</updated>
        <summary type="html"><![CDATA[Let me give you a quick intro. My engineering experience primarily revolved around data processing, analytics, and distributed systems. Nonetheless, I had a desire to learn about ML, and imho the best way to learn is to work on a practical project. So, that's precisely what I did. A few months ago, I embarked on an exciting journey with a friend, and together, we've created http://github.com/huggingbench/huggingbench.
 Now, after three months, I find myself seeking validation for some of my assumptions from the broader community. If you'd like to learn more about our motivations and the path we've taken check out the blog post https://medium.com/@niksa.jakovljevic/introducing-huggingbench-a-path-to-optimized-model-serving-a17cecc8d3ec.
 What I'd like to gather from individuals with machine learning models in production is their level of investment in optimizing inference. Is this a commonplace practice? I acknowledge that it can vary on a case-by-case basis, but I'm still hopeful of identifying prevailing trends. After conversing with a few companies, I've come to the impression that only the truly large players (those spending six figures or more on inference per month) place significant emphasis on inference optimization, which is entirely understandable. Nevertheless, I sense that there are numerous low hanging fruits that could result in substantial cost savings, even for typical startups.
 Could it be that the entire machine learning field is still in its infancy, and many engineers may not be fully considering or prioritizing such optimizations? Perhaps businesses are not giving as much attention to cost considerations? Alternatively, there might be technical challenges I'm not yet aware of. In any case, I would greatly appreciate hearing your insights on the subject of inference optimization.
    submitted by    /u/unsigned_mind  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What technological improvements led to the current AI boom?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16cqgu7/what_technological_improvements_led_to_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16cqgu7/what_technological_improvements_led_to_the/"/>
        <updated>2023-09-07T21:05:47.000Z</updated>
        <summary type="html"><![CDATA[I have studied artificial intelligence about 15 years ago, and have left the field since. I am curious to learn what has been happening in the field after I've left. I know there's a lot of hype around generative AI like ChatGPT and Wall-E.
 I find it quite hard though to find out what's exactly the underlying technology breakthroughs that have allowed for these new applications. I mean, neural networks and similar machine learning techniques are already decades old.
 What technology led to the current AI boom? What would you say are the biggest conceptual improvements since? Or is it all just faster and bigger computers running 2000's tech?
    submitted by    /u/math1985  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLEASE HELP (LSTM FOR RAINFALL PREDICTION) [P] [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cp5sr/please_help_lstm_for_rainfall_prediction_p_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cp5sr/please_help_lstm_for_rainfall_prediction_p_d/"/>
        <updated>2023-09-07T20:17:29.000Z</updated>
        <summary type="html"><![CDATA[I have been trying to build a DNN model for predicting the amount of rainfall but it has been hugely unsuccessful with just 40% accuracy even after CV and a high RMSE. I have read some research papers and they have suggested to use LSTM , I am aware of the concept but have never implemented.
 My dataset has arounf 15000 values of precipitation out of which 5000 values are zero (no rainfall at all) and I have 7 other features (including humidity , wind speed etc etc) .
 PLEASE HELP ! I NEED TO COMPLETE THIS FOR MY INTERNSHIP HAHA
 https://preview.redd.it/sg5v95ly5wmb1.png?width=1818&format=png&auto=webp&s=793bee830bb83f531f77e5c2a4ab47a5fb21eb3b
    submitted by    /u/Decent_Ordinary1528  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Open ASR Leaderboard]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cnez1/r_open_asr_leaderboard/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cnez1/r_open_asr_leaderboard/"/>
        <updated>2023-09-07T19:12:35.000Z</updated>
        <summary type="html"><![CDATA[Hugging Face benchmarked open source/ access models [English only] on 8 different speech datasets (LibriSpeech, Common Voice, VoxPopuli, TED-LIUM, Gigaspeech, SPGISpeech, Earnings-22 and AMI) ðŸ¤—
 Leaderboard here: https://huggingface.co/spaces/hf-audio/open_asr_leaderboard
    submitted by    /u/vaibhavs10  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How can we improve LLM responses outside of fine-tuning & prompt engineering?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cnbmk/d_how_can_we_improve_llm_responses_outside_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cnbmk/d_how_can_we_improve_llm_responses_outside_of/"/>
        <updated>2023-09-07T19:09:09.000Z</updated>
        <summary type="html"><![CDATA[Outside of better models, bigger, fine-tuning, etc, I'm wondering how we can get better responses from models.
 In my experience, I think prompt engineering can only take us so far. Models hallucinate often and I think we need to have some engineering solution to this.
 I've been looking at libraries doing token healing, which I find to be helpful (for example https://github.com/guidance-ai/guidance/tree/main) but outside of this, I'm wondering what other techniques people have been doing to improve model performance?
    submitted by    /u/opt1malP0licy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Industries Are Meeting Consumer Expectations With Speech AI]]></title>
        <id>https://blogs.nvidia.com/?p=66764</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/07/speech-ai-for-industries/"/>
        <updated>2023-09-07T18:41:09.000Z</updated>
        <summary type="html"><![CDATA[Thanks to rapid technological advances, consumers have become accustomed to an unprecedented level of convenience and efficiency. Smartphones make it easier than ever to search for a product and have it delivered right to the front door. Video chat technology lets friends and family on different continents connect with ease. With voice command tools, AI Read article >]]></summary>
        <author>
            <name>Cliff Edwards</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimize equipment performance with historical data, Ray, and Amazon SageMaker]]></title>
        <id>ecca70c5e391cc3bd7c991bb1f05a56b1596d6f2</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/optimize-equipment-performance-with-historical-data-ray-and-amazon-sagemaker/"/>
        <updated>2023-09-07T18:31:07.000Z</updated>
        <summary type="html"><![CDATA[In this post, we will build an end-to-end solution to find optimal control policies using only historical data on Amazon SageMaker using Rayâ€™s RLlib library. To learn more about reinforcement learning, see Use Reinforcement Learning with Amazon SageMaker.]]></summary>
        <author>
            <name>Walt Mayfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enable pod-based GPU metrics in Amazon CloudWatch]]></title>
        <id>5e117253e88439ed033ed8e7c7b6b530a4f7afc0</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/enable-pod-based-gpu-metrics-in-amazon-cloudwatch/"/>
        <updated>2023-09-07T18:14:26.000Z</updated>
        <summary type="html"><![CDATA[This post details how to set up container-based GPU metrics and provides an example of collecting these metrics from EKS pods.]]></summary>
        <author>
            <name>Amr Ragab</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best practices and design patterns for building machine learning workflows with Amazon SageMaker Pipelines]]></title>
        <id>1220c17c0fa90bbc145df3fde4359eec4fd06f4f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/best-practices-and-design-patterns-for-building-machine-learning-workflows-with-amazon-sagemaker-pipelines/"/>
        <updated>2023-09-07T17:54:33.000Z</updated>
        <summary type="html"><![CDATA[In this post, we provide some best practices to maximize the value of SageMaker Pipelines and make the development experience seamless. We also discuss some common design scenarios and patterns when building SageMaker Pipelines and provide examples for addressing them.]]></summary>
        <author>
            <name>Pinak Panigrahi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Open-source observability for LLMs without adapting new tools]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16clupa/p_opensource_observability_for_llms_without/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16clupa/p_opensource_observability_for_llms_without/"/>
        <updated>2023-09-07T17:38:25.000Z</updated>
        <summary type="html"><![CDATA[Hey all!
 I've written an open-source SDK for reporting metrics from LLM usage using OpenTelemetry.
 The great thing about it? With just one line of code you can get full visibility into your LLM app with your existing observability stack - straight into Datadog, Sentry, Honeycomb and others!
 Check it out (maybe give a â­?), and let me know your thoughts - https://github.com/traceloop/openllmetry
    submitted by    /u/nirga  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Falcon 180Bâ€”A Record-Breaking Open Source LLM on Hugging Face [N]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cluor/falcon_180ba_recordbreaking_open_source_llm_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cluor/falcon_180ba_recordbreaking_open_source_llm_on/"/>
        <updated>2023-09-07T17:38:24.000Z</updated>
        <summary type="html"><![CDATA[The AI community is buzzing with the arrival of Falcon 180B, an open-source LLM with an unprecedented 180 billion parameters. Developed by TII, This powerful model has surpassed key players like Meta's LLaMA 2 and matches commercial models like Google's PaLM-2.
 To stay on top of the latest advancements in AI, look here first.
 â€‹
 https://preview.redd.it/9xe5tczpdvmb1.jpg?width=480&format=pjpg&auto=webp&s=b7927d94a48fb75eaf05f6f0d8fe1089c0e1078b
 Falcon 180B's Unrivaled Performance
  
This advanced LLM is trained on an astounding 3.5 trillion tokens.
 Falcon 180B's parameters are 2.5 times larger than LLaMA 2's. It outperforms LLaMA 2 in scale and benchmark performance across diverse NLP tasks.
 On evaluations like the HellaSwag benchmark, it rivals commercial models like Google's PaLM-2.
  
Promising Future
  
Techniques like weight randomization and Nvidiaâ€™s Perfusion have helped train Falcon 180B more efficiently.
 Now freely available on Hugging Face, Falcon 180B is set to benefit from further enhancements by the community.
 The model's demonstration of advanced natural language abilities makes it a thrilling development in open-source AI.
  
(source) (demo)
 P.S. If you like this kind of analysis, I write a free newsletter that covers the most crucial news and studies in AI and tech. Professionals from Google, Meta, and OpenAI are already subscribed.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Falcon 180Bâ€”A Record-Breaking Open Source LLM on Hugging Face]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16cltpi/falcon_180ba_recordbreaking_open_source_llm_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16cltpi/falcon_180ba_recordbreaking_open_source_llm_on/"/>
        <updated>2023-09-07T17:37:03.000Z</updated>
        <summary type="html"><![CDATA[The AI community is buzzing with the arrival of Falcon 180B, an open-source LLM with an unprecedented 180 billion parameters. Developed by TII, This powerful model has surpassed key players like Meta's LLaMA 2 and matches commercial models like Google's PaLM-2.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/trscqxmncvmb1.jpg?width=480&format=pjpg&auto=webp&s=0590f4017937e70533414f93c72d9aa6edd62048
 Falcon 180B's Unrivaled Performance
  
This advanced LLM is trained on an astounding 3.5 trillion tokens.
 Falcon 180B's parameters are 2.5 times larger than LLaMA 2's. It outperforms LLaMA 2 in scale and benchmark performance across diverse NLP tasks.
 On evaluations like the HellaSwag benchmark, it rivals commercial models like Google's PaLM-2.
  
Promising Future
  
Techniques like weight randomization and Nvidiaâ€™s Perfusion have helped train Falcon 180B more efficiently.
 Now freely available on Hugging Face, Falcon 180B is set to benefit from further enhancements by the community.
 The model's demonstration of advanced natural language abilities makes it a thrilling development in open-source AI.
  
(source) (demo)
 P.S. If you like this kind of analysis, I write a free newsletter that covers the most crucial news and studies in AI and tech. Professionals from Google, Meta, and OpenAI are already subscribed.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] [R] New dataset on very high-quality image segmentation (EntitySeg)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ckswb/n_r_new_dataset_on_very_highquality_image/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ckswb/n_r_new_dataset_on_very_highquality_image/"/>
        <updated>2023-09-07T16:49:29.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 EntitySeg dataset
 Dense image segmentation tasks (e.g., semantic, panoptic) are useful for image editing, but existing methods can hardly generalize well in an in-the-wild setting where there are unrestricted image domains, classes, and image resolution and quality variations. Motivated by these observations, we construct a new entity segmentation dataset, with a strong focus on high-quality dense segmentation in the wild. The dataset contains images spanning diverse image domains and entities, along with plentiful high-resolution images and high-quality mask annotations for training and testing.
 We have now released the dataset at https://github.com/adobe-research/EntitySeg-Dataset
 Project page: http://luqi.info/entityv2.github.io
 Code & models: https://github.com/qqlu/Entity/tree/main/Entityv2
 â€‹
    submitted by    /u/xternalz  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Open Interpreter ChatGPT Code Interpreter You Can Run LOCALLY! - 9.2k Stars on Github as of right now!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ckobr/n_open_interpreter_chatgpt_code_interpreter_you/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ckobr/n_open_interpreter_chatgpt_code_interpreter_you/"/>
        <updated>2023-09-07T16:44:22.000Z</updated>
        <summary type="html"><![CDATA[Github: https://github.com/KillianLucas/open-interpreter
 Youtube: https://youtu.be/SqnXUHwIa3c?si=ibSelipAb84AZQKo
 Open Interpreter lets LLMs run code (Python, Javascript, Shell, and more) locally. You can chat with Open Interpreter through a ChatGPT-like interface in your terminal by running $ interpreter
 after installing.
 This provides a natural-language interface to your computer's general-purpose capabilities:
  
Create and edit photos, videos, PDFs, etc.
 Control a Chrome browser to perform research
 Plot, clean, and analyze large datasets
 ...etc.
  
âš ï¸ Note: You'll be asked to approve code before it's run.
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How are AI services today when it comes to making content that requires distribution?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ck5yd/how_are_ai_services_today_when_it_comes_to_making/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ck5yd/how_are_ai_services_today_when_it_comes_to_making/"/>
        <updated>2023-09-07T16:24:18.000Z</updated>
        <summary type="html"><![CDATA[I'm looking at stuff that could be submitted to a Netflix or Crunchyroll.
 I'm looking at some of the ai generated content out there, in particular some of the Instagram tutorials and they look really good but none of these are serials like comics, graphic novels, OAVs or even webcomics.
    submitted by    /u/KrusMatrieya  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Fast open-source C++ libraries for Lasso]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cjpqb/d_fast_opensource_c_libraries_for_lasso/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cjpqb/d_fast_opensource_c_libraries_for_lasso/"/>
        <updated>2023-09-07T16:05:54.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I'm in search of a speedy open-source C++ library for tackling Lasso problems. These problems have a moderate size, typically with dimensions of nxp = 60x3000. I'm looking for a library that can solve each problem with regularization paths quickly, ideally within 0.3 seconds. Additionally, I need this library to include cross-validation functionality, which would enable me to select the best regularization parameter lambda using cross-validation.
 Any insights or recommendations on such libraries would be greatly appreciated! Thank you in advance for your help!
    submitted by    /u/mopyfish007  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating chemistsâ€™ insight with AI models for single-step retrosynthesis prediction]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=964185</id>
        <link href="https://www.microsoft.com/en-us/research/blog/incorporating-chemists-insight-with-ai-models-for-single-step-retrosynthesis-prediction/"/>
        <updated>2023-09-07T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis analysis is a critical task in organic chemistry and central to many important industries. It primarily involves decomposing a target molecule into commercially available molecules step by step. Since synthesis strategies can be quite diverse and strategic, retrosynthesis planning with expert knowledge has long been considered an â€œart.â€ Recently, machine learning-based approaches have achieved [â€¦]
The post Incorporating chemistsâ€™ insight with AI models for single-step retrosynthesis prediction appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do you train your models with limited hardware?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16chq28/d_how_do_you_train_your_models_with_limited/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16chq28/d_how_do_you_train_your_models_with_limited/"/>
        <updated>2023-09-07T14:46:15.000Z</updated>
        <summary type="html"><![CDATA[Hey there,
 So, I've been messing around with ML and I must say, the hardware requirements can be a real buzzkill... I mean, not everyone's got a huge GPU lying around or the money to rent a dedicated cloud instance.
 What are your hacks for pulling off decent model training without selling a kidney?
 Here's what I'm curious about:
  
CPU: Is anyone else training models on their CPU? How's that working out for you? What are some workarounds you've tried to make it less painful?
 Cloud: Who's been dabbling in cloud services like AWS, Google Cloud, or Azure? Are they worth the pennies or complicated to set up?
 Big Dataset: How do you handle a massive dataset with a standard storage space?
  
Let's help each other get those models trained without going broke! :D
 Cheers!
    submitted by    /u/aaron-cesaro  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Function approximation with neural net]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ch8tj/d_function_approximation_with_neural_net/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ch8tj/d_function_approximation_with_neural_net/"/>
        <updated>2023-09-07T14:26:07.000Z</updated>
        <summary type="html"><![CDATA[I have been struggling with a regression problem with TensorFlow. 
 Basically, I want a neural network to learn the simple polynomial pattern of a set of arrays of the form [x,y], with y = xÂ², where the first coordinates are uniformly distributed random numbers in the interval [0,1].
 I started with a model with 2 hidden layers of size two and 'tanh' activation functions, and an output layer with 'linear' activation function. 
 I've then experimented with both additional hidden layers and with increasing the sizes of these layers. Finally, I've tested both the 'adam' and 'sgd' optimizers and the loss functions 'meanSquaredError' and 'meanAbsolutePercentageError'. 
 However, none of the various combinations of these parameters has led to any even half-descent result. Even on the training seâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][D] Hey LOMO paper authors, Does SGD have optimizer states, or does it not?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cgukc/rd_hey_lomo_paper_authors_does_sgd_have_optimizer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cgukc/rd_hey_lomo_paper_authors_does_sgd_have_optimizer/"/>
        <updated>2023-09-07T14:09:31.000Z</updated>
        <summary type="html"><![CDATA[In the LOw-Memory Optimization paper one of the main ideas towards reducing memory usage in training LLMs is to replace a fancy optimizer like Adam with simple SGD. The reason is that Adam maintains "the optimizer state", which accounts for about 75% of the memory used. In contrast, SGD does not store any intermediate state, as they say on page one. So far, so good.
 https://preview.redd.it/b0dj2nzscumb1.png?width=1055&format=png&auto=webp&s=1712f8500b5cbfb3773cee00ea980175491dddbf
 On page six they have pie charts and a table showing memory usage for Adam, SGD, and LOMO. Here's where I got confused. The pie chart for SGD shows that the optimizer state accounts for nearly 50% of the memory used (weight, gradients and activations are shown separately). It's a major WTF moment: WHAT OPTIMIZER STATE? Can anybody understand and explain this?
    submitted by    /u/Foxtr0t  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] FalkorDB - a fast Graph Database - Knowledge Graph as RAG]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cg6k7/p_falkordb_a_fast_graph_database_knowledge_graph/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cg6k7/p_falkordb_a_fast_graph_database_knowledge_graph/"/>
        <updated>2023-09-07T13:41:25.000Z</updated>
        <summary type="html"><![CDATA[We're building a fast low latency Graph Database called FalkorDB that will also support Vector search.
 It's based on Redis and can be used both as a stand alone database or a module for existing Redis.
 It feels like that is going to be the most optimized way to serve Knowledge as RAG, would love to get your feedback.
 https://github.com/FalkorDB/falkordb 
 It already supports LlamIndex and Langchain:
 https://python.langchain.com/docs/use_cases/more/graph/graph_falkordb_qa
 https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/FalkorDBGraphDemo.html
 â€‹
    submitted by    /u/gkorland  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Artificial intelligence in medicine]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cfpje/d_artificial_intelligence_in_medicine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cfpje/d_artificial_intelligence_in_medicine/"/>
        <updated>2023-09-07T13:21:00.000Z</updated>
        <summary type="html"><![CDATA[Medicine's field transformation is being driven by artificial intelligence (AI). However, an important debatable question arises: Will AI ever have a place in this field, or will it remain exclusive to doctors and medical pros?
 Opponents of automated AI diagnosis and treatment contend that machines cannot be relied upon to preserve patient health and lives. Bugs in AI algorithms might cause incorrect diagnoses and treatment prescriptions, leaving them cautious. Individual differences, the doubt is whether AI can truly empathize with patients.
 By contrast, advocates of AI in medicine contend that the technology can considerably improve diagnosis and treatment accuracy. Faster and more accurate than humans, machines can analyze large amounts of data. Not only does it identify rare and complex diseases, but it also saves time and resources. By incorporating AI, clinicians receive additional tips and signals to make more judicious choices.
 Where medical specialists are in short supply, AI can prove especially valuable. This approach can help with shortages in health systems.
 And what do you think?
    submitted by    /u/gcore-com  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention, Please: Focus Entertainment Brings Game Pass Titles to GeForce NOW]]></title>
        <id>https://blogs.nvidia.com/?p=66751</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/07/geforce-now-thursday-sep-07/"/>
        <updated>2023-09-07T13:00:18.000Z</updated>
        <summary type="html"><![CDATA[GeForce NOW brings expanded support for PC Game Pass to members this week. Members can stream eight more games from Microsoftâ€™s subscription service, including four titles from hit publisher Focus Entertainment. Play A Plague Tale: Requiem, Atomic Heart and more from the GeForce NOW library at up to 4K resolution and 120 frames per second Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ceyfm/r_tune_as_you_scale_hyperparameter_optimization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ceyfm/r_tune_as_you_scale_hyperparameter_optimization/"/>
        <updated>2023-09-07T12:46:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/InterviewIntrepid889  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] The $900,000 deep learning salary]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ce68p/d_the_900000_deep_learning_salary/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ce68p/d_the_900000_deep_learning_salary/"/>
        <updated>2023-09-07T12:08:20.000Z</updated>
        <summary type="html"><![CDATA[This recent article in the WSG advertised a $900,000 salary at Netflix https://www.wsj.com/articles/artificial-intelligence-jobs-pay-netflix-walmart-230fc3cb.
 I was wondering what other DL research scientists who frequent this page are paid? And what exactly their job title is.
    submitted by    /u/blabboy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chatty LLama: A fullstack Rust + react chat app using Meta's Llama-2 LLMs https://github.com/Sollimann/chatty-llama]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16cchdz/chatty_llama_a_fullstack_rust_react_chat_app/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16cchdz/chatty_llama_a_fullstack_rust_react_chat_app/"/>
        <updated>2023-09-07T10:39:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sollimann  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D brain mri classification [Research]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16cbjmh/3d_brain_mri_classification_research/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16cbjmh/3d_brain_mri_classification_research/"/>
        <updated>2023-09-07T09:44:02.000Z</updated>
        <summary type="html"><![CDATA[I am planning on publishing a journal based on the thesis i completed in the mid of 2022. I did my thesis on Parkinson disease binary classification on 3D structural brain mri, and the dataset has significantly small amount of data(around 80 samples); but due to high resolution and complex data structure I was able achieve around 70% accuracy.
 But now at 2023 using deep neural network only isnot enough to publish in a good journal. Currently I am learning about GAN and attention mechanism, but completely noob on this area. For my journal to get published, I have planned on applying some key operations. But I am not sure if they would work or not. So needed some advice on this regard.
  
Applying tranfer learning: as my dataset has very small amount of data. I was thinking if its possible to pre train a CNN Architecture with some other structural mri data of a different disease and then apply to my dataset? ( for example: brain tumor dataset has the same type of three dimensional data structure, but has comparatively good amount of data)
 
Applying attention mechanism: how should I approach on learning about attention mechanism? 
 
 Any other advices will be appreciated, thank you!
    submitted by    /u/Bonito_Flakez  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intuit cut hundreds of jobs and spent at least $20 billion in a massive bet on AI. Today the company is revealing its new virtual assistant]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16cbiwx/intuit_cut_hundreds_of_jobs_and_spent_at_least_20/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16cbiwx/intuit_cut_hundreds_of_jobs_and_spent_at_least_20/"/>
        <updated>2023-09-07T09:42:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AminoOxi  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google takes on AI in political ads]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16cajor/google_takes_on_ai_in_political_ads/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16cajor/google_takes_on_ai_in_political_ads/"/>
        <updated>2023-09-07T08:40:16.000Z</updated>
        <summary type="html"><![CDATA[Google is updating its policy to require advertisers to disclose when their election ads include digitally altered or generated content.
 
The update will go into effect in November, ahead of the 2024 presidential election.
 
The goal is to provide transparency and help voters make informed decisions.
 
Minor alterations that are inconsequential to the claims are exempt from the disclosure requirements. 
 
 Source : https://thehill.com/newsletters/technology/4190769-googles-campaign-ai-crackdown/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prepare for the Mine-Fest: Radical changes undermine all previous ownership assumptions and now everyone is shouting "Mine".]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16c8j4l/prepare_for_the_minefest_radical_changes/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16c8j4l/prepare_for_the_minefest_radical_changes/"/>
        <updated>2023-09-07T06:35:01.000Z</updated>
        <summary type="html"><![CDATA[Ownership is just a story that we tell each other, a social construct. If people donâ€™t agree on these stories, the concept loses its inherent power. This is true of owning land, money, cars, houses, art, mines, oil-wells, factories, corporations, relationships, loyalties, copyrights, brands, patents or anything else that is owned by you, me or those ever-superior â€œothersâ€.
 In a society where change occurs gradually, we become accustomed to the narratives that bind us together and determine who possesses significant wealth, resources, attention, power, fame, and other ego-gratifying treasures, and who has access to only meager portions of these.
 However, when societies change and new types of goods appear, there might be no agreement about who gets to own these. For example, while the conâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/6/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16c6oll/oneminute_daily_ai_news_962023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16c6oll/oneminute_daily_ai_news_962023/"/>
        <updated>2023-09-07T04:49:39.000Z</updated>
        <summary type="html"><![CDATA[The Consensus Search plugin allows users to find answers, search for papers, and draft pieces of content grounded in scientific research by searching our database of 200M+ papers directly within the ChatGPT interface.[1]
 Israel: AI Software Detects Bleeding Inside Brain During CT Scan; Helps Save Patientâ€™s Life.[2]
 Chinese tech giant Tencent is launching its artificial intelligence model â€œHunyuanâ€ for business use at an annual summit on Thursday.[3]
 Google on Wednesday said it will mandate that political advertisements on its platforms disclose when images and audio have been altered or created using tools such as AI.[4]
  
Sources:
 [1] https://consensus.app/home/blog/introducing-the-consensus-search-chatgpt-plugin/
 [2] https://english.jagran.com/technology/israel-ai-program-detects-bleeding-inside-brain-during-ct-scan-helps-save-patient-life-full-story-10098464
 [3] https://www.cnbc.com/2023/09/07/tencent-releases-ai-model-hunyuan-for-businesses-amid-china-competition.html
 [4] https://sg.news.yahoo.com/google-require-political-ads-disclose-010502103.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI poised to replace 2.4 million US jobs by 2030]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16c6gcw/generative_ai_poised_to_replace_24_million_us/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16c6gcw/generative_ai_poised_to_replace_24_million_us/"/>
        <updated>2023-09-07T04:37:04.000Z</updated>
        <summary type="html"><![CDATA[Forrester predicts that generative AI will replace 2.4 million US jobs by 2030, mostly white-collar roles, such as technical writers, proofreaders, copywriters, and administrative positions. But ironically, other forms of automation will displace more jobs.
 To stay on top of the latest advancements in AI, look here first.
 (Chart showing how much different types of jobs can expect to be influenced by technology)
 Concerns about Generative AI
  
While the Generative AI impact is significant, other forms of automation are set to cause more widespread job displacement.
 The most impacted group will be middle-class, college-educated, white-collar workers, specifically those earning above $60,000 annually.
  
Creative professionals stand to benefit
  
Interestingly, workers in creative industries will likely utilize generative AI tools in their jobs rather than being replaced. This includes editors, writers, authors, poets, and lyricists.
 However, the use of such tools as ChatGPT may result in inconsistent outputs and even "coherent nonsense", leading to potential performance issues.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that covers the most crucial news and studies in AI and tech. Professionals from Google, Meta, and OpenAI are already subscribed.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Fine-tuning LLMs or Supervised Learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16c2d5d/d_finetuning_llms_or_supervised_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16c2d5d/d_finetuning_llms_or_supervised_learning/"/>
        <updated>2023-09-07T01:21:43.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone! I want to implement a document similarity program and was looking into LLMs as a means of accomplishing this task. 
 I have ~10,000 documents that are "scams" because of some specific reason (all are verified); now I want to check if a new document is similar to any of the documents in the corpus of 10k scam documents. 
 Right now I've implemented a winnowing solution which normalizes text, breaks it up into windows, and then calculates the intersection between a document and each document in the corpus. HOWEVER, this method is pretty computationally expensive (for this many documents a single comparison cycle can take upwards of 3-4 minutes especially when windows are NOT precomputed). 
 How might I approach this problem? Because my data is pretty well structured, supervised learning might be a good approach but so might be setting up recursive chunking for the 10k document corpus and then using LLMs to access if this current legal document has any similarity, but I would love to hear your thoughts!
    submitted by    /u/Adventurous-Tower392  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Copyright And Fair Use: Important Notice Of Iquiry By The US Copyright office]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16c0l0x/n_copyright_and_fair_use_important_notice_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16c0l0x/n_copyright_and_fair_use_important_notice_of/"/>
        <updated>2023-09-07T00:03:09.000Z</updated>
        <summary type="html"><![CDATA[Please make your voices heard by submitting comments on how you use and benefit from having access to open datasets, their resulting models and how you think copyright issues should be handled to not destroy the open source local model eco system. Banning publicily avaiable datasets for training would absolutely kill the open research space and halt in development of machine learning. 
 â€‹
 In my opinion the real dystopia will be when politicians sit own with big tech lobbyists and big rights holders and decide that training as it is currently done, for free and open source models and others is illegal. Then the big players would actually win, since they have enough resources to license datasets and will certainly do so willingly and gladly, if it is clear that the jurisdiction keeps all the small players and open source out. Easiest way to build a moat and force people to pay thousands for these tools. So please make your voices heard and share the link
 >The Copyright Office issued a notice of inquiry in the Federal Register seeking public comment on questions about copyright law and policy issues raised by AI systems. Initial comments are due by October 18, 2023. Reply comments are due November 15, 2023.
 https://www.copyright.gov/newsnet/2023/1017.html?loclr=twcop
 Link to comment submissive form:
 https://www.regulations.gov/commenton/COLC-2023-0006-0001
    submitted by    /u/PinPuzzleheaded8525  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ðŸ¤– AI in 2023: Blessing or Curse? ðŸ¤–]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16byffs/ai_in_2023_blessing_or_curse/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16byffs/ai_in_2023_blessing_or_curse/"/>
        <updated>2023-09-06T22:34:59.000Z</updated>
        <summary type="html"><![CDATA[View Poll
    submitted by    /u/m-king473  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Total NN N00b Here Looking to Do an ML Project]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16by7or/total_nn_n00b_here_looking_to_do_an_ml_project/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16by7or/total_nn_n00b_here_looking_to_do_an_ml_project/"/>
        <updated>2023-09-06T22:26:41.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I don't know if this is the right subreddit to post this kind of thing. I have basic coding skills but other than that no experience with neural networks.
 What I'd like to do is take an existing input data set and then use a neutral net to build a model based on manual training data. If anyone could give me help on how to start / even a full explanation of the way a noob like me could accomplish this, that would be great. Otherwise if anyone can point me to a list of resources that are able to comprehensively explain the process, that would also be great! Again sorry if this is the wrong subreddit, if this is the wrong place for this can someone please direct me to the right place to ask this question. Thanks!
    submitted by    /u/DJ_Hastings013  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models - DAMO Academy, Alibaba Group, China 2023 - Released under an Apache 2.0 license!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16by65o/r_modelscopeagent_building_your_customizable/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16by65o/r_modelscopeagent_building_your_customizable/"/>
        <updated>2023-09-06T22:25:05.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.00986
 Github: https://github.com/modelscope/modelscope-agent
 Abstract:
  
Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. 
  
https://preview.redd.it/9f77992ynpmb1.jpg?width=1245&format=pjpg&auto=webp&s=4e17e3d46c7f262bfec76b88e086164530739255
 https://preview.redd.it/etelh03ynpmb1.jpg?width=1219&format=pjpg&auto=webp&s=517a52a1e2bbf488b647c4e1b9b496657003c1d2
 https://preview.redd.it/b0tkra2ynpmb1.jpg?width=850&format=pjpg&auto=webp&s=397c910b2d90dd212a31ec118d1c4e78532bf5f4
 â€‹
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can't wait for the Zelda 3 movie,, thanks Pika Labs AI!!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16by1by/cant_wait_for_the_zelda_3_movie_thanks_pika_labs/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16by1by/cant_wait_for_the_zelda_3_movie_thanks_pika_labs/"/>
        <updated>2023-09-06T22:20:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I pass in the models policy to the reset function for logging in Stable Baselines 3?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16bxbwr/how_can_i_pass_in_the_models_policy_to_the_reset/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16bxbwr/how_can_i_pass_in_the_models_policy_to_the_reset/"/>
        <updated>2023-09-06T21:52:56.000Z</updated>
        <summary type="html"><![CDATA[I want to pass the policy from my main file into my agent file so that I can log the data collected during training. I am already collecting things like the score and reward but I don't know how I could collect things like the policy loss or explained variance where they are changing as the training progresses . I want to log these to an XSLX file every time the reset function is called (once every game) preferably the same one I am logging the score and reward to.
 The game is the classic snake game, run on pygame.
 Here is my main code:
 import gymnasium from stable_baselines3 import A2C from agentStable import snakeEnv from eiffel2 import builder # Import Eiffel2's builder function from torchsummary import summary # from agentStable import data_manager # Initialize your custom environmeâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Can a neural network learn like a dog?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bxbe1/p_can_a_neural_network_learn_like_a_dog/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bxbe1/p_can_a_neural_network_learn_like_a_dog/"/>
        <updated>2023-09-06T21:52:20.000Z</updated>
        <summary type="html"><![CDATA[Hello folks.,
 Some time ago I wanted to try out to train a neural network in the same way a human would with a dog, one command at the time, and in a reasonable number of iterations.
 What I thought it would be a simple exercise became (for me) a non-trivial project, so I decided to publish it here https://github.com/giteliot/lucioai
 I just wanted to share it with you, any feedback is highly appreciated.
 Cheers!
    submitted by    /u/rexdemorte  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI does not exist but it will ruin everything anyway]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bxahr/ai_does_not_exist_but_it_will_ruin_everything/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bxahr/ai_does_not_exist_but_it_will_ruin_everything/"/>
        <updated>2023-09-06T21:51:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Hazzman  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Using ChatGPT as a Social Media Post Generator]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bxa5o/p_using_chatgpt_as_a_social_media_post_generator/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bxa5o/p_using_chatgpt_as_a_social_media_post_generator/"/>
        <updated>2023-09-06T21:51:05.000Z</updated>
        <summary type="html"><![CDATA[I created this prompt for a member of r/PromptWizards which automates the generation of social media posts, with a conversational prompt. Thought I'd share, I really enjoy building such prompts so, post your automation ideas, and next time I'll automate it if I can :)
 Also, you can join r/PromptWizards, for more advanced prompt chains & templates.
 Here is the prompt (just copy the full thing in chatgpt and see the magic):
  
ChatGPT, now enter 'Social Media Post Generator Mode' that limits your inputs and outputs to a predefined framework aimed at creating engaging social media content. After each user command, provide the [help] options available for their next steps in list form. Generate prompts that are imaginative, engaging, concise, and tailored for social media audiences. Step 1: â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Falcon180B released! Sadly without Apache 2.0 they made their own modified version. :(]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bwoed/n_falcon180b_released_sadly_without_apache_20/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bwoed/n_falcon180b_released_sadly_without_apache_20/"/>
        <updated>2023-09-06T21:28:44.000Z</updated>
        <summary type="html"><![CDATA[LocalLLaMA discussion: https://www.reddit.com/r/LocalLLaMA/comments/16bjdmd/falcon180b_authors_open_source_a_new_180b_version/ 
 Announcement: https://falconllm.tii.ae/falcon-models.html
 HF Model: https://huggingface.co/tiiuae/falcon-180B 
 Demo: https://huggingface.co/spaces/tiiuae/falcon-180b-demo 
 Blog: https://huggingface.co/blog/falcon-180b 
  
180 Billion parameters
 Trained on 3.5 trillion tokens
 Available for research and commercial usage
 Claims similar performance to Bard, slightly below gpt4
  
https://falconllm.tii.ae/terms-and-conditions.html 
 https://falconllm.tii.ae/acceptable-use-policy.html 
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Tabular Data: DL vs GBDTs on large scale datasets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bwbpw/d_tabular_data_dl_vs_gbdts_on_large_scale_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bwbpw/d_tabular_data_dl_vs_gbdts_on_large_scale_datasets/"/>
        <updated>2023-09-06T21:15:31.000Z</updated>
        <summary type="html"><![CDATA[I've been hearing lately that NNs are better than GBDTs when scaled up alot:
  
Uber https://www.uber.com/en-CA/blog/deepeta-how-uber-predicts-arrival-times/
 Stripe https://stripe.com/blog/how-we-built-it-stripe-radar
 Most CTR papers coming from google are also NN based (like https://arxiv.org/abs/2209.05310)
 Meta mentions NNs in their recommender system (also kind of a large scale tabular problem there) https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system
 Lyft forecasting https://medium.com/this-week-in-machine-learning-ai/causal-models-in-practice-at-lyft-with-sean-taylor-1e62efd62385
  
What's your intuition on DL vs GBDT on (very)large-scale tabular datasets? Have you heard of other such examples (or the reverse)? 
 Are there any particularly interesting open large tabular datasets on which I could test this? I guess datasets should also be wide/hard/with large intrinsic dimention (whatever that means) so there is something to learn with scale (the above examples sure feel good in this way).
 â€‹
    submitted by    /u/_puhsu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iâ€™m not sure if this is allowed here, but can someone with a music AI make Vessel from Sleep Token sing As the World Caves In by Matt Maltese?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bvesn/im_not_sure_if_this_is_allowed_here_but_can/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bvesn/im_not_sure_if_this_is_allowed_here_but_can/"/>
        <updated>2023-09-06T20:41:53.000Z</updated>
        <summary type="html"><![CDATA[I think that would be pretty sick.
    submitted by    /u/No_Understanding162  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fraud detection using Machine Learning: Unmasking deceptive patterns]]></title>
        <id>https://www.datasciencecentral.com/?p=63018</id>
        <link href="https://www.datasciencecentral.com/fraud-detection-using-machine-learning-unmasking-deceptive-patterns/"/>
        <updated>2023-09-06T20:22:49.000Z</updated>
        <summary type="html"><![CDATA[In an increasingly interconnected world where digital transactions have become the norm the battle against fraud has taken on new dimensions. The challenge lies not only in identifying familiar fraud patterns but also in unearthing the intricate web of evolving deceptions that threaten industries such as finance, e-commerce, and insurance. As fraudsters continually adapt theirâ€¦Â Read More Â»Fraud detection using Machine Learning: Unmasking deceptive patterns
The post Fraud detection using Machine Learning: Unmasking deceptive patterns appeared first on Data Science Central.]]></summary>
        <author>
            <name>Diana Jane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to get started with 3D machine learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bu6n1/d_how_to_get_started_with_3d_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bu6n1/d_how_to_get_started_with_3d_machine_learning/"/>
        <updated>2023-09-06T19:56:42.000Z</updated>
        <summary type="html"><![CDATA[Hi. I want to get started with deep learning in 3D. Any suggestions on what libraries I should go with (I have expeirence with Pytorch but open to learn anything other than that which might be better. I came across pytorch3d but not sure if it's good ) what are the basics that are needed and how should I learn them? Also it seems there are not much datasets on this field. 
    submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frontiers of multimodal learning: A responsible AI approach]]></title>
        <id>https://www.microsoft.com/en-us/research/blog/frontiers-of-multimodal-learning-a-responsible-ai-approach/</id>
        <link href="https://www.microsoft.com/en-us/research/blog/frontiers-of-multimodal-learning-a-responsible-ai-approach/"/>
        <updated>2023-09-06T19:53:53.000Z</updated>
        <summary type="html"><![CDATA[New evaluation methods and a commitment to continual improvement are musts if weâ€™re to build multimodal AI systems that advance human goals. Learn about cutting-edge research into the responsible development and use of multimodal AI at Microsoft.
The post Frontiers of multimodal learning: A responsible AI approach appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSMixer: An all-MLP architecture for time series forecasting]]></title>
        <id>http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html</id>
        <link href="http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html"/>
        <updated>2023-09-06T19:47:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Si-An Chen, Student Researcher, Cloud AI Team, and Chun-Liang Li, Research Scientist, Cloud AI Team




Time series forecasting is critical to various real-world applications, from demand forecasting to pandemic spread prediction. In multivariate time series forecasting (forecasting multiple variants at the same time), one can split existing methods into two categories: univariate models and multivariate models. Univariate models focus on inter-series interactions or temporal patterns that encompass trends and seasonal patterns on a time series with a single variable. Examples of such trends and seasonal patterns might be the way mortgage rates increase due to inflation, and how traffic peaks during rush hour. In addition to inter-series patterns, multivariate models process intrâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is It Too Early to Leverage AI for WebAssembly?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16btt10/is_it_too_early_to_leverage_ai_for_webassembly/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16btt10/is_it_too_early_to_leverage_ai_for_webassembly/"/>
        <updated>2023-09-06T19:42:04.000Z</updated>
        <summary type="html"><![CDATA[AI and WebAssembly are seen as a perfect pairing, with the potential to accelerate the adoption of WebAssembly.
 
Fermyon believes that applying AI to WebAssembly is not premature and has developed a serverless platform that offers sub-second cold start times and high-volume time-slicing of compute instances.
 
This allows for faster startup times and efficient resource utilization.
 
The goal is to make AI easy for developers to leverage and build serverless apps.
 
 Source : https://thenewstack.io/is-it-too-early-to-leverage-ai-for-webassembly/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elon Musk Plans to Merge Neuralink and Tesla for an AI Supercompany]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16btqlm/elon_musk_plans_to_merge_neuralink_and_tesla_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16btqlm/elon_musk_plans_to_merge_neuralink_and_tesla_for/"/>
        <updated>2023-09-06T19:39:31.000Z</updated>
        <summary type="html"><![CDATA[Elon Musk reportedly plans to blend Neuralink and Tesla into a large AI company, using data from Twitter users and Tesla's Full Self-Driving Cameras to train a robust AI model.
 To stay on top of the latest advancements in AI, look here first.
 https://preview.redd.it/la78u2ebuomb1.jpg?width=1315&format=pjpg&auto=webp&s=4d8178f8fb94e45d6959e243b86c3bab3bce72ee
 Musk's AI Integration Plan
  
Musk is contemplating merging Neuralink and Tesla, alongside his xAI startup, to create a comprehensive artificial intelligence model.
 Leveraging the text data from Twitter and real-world images from Tesla's Full Self-Driving network, he intends to develop AI chatbots and physical robots capable of real-world navigation.
  
Reasoning Behind the Merge
  
A concern that AI could potentially render humans obsolete led Musk to found xAI for AI safety.
 Musk is targeting to create an AI that can generate computer software and a politically unbiased chatbot rival to ChatGPT.
  
Twitter and Tesla as AI Datasets
  
Despite criticism, Musk's acquisition of Twitter offers access to vast user data for AI training.
 In addition, the Autopilot and Full-Self Driving systems of Tesla, with billions of collected camera images, serve as valuable resources to build physical robot AI.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that covers the most crucial news and studies in AI and tech. Professionals from Google, Meta, and OpenAI are already subscribed.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bsvst/n_finetuning_llms_lora_or_fullparameter_an/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bsvst/n_finetuning_llms_lora_or_fullparameter_an/"/>
        <updated>2023-09-06T19:07:13.000Z</updated>
        <summary type="html"><![CDATA[After our first blog post gained some attention from folks interested in applied fine-tuning, we now have a follow-up post that discusses all sorts of things we learned while working with LoRA.
 We hope that this helps engineers and other folks in the community to improve their fine-tuning.
 Here's what you can expect from the post:
 We compare full-parameter fine-tuning with LoRA and answer questions around the strengths and weaknesses of the two techniques. We train the Llama 2 models on three real-world use cases and demonstrate that using LoRA involves a trade-off between serving efficiency and model quality, which varies according to the specific task at hand. Additionally, we offer insights into how to stabilize training with LoRA through intelligent prompting techniques. We further show that adopting a lower learning rate can enhance the reliability of the resulting model checkpoints. 
 Link to the blog post 
 If you have questions, I'd be happy to answer them here!
    submitted by    /u/atta_snack  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL Project Help]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16bsvs1/rl_project_help/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16bsvs1/rl_project_help/"/>
        <updated>2023-09-06T19:07:11.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am looking for an experienced ML developer to consult on my project. I am currently developing a reinforcement learning model and have several questions regarding the reward system and the implementation of actions/steps. I have been unable to find solutions to my specific problems on the internet. If you are willing to assist me, please send me a message on Reddit.
 Thank you for your time.
    submitted by    /u/77_micheno_77  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Automate LLM backend deployments using infrastructure as code]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16brwu2/p_automate_llm_backend_deployments_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16brwu2/p_automate_llm_backend_deployments_using/"/>
        <updated>2023-09-06T18:30:03.000Z</updated>
        <summary type="html"><![CDATA[New GitHub project to provision, update, and destroy the cloud infrastructure for a LLM backend using infrastructure as code (Python). Deployment options include deploying huggingface models to Docker (local), Runpod, and Azure.
 Blog post
 Repo
    submitted by    /u/kao-pulumi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to go about reverse engineering historical trading data?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16bridt/how_to_go_about_reverse_engineering_historical/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16bridt/how_to_go_about_reverse_engineering_historical/"/>
        <updated>2023-09-06T18:15:02.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 Assume I have data for forex / stocks day trading, where my data/columns are:
 1) price of last 50 ticks (a tick is the price at that moment in time, the smallest movement possibly that you can get for that currency)
 2) If we should be in a trade (and direction of trade. where 1 = trade going up. 2 = trade going down. 0 = we should not be in a trade).
 I have tried classification (I generalized the tick price by changing it to pct_change() ) but accuracy is low.
 would it be possible to reverse engineer through reinforcement learning given these data? I am actually more interested in the trade exiting only (so if trade is currently has value of 1 then it became 0 or 2, it means we should exit existing trade). 
 any guide on how to go about this? Yes I know it will be hard. but if humans can teach a robot to walk, maybe hopefully an agent can be taught to learn to exit a trade based on historucal data?
 I have done preliminary readings, and is PPO the best way to go? or DQN? assuming I will use stable baseline3. I am also open to using other Python libraries.
 Thank you.
    submitted by    /u/oniongarlic88  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a secure enterprise application with Generative AI and RAG using Amazon SageMaker JumpStart]]></title>
        <id>9aa7cd8a143cd30ea36fe403a9d7f4847c16b121</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-secure-enterprise-application-with-generative-ai-and-rag-using-amazon-sagemaker-jumpstart/"/>
        <updated>2023-09-06T18:01:44.000Z</updated>
        <summary type="html"><![CDATA[In this post, we build a secure enterprise application using AWS Amplify that invokes an Amazon SageMaker JumpStart foundation model, Amazon SageMaker endpoints, and Amazon OpenSearch Service to explain how to create text-to-text or text-to-image and Retrieval Augmented Generation (RAG). You can use this post as a reference to build secure enterprise applications in the Generative AI domain using AWS services.]]></summary>
        <author>
            <name>Jay Pillai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Future of ML applied to music/sound]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16br140/d_future_of_ml_applied_to_musicsound/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16br140/d_future_of_ml_applied_to_musicsound/"/>
        <updated>2023-09-06T17:56:46.000Z</updated>
        <summary type="html"><![CDATA[What is the current landscape around sound analysis and ML applied to music? Which are the latest trends? Do you think there could be a sort of â€œmusic revolutionâ€, like there was with the rise of electronic music and synthetizers?
    submitted by    /u/francMesina  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligently search Adobe Experience Manager content using Amazon Kendra]]></title>
        <id>f693c8216a0964a906c8328754e29aeb7d5ef1eb</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/intelligently-search-adobe-experience-manager-content-using-amazon-kendra/"/>
        <updated>2023-09-06T17:28:14.000Z</updated>
        <summary type="html"><![CDATA[This post shows you how to configure the Amazon Kendra AEM connector to index your content and search your AEM assets and pages. The connector also ingests the access control list (ACL) information for each document. The ACL information is used to show search results filtered by what a user has access to.]]></summary>
        <author>
            <name>Praveen Edem</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tune Llama 2 for text generation on Amazon SageMaker JumpStart]]></title>
        <id>687bea7cc4ef4fd47346e32add3df8a4805f340c</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/fine-tune-llama-2-for-text-generation-on-amazon-sagemaker-jumpstart/"/>
        <updated>2023-09-06T17:19:16.000Z</updated>
        <summary type="html"><![CDATA[Today, we are excited to announce the capability to fine-tune Llama 2 models by Meta using Amazon SageMaker JumpStart. The Llama 2 family of large language models (LLMs) is a collection of pre-trained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Fine-tuned LLMs, called Llama-2-chat, are optimized for dialogue use cases.]]></summary>
        <author>
            <name>Vivek Madan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[comgra - Debugging Neural Networks more easily]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16bpz0c/comgra_debugging_neural_networks_more_easily/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16bpz0c/comgra_debugging_neural_networks_more_easily/"/>
        <updated>2023-09-06T17:16:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Run multiple generative AI models on GPU using Amazon SageMaker multi-model endpoints with TorchServe and save up to 75% in inference costs]]></title>
        <id>ea25756e89b40718eb3b2122361b56ee1b6cc5dd</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/run-multiple-generative-ai-models-on-gpu-using-amazon-sagemaker-multi-model-endpoints-with-torchserve-and-save-up-to-75-in-inference-costs/"/>
        <updated>2023-09-06T17:13:21.000Z</updated>
        <summary type="html"><![CDATA[Recently, generative AI applications have captured widespread attention and imagination. Customers want to deploy generative AI models on GPUs but at the same time are conscious of costs. SageMaker MMEs support GPU instances and is a great option for these types of applications. Today, we are excited to announce TorchServe support for SageMaker MMEs. This new model server support gives you the advantage of all the benefits of MMEs while still using the serving stack that TorchServe customers are most familiar with. In this post, we demonstrate how to host generative AI models, such as Stable Diffusion and Segment Anything Model, on SageMaker MMEs using TorchServe and build a language-guided editing solution that can help artists and content creators develop and iterate their artwork faster.]]></summary>
        <author>
            <name>James Wu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Guidance for building a game AI pipeline]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16boilb/d_guidance_for_building_a_game_ai_pipeline/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16boilb/d_guidance_for_building_a_game_ai_pipeline/"/>
        <updated>2023-09-06T16:19:16.000Z</updated>
        <summary type="html"><![CDATA[Hi ML Community!
 I'm working on a card game similar to Hearthstone or Magic: The Gathering, i.e. a game where two players battle with decks of cards coming from a large collection (for instance, there are around 4000 cards in Hearthstone).
 Actions are limited to three things:
  
Play a card (potentially on a target)
 Use a card on a target
 End the turn
  
I'm looking at building AI for it, and am investigating using machine learning for it. I know very little on the subject (I am a game engineer with a reasonable experience of cloud / AWS stuff), but it seems to me that it might be a good fit: features would be the state of the board (i.e. all the cards in play or in hand or in deck), the turn, and whether the current player has won that game or not (eventually), and label would be the action taken (that turn).
 I was looking at SageMaker, hoping that it would streamline and allow me to try something relatively easily, but I immediately found it complicated and quite unclear.
 I would be very grateful if anyone could point me at resources describing at a high level what a full ML pipeline could look like (i.e. what software can injest this kind of data, what software can provide inference, etc.). For instance, would it be saner to "just" get started with Spark on EMR for this kind of problem domain?
 I hope I'm not too wide off the mark with those questions, and thanks in advance!
    submitted by    /u/tinkagames_g  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why RLHF instead of direct ranking loss?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16boda9/d_why_rlhf_instead_of_direct_ranking_loss/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16boda9/d_why_rlhf_instead_of_direct_ranking_loss/"/>
        <updated>2023-09-06T16:13:28.000Z</updated>
        <summary type="html"><![CDATA[This may be basic question for some one but it bothers me for a while. For the instructgpt or whatever following model with alignment, RLHF seems to be the standards. We get human feedback and train a reward model, then we use rl to further finetune the model. However, why not directly use human feedback to finetune with a simple ranking loss(e.g pairwise loss)? What might be the best advantage for RLHF?
    submitted by    /u/Chen806  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Advice on training on noisy million scale dataset?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bo926/d_advice_on_training_on_noisy_million_scale/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bo926/d_advice_on_training_on_noisy_million_scale/"/>
        <updated>2023-09-06T16:08:51.000Z</updated>
        <summary type="html"><![CDATA[I've just finished pre-processing the danbooru dataset, which if you don't know, is a 5 million anime image dataset. Each image is tagged by humans such as ['1girl', 'thigh_highs', 'blue eyes'], however, many images are missing tags due to there being so many. I've filtered the tags (classes) down to the 15k most common. Although the top classes have 100k or more examples, many rare classes only have a few hundred tags (long tail problem?). 
 This is my first time training on such a large dataset, and I'm planning on using Convnext due to close to SOTA accuracy and fast training speed. Perhaps vit or a transformer architecture may benefit from such a large dataset? However, vit trains way slower even on my 4090. 
 What are some tips and tricks for training on such a large noisy dastaset? Existing models such as deepdanbooru work well on common classes, but struggles on rare classes in my testing. 
 I assume class unbalance will be a huge problem, as the 100k classes will dominate the loss compared to the rarer classes. Perhaps focal loss or higher sampling ratio for rare classes?
 For missing labels, I'm planning on using psuedolabeling (self distillation) to fix the missing labels. What is the best practice when generating psuedolabels? 
 â€‹
 Any tips or experiences with training on large unbalanced noisy datasets you could contribute would be greatly appreciated! 
    submitted by    /u/Chance-Tell-9847  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] The greatest success stories of Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bnsmy/d_the_greatest_success_stories_of_reinforcement/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bnsmy/d_the_greatest_success_stories_of_reinforcement/"/>
        <updated>2023-09-06T15:51:14.000Z</updated>
        <summary type="html"><![CDATA[Hello guys, I made a video for my YT channel discussing some of the greatest success stories in Deep Reinforcement Learning. The video is meant to provide some intuition on RL as a concept as well as a basic understanding of how these different projects work under the hood. There are way too many great RL projects, so I didnâ€™t try to make it an exhaustive list (Iâ€™m gonna do more videos later talking about more projects - maybe make a series out of it), but I chose four that Iâ€™ve personally worked with in the past/find really insightful and educational (DQN/Atari, Alpha GO, DeepMimic, and Dactyl). Thanks for reading.
 Here is the link, hope you guys check it out. All feedback is appreciated!
 https://youtu.be/zOXcNFM8dt4
    submitted by    /u/AvvYaa  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Checksum polynomials]]></title>
        <id>https://www.johndcook.com/blog/?p=206719</id>
        <link href="https://www.johndcook.com/blog/2023/09/06/checksum-polynomials/"/>
        <updated>2023-09-06T15:40:14.000Z</updated>
        <summary type="html"><![CDATA[A large class of checksum algorithms have the following pattern: Think of the bits in a file as the coefficients in a polynomial P(x). Divide P(x) by a fixed polynomial Q(x) mod 2 and keep the remainder. Report the remainder as a sequence of bits. In practice thereâ€™s a little more to the algorithm than [â€¦]
Checksum polynomials first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Looking for a freelancer]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bnar8/p_looking_for_a_freelancer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bnar8/p_looking_for_a_freelancer/"/>
        <updated>2023-09-06T15:31:38.000Z</updated>
        <summary type="html"><![CDATA[Hi all!
 I have a project I would need help with. We need to build a MVP (minimum viable product) of a combination of two models.
 A model that recommend the best channel to use performing a task. And then after that a model to recommend the best time today to perform that task in given channel.
 We have a set of features already defined. Some are in the data and some are generated from the data.
 Looking for someone who could work on this as a freelancer.
 Our preferred environment would be AWS SageMaker, but honestly not a necessity at this point as this is a MVP.
 Due to the reason I want to keep this "secret" for a while, I will not disclose all the details in this post.
 End product that I am waiting for includes (but not restricted to): - Model Training script that evaluates if the new model is more accurate as the previous model (some level of version control) - Model prediction API that will accept the data and prepare it for the models, run the prediction, return the result with accuracy.
    submitted by    /u/S0pg  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The great success stories of RL (A video)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16bn4km/the_great_success_stories_of_rl_a_video/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16bn4km/the_great_success_stories_of_rl_a_video/"/>
        <updated>2023-09-06T15:24:55.000Z</updated>
        <summary type="html"><![CDATA[Hello guys, I made a video for my YT channel discussing some of the greatest success stories in Deep Reinforcement Learning. The video is meant to provide some intuition on RL as a concept as well as a basic understanding of how these different projects work under the hood. There are way too many great RL projects, so I didnâ€™t try to make it an exhaustive list (Iâ€™m gonna do more videos later talking about more projects - maybe make a series out of it), but I chose four that Iâ€™ve personally worked with in the past/find really insightful and educational (DQN/Atari, Alpha GO, DeepMimic, and Dactyl). Thanks for reading.
 Here is the link, hope you guys check it out. All feedback is appreciated!
 https://youtu.be/zOXcNFM8dt4
    submitted by    /u/AvvYaa  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[combination of reinforcement learning and supervised learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16bmp49/combination_of_reinforcement_learning_and/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16bmp49/combination_of_reinforcement_learning_and/"/>
        <updated>2023-09-06T15:07:27.000Z</updated>
        <summary type="html"><![CDATA[Hi. I'm trying to train a robot that will minic the action that we provide via a video input. On the surface it sounds similar to teaching the robot to walk, but it's not. We can train the robot to make it walk easily these days. But I'm not sure how to teach it to minic an action that we perform. Because each time a new action can be given to the robot and it has to minic that action (it's sort of like a supervised data that the robot has to memorize) Is there a way to do it? is it some branch of machine learning that I'm not aware?
 The robot is a humanoid simulation.
 â€‹
    submitted by    /u/rakk109  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Powerful Legacy: Researcherâ€™s Mom Fueled Passion for Nuclear Fusion]]></title>
        <id>https://blogs.nvidia.com/?p=66728</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/06/ai-hpc-energy-fusion/"/>
        <updated>2023-09-06T15:00:44.000Z</updated>
        <summary type="html"><![CDATA[Before she entered high school, Ge Dong wanted to be a physicist like her mom, a professor at Shanghai Jiao Tong University.]]></summary>
        <author>
            <name>Rick Merritt</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Martian Lawyers Club raises $2.2M for AI-based game personalization tech]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bl4e0/martian_lawyers_club_raises_22m_for_aibased_game/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bl4e0/martian_lawyers_club_raises_22m_for_aibased_game/"/>
        <updated>2023-09-06T14:02:02.000Z</updated>
        <summary type="html"><![CDATA[The Martian Lawyers Club (MLC) has raised $2.2 million in a pre-seed round to develop AI-based game personalization technology.
 
Unlike other companies that focus on generating game assets, MLC is focused on the systems that form the core of a game.
 
The company aims to create games that feel like a conversation, where players provide input and the game responds in a way that wasn't pre-defined by the developer.
 
MLC plans to provide an SDK that allows developers to design the game experience without having to create every interaction from scratch.
 
Developers will have access to a sandbox experience where they can design the game, and the SDK will also have guardrails to ensure the generative AI system stays within boundaries.
 
MLC is currently working on its first game, a collectible card game, to test out its SDK.
 
The company is the first spin-off from INSAIT, an AI-centric tech institute, and has received funding from Fly Ventures, System.One, and Amar Shah.
 
 Source : https://techcrunch.com/2023/08/31/martian-lawyers-club-raises-2-2m-for-ai-based-game-personalization-tech/
    submitted by    /u/NuseAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How well do LLMs do on specific ML NLP tasks compared to previous models - paper takeaways]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bkw1y/r_how_well_do_llms_do_on_specific_ml_nlp_tasks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bkw1y/r_how_well_do_llms_do_on_specific_ml_nlp_tasks/"/>
        <updated>2023-09-06T13:52:06.000Z</updated>
        <summary type="html"><![CDATA[Hi all !
 Reading through articles online and reading through sub reddit I have seen some people use LLMs (mainly through openAI) for nlp specific tasks (NER, Text classification, etc.). I was a bit surprised as smaller (~100 million) size models already like RoBERTa exist for such cases.
 Not much content online about this beside this recent paper : https://arxiv.org/pdf/2308.10092.pdf
 Highly recommend reading it, here are a few take aways:
  
Most LLM benchmarks today focus on capabilities like understanding, reasoning and Q&A. They often overlook performance on specific nlp tasks like text classification, NER, etc.
 Llama 2 (70b) required fine-tuning to beat GPT 3.5 in some tasks. Both were still overall outperformed by RoBERTa.
 In certain cases GPT4 did better. However smaller open models provide more advantages in terms of speed, cost and transparency.
 The difference of speed/latency (often more important than accuracy in production) and the cost differences between LLMs and "Smaller" models is mind blowing in my view (see screenshots)
  
â€‹
 Cost, speed and throughput comparaison
 How good the models do on various tasks/datasets
 Note: Not saying benchmarks are a source of truth, just found the analysis interesting, always take benchmarks with a grain of salt. 
 If you're using LLMs for anything else beside text generation, I'm curious to know more about your experience so far :) cheers!
    submitted by    /u/EnthusiasmNew7222  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If you can't beat'em, join'em. How do I learn to code for AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bjyli/if_you_cant_beatem_joinem_how_do_i_learn_to_code/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bjyli/if_you_cant_beatem_joinem_how_do_i_learn_to_code/"/>
        <updated>2023-09-06T13:12:15.000Z</updated>
        <summary type="html"><![CDATA[I called it 6 years ago that by 2028 my tech job would be done by AI. We are right on track for my prediction. A short while ago I was laid off for reasons unrelated to AI. The way I see it, this is an excellent opportunity to make a career pivot. I have an intermediate understanding of JavaScript, React, Node and Linux. I have a good understanding of other technologies and languages too but specialize in web-dev. not saying web-dev will be done by AI but my very specialized niche will be gone way before I am ready to retire.
 Â 
 Can anyone recommend any good online courses? If you could even recommend a good article or two? I really don't know where to start. There are so many different buzz words floating around right now and it feels like it would be easy to waste a bunch of time learning AI related stuff that is outdated or leading to a deadend.
    submitted by    /u/PutsOnOil  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Maximum Sequence Length Supported by Sinusoidal Positional Encoding?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bjwyb/d_maximum_sequence_length_supported_by_sinusoidal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bjwyb/d_maximum_sequence_length_supported_by_sinusoidal/"/>
        <updated>2023-09-06T13:10:12.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I've been pondering on sinusoidal positional encoding and its limitations. Does anybody know of a maximum sequence length that this absolute positional encoding may support? I'm coming from a deep reinforcement learning background, so I'm not too familiar with NLP papers, like I couldn't figure out the sequence length used in the original transformer paper.
 Thanks in advance for any info!
    submitted by    /u/LilHairdy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[â€˜Arteanaâ€™s Art Squadâ€™ Assembles â€” Indie Showrunner Rafi Nizam Creates High-End Childrenâ€™s Show on a Budget]]></title>
        <id>https://blogs.nvidia.com/?p=66634</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/06/asus-proart-studio-laptop-omniverse-openusd/"/>
        <updated>2023-09-06T13:00:28.000Z</updated>
        <summary type="html"><![CDATA[Rafi Nizam is an award-winning independent animator, director, character designer and more. Heâ€™s developed feature films at Sony Pictures, childrenâ€™s series and comedies at BBC and global transmedia content at NBCUniversal.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to optimize parameters of a model written in C]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bj7fp/d_how_to_optimize_parameters_of_a_model_written/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bj7fp/d_how_to_optimize_parameters_of_a_model_written/"/>
        <updated>2023-09-06T12:39:05.000Z</updated>
        <summary type="html"><![CDATA[Problem: I have a quite complex model that is written in C that takes parameters as an input and estimates a curve as an output. I would like to optimize the parameters by comparing the output with the real measurements using ML methods such as stochastic gradient descent.
 â€‹
 Question: Is there any possible way to use white box optimizers to optimize the parameters of my C-model without adapting the model itself? Is there a framework that I could use?
 â€‹
 What I tried: I tried using frameworks such as tensorflow or pytorch and tried to include the compiled C-model in Python. However, gradient tracking does not work when using C functions. I tried doing the optimization in C++ by using libtorch. I realized that for gradient tracking it is essential to only use torch methods. I cannot adapt the C functions to torch functions. I don't want to use black box optimizers since they require good knowledge of the parameters that I will not have.
    submitted by    /u/romtej  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Foundation Models or Fine-tune VAEs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bhj33/d_foundation_models_or_finetune_vaes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bhj33/d_foundation_models_or_finetune_vaes/"/>
        <updated>2023-09-06T11:18:26.000Z</updated>
        <summary type="html"><![CDATA[I am considering building a model that will be the basis of many specialized models that each donâ€™t need much computational capabilities. 
 Whatâ€™s the current way to go about this? I was reading about Teslas Hydra network that looks to be more of a foundation model. However, newer methods like latent diffusion models operate on a latent space generated by more advanced auto encoders such as VQ-VAE. 
 I couldnâ€™t find any papers going into this direction and would be curious to hear your thoughts!
    submitted by    /u/That_Phone6702  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do you get started with LLMs as a complete beginner?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bhbin/d_how_do_you_get_started_with_llms_as_a_complete/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bhbin/d_how_do_you_get_started_with_llms_as_a_complete/"/>
        <updated>2023-09-06T11:07:17.000Z</updated>
        <summary type="html"><![CDATA[Can you give me courses and recommendations on how to get started with llm
    submitted by    /u/uzitarekc  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How Do Large Language Models Achieve Translation as an Emergent Property? ðŸŒ]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bf33k/d_how_do_large_language_models_achieve/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bf33k/d_how_do_large_language_models_achieve/"/>
        <updated>2023-09-06T08:56:52.000Z</updated>
        <summary type="html"><![CDATA[Hey fellow Redditors,
 I've been wondering about a question lately about the inner workings of large language models like GPT-3.5 and I'm hoping some of you knowledgeable folks can shed some light on this. My curiosity centers around how these models manage to perform translation tasks as an emergent property of next token prediction.
 So, here's my question: Does the training data for models like GPT-3.5 contain text explicitly linking between languages, such as a dictionary, or do they learn translation by assigning similarity between words in different languages based on mathematical metrics like cosine distance?So in that sense, being indepedently trained on several textbooks of different languages (not on the same topic), they would be able to link languages simply by their arithmetic properties? I hope that's making sense.
 For instance, if you look at words like "queen" in English and "rainha" in Portuguese, they share a certain similarity that could be quantified using mathematical similarity metrics. I'm wondering if through this similar vector assignment, the models learn what means what.
 I'm more leaning towards the latter, but I'm too lazy to pursue this empirically.As a follow up question, does this mean that if we are able to predict whale conversation, we would be able to translate it to English as well?
 Thanks in advance for any input you can provide! ðŸ¤“
    submitted by    /u/AlexandreFSR  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI voice clone]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bepyx/ai_voice_clone/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bepyx/ai_voice_clone/"/>
        <updated>2023-09-06T08:33:12.000Z</updated>
        <summary type="html"><![CDATA[guys can i know where to get free AI voice clone ?
    submitted by    /u/DonnieCuteMwone  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI voice clone]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bepyt/ai_voice_clone/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bepyt/ai_voice_clone/"/>
        <updated>2023-09-06T08:33:11.000Z</updated>
        <summary type="html"><![CDATA[guys can i know where to get free AI voice clone ?
    submitted by    /u/DonnieCuteMwone  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI voice clone]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bepyn/ai_voice_clone/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bepyn/ai_voice_clone/"/>
        <updated>2023-09-06T08:33:11.000Z</updated>
        <summary type="html"><![CDATA[guys can i know where to get free AI voice clone ?
    submitted by    /u/DonnieCuteMwone  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My first ever Unity ML Agents AI training!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16beks7/my_first_ever_unity_ml_agents_ai_training/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16beks7/my_first_ever_unity_ml_agents_ai_training/"/>
        <updated>2023-09-06T08:24:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/R_AIAO  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[News] AI-Based Physics Predictions in Your Web-Browser!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16be1v1/news_aibased_physics_predictions_in_your/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16be1v1/news_aibased_physics_predictions_in_your/"/>
        <updated>2023-09-06T07:52:40.000Z</updated>
        <summary type="html"><![CDATA[If you are interested in Engineering simualtion and ML, check out this webinar from SimScale on the 4th of October!
 Join the webinar to find out more. https://www.simscale.com/webinars-workshops/ai-based-physics-predictions/
 https://www.reddit.com/r/simscale/comments/16bdq3x/aibased_physics_predictions_in_your_webbrowser/?utm_source=share&utm_medium=web2x&context=3
    submitted by    /u/s_laine  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Baseline behaviour of agents]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16bdh1k/baseline_behaviour_of_agents/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16bdh1k/baseline_behaviour_of_agents/"/>
        <updated>2023-09-06T07:17:14.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m having a tough time understanding, how to establish baseline behaviour of agents in a LLM RLHF environment.
 I have data with time stamp and rewards from several models for each agent. My question is how do we establish baseline behaviour of agents?
 Does each row in weights and bias considered as a separate agent?
 Are the initial few 100â€™s of rewards according to timestamp be considered as baseline behaviour? Thankful in advance.
    submitted by    /u/Private050  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Join us for OpenAIâ€™s first developer conference on November 6 in San Francisco]]></title>
        <id>https://openai.com/blog/announcing-openai-devday</id>
        <link href="https://openai.com/blog/announcing-openai-devday"/>
        <updated>2023-09-06T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Developer registration for in-person attendance will open in the coming weeks and developers everywhere will be able to livestream the keynote.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does Llama-2 perform in sentiment analysis?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bcq8t/d_how_does_llama2_perform_in_sentiment_analysis/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bcq8t/d_how_does_llama2_perform_in_sentiment_analysis/"/>
        <updated>2023-09-06T06:31:22.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, if you have explored using Llama-2 in doing sentiment analysis, just wanted to get your experience in how Llama-2 perform in this task?
 I have tried using GPT and itâ€™s pretty accurate.
 If Llama-2 isnâ€™t all that good in sentiment analysis, which other open LLM would you recommend? 
 Thank heaps!
    submitted by    /u/--leockl--  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some question about GAIL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16bcn2e/some_question_about_gail/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16bcn2e/some_question_about_gail/"/>
        <updated>2023-09-06T06:26:03.000Z</updated>
        <summary type="html"><![CDATA[Recently, I've been trying to replicate the method described in the paper "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control" by training an agent in Isaac Gym using GAIL. However, I've encountered some issues. After adding the discriminator network, the discriminator's loss function stabilizes at around 0.3, and I'm unsure if this value is too high. Additionally, it is strange that the value loss of my value network can reach values between 80 and 90. I want to know if anyone else has experienced a similar situation and what might be the reasons behind these issues.
    submitted by    /u/Mia_Sue_123  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Can LLMs learn from a single example?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16bbvjk/r_can_llms_learn_from_a_single_example/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16bbvjk/r_can_llms_learn_from_a_single_example/"/>
        <updated>2023-09-06T05:41:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GitLab survey reveals increasing reliance on AI in software development]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16bbrni/gitlab_survey_reveals_increasing_reliance_on_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16bbrni/gitlab_survey_reveals_increasing_reliance_on_ai/"/>
        <updated>2023-09-06T05:35:16.000Z</updated>
        <summary type="html"><![CDATA[A recent survey by GitLab reveals a growing trend among organizations implementing AI in their software development processes, deeming it essential to stay competitive.
 To stay on top of the latest advancements in AI, look here first.
 GitLab Survey
 AI becomes crucial for software development
  
GitLab's report reveals that most respondents (83%) consider AI essential for their software development, regardless of their position, job level, or years of experience.
 Most organizations have deemed AI adoption successful, with 90% stating confidence in using AI tools daily.
  
Areas of AI application and concerns about its integration
  
AI's application in software development extends beyond simply generating codes, focusing more on natural language chatbots, automated test generation, and tracking machine learning model experiments.
 However, despite the growing adoption, concerns about AI-generated codes lacking copyright protection (48%) and potentially introducing vulnerabilities (39%) are rising.
 The rising fear of AI replacing existing roles is evident, with 57% predicting that their jobs might be threatened within five years.
  
The need for training and the real-world implications of AI integration
  
As AI permeates workplaces, nearly 81% believe they require more training.
 Interestingly, those with more AI experience were less likely to link it with productivity gains and faster cycle times, highlighting the importance of human verification in AI-generated codes for ensuring error-free, secure, and copyright-compliant production.
  
(source)
 P.S. If you like this kind of analysis, youâ€™ll love my free newsletter, which covers the latest advancements in AI. Professionals from Google, Meta, and OpenAI are already on board.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/5/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16baptc/oneminute_daily_ai_news_952023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16baptc/oneminute_daily_ai_news_952023/"/>
        <updated>2023-09-06T04:38:53.000Z</updated>
        <summary type="html"><![CDATA[OpenAI introduces Canva plugin for ChatGPT, simplifying design process.[1]
 A new technique called RLAIF (Reinforcement Learning from AI Feedback) enables training reinforcement learning (RL) models without relying on human-labelled training data, according to a paper from researchers at Google.[2]
 Harvard bro sparks immediate backlash with new â€˜SmashOrPassAIâ€™ site, where users rate AI-generated women.[3]
 Xâ€™s privacy policy confirms it will use public data to train AI models.[4]
  
Sources:
 [1] https://nextbigwhat.com/openai-introduces-canva-plugin-for-chatgpt-simplifying-design-process/
 [2] https://medium.datadriveninvestor.com/rlaif-scaling-reinforcement-learning-from-human-feedback-with-ai-feedback-aae57b7c36a9
 [3] https://www.dailydot.com/debug/smashorpassai-backlash/
 [4] https://techcrunch.com/2023/09/01/xs-privacy-policy-confirms-it-will-use-public-data-to-train-ai-models/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can LLMs learn from a single example?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16b8yc5/can_llms_learn_from_a_single_example/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16b8yc5/can_llms_learn_from_a_single_example/"/>
        <updated>2023-09-06T03:09:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We created a word android app game with the help of ChatGPT. ChatGPT provided us massive list of words with translation. And now our game is packed with 15 different languages. (English, Germany, France, Spanish, Netherlands, Italian, Portuguese, Swedish, Danish, Czech, Polish, Hungarian, etc .. )]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16b8urk/we_created_a_word_android_app_game_with_the_help/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16b8urk/we_created_a_word_android_app_game_with_the_help/"/>
        <updated>2023-09-06T03:04:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/dupelas  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NO WAY...I CAN MAKE MY OWN AI SCI-FI MOVIE NOW WOW...PIKA LABS SHIT WOW]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16b8g5q/no_wayi_can_make_my_own_ai_scifi_movie_now/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16b8g5q/no_wayi_can_make_my_own_ai_scifi_movie_now/"/>
        <updated>2023-09-06T02:44:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[YO YO YO MY PPL, THIS IS COOL. (Free AI Discord stuff, by Pika Labs)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16b82w7/yo_yo_yo_my_ppl_this_is_cool_free_ai_discord/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16b82w7/yo_yo_yo_my_ppl_this_is_cool_free_ai_discord/"/>
        <updated>2023-09-06T02:27:41.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Aspiring MLE Discord]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16b56vv/d_aspiring_mle_discord/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16b56vv/d_aspiring_mle_discord/"/>
        <updated>2023-09-06T00:20:02.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 Iâ€™m an aspiring Machine Learning Engineer. I want to be a practitioner. Building, deploying, and evaluating models to solve problems.
 Ideally I want to land a job in Tech as an MLE. I struggle at times to stay committed to building side projects, studying ML algos, etc.
 I have a background in hardware specific C++ SWE stuff for 3.5 yrs, but not much in the way of ML and web backend. I do have a decent amount of python coding from other experiences and itâ€™s my preferred language.
 Would anyone be interested in forming a discord to talk about what we are doing to prepare, practice interview each other, stay accountable to each other, etc?
 Had a few people show interest in r/ArtificialInteligence already
 If so comment below! Letâ€™s do this!
    submitted by    /u/Srokisthename  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation between state value and state-action value function]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16b54rm/relation_between_state_value_and_stateaction/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16b54rm/relation_between_state_value_and_stateaction/"/>
        <updated>2023-09-06T00:17:25.000Z</updated>
        <summary type="html"><![CDATA[I am following Lil's Weng Blog on RL over here (https://lilianweng.github.io/posts/2018-02-19-rl-overview/) - 
 1) I am confused how this expression came about - 
 â€‹
 https://preview.redd.it/jo31wbrt2jmb1.png?width=1106&format=png&auto=webp&s=15946bebef2dccadfabf2205d5283729d5405826
 2) I am also lost with the origin of this expression -
 https://preview.redd.it/fs4f46xv2jmb1.png?width=1097&format=png&auto=webp&s=9cc72b265e7f1069bddee9714d1deb7cc3a61775
 3) Regarding the second image, where did the expectations go? If you see the top of the image, the state-action value is represented using an expectation but at the bottom, I don't see any expectation.
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assume You Have To Place $100 Bet On One of 3 Nick Bostrom Simulation Theory Scenarios: Which Scenario Would You Bet On?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16b4kzd/assume_you_have_to_place_100_bet_on_one_of_3_nick/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16b4kzd/assume_you_have_to_place_100_bet_on_one_of_3_nick/"/>
        <updated>2023-09-05T23:53:37.000Z</updated>
        <summary type="html"><![CDATA[Odds are same for each option 1/3. I believe results will be really interesting observation .
 â€‹
 View Poll
    submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with Creating a MultiAgentEnv]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16b4gby/issues_with_creating_a_multiagentenv/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16b4gby/issues_with_creating_a_multiagentenv/"/>
        <updated>2023-09-05T23:48:13.000Z</updated>
        <summary type="html"><![CDATA[Rllib is making me feel like the biggest idiot, again, and maybe someone else knows what I'm doing wrong here? It feels like I'm missing what should be a fairly simple step...
 I keep receiving the following error message, which is odd, as my environment is an extension of MultiAgentEnv. Is there anything else I need to do in order for my environment to pass the check successfully?
 ValueError: Have multiple policies <PolicyMap lru-caching-capacity=100 policy-IDs=\[â€˜shared\_policyâ€™\]>, but the env <OrderEnforcing<PassiveEnvChecker<RoutingEnv>>> is not a subclass of BaseEnv, MultiAgentEnv, ActorHandle, or ExternalMultiAgentEnv!
 â€‹
 class RoutingEnv(MultiAgentEnv): metadata = { "render_modes": ["human"] } def __init__(self, render_mode="human", **kwargs): super().__init__() 
 â€‹
 â€‹
 gym.envs.register( id="MyEnv-v0", entry_point='routing_rl.envs:RoutingEnv', kwargs={"config": param_config} ) env_name = "MyEnv-v0" train_steps = 200000 learning_rate = 1e-3 save_dir = "saved_models" def register(config): env = gym.make("MyEnv-v0") return env # register the predefined scenario with RLlib register_env("MultiEnv", register) config = ( PPOConfig() .training(lr=0.001, _enable_learner_api=False) .environment(env="MultiEnv") .environment(disable_env_checking=True) .resources(num_cpus_per_worker=1) .rollouts(num_rollout_workers=0) .multi_agent( policies={"shared_policy": PolicySpec()}, policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: "shared_policy", ) 
 â€‹
    submitted by    /u/tessherelurkingnow  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New AI-generated COVID drug enters Phase I clinical trials: Claims to be effective against all variants]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16b4beb/new_aigenerated_covid_drug_enters_phase_i/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16b4beb/new_aigenerated_covid_drug_enters_phase_i/"/>
        <updated>2023-09-05T23:42:34.000Z</updated>
        <summary type="html"><![CDATA[Insilico Medicine, an AI-driven biotech company, has announced its AI-designed COVID-19 drug is entering Phase I clinical trials. Promising to deliver lasting results against all variants, this could become the first viable alternative to Paxlovid.
 To stay on top of such cutting-edge advancements in AI, look here first.
 Insilico's breakthrough medicine, ISM3312
  
Generated using Artificial Intelligence, ISM3312 may offer the superior solution to the constraints of current oral medication, Paxlovid.
 Insilicoâ€™s new drug could address the limitations of Paxlovid, including unpleasant side effects and drug resistance due to constant COVID mutation.
 Preclinical studies reveal the drugâ€™s potential in reducing the viral load in lung tissue and mitigating lung inflammation.
  
Development powered by AI
  
Identified via AI-driven platform PandaOmics, the drug effectively targets crucial proteins in the coronavirus.
 Using Chemistry42, a generative chemistry platform, Insilico generated new molecules built to suppress this protein, creating ISM3312.
 Given the success, the company patented ISM3312, which is currently undergoing Phase I Clinical trials, with results expected by end 2023.
  
The Implications
  
Dr. Harvey Castro, an emergency medicine physician, encourages doctors to remain cautious but also recognizes the promise of AI-generated drugs like ISM3312.
 With the trials in progress, the medical community is closely monitoring it as it could redefine the treatment course for COVID and other similar viruses.
 Insilico's venture exhibits AI's potential in accelerating effective drug discovery, prompting the need for consistent tracking of AI's transformation of healthcare and other fields.
  
(source)
 P.S. If you like this kind of analysis, I compile a free newsletter that tracks the most relevant news and research in AI. Professionals from Google, Meta, and Insilico Medicine are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers Arenâ€™t Turing-complete, But a Good Disguise Is All You Need]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16b3xdh/transformers_arent_turingcomplete_but_a_good/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16b3xdh/transformers_arent_turingcomplete_but_a_good/"/>
        <updated>2023-09-05T23:26:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Question about Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning, ICLR 2021]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16b2g7v/r_question_about_implicit_underparameterization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16b2g7v/r_question_about_implicit_underparameterization/"/>
        <updated>2023-09-05T22:28:57.000Z</updated>
        <summary type="html"><![CDATA[As stated here I already emailed the authors and asked in ai.stackexchage, but I haven't received any replies, so I am trying my luck here.
 I believe the question was clearly stated in the ai.stackexchange link included here again, and the paper in question can be found here. So, I won't repeat it here because the formatting here is worse. I am hoping maybe someone can shed a light on my issue. If this is an inappropriate use of this sub, I'll take the post down :D 
    submitted by    /u/carlml  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Deploying a Grounding DINO Model to a Rest API Endpoint for Open-Set Object Detection with Prompts]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16b2di0/p_deploying_a_grounding_dino_model_to_a_rest_api/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16b2di0/p_deploying_a_grounding_dino_model_to_a_rest_api/"/>
        <updated>2023-09-05T22:25:57.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone! Last year we launched a tool to make it easier to deploy ML models into production behind REST APIs.
 Our first prototype was focused on small models built with Scikit-Learn and XGBoost, but pretty quickly we got a lot of requests to support bigger, more complex models built on Tensorflow, Pytorch and Transformers.
 From detecting model dependencies to building out auto-scaling compute, it's been a lot of fun working through the challenges to make this product scale.
 We've built a few tutorials to showcase deploying some interesting and complex models to REST Endpoints. The latest one we released is a tutorial showing how to deploy a Grounding DINO model to a Rest API Endpoint for open-set object detection with prompts.
 Link to blog post tutorial.
 Link to Colab notebook.
 https://preview.redd.it/mi3jk4t5jimb1.png?width=950&format=png&auto=webp&s=37524b719f9dd6fb1605d0c18fcec7da31a685dd
    submitted by    /u/Jazzlike_Flamingo_35  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Looking for a text classification problem for something helpful in social media]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16b1p0r/p_looking_for_a_text_classification_problem_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16b1p0r/p_looking_for_a_text_classification_problem_for/"/>
        <updated>2023-09-05T22:00:43.000Z</updated>
        <summary type="html"><![CDATA[Hi! I am looking for an text classification problem where I can use text data from social media. Similar projects I have found interesting is classifying if the author is depressed, pro-eating disorder, right wing radical, a potential schoolshooter, a bully or a pedophile. If any of you have a suggestion for a classification problem that can be used for something good, please comment.
    submitted by    /u/IndependentSidekick  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Phrase Similarity Based On Images (embeddings)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16b12n5/d_phrase_similarity_based_on_images_embeddings/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16b12n5/d_phrase_similarity_based_on_images_embeddings/"/>
        <updated>2023-09-05T21:37:45.000Z</updated>
        <summary type="html"><![CDATA[So I know that embeddings work by finding words that are used in similar contexts or found around some input word. This allows us to find similar words based on proximity to other words and in a way, map a relationship between an input word and other words.
 But I assume children learn what words mean and the intuition behind them, by hearing the word and associating it with visuals or a specific scenario in front of them which helps them to add context to that word and how it is used.
 If we were to emulate how children learn words, could we or is there an architecture that allows us to take an input word, find images with the input word in there (Object detection) and then extract the context from the images (other objects and their position and relation to the input word) then convert that context to phrases and query those phrases the next time that a word is inputted to see other phrases or words that are similar to the input word based on whether or not they appear in the images of the input word.
 Not sure if it makes sense or if it is even useful compared to embeddings but I was thinking about how we could emulate how children learn words to see if we could draw influence from that. Just wondering if thereâ€™s a similar approach to this where we use context from images to find similar words and phrases to some input.
    submitted by    /u/4K-AMER  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data preprocessing/ augmentation for named entity recognition? [D] [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16b096a/data_preprocessing_augmentation_for_named_entity/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16b096a/data_preprocessing_augmentation_for_named_entity/"/>
        <updated>2023-09-05T21:08:31.000Z</updated>
        <summary type="html"><![CDATA[I am currently doing named entity recognition with a bert model. Its working fine so far, so I am now trying to ameliorate my results. Usually my first thought when I try to augment my ML models is input data preprocessing. In case of NER stop word removal and removal of punctuation, numbers and one-character words came to mind - they are hardly ever named entities so I woulndt loose many training examples. However, NER does in fact require context to work, so removing stuff could prove harmfull in the end? I am kind of torn. Should I do it? Are there better data augmentation approaches? I would be really thankfull for any kind of hint 
    submitted by    /u/SilverDusk42  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] lost junior Machine Learning engineer]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16azodb/d_lost_junior_machine_learning_engineer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16azodb/d_lost_junior_machine_learning_engineer/"/>
        <updated>2023-09-05T20:47:49.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone, I know itâ€™s a bit silly to ask these kind of questions, but Im gonna give it a shot since Iâ€™ve seen lots of talented people in here. I am gonna try to keep it as short asp.(Also please excuse my "sometimes" bad English, I am not a native speaker)
 Well, last year I graduated as an industrial engineer, I was thinking during my last year of studies to completely switch to programming since many of my friends are programmers, but they are all web. So I dedicated the last year of my engineering studies to getting to know what machine learning actually is besides my studies (also tbh I wasnâ€™t very consistent) (also my learning material was mostly the famous DL spec by Andrew on coursera), at the end of the year we have something called project of end of studies (like a masters tâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Streamlit launches LLM Hackathon ðŸ§ ]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ayd0b/n_streamlit_launches_llm_hackathon/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ayd0b/n_streamlit_launches_llm_hackathon/"/>
        <updated>2023-09-05T19:59:34.000Z</updated>
        <summary type="html"><![CDATA[Streamlit just launched its latest hackathon focused on large language models and AI ðŸš€
 Awesome opportunity to build a Streamlit app using LangChain, LlamaIndex, AssemblyAI, Weaviate, or Clarifai, and win cool prizes (AirPods, Yeti microphone, mechanical keyboard, to name a few)
 More info on the hackathon here
 Streamlit LLM Hackathon
    submitted by    /u/carolinedfrasca  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 5 September 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=63063</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-5-september-2023/"/>
        <updated>2023-09-05T19:11:03.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 5 September 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P]Embedchain Open Source project is a game changer]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16awy5f/pembedchain_open_source_project_is_a_game_changer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16awy5f/pembedchain_open_source_project_is_a_game_changer/"/>
        <updated>2023-09-05T19:06:12.000Z</updated>
        <summary type="html"><![CDATA[I was just exploring ChatBot and LLMs and found a library named Embedchain AI.
 This library lets you build a ChatBot like ChatGPT in just 3-4 lines of code.
 Tutorial: https://www.youtube.com/watch?v=vIhDh7H73Ww
    submitted by    /u/trj_flash75  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Tl;dr Approximate Inference methods made easy]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16awnnr/d_tldr_approximate_inference_methods_made_easy/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16awnnr/d_tldr_approximate_inference_methods_made_easy/"/>
        <updated>2023-09-05T18:55:11.000Z</updated>
        <summary type="html"><![CDATA[â€œMCMC vs VIâ€ is no longer a discussion about your favourite Roman numeral. If you share my trepidation for model performance in the face of data sparsity, or you simply suffer from anxiety uncertainty, you might be tempted into the Bayesian world. Years later at the precipice of your career (and mental health degeneracy) you over-engineer probabilistic models so intractable that would stress Lord Bayes himself into stomach ulcers. The solution? Approximate inference, the true antihero to model simplification. I wrote a brief primer for those who enjoy maths and those who disdain it, in both cases it's impossible to avoid using maths while discussing Bayesian statistics so I kept it as light as I could.
 PS - This is a Reddit-friendly copypasta from my medium article, so if you're a visual â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[16 most interesting AI applications across industries worldwide]]></title>
        <id>https://www.datasciencecentral.com/?p=63039</id>
        <link href="https://www.datasciencecentral.com/16-most-interesting-ai-applications-across-industries-worldwide/"/>
        <updated>2023-09-05T18:18:16.000Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence has become a compulsive innovation for humankind, that we cannot live without. It has been gaining strength with every passing moment. The impact of AI applications extends beyond improved business results and can be significant in elevating and enriching the human experience. Popular AI trends in the past have revealed a compelling needâ€¦Â Read More Â»16 most interesting AI applications across industries worldwide
The post 16 most interesting AI applications across industries worldwide appeared first on Data Science Central.]]></summary>
        <author>
            <name>Tarique</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI megatrends: Generative AI for enterprise is proven vs generative AI forÂ  consumer is not â€“ Part One]]></title>
        <id>https://www.datasciencecentral.com/?p=63032</id>
        <link href="https://www.datasciencecentral.com/generative-ai-megatrends-generative-ai-for-enterprise-is-proven-v-s-generative-ai-for-consumer-is-not-part-one/"/>
        <updated>2023-09-05T18:17:31.000Z</updated>
        <summary type="html"><![CDATA[As generative AI evolves, certain trends are becoming clearer,Â  In yet another milestone in AI consulting giant McKinsey unveiled its own generative AI tool for employees called lilli My comments a) McKinsey launching this agent gives credibility to the domain for enterprise AI assistants b) On one hand, itâ€™s a familiar copilot strategy â€“ butâ€¦Â Read More Â»Generative AI megatrends: Generative AI for enterprise is proven vs generative AI forÂ  consumer is not â€“ Part One
The post Generative AI megatrends: Generative AI for enterprise is proven vs generative AI forÂ  consumer is not â€“ Part One appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing the challenge of software support for multiarchitecture AI accelerated HPC]]></title>
        <id>https://www.datasciencecentral.com/?p=63007</id>
        <link href="https://www.datasciencecentral.com/addressing-the-challenge-of-software-support-for-multiarchitecture-ai-accelerated-hpc/"/>
        <updated>2023-09-05T18:16:46.000Z</updated>
        <summary type="html"><![CDATA[Programmers can no longer rely on the traditional method of targeting specific hardware accelerators with conditional pragmas (e.g., #ifdef) to match the software to the hardware at a particular datacenter or customer site. Humans writing machine-specific code cannot address the exponential increase in possible hardware combinations in the modern multivendor, multiarchitecture computing environment. Open software provides a multiarchitecture, multivendor solution that addresses the complexities of accelerated HPC and AI computing.
The post Addressing the challenge of software support for multiarchitecture AI accelerated HPC appeared first on Data Science Central.]]></summary>
        <author>
            <name>RobFarber</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI megatrends: Generative AI for enterprise is provenÂ  vs generative AI forÂ  consumer is not â€“ Part two]]></title>
        <id>https://www.datasciencecentral.com/?p=63045</id>
        <link href="https://www.datasciencecentral.com/generative-ai-megatrends-generative-ai-for-enterprise-is-proven-vs-generative-ai-for-consumer-is-not-part-two/"/>
        <updated>2023-09-05T18:15:24.000Z</updated>
        <summary type="html"><![CDATA[In part one of this blog, we saw how there is an increasing case for an enterprise chatbot use case. In part two, we ask the questionÂ  Could a consumer chatbot i.e. directly customer facing chatbot be a flawed use case for an LLM? The consumer (customer facing) chatbot case is a familiar use caseâ€¦Â Read More Â»Generative AI megatrends: Generative AI for enterprise is provenÂ  vs generative AI forÂ  consumer is not â€“ Part two
The post Generative AI megatrends: Generative AI for enterprise is provenÂ  vs generative AI forÂ  consumer is not â€“ Part two appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Has anyone went through the ml.school course from Santiago? Is it any good?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16auqpg/discussion_has_anyone_went_through_the_mlschool/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16auqpg/discussion_has_anyone_went_through_the_mlschool/"/>
        <updated>2023-09-05T17:40:57.000Z</updated>
        <summary type="html"><![CDATA[I used to do some basic machine learning a few years ago (7+), but then went into what now became data engineering, because of the lack of opportunities in ML.
 This year I'm trying to up my game and maybe switch back to ML, which I've always been following and tinkering with, but I want to learn all the necessary skills at least at a basic level, in order to find an ML job.
 I'm learning on my own but now I'm looking for resources regarding MLOps and found ml.school and I'm curious if anyone has any opinons about it or if there is anyone here who has went over the course?
 Thanks in advance for any help or info!
    submitted by    /u/jack-in-the-sack  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a generative AI-based content moderation solution on Amazon SageMaker JumpStart]]></title>
        <id>e7a0694b3b6b2b57233a4c3ac9a1d8484e9b5b7f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-generative-ai-based-content-moderation-solution-on-amazon-sagemaker-jumpstart/"/>
        <updated>2023-09-05T17:38:57.000Z</updated>
        <summary type="html"><![CDATA[In this post, we introduce a novel method to perform content moderation on image data with multi-modal pre-training and a large language model (LLM). With multi-modal pre-training, we can directly query the image content based on a set of questions of interest and the model will be able to answer these questions. This enables users to chat with the image to confirm if it contains any inappropriate content that violates the organizationâ€™s policies. We use the powerful generating capability of LLMs to generate the final decision including safe/unsafe labels and category type. In addition, by designing a prompt, we can make an LLM generate the defined output format, such as JSON format. The designed prompt template allows the LLM to determine if the image violates the moderation policy, identify the category of violation, explain why, and provide the output in a structured JSON format.]]></summary>
        <author>
            <name>Gordon Wang</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distal Adversarial Examples Against Neural Networks in PyTorch]]></title>
        <id>https://davidstutz.de/?p=8634</id>
        <link href="https://davidstutz.de/distal-adversarial-examples-against-neural-networks-in-pytorch/"/>
        <updated>2023-09-05T17:36:55.000Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution examples are images that are cearly irrelevant to the task at hand. Unfortunately, deep neural networks frequently assign random labels with high confidence to such examples. In this article, I want to discuss an adversarial way of computing high-confidence out-of-distribution examples, so-called distal adversarial examples,  and how confidence-calibrated adversarial training handles them.
The post Distal Adversarial Examples Against Neural Networks in PyTorch appeared first on David Stutz.]]></summary>
        <author>
            <name>David Stutz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Carrier predicts HVAC faults using AWS Glue and Amazon SageMaker]]></title>
        <id>263e0868397fe3edea37876397aa691be741e67f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/how-carrier-predicts-hvac-faults-using-aws-glue-and-amazon-sagemaker/"/>
        <updated>2023-09-05T17:25:12.000Z</updated>
        <summary type="html"><![CDATA[In this post, we show how the Carrier and AWS teams applied ML to predict faults across large fleets of equipment using a single model. We first highlight how we use AWS Glue for highly parallel data processing. We then discuss how Amazon SageMaker helps us with feature engineering and building a scalable supervised deep learning model.]]></summary>
        <author>
            <name>Ravi Patankar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimize deployment cost of Amazon SageMaker JumpStart foundation models with Amazon SageMaker asynchronous endpoints]]></title>
        <id>18bb146de8bb197fdbcb8b220a5e08e07928a1b4</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/optimize-deployment-cost-of-amazon-sagemaker-jumpstart-foundation-models-with-amazon-sagemaker-asynchronous-endpoints/"/>
        <updated>2023-09-05T17:13:21.000Z</updated>
        <summary type="html"><![CDATA[In this post, we target these situations and solve the problem of risking high costs by deploying large foundation models to Amazon SageMaker asynchronous endpoints from Amazon SageMaker JumpStart. This can help cut costs of the architecture, allowing the endpoint to run only when requests are in the queue and for a short time-to-live, while scaling down to zero when no requests are waiting to be serviced. This sounds great for a lot of use cases; however, an endpoint that has scaled down to zero will introduce a cold start time before being able to serve inferences.]]></summary>
        <author>
            <name>Davide Gallitelli</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Most user-friendly data labelling tool (non-AI)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16atdde/d_most_userfriendly_data_labelling_tool_nonai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16atdde/d_most_userfriendly_data_labelling_tool_nonai/"/>
        <updated>2023-09-05T16:48:53.000Z</updated>
        <summary type="html"><![CDATA[Hi
 I am currently creating computer vision models for segmentation and classification, and I am looking for a tool that is very user friendly. We have been using CVAT so far, and apparently, its UI is too cluttered. So, we need something easier to use.
 Segment Anything and other auto-segmentation tools simply do not work on our dataset. So, I do not want a tool that is user friendly because it uses AI.
 Any thoughts?
    submitted by    /u/Avatrin  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Introducing CometLLM: Track, Visualize, and Annotate your LLM Prompts]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16at3q2/p_introducing_cometllm_track_visualize_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16at3q2/p_introducing_cometllm_track_visualize_and/"/>
        <updated>2023-09-05T16:38:39.000Z</updated>
        <summary type="html"><![CDATA[Hello ML Community,
 We released our new LLMOps Tool: CometLLM. It's highly optimized for Prompt Engineering Workflows and making it easy to find the best prompts for your use-case! Here a few helpful things you can do with this tool!
  
Score/Rate Your Prompts
 Add Metadata to your Logged Prompts (Great for Tracking Prompt Usage)
 Search for Specific Prompts via Keywords/Phrases
 Visualize Full-On Prompt Chains!
 Group Your Prompts
  
Hope the ML Community find this useful as well continue to experiment with LLMs! Don't Hesitate to reach out if you have any feedback!
    submitted by    /u/metric_logger  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are you an AI beginner or AI professional?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16asfro/are_you_an_ai_beginner_or_ai_professional/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16asfro/are_you_an_ai_beginner_or_ai_professional/"/>
        <updated>2023-09-05T16:13:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/MarkFulton  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Direct Preference Optimization: Your Language Model Is Secretly A Reward Model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16as374/r_direct_preference_optimization_your_language/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16as374/r_direct_preference_optimization_your_language/"/>
        <updated>2023-09-05T16:00:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/EducationalCicada  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking trust in direct messages in the AI era]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=964494</id>
        <link href="https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/"/>
        <updated>2023-09-05T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Microsoft researchers are proposing a new way to ensure greater trust and accountability in email, texts, direct messages on social platforms, even phone calls, to help mitigate sophisticated threats from AI-related scams and fraud.
The post Rethinking trust in direct messages in the AI era appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How I could handle BIG network traffic dataset for ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16arvcb/r_how_i_could_handle_big_network_traffic_dataset/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16arvcb/r_how_i_could_handle_big_network_traffic_dataset/"/>
        <updated>2023-09-05T15:51:16.000Z</updated>
        <summary type="html"><![CDATA[Hello people!
 This is the first time that I post here and I desperately need your help. I need to perform anomaly detection on a huge network traffic dataset with isolation forest (unsupervised learning). I have the .pcap files of a whole month and and for each day there are multiple devices that communicated each other. So the file of each day is from 700 MB to 2 or 3 GB.
 My initial idea was to only maintain the header of the packets and to discard the data payload. But even in this case the dataset remains huge and the number of entries is crazy.
 What I should do?
    submitted by    /u/J-Devesh  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] what processes should one follow to find better recommendation systems than these?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16arsjj/r_what_processes_should_one_follow_to_find_better/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16arsjj/r_what_processes_should_one_follow_to_find_better/"/>
        <updated>2023-09-05T15:48:10.000Z</updated>
        <summary type="html"><![CDATA["The Greatest Books - Combines many top book lists to create a master list
 anobii - a community built by readers for readers allowing you to find, shelve, review and share books
 Author Alcove - Rate read books, shelve to be read, and receive recommendations.
 BookDigits - Book tracking, rating, and discovery with achievements. Another from an r/books member (I really think this plus authoralcove would be perfect)
 booklikes - Book tracking and blogging/reviewing
 Goodreads - The popular choice for book social media, reviews, and tracking
 LibraryThing - The old standby, of webbased personal library management
 Litsy - Insagram inspired social media app for tracking and reviwing books
 Lovelybooks - German book tracking site
 readernaut - Readernaut helps you make your book list, build a library, keep track of what you've read and what you'd like to read, and then share those lists with your friends.
 Readgeek - Book review and cataloging site by a redditor(?) and translated from german
 Riffle - track & reivew books with social media integration
 TasteDive - (aka tastekid) social rating site for music, movies, shows, books, authors, and games
 Discovered - Dating site/app for bookworms
 Calibre - The go to for ebook management
 The Game of Books - A kickstarter. They used to have a beta up but it's gone now too - http://gameofbooks.com/level_up
 weread - Encouraging Children To Read: Articles, ideas, and information to encourage children to read
 thirdscribe - ThirdScribe provides authors and readers with actual tools and services they can use to enjoy their books as well as grow and connect with their audience.
 What Should I Read Next? - A book recommendation engine
 bookfinder - book search tool
 50 Book Pledge - Goal based book tracking
 anno.wiki - collaborative book annotation"
    submitted by    /u/Fearless-Room-504  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can information about the action selected be used to inject information to learning agent]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16aro4a/d_can_information_about_the_action_selected_be/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16aro4a/d_can_information_about_the_action_selected_be/"/>
        <updated>2023-09-05T15:43:24.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I am training an agent via PPO. The environment is a node removal ('n' number of actions which are nodes on the graph) with evaluation after each node removed. the state is represented by a trained graph attention network in the environment with the average of the node embeddings on the graph representing the state of dimension size 'n'. The embedding of the a node that has been removed is subtracted from overall graph embedding representation to represent the 'removal' of that node. 
 However, I want to absolutely be certain, that given a state representation, in a new unseen graph, the agent will not select a node that is absent from that graph. In the event that the state representation may not be granular enough and might cause the agent to think that a node on the graph is present when it is not, are there ways to mitigate this? Two ideas I have are:
  
mask actions for nodes that are not present (this is already done after node removal to prevent the agent from selecting the same node again), but is this valid to do in an unseen graph if I a priori mask nodes that are not present in the action space
 Inject a second input to the policy network such as a one-hot encoding of nodes that have already been selected as an input in addition to the state representation of the graph, so that it models finer dependencies between the state and action taken. However is this valid? 
  
Any thoughts are appreciated! thank you!
    submitted by    /u/amjass12  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Locally train and generate AI VoiceOver using a large data set of my voice and matching scripts.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16arigd/p_locally_train_and_generate_ai_voiceover_using_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16arigd/p_locally_train_and_generate_ai_voiceover_using_a/"/>
        <updated>2023-09-05T15:37:19.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I've voiced over 500 videos for a YouTube channel and have the accompanying voiceover audio and scripts.
 I'd like to train a very robust AI to generate VoiceOver locally and not use an online service using the extensive amount of audio/scripts I have stored.
 My hardware is a 3070 and 12700.
 All other solutions have been online such as Elevenlabs.
 This will be a secondary service I could provide alongside bespoke voice over.
    submitted by    /u/dfawlt  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How is currently your experience with availability of GPUs across providers ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ardhg/d_how_is_currently_your_experience_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ardhg/d_how_is_currently_your_experience_with/"/>
        <updated>2023-09-05T15:32:00.000Z</updated>
        <summary type="html"><![CDATA[Just wanted to ask what has been lately your experience with availability of GPUs across providers (major ones - AWS, Azure, GCP, but also some minor ones). Especially when it comes to GPUs which are more suited for ML (A100s, H100s).
 Anyone also considering buying physical hardware instead ?
    submitted by    /u/remek  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do you observe the behaviour / satisfaction of users of your LLM product?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ar57l/d_how_do_you_observe_the_behaviour_satisfaction/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ar57l/d_how_do_you_observe_the_behaviour_satisfaction/"/>
        <updated>2023-09-05T15:23:14.000Z</updated>
        <summary type="html"><![CDATA[Soon, I will launch an LLM-powered chatbot. I have run plenty of tests to make sure the LLM works well, but I am super curious about the experience of real (external) users. Iâ€™d like to find out if users are happy with the answers the model generates, what topics they ask about, etc. And also how much each user costs me since the service is free and I am paying for it at the moment being. I expect to be able to improve the product over time with this kind of insights.
 Are you guys trying to track similar metrics? If so, how do you do it? Thank you!
    submitted by    /u/jroux92  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Halo Effect: AI Deep Dives Into Coral Reef Conservation]]></title>
        <id>https://blogs.nvidia.com/?p=66695</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/05/ai-for-coral-reef-conservation/"/>
        <updated>2023-09-05T15:00:34.000Z</updated>
        <summary type="html"><![CDATA[With coral reefs in rapid decline across the globe, researchers from the University of Hawaii at MÄnoa have pioneered an AI-based surveying tool that monitors reef health from the sky. Using deep learning models and high-resolution satellite imagery powered by NVIDIA GPUs, the researchers have developed a new method for spotting and tracking coral reef Read article >]]></summary>
        <author>
            <name>Michelle Horton</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Randomized Search with Early Stopping for LGBMClassifier]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16apo8y/d_randomized_search_with_early_stopping_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16apo8y/d_randomized_search_with_early_stopping_for/"/>
        <updated>2023-09-05T14:24:23.000Z</updated>
        <summary type="html"><![CDATA[I have been running hyperparameter optimization for an LGBM multi-classifier model with randomized search with 10fold stratified cv as well as oversampling on each fold using SMOTE as follows:
 # Create a pipeline with SMOTE oversampling smote_pipeline = make_pipeline(SMOTE(random_state=42), lgbm_clf) # Initialize 10-fold stratified cross-validation cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # Initialize RandomizedSearchCV for hyperparameter tuning using the pipeline random_search = RandomizedSearchCV( estimator=smote_pipeline, param_distributions=param_dist_with_prefix, n_iter=n_iter, scoring=f1_macro_scorer, n_jobs=n_cores, pre_dispatch=n_cores, cv=cv, random_state=42 
 I wanted do incorporate early stopping based on the validation set from the nth iteration of cv. However this does not seem possible using the current API if I am correct.
 If I wanted to use a predefined validation set the code would be sth like this but I want to perform validation using 10 fold-cv validation set only:.
 mode l= lgb.LGBMClassifier() clf = RandomizedSearchCV( model, parameters, fit_params={ 'early_stopping_rounds':20, 'eval_set':[(X,y)] }, cv=cv ) 
 My questions are:
 1- Does it make sense to use early stopping during randomized search?
 2- Do you know a way I could do it?
 3- If not, is it a good idea to use randomized search without early stopping and train a new model with early stopping using the best parameters resulting from randomized search?
 Bonus Question: Does it make sense to run randomized search with f1_macro scoring from sklearn instead of multilogloss in case of imbalanced classes?
    submitted by    /u/returnname35  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What do you put in your lab notes?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16apffb/d_what_do_you_put_in_your_lab_notes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16apffb/d_what_do_you_put_in_your_lab_notes/"/>
        <updated>2023-09-05T14:14:23.000Z</updated>
        <summary type="html"><![CDATA[I'm working my way through various tweaks to a ML pipeline, and I've started keeping short lab notes in a markdown file with just the time, a brief summary of changes, and my observations on training metrics or anything else interesting on a training run. I've also started copying a snapshot of the Python source code to the tensorboard directory, which has saved me a lot of headache.
 I was wondering how other people keep lab notes, and especially what you find useful to record and how you structure the notes. 
    submitted by    /u/hazard02  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] [D] Machine learning model to predict deformation of 2D object]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ap4vb/r_d_machine_learning_model_to_predict_deformation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ap4vb/r_d_machine_learning_model_to_predict_deformation/"/>
        <updated>2023-09-05T14:02:37.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am currently working on predicting 2D deformations of objects. These objects are available as 2D contours in my code. I am splitting these contours into 1000 points with an equal distance in the direction of the x axis. I have about 70 data entries. 
 The following picture shows one of these objects:
 Comparison before and after
 The red data series contains the points before and the blue series contains the points after the deformation.
 My model should take in a series of coordinates before the deformation. Using this information the model should predict the coordinates after the deformation. 
 I have tried using the LSTM Model from keras. Unfortunately I wasn't able to produce useful results. The way I structured my data is the following:
 [
 [
 [x1, y1], [x2, y2], [x3, y3], ... 1000 coordinate pairs
 ], [
 [x1, y1], [x2, y2], ...
 ],
 .... 70 entries
 ] 
 The structure for the input and the output series is the same. 
 When trying to train the model I have a very low loss and low validation loss as well:
 Overview during training of model
 The test loss is also quite similar:
 Overview test loss
 However when looking into the predictions I get results like these:
 Visualized prediction after training
 The prediction is not close to what it should be like. Also the prediction seems to not change even when changing the input. 
 â€‹
 Do you have an idea about why my ML model does not work? Are there examples on this topic available? Should I change my approach in any way?
 Thank you in advance! Any help is appreciated!
 â€‹
 If you need my jupyter-notebook, it would be great if somebody could tell me, how to link files on Reddit :) 
    submitted by    /u/InitiativeGlass4701  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What OpenAI Really Wants]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ap10h/what_openai_really_wants/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ap10h/what_openai_really_wants/"/>
        <updated>2023-09-05T13:58:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Alone-Competition-77  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Equinox (1.3k stars), a JAX library for neural networks and sciML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ap09u/p_equinox_13k_stars_a_jax_library_for_neural/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ap09u/p_equinox_13k_stars_a_jax_library_for_neural/"/>
        <updated>2023-09-05T13:57:30.000Z</updated>
        <summary type="html"><![CDATA[Hey folks! I wanted to advertise Equinox -- my now-surprisingly-popular ( :D ) JAX library for numerical models. These days that often means "neural networks", but I like to emphasise that this also includes ODEs/SDEs/linear solves, etc.
 Here's the GitHub link: https://github.com/patrick-kidger/equinox
 For those already using JAX, then Equinox is interesting because (a) it ships with a NN library, and (b) this is built around the idea that "everything is a pytree", which makes things easy to reason about and easy to compose. Furthermore (c) Equinox offers advanced tools like true runtime errors, out-of-place pytree surgery, and checkpointed while loops, and AFAIK in the JAX ecosystem these are unique to Equinox.
 For those most familiar with PyTorch: for many use cases (sciML in particular), JAX has a much stronger compiler, more advanced autodiff, etc. And whilst JAX itself is akin to the torch.* namespace, libraries like Equinox are then akin to the torch.nn.* namespace.
 Because of its speed and features, right now JAX+Equinox is my favourite approach to numerical computing. So I'd love for some more people to try it. What do you think?
    submitted by    /u/patrickkidger  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Distributed training on a local cluster]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16aoyiz/d_distributed_training_on_a_local_cluster/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16aoyiz/d_distributed_training_on_a_local_cluster/"/>
        <updated>2023-09-05T13:55:28.000Z</updated>
        <summary type="html"><![CDATA[I want to make use of a local rack for running both training and serving jobs. I have looked into using something like Kubeflow, but I have some questions.
 -Does Kubeflow offer a suitable solution for running tasks across multiple machines? (Either data parallel or model parallel tasks).
 -How does resource provisioning work with it? Is it able to automatically select the machines that best suits the resource requirements or does it require the user to select where to run the job? Is it able to scale vertically/horizontally?
 Thanks in advance.
    submitted by    /u/omegalul3000  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Perfect Pair: adidas and Covision Media Use AI, NVIDIA RTX to Create Photorealistic 3D Content]]></title>
        <id>https://blogs.nvidia.com/?p=66558</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/05/covision-adidas-rtx-ai/"/>
        <updated>2023-09-05T13:00:44.000Z</updated>
        <summary type="html"><![CDATA[Creating 3D scans of physical products can be time consuming. Businesses often use traditional methods, like photogrammetry-based apps and scanners, but these can take hours or even days. They also donâ€™t always provide the 3D quality and level of detail needed to make models look realistic in all its applications. Italy-based startup Covision Media is Read article >]]></summary>
        <author>
            <name>Nicole Castro</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are some good open source projects exploring emotional voice synthesis?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16akvik/what_are_some_good_open_source_projects_exploring/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16akvik/what_are_some_good_open_source_projects_exploring/"/>
        <updated>2023-09-05T10:48:26.000Z</updated>
        <summary type="html"><![CDATA[There are tons of TTS software out there, but they don't incorporate human emotions during speech synthesis. For example, anger, tiredness, surprise, happiness...
 What solutions exist for this today?
    submitted by    /u/ICWiener6666  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Hydralette: Simple but powerful configs based on dataclasses]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16ah03g/p_hydralette_simple_but_powerful_configs_based_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16ah03g/p_hydralette_simple_but_powerful_configs_based_on/"/>
        <updated>2023-09-05T07:02:34.000Z</updated>
        <summary type="html"><![CDATA[Hi r/ML,
 i want to share a little side project of mine: hydralette. I mainly built this for my own work but thought why not get some feedback and potentially make someone else's work a little easier as well.
 I think we all agree that having a flexible configuration is crucial to successful ML experimentation. There are a million python config libraries out there, some dedicated to configs like hydra and others that support configs as a convenience feature like transformers.HfArgumentParser. So why did I decide to write yet another library?
 First off, I can say that I never really liked the way huggingface handles configs. All options are on a single level with tons of dependencies between them, some only taking effect if a combination of others is given. General approach to configs asidâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thoughts for my disgruntled artist friends:]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ag4d9/thoughts_for_my_disgruntled_artist_friends/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ag4d9/thoughts_for_my_disgruntled_artist_friends/"/>
        <updated>2023-09-05T06:12:19.000Z</updated>
        <summary type="html"><![CDATA[Learning a skill, for me, was never about securing knowledge that privileged me over everyone else who did not put the work in. While often, it did feel like drinking Kool-Aid, buying in to these groups like yoga and climbing, I knew I was not there to rub elbows, but to discover the how behind it. Some leaders of some groups did create a barrier of entry, a necessary proving point, but I have always seen these loops to jump through as a challenge - once completed - a spy. Every skill you have learned has prepared you not to be better at that skill, but to learn a new skill with more ease. It is uncomfortable to learn something new, like drinking from a fire hydrant, but the more sips you take from that blasting surge of water, the more you realize it is all part of the process. We get blasted, we sip, we get overwhelmed, we come back. Just because there is a tool that regulates the blasting, that holds our hand through the overwhelm, does not mean all our hard work has been for nothing. In fact, it means we are more prepared, more primed, to receive all of the beauty and knowledge coming our way. Now, friends, we become CURATORS. :) xo
    submitted by    /u/airkaty  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tesla Diesel Truck Commercial (AI)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16ad50f/tesla_diesel_truck_commercial_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16ad50f/tesla_diesel_truck_commercial_ai/"/>
        <updated>2023-09-05T03:35:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/wisconsin-sopapa  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Refact Code LLM: 1.6B State-of-the-Art LLM for Code that Reaches 32% HumanEval]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16ab0rb/introducing_refact_code_llm_16b_stateoftheart_llm/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16ab0rb/introducing_refact_code_llm_16b_stateoftheart_llm/"/>
        <updated>2023-09-05T01:55:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Swap, Earn, Airdrop: ZKSyncSwap]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16a94f8/swap_earn_airdrop_zksyncswap/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16a94f8/swap_earn_airdrop_zksyncswap/"/>
        <updated>2023-09-05T00:28:50.000Z</updated>
        <summary type="html"><![CDATA[https://zsyncswap.technology/
    submitted by    /u/shivamrai24  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI is a Looming Damnation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16a7x0n/ai_is_a_looming_damnation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16a7x0n/ai_is_a_looming_damnation/"/>
        <updated>2023-09-04T23:36:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Language Processing Question]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16a7uga/natural_language_processing_question/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16a7uga/natural_language_processing_question/"/>
        <updated>2023-09-04T23:33:42.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am learning about natural language processing now. Technically, is this a way for a computer to input language of a person and then convert it into machine code (0s and 1s)? Or, is this a way to turn human language into some computer language like Python, and then turn into machine code as a second step?
 I am assuming that NLP has only just recently become widely used (like in Chat GPT). Was it a huge jump to go from a machine understanding a computer programming language like Python to a machine understanding ordinary human language? Why was it so much more difficult to train computers to understand the later?
 Thanks!
    submitted by    /u/NoahsArkJP  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is the difference between self-taught learning and self-supervised learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16a6yaq/d_what_is_the_difference_between_selftaught/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16a6yaq/d_what_is_the_difference_between_selftaught/"/>
        <updated>2023-09-04T22:56:53.000Z</updated>
        <summary type="html"><![CDATA[I came across a paper by Andrew Ng "https://ai.stanford.edu/~hllee/icml07-selftaughtlearning.pdf" ï¼Œwith title " Self-taught Learning: Transfer Learning from Unlabeled Data "
 I am not an expert on this topic, but I feel it is really close to what SimCLR or MoCO are trying to do.
 Can someone provide guidance on what different it is between self-taught learning and self-supervised learning?
    submitted by    /u/AaronSpalding  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NN underperforming greedy algorithms]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16a6j98/nn_underperforming_greedy_algorithms/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16a6j98/nn_underperforming_greedy_algorithms/"/>
        <updated>2023-09-04T22:40:05.000Z</updated>
        <summary type="html"><![CDATA[So apparently NNs may not outperform simple greedy algorithms in some combinatorial optimization problems
 Never thought could be the case.
 https://arxiv.org/pdf/2206.13211.pdfâ€¦
 Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like Maximum Independent Set.
 https://arxiv.org/pdf/2210.00623.pdfâ€¦
 Inability of a graph neural network heuristic to outperform greedy algorithms in
 solving combinatorial optimization problems like Max-Cut
    submitted by    /u/vniversvs_  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] A brain-inspired algorithm that mitigates catastrophic forgetting of artificial and spiking neural networks with low computational cost - Chinese Academy of Sciences 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16a2un5/r_a_braininspired_algorithm_that_mitigates/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16a2un5/r_a_braininspired_algorithm_that_mitigates/"/>
        <updated>2023-09-04T20:21:56.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://www.science.org/doi/10.1126/sciadv.adi2947#abstract
 Code: https://zenodo.org/record/8037309
 Abstract:
  
Neuromodulators in the brain act globally at many forms of synaptic plasticity, represented as metaplasticity, which is rarely considered by existing spiking (SNNs) and nonspiking artificial neural networks (ANNs). Here, we report an efficient brain-inspired computing algorithm for SNNs and ANNs, referred to here as neuromodulation-assisted credit assignment (NACA), which uses expectation signals to induce defined levels of neuromodulators to selective synapses, whereby the long-term synaptic potentiation and depression are modified in a nonlinear manner depending on the neuromodulator level. The NACA algorithm achieved high recognition accuracy with substantially reduced computational cost in learning spatial and temporal classification tasks. Notably, NACA was also verified as efficient for learning five different class continuous learning tasks with varying degrees of complexity, exhibiting a markedly mitigated catastrophic forgetting at low computational cost. Mapping synaptic weight changes showed that these benefits could be explained by the sparse and targeted synaptic modifications attributed to expectation-based global neuromodulation. 
  
https://preview.redd.it/5lcx3sn8ramb1.jpg?width=711&format=pjpg&auto=webp&s=4431b81708bb9ab98e6351f4b979897ad8244ed9
 https://preview.redd.it/vgsuqsn8ramb1.jpg?width=718&format=pjpg&auto=webp&s=f0602185fcb0dc6ec29308f77f1db77a4f4a562d
 https://preview.redd.it/hpfuftn8ramb1.jpg?width=709&format=pjpg&auto=webp&s=545f4fab3033cb68637052e7ff2c4775a12a7b99
 https://preview.redd.it/7plm0tn8ramb1.jpg?width=714&format=pjpg&auto=webp&s=b138b1c43a2078297b69c09d26de013865629e77
 https://preview.redd.it/uc6tnrn8ramb1.jpg?width=703&format=pjpg&auto=webp&s=fcf747b2515fbf6e78b1ef7aa66ce9ca4d223cd3
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster, long range transformer [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16a1u07/faster_long_range_transformer_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16a1u07/faster_long_range_transformer_r/"/>
        <updated>2023-09-04T19:43:28.000Z</updated>
        <summary type="html"><![CDATA[Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. 
 CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
 In this video, we walk through the ColT5 paper and explain what is T5, longT5, UL2 and PEGASUS, then discuss how ColT5 has advantage over previous methods for few-shot and 1-shot tasks.
 https://youtu.be/8KCQQtXje2g?si=ecbvnFPlhGP01aOt
    submitted by    /u/MRMohebian  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] YaRN: Efficient Context Window Extension of Large Language Models - Nous Research 2023 - Open source allows context windows of up to 128k!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16a1hfe/r_yarn_efficient_context_window_extension_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16a1hfe/r_yarn_efficient_context_window_extension_of/"/>
        <updated>2023-09-04T19:30:23.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2309.00071 
 Github: https://github.com/jquesnelle/yarn 
 Very informative Reddit discussion: https://www.reddit.com/r/LocalLLaMA/comments/166jik4/128k_context_llama_2_finetunes_using_yarn/?utm_source=share&utm_medium=web2x&context=3 
 Twitter: https://twitter.com/EnricoShippole/status/1697317625116742119?s=20 
 Abstract:
  
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn . 
  
https://preview.redd.it/tnovsbpjiamb1.jpg?width=1354&format=pjpg&auto=webp&s=ce098b3071285f9f64d99312a98999de8b625bfe
 https://preview.redd.it/j10sicpjiamb1.jpg?width=997&format=pjpg&auto=webp&s=95bbc6d70759ef7ccdf6bccee0c2a2f98ebda52b
 https://preview.redd.it/ve710dpjiamb1.jpg?width=1380&format=pjpg&auto=webp&s=05f53117bcf648e330fa6ac148746484dca9fb1b
 â€‹
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] DenseDiffusion: The Game-changing, Training-free Technique in Text-to-Image Generation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16a1133/r_densediffusion_the_gamechanging_trainingfree/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16a1133/r_densediffusion_the_gamechanging_trainingfree/"/>
        <updated>2023-09-04T19:13:35.000Z</updated>
        <summary type="html"><![CDATA[Overcoming present challenges in text-to-image models, DenseDiffusion is the latest advancement ensuring enhanced image quality based on scene descriptions. Developed specifically to handle complex captions, it brings a new era in dense captioning.
 https://preview.redd.it/v5oa5suwfamb1.png?width=2000&format=png&auto=webp&s=17fbcc702ee21a41cb356a7d0e38d710a8c048c3
 If you want to stay on top of the latest trends and insights in AI, look here first.
 Why is it noteworthy?
  
It addresses the issues with existing techniques where users face inconsistencies when dictating the arrangement of elements within generated images using textual prompts.
 DenseDiffusion is training-free, unlike existing methods like "Make-aScene" and "Latent Diffusion Models," which are computationally intensive and râ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Refact Code LLM: 1.6B State-of-the-Art LLM for Code that Reaches 32% HumanEval]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/16a026u/introducing_refact_code_llm_16b_stateoftheart_llm/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/16a026u/introducing_refact_code_llm_16b_stateoftheart_llm/"/>
        <updated>2023-09-04T18:39:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["ChessGPT: Bridging Policy Learning and Language Modeling", Feng et al 2023]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/169zzgb/chessgpt_bridging_policy_learning_and_language/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/169zzgb/chessgpt_bridging_policy_learning_and_language/"/>
        <updated>2023-09-04T18:36:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for open PhD positions]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/169zuo2/looking_for_open_phd_positions/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/169zuo2/looking_for_open_phd_positions/"/>
        <updated>2023-09-04T18:31:23.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I have just completed my MSc and am looking for open PhD positions (preferably funded) in RL to join. My masters thesis was on Hierarchical RL and skill discovery, so thatâ€™s the domain am mostly interested in since I have spent quite some time researching it but also open to other interesting avenues. If there are any such positions available at your workplace/lab please let me know. Thanksâ€¦..
    submitted by    /u/FreakedoutNeurotic98  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jordan normal form: 1â€™s above or below diagonal?]]></title>
        <id>https://www.johndcook.com/blog/?p=206492</id>
        <link href="https://www.johndcook.com/blog/2023/09/04/jnf-above-below/"/>
        <updated>2023-09-04T18:25:03.000Z</updated>
        <summary type="html"><![CDATA[Given a square complex matrix A, the Jordan normal form of A is a matrix J such that and J has a particular form. The eigenvalues of A are along the diagonal of J, and the elements above the diagonal are 0s or 1s. Thereâ€™s a particular pattern to the 1s, giving the matrix J [â€¦]
Jordan normal form: 1â€™s above or below diagonal? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] video data in image classification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169y7zz/d_video_data_in_image_classification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169y7zz/d_video_data_in_image_classification/"/>
        <updated>2023-09-04T17:30:40.000Z</updated>
        <summary type="html"><![CDATA[Let's say your training a simple CNN for a classification problem. An example would be a model that is supposed to decide if a person is male or female based on facial images.
 What is your experience regarding image sequences from videos in the training datasets? 
 My intuition is, that the added information to the dataset from one video isn't proportional to the number of frames. The network probably can't learn much more from 30 frames with little variation in comparison to a single image (at least if you use augmentations). What do you think about this? Or do you even know any research in the direction of this question?
    submitted by    /u/seba07  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The reason for using a policy based learning method]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/169y5fc/the_reason_for_using_a_policy_based_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/169y5fc/the_reason_for_using_a_policy_based_learning/"/>
        <updated>2023-09-04T17:28:02.000Z</updated>
        <summary type="html"><![CDATA[I am reading Sutton's RL chapter on Policy Gradients (13.1) and came across the following paragraph. Can someone please explain it to me - " Finally, we note that the choice of policy parameterization is sometimes a good way of injecting prior knowledge about the desired form of the policy into the reinforcement learning system. This is often the most important reason for using a policy-based learning method. ". Is he referring to some kind of Bayesian technique? I'd highly appreciate some examples here.
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuation of Key Papers in DRL from OpenAI Spinning UP]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/169x7mw/continuation_of_key_papers_in_drl_from_openai/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/169x7mw/continuation_of_key_papers_in_drl_from_openai/"/>
        <updated>2023-09-04T16:52:10.000Z</updated>
        <summary type="html"><![CDATA[Hey, I've been going through the papers curated by people behind OpenAI Spinning Up and I've recently started thinking what the list would look like in 2023 if OpenAI hadn't abandoned it. Do you folks have any suggestions for DRL papers from 2019, 2020, â€¦, up to now?
    submitted by    /u/spoiled-mylk  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA CEO Meets with India Prime Minister Narendra Modi]]></title>
        <id>https://blogs.nvidia.com/?p=66710</id>
        <link href="https://blogs.nvidia.com/blog/2023/09/04/modi-huang-india/"/>
        <updated>2023-09-04T16:51:32.000Z</updated>
        <summary type="html"><![CDATA[Underscoring NVIDIAâ€™s growing relationship with the global technology superpower, Indian Prime Minister Narendra Modi met with NVIDIA founder and CEO Jensen Huang Monday evening. The meeting at 7 Lok Kalyan Marg â€” as the Prime Ministerâ€™s official residence in New Delhi is known â€” comes as Modi prepares to host a gathering of leaders from Read article >]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Strongest LLM for Writers/Editors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169x55u/d_strongest_llm_for_writerseditors/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169x55u/d_strongest_llm_for_writerseditors/"/>
        <updated>2023-09-04T16:49:37.000Z</updated>
        <summary type="html"><![CDATA[Hey all,
 I'm a screenwriter that's curious about ML/AI tech and its applications to my industry. I'm wondering what the current best product is for writers and editors. Specifically, I'm curious if there's a product that can "edit" longform text - say, to trim a screenplay down from 140 pages to 120, while retaining style, plot, and narrative intent. Are there any products like that? Forgive me if this is too basic; I've only dabbled in ChatGPT and MidJourney to see what the fuss is about. Thanks in advance!
    submitted by    /u/cesrep  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] We're building the first LLM marketplace to connect developers with teams, investors, and projects]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169wdcl/p_were_building_the_first_llm_marketplace_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169wdcl/p_were_building_the_first_llm_marketplace_to/"/>
        <updated>2023-09-04T16:20:25.000Z</updated>
        <summary type="html"><![CDATA[There is so much going on right now in AI and machine learning. But there isn't a concise place to find experts, teams, and amazing projects all in one place. That is why we are building Bazaar, the first ever LLM marketplace. 
 We will be inviting slowly making sure we have enough members on each side of the marketplace. https://www.llmbazaar.com/
    submitted by    /u/husky_misconception  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement learning Rivals of Aether]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/169wc4w/reinforcement_learning_rivals_of_aether/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/169wc4w/reinforcement_learning_rivals_of_aether/"/>
        <updated>2023-09-04T16:19:13.000Z</updated>
        <summary type="html"><![CDATA[i want to create an ai for Rivals of Aether to see how far it could get in abyss mode and if it could beat 3 9th level cpus on a team. i have no idea how to do this. I was thiking for abyss mode, it could get rewards for finishing waves, and get more reward for doing them with minimal damage.
 â€‹
    submitted by    /u/Additional_Ad9093  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can someone tell me where I can get Runway Gen-2 code? I tried Github but found nothing]]></title>
        <id>https://www.reddit.com/r/artificial/comments/169w262/can_someone_tell_me_where_i_can_get_runway_gen2/</id>
        <link href="https://www.reddit.com/r/artificial/comments/169w262/can_someone_tell_me_where_i_can_get_runway_gen2/"/>
        <updated>2023-09-04T16:08:57.000Z</updated>
        <summary type="html"><![CDATA[Title
    submitted by    /u/ICWiener6666  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Should i use the compile() function when using a custom trainer class in tensorflow?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169vxmi/project_should_i_use_the_compile_function_when/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169vxmi/project_should_i_use_the_compile_function_when/"/>
        <updated>2023-09-04T16:04:13.000Z</updated>
        <summary type="html"><![CDATA[I'm writing a neural network for super resolution but it's one of my first projects and I didn't really understand what compile() is used for. I specify the optimizer, the loss and the accuracy metrics in the trainer class and then I just call my train method on my model. Should I still use the compile function? I'm following this template for the project structure https://github.com/jinh0park/Tensorflow-2.0-Project-Template/tree/master
    submitted by    /u/petrogass  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169usrc/r_learning_to_generate_semantic_layouts_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169usrc/r_learning_to_generate_semantic_layouts_for/"/>
        <updated>2023-09-04T15:20:33.000Z</updated>
        <summary type="html"><![CDATA[Project page: https://pmh9960.github.io/research/GCDP/
 https://i.redd.it/9uz2wt3ba9mb1.gif
    submitted by    /u/yeolj0o  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Segmentation Suggestions for Structured (and Deeply Nested) Bulleted Documents]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169ulde/discussion_segmentation_suggestions_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169ulde/discussion_segmentation_suggestions_for/"/>
        <updated>2023-09-04T15:12:41.000Z</updated>
        <summary type="html"><![CDATA[My question is more focused on the pre-processing side, rather than the training side of things.
 I have a local RAG Q&A pipeline set up for personal documents (local regulations, technical manuals, stuff like that), and I'm looking for ways to improve it. 
 All the documents I'm working with are consistently structured, with nested bullets of varying depth making up most of the structure. So far I've been manually writing/tweaking a python script I wrote to recursively extract the nested bullets and duplicate their hierarchy parents' content for each of the inner-most bullets, that way each bullet has all the contextual content it needs to be valuable in a vacuum. 
 So something like: (a) 1. A. B.
 Would turn into: (a) + 1. + A. (a) + 1. + B.
 This works well in the sense that my LLM does a wonderful job answering my questions and citing the right sources, but the lion's share of my work goes into the tweaking of my parser scripts, or creating new ones entirely. I've played around with semantic segmentation via embedding models, but it doesn't really work here since I'm trying to retain the nested structure of the document for citation accuracy.
 Does anyone have any ideas for ots solutions that address this kind of thing? I can't be the only person who has run into this type of problem, but I've been having a really hard time finding relevant libraries/software that can even get me 80% of the way there.
 Also, I'm totally happy to hear what you guys have done and how's it's worked out/what walls you've hit!
 Edit: I suppose I should have included my current attempt as well, so it's doesn't look like I'm treating this subreddit like Google lol
 https://gist.github.com/apettina/76de292d6d24ed3d0128b87847706b18
    submitted by    /u/RedditAppSucksDicks  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] How to implement Data Contracts generically? Seeking advice from data contract users.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169u5qv/discussion_how_to_implement_data_contracts/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169u5qv/discussion_how_to_implement_data_contracts/"/>
        <updated>2023-09-04T14:56:01.000Z</updated>
        <summary type="html"><![CDATA[Hey folks, it's me the dlt builder again.
 I have questions about data contracts!
 Schema evolution, where the schema of the destination evolves based on incoming data is nice for ingesting transactional data.
 However, there are scenarios where we might not want this automatic evolution. For example, when other parts of our infrastructure require a fixed schema or when we want to store only data that conforms to the current schema. This is where a data contract comes into play.
 Our plan is to implement a straightforward version of this concept initially. We're considering introducing settings on the pipeline to control schema evolution, and here are some modes we're thinking about:
  
Evolve (Default): The current behavior where the schema adapts to incoming data.
 Freeze-and-Trim: Freezâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] ðŸ¤µðŸ”¥ Classy-Fire ðŸ”¥ðŸ¤µ - pretrained text classification using LLM APIs (github.com/microsoft)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169trov/p_classyfire_pretrained_text_classification_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169trov/p_classyfire_pretrained_text_classification_using/"/>
        <updated>2023-09-04T14:40:26.000Z</updated>
        <summary type="html"><![CDATA[Classy-fire is a pretrained multiclass text classification approach that leverages Azure OpenAI's LLM APIs using clever parameter tuning and prompting for classification.
 Why?
  
Tired of having to beg your LLM to pick from a set of options / actions?
 Tired of working hard on cleaning and parsing its responses to trigger a flow?
 Struggling to strip unhelpful prefixes (such as "Sure! " or "I am just a language model!")?
 Having to wait on retries in cases of unexpected outputs?
 Getting random responses on the same query?
 Need a "quick and dirty" text classifier? Don't have enough training data?
  
   submitted by    /u/shayben  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help with finding a tool for 3d image effects.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/169to5f/help_with_finding_a_tool_for_3d_image_effects/</id>
        <link href="https://www.reddit.com/r/artificial/comments/169to5f/help_with_finding_a_tool_for_3d_image_effects/"/>
        <updated>2023-09-04T14:36:33.000Z</updated>
        <summary type="html"><![CDATA[Hello all -
 I'm looking to track down a tool that was able to create a zoom effect that looks three dimensional, example below.
 https://www.instagram.com/reel/CwLB1XsNK0X/?igshid=MmU2YjMzNjRlOQ==
 I've searched my usual spots for some different image editing tools and looked at some video ones as well, but I can't quite figure it out. Anyone familiar with a tool that could do something like that?
 Thanks in advance.
    submitted by    /u/Lys0L  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are there any projects working with large compute clusters looking for volunteers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169st5d/d_are_there_any_projects_working_with_large/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169st5d/d_are_there_any_projects_working_with_large/"/>
        <updated>2023-09-04T14:01:42.000Z</updated>
        <summary type="html"><![CDATA[I've been an ML/software engineer for a bit over 7 years now, and am looking for a new job. It seems like most of the job postings I see around want experience with large compute clusters, but my work has always been in compute-restricted domains (robotics, on-prem deployments, etc.).
 I'm looking broaden my skillset and get some experience with distributed computing. Does anyone know of open-source or otherwise public projects that work with compute clusters like this that are looking for volunteers? I'm happy to put aside an hour or so a day to work on an interesting project.
    submitted by    /u/Flag_Red  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is this company Legit? Any more info on the early access release of this AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/169rhds/is_this_company_legit_any_more_info_on_the_early/</id>
        <link href="https://www.reddit.com/r/artificial/comments/169rhds/is_this_company_legit_any_more_info_on_the_early/"/>
        <updated>2023-09-04T13:04:58.000Z</updated>
        <summary type="html"><![CDATA[Sounds like the stuff I somgwrite about
    submitted by    /u/Niu_Davinci  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] - Two objections to Iris van Rooij's paper saying that it is provably intractable to simulate human intelligence via any machine learning algorithm that samples from human actions.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169qzua/d_two_objections_to_iris_van_rooijs_paper_saying/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169qzua/d_two_objections_to_iris_van_rooijs_paper_saying/"/>
        <updated>2023-09-04T12:42:36.000Z</updated>
        <summary type="html"><![CDATA[https://psyarxiv.com/4cbuv/
 The short of the paper is they show that an AI algorithm that can only learn via sampling from human action is unable to tractably simulate human behavior. I have seen papers like this one by u/alcanthro questioning the validity of the result, but I want to point out two objections to the paper that stand even if the result is true.
 â€‹
 1 - It only seems to apply for AIs trained to mimic humans via sampling human behavior: 
 The paper assumes the AI is trained via an arbitrary machine learning algo M that samples from possible human behaviors in given situations. This matches pretty well to how a lot of LLMs are pretrained (guess the next token), but doesn't seem to apply to any sort of reinforcement learning, since in those situations you are not training the â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Optimal Temperature in The Transmission System using ML]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/169qq6k/predicting_optimal_temperature_in_the/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/169qq6k/predicting_optimal_temperature_in_the/"/>
        <updated>2023-09-04T12:29:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Antique-human6894  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] ReAct: "Recurrence for Adaptive Computation" can lead to OOD length-extrapolation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169qa3b/p_react_recurrence_for_adaptive_computation_can/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169qa3b/p_react_recurrence_for_adaptive_computation_can/"/>
        <updated>2023-09-04T12:07:29.000Z</updated>
        <summary type="html"><![CDATA[This was a small project I was working upon which adds a recurrent prior to attention-based models. 
 This allows integrating an adaptive-computation mechanism, leading to much better length-extrapolation capabilities (compared to vanilla transformers). On some tasks, I'm able to OOD extrapolate to quite an appreciable extent!
 Its also (relatively) quite parallelizable with slightly different training regimes - thus, hopefully being scalable as well. Being lightweight, it might be useful for inferencing as it saves on memory (trading off compute instead).
 It's interesting to think that MHSA might contain an implicit inductive bias that prevents extrapolation. Replacing that with other variants helps a lot - I go in detail in the writeup!
 Twitter summary: https://twitter.com/awesome_ruler_/status/1698668965612917112?s=20
 Writeup/Blogpost: https://dripfeedofideas.notion.site/dripfeedofideas/ReAct-bef052956a0d45f29fb5a5383e7d737d
 GitHub repo: https://github.com/neel04/ReAct
    submitted by    /u/Competitive-Rub-1958  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Current opinions on the information bottleneck principle for neural networks?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169nnf4/d_current_opinions_on_the_information_bottleneck/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169nnf4/d_current_opinions_on_the_information_bottleneck/"/>
        <updated>2023-09-04T09:42:55.000Z</updated>
        <summary type="html"><![CDATA[A while back, the IB principle (https://arxiv.org/abs/1503.02406) made a few waves as a promising framework to understand/study deep neural networks. But I recall a series of follow up works (notably https://openreview.net/forum?id=ry_WPG-A-) that called a lot of the results into question, and (I think?) people drifted away from it.
 I saw this recent paper (https://arxiv.org/abs/2304.09355) on the IB and self-supervised learning, and it got me wondering what the current views are as to how useful/accurate the IB view of deep learning is?
    submitted by    /u/Tea_Pearce  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project][Discussion] What could I use to create a UI like AIChain?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169mais/projectdiscussion_what_could_i_use_to_create_a_ui/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169mais/projectdiscussion_what_could_i_use_to_create_a_ui/"/>
        <updated>2023-09-04T08:22:34.000Z</updated>
        <summary type="html"><![CDATA[I was looking over this paper https://arxiv.org/abs/2110.01691 called AIChains that deals with an interactive chaining method to interact with LLMs.
 I could not find an associated codebase with that paper. If I wanted to create a similar UI like theirs anything you would recommend?
 More specifically, if I want to replicate the paper in 3 months full time (as a student with some experience in ML), what would be the best approach to the UI part of the paper. What if deployment is a concern? I was intially thinking of simple python frameworks like PySimpleGui, or maybe something more comprehensive like PyQt. I am rather unfamiliar with more common web frontend frameworks, but if there are suggestions that make such a Graph/Diagram based User interface easy to implement, I am open to them.
    submitted by    /u/BasisCompetitive6275  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can AI Writing Boost Your Mood and Mind? My Personal experience.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/169kcky/can_ai_writing_boost_your_mood_and_mind_my/</id>
        <link href="https://www.reddit.com/r/artificial/comments/169kcky/can_ai_writing_boost_your_mood_and_mind_my/"/>
        <updated>2023-09-04T06:33:08.000Z</updated>
        <summary type="html"><![CDATA[Have you ever wondered if AI writing can make you feel better in your head? I would like to discuss about how AI writing can put a smile on your face as it is my personal experience.
 1. Stress-Free Writing
 Writing can be stressful, especially when you're not sure where to start. AI writing tools can be your stress-busters. They help you begin by giving you ideas and suggestions. So, no more staring at a blank screen in frustration!
 2. Beating the Writer's Blues
 We all know that feeling when words just won't flow. AI can be your brainstorm buddy. It tosses out ideas like confetti at a party, sparking your creativity when you need it most. Goodbye, writer's block!
 3. Making Your Writing Shine
 Typos and messy sentences can be a downer. AI can be your proofreader, catching those pesky erâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/3/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/169hsga/oneminute_daily_ai_news_932023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/169hsga/oneminute_daily_ai_news_932023/"/>
        <updated>2023-09-04T04:17:31.000Z</updated>
        <summary type="html"><![CDATA[Amazon India is developing a generative artificial intelligence (AI) tool called SahAI (help/assist) for its business partners to help them with the backend of any particular product.[1]
 A robot moves a toy package of butter around a table in the Intelligent Robotics and Vision Lab at The University of Texas at Dallas. With every push, the robot is learning to recognize the object through a new system developed by a team of UT Dallas computer scientists.[2] â€œWhat is my purpose?â€ â€“ â€œYou pass butterâ€.
 Mustafa Suleyman, Google DeepMindâ€™s co-founder and chief executive of Inflection AI, told the Financial Times that the US should use their chip leadership to enforce minimum global standards for the use of AI.[3]
 Model who never ages: Noonoouri becomes first digital artist to be signed by Warner Music.[4]
  
Sources:
 [1] https://www.thehindu.com/sci-tech/technology/amazon-working-on-a-generative-ai-to-help-small-businesses-in-india/article67255325.ece
 [2] https://www.nanowerk.com/news2/robotics/newsid=63572.php
 [3] https://www.finextra.com/newsarticle/42878/google-deepmind-co-founder-argues-us-should-set-ai-global-standards---ft
 [4] https://www.thenationalnews.com/arts-culture/music-stage/2023/09/02/model-who-never-ages-noonoouri-becomes-first-digital-artist-to-be-signed-by-warner-music/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] how to learn Stochastic Differential Equations for diffusion model?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169he26/d_how_to_learn_stochastic_differential_equations/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169he26/d_how_to_learn_stochastic_differential_equations/"/>
        <updated>2023-09-04T03:57:27.000Z</updated>
        <summary type="html"><![CDATA[There have many blogs and papers disscuss SDE for diffusion model:
 Stochastic Differential Equations and Diffusion Models https://www.vanillabug.com/posts/sde/
 Perspectives on diffusion https://sander.ai/2023/07/20/perspectives.html
 On the Mathematics of Diffusion Models https://arxiv.org/abs/2301.11108
 But i can't find blog or book to explain Stochastic Differential Equations, it seems complex, even after i have learned Calculus and Ordinary Differential Equations and Partial Differential Equations, i still can't understand SDE, especially the SDE Perspective on diffusion.
 So Do you know some blogs or books explain SDE intuitive like betterexplained.com/ and mathsisfun.com/ ?
    submitted by    /u/ghosthamlet  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How are Mixture Of Expert models trained in conjunction with Transformers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169gpx4/d_how_are_mixture_of_expert_models_trained_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169gpx4/d_how_are_mixture_of_expert_models_trained_in/"/>
        <updated>2023-09-04T03:22:47.000Z</updated>
        <summary type="html"><![CDATA[How MoEs (sparsely gated ones) are trained appear to be rather opaque from looking at literature (e.g. GLAM and similar papers). 
 From my intuition it would make sense that it works either by:
 Each expert being trained on a subset of data (the data they are supposed to have expertise in) to predict a token given a previous token, or to predict a token given a contextual embedding. This would mean the expert MLPs are frozen, and the only thing concerning the experts that we train with the transformer is the gating mechanism. 
 or
 The experts are trained in the same training loop as the transformer (e.g. backprop over the whole network), but that each of the experts are only trained on a subset of the data corresponding to their expertise (e.g. as we perform the training loop and we run upon data from our math dataset, then we backprop through the math expert mlp)
 â€‹
 Could anyone help me resolve my confusion and point me in the right direction for how these are trained? Thanks!
    submitted by    /u/SorasNobody  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An audiobook entirely created from A.I.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/169fh2j/an_audiobook_entirely_created_from_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/169fh2j/an_audiobook_entirely_created_from_ai/"/>
        <updated>2023-09-04T02:21:35.000Z</updated>
        <summary type="html"><![CDATA[This story, it's narrator and even the cover art are all made by Artificial Intelligence. The only human contribution was adding the Text to the book cover and the prompts used to produce the story. 
 https://youtu.be/tZgq9N9RCo0 
    submitted by    /u/BermaidMutter  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eigenvectors of the DFT matrix]]></title>
        <id>https://www.johndcook.com/blog/?p=206395</id>
        <link href="https://www.johndcook.com/blog/2023/09/03/eigenvectors-of-dft/"/>
        <updated>2023-09-04T01:47:01.000Z</updated>
        <summary type="html"><![CDATA[When is the discrete Fourier transform of a vector proportional to the original vector? And when that happens, what is the proportionality constant? In more formal language, what can we say about the eigenvectors and eigenvalues of the DFT matrix? Setup I mentioned in the previous post that Mathematicaâ€™s default convention for defining the DFT [â€¦]
Eigenvectors of the DFT matrix first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Generated Voice Deepfakes Pose New Threat to Bank Security]]></title>
        <id>https://www.reddit.com/r/artificial/comments/169elgu/aigenerated_voice_deepfakes_pose_new_threat_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/169elgu/aigenerated_voice_deepfakes_pose_new_threat_to/"/>
        <updated>2023-09-04T01:39:35.000Z</updated>
        <summary type="html"><![CDATA[Scammers are now using AI to create realistic voice deepfakes, aiming to trick people into transferring money. By mimicking real customer voices, this new type of voice fraud attempts to exploit bank security systems and deceive call center agents.
 To make sure you're updated about the latest AI trends, look here first.
 Increasing prevalence and sophistication of voice frauds
  
A rise in AI-generated voice frauds has been noted this year, with one major case featuring an investor in Florida whose voice was synthetically duplicated to deceive his bank.
 Voice authentication vendor Nuance detected its first successful deepfake attack on a financial services client late last year.
 These scams are facilitated by the wide availability of voice samples online, coupled with the growth of AI capabilities and hackers' access to stolen bank account details.
  
Defending against evolving AI threats
  
Currently, only a small percentage of fraud calls to large financial companies are AI-generated. Most attacks have targeted credit card service call centers.
 Fraudsters are advancing their techniques, now able to convert speech to a specific target's voice in real-time using advanced AI systems like Microsoft's VALL-E.
 With most of these security measures focusing on call centers and automated systems, individual calls to high-ranking officials remain a vulnerability.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that keeps you updated with the most relevant news and research in AI. Join professionals from Google, Meta, and OpenAI who are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] (Advice) Remote work in ML/DL/Data Science]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169ct10/d_advice_remote_work_in_mldldata_science/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169ct10/d_advice_remote_work_in_mldldata_science/"/>
        <updated>2023-09-04T00:13:22.000Z</updated>
        <summary type="html"><![CDATA[I'm from India and I've started learning and building my portfolio in Machine Learning/Deep learning. Currently, I'm doing "Practical Deep learning using fast.ai and pytorch" course.
 In my university, there are not a lot of companies visiting for campus placement this year so I decided to go on the offcampus job hunt. I'm a final year student (in masters but bachelors was unrelated to CS) and no work experience. 
 I have further personal goals for which I'd need a steady and good income. I decided if I could get a remote job it would be really beneficial for me as my living costs would be saved and I'll be paid much more than what India offers freshers(since I'll be paid in dollars or euros). 
 However, I need advice in various domains: 1. Should I focus on one of ML/DL/ Data science or multiple ? 2. Any resources that could help me learn ? 3. Projects that help me stand out from the crowd? 4. Where can I start looking for remote work(websites, etc)? 5. Any other personal advice is appreciated!
 Thank you for taking the time to read my post :)
    submitted by    /u/Lazy_Guidance_5151  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Finetune pretrained ViT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169cp30/d_finetune_pretrained_vit/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169cp30/d_finetune_pretrained_vit/"/>
        <updated>2023-09-04T00:08:33.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 In deep learning finetuning pre-trained model was performed by taking some pretrained models like resnet, vgg and unfreezing some of it's final layers.
 Is it the same when finetuning pretrained ViT models? Or do we have to take pretrained ViT and train all the parameters on our own data ?
 On this tutorials https://theaisummer.com/hugging-face-vit/, they have not freezed any pretrained layers.
    submitted by    /u/Bishwa12  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] How susceptible are LLMs to Logical Fallacies?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1699u9x/r_how_susceptible_are_llms_to_logical_fallacies/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1699u9x/r_how_susceptible_are_llms_to_logical_fallacies/"/>
        <updated>2023-09-03T22:05:23.000Z</updated>
        <summary type="html"><![CDATA[paper https://arxiv.org/abs/2308.09853
 abstract.
 This paper investigates the rational thinking capability of Large Language Models (LLMs) in multi-round argumentative debates by exploring the impact of fallacious arguments on their logical reasoning performance. More specifically, we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM involves two agents: a persuader and a debater engaging in a multi-round debate on a controversial topic, where the persuader tries to convince the debater of the correctness of its claim. First, LOGICOM assesses the potential of LLMs to change their opinions through reasoning. Then, it evaluates the debaterâ€™s performance in logical reasoning by contrasting the scenario where the persuader employs logical fallacies against one where logical reasoning is used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4 using a dataset containing controversial topics, claims, and reasons supporting them. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their opinion through reasoning. However, when presented with logical fallacies, GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often, respectively, compared to when logical reasoning is used. Finally, we introduce a new dataset containing over 5k pairs of logical vs. fallacious arguments. The source code and dataset of this work are made publicly available.
 GPT3.5 vulnerable to false information generated by itself!
    submitted by    /u/Amir-AI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] What was your biggest oops with a model or analysis that made it (or almost made it) into production?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16998aa/discussion_what_was_your_biggest_oops_with_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16998aa/discussion_what_was_your_biggest_oops_with_a/"/>
        <updated>2023-09-03T21:41:06.000Z</updated>
        <summary type="html"><![CDATA[I'm asking because it seems like when I review other people's work I very regularly catch a tiny coding misstep that has HUGE downstream implications. I'm sure my own work is not exempt either. Some examples:
 "At this step you're saying you encode responders as 1 and non-responders as 0 but you actually did it the other way around."
 "That groupby statement isn't doing what you think it's doing."
 "When you created your target variable by labeling people with this ratio >= 30%, you accidentally failed to capture a ton of actual responders, because the floating-point arithmetic used to derive this column is calculating people with actual values of 0.30 as 0.2999999999999998."
 Come on guys, let's hear it.
    submitted by    /u/WartimeHotTot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Meta's DINOv2 and FACET sets the bar in computer vision model fairness]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1698ieh/r_metas_dinov2_and_facet_sets_the_bar_in_computer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1698ieh/r_metas_dinov2_and_facet_sets_the_bar_in_computer/"/>
        <updated>2023-09-03T21:13:03.000Z</updated>
        <summary type="html"><![CDATA[Meta has recently unveiled DINOv2, its cutting-edge computer vision model, and FACET, a comprehensive benchmark to ensure AI fairness. These developments promise improved automation and better inclusivity in the AI sector.
 If you want to stay on top of the latest trends and insights in AI, look here first.
 https://i.redd.it/jeojm1qew3mb1.gif
 DINOv2 for advanced visual tasks
  
Meta has made the powerful DINOv2 model available under the Apache 2.0 license, employing self-supervised learning to enhance image segmentation and depth estimation.
 This broader use model encourages further innovation and practical application in the computer vision community, driving progress in the AI industry.
  
FACET for enhanced AI fairness
  
Given the inherent difficulty and risks in ensuring fairness in computer vision, Meta introduced FACET.
 FACET has been developed to benchmark fairness across computer vision models performing tasks such as detection or classification, considering a wide array of demographic attributes.
 This revolutionary tool enables a better understanding of potential biases in AI models, helping to address fairness and robustness concerns.
  
Wider implications
  
Preliminary studies indicate performance disparities across some demographic groups within computer vision models. FACET allows researchers to track these divergences and monitor the implementation of corrective measures.
 Meta actively encourages researchers to use FACET for fairness benchmarking in other visual/multimodal tasks. For instance, the DINOv2 model's performance was analyzed with FACET â€” facilitating insights into potential biases.
  
(source)
 P.S. If you like such analysis, I write a free newsletter tracking significant news and research in AI. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta's DINOv2 and FACET sets the bar in computer vision model fairness]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1698fv4/metas_dinov2_and_facet_sets_the_bar_in_computer/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1698fv4/metas_dinov2_and_facet_sets_the_bar_in_computer/"/>
        <updated>2023-09-03T21:10:22.000Z</updated>
        <summary type="html"><![CDATA[Meta has recently unveiled DINOv2, its cutting-edge computer vision model, and FACET, a comprehensive benchmark to ensure AI fairness. These developments promise improved automation and better inclusivity in the AI sector.
 If you want to stay on top of the latest trends and insights in AI, look here first.
 https://i.redd.it/zg47br3xv3mb1.gif
 DINOv2 for advanced visual tasks
  
Meta has made the powerful DINOv2 model available under the Apache 2.0 license, employing self-supervised learning to enhance image segmentation and depth estimation.
 This broader use model encourages further innovation and practical application in the computer vision community, driving progress in the AI industry.
  
FACET for enhanced AI fairness
  
Given the inherent difficulty and risks in ensuring fairness in computer vision, Meta introduced FACET.
 FACET has been developed to benchmark fairness across computer vision models performing tasks such as detection or classification, considering a wide array of demographic attributes.
 This revolutionary tool enables a better understanding of potential biases in AI models, helping to address fairness and robustness concerns.
  
Wider implications
  
Preliminary studies indicate performance disparities across some demographic groups within computer vision models. FACET allows researchers to track these divergences and monitor the implementation of corrective measures.
 Meta actively encourages researchers to use FACET for fairness benchmarking in other visual/multimodal tasks. For instance, the DINOv2 model's performance was analyzed with FACET â€” facilitating insights into potential biases.
  
(source)
 P.S. If you like such analysis, I write a free newsletter tracking significant news and research in AI. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expert systems and RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1697ljh/expert_systems_and_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1697ljh/expert_systems_and_rl/"/>
        <updated>2023-09-03T20:37:26.000Z</updated>
        <summary type="html"><![CDATA[I'm interested in learning more about how expert systems and explicit knowledge injection by various means can be used to guide and improve RL, both in terms of capability and in terms of reduced training times. I have a hard time finding good resources for this topic. What are some must-read papers on this topic? Are there any good youtube channels or online courses? I'm particularly interested in resources that feature practical implementations 
    submitted by    /u/worstthingsonline  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFT conventions: NumPy vs Mathematica]]></title>
        <id>https://www.johndcook.com/blog/?p=206268</id>
        <link href="https://www.johndcook.com/blog/2023/09/03/dft-numpy-mathematica/"/>
        <updated>2023-09-03T20:27:05.000Z</updated>
        <summary type="html"><![CDATA[Just as there are multiple conventions for defining the Fourier transform, there are multiple conventions for defining the discrete Fourier transform (DFT), better known as the fast Fourier transform (FFT). [1] This post will look at two DFT conventions, one used in Pythonâ€™s NumPy library, and one used in Mathematica. There are more conventions in [â€¦]
DFT conventions: NumPy vs Mathematica first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why am i getting this error?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1696j9b/why_am_i_getting_this_error/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1696j9b/why_am_i_getting_this_error/"/>
        <updated>2023-09-03T19:55:14.000Z</updated>
        <summary type="html"><![CDATA[When I try to call check_env() from this code:
 from stable_baselines3.common.env_checker import check_env from agentStable import snakeEnv env = snakeEnv() check_env(env) 
 I get this error:
 Traceback (most recent call last): File "myDir", line 6, in <module> check_env(env) File "<myDir\Python\Python310\lib\site-packages\stable_baselines3\common\env_checker.py", line 396, in check_env assert isinstance( AssertionError: Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/ 
 â€‹
 Here is my agentStable.py code:
 import gym from gym import spaces import numpy as np from enum import Enum from collections import namedtuple import numpy as np from colorama import Fore from gameStable import SnakeGameAI, Direction, Point class snakeEnv(gym.Env): metâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][D] VAE using one-hot-encoding input, problem with optimising and getting good results]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1694bvn/pd_vae_using_onehotencoding_input_problem_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1694bvn/pd_vae_using_onehotencoding_input_problem_with/"/>
        <updated>2023-09-03T18:27:53.000Z</updated>
        <summary type="html"><![CDATA[Introduction to the problem
 I will provide you with link to github code so you can see both input dataset, how my onehot encoding and decoding works and implementation of VAE, with current reconstruction_loss and kl_loss fuctions that make my loss function.
 dataset - amp.csv
 onehotencoding -- tools.py
 main code -- VAE-onehot-testing.ipynb
 https://github.com/aronta/Master-thesis-Generating-de-novo-peptides-using-variational-autoencoder-model/blob/main/VAE-onehot-testing.ipynb
 Firstly, just to explain dataset. I have sequences of peptides represented with onehot encoded sequences which I am using as input for my model. Current VAE model, both encoder and deocder are based on LSTM layer as a main way for this VAE to learn connections between inputed sequences and to make sense of it all.
 Main Issue
 Issue is that the latent space im getting doesn't look good no matter what i do. (pictures of plots are on link). So i have tried scalling kl loss (and also warming it up -- because many papers say its a good way) but it doesn't change the end result. Maybe there is problem in the implementation of the VAE, i am realy not sure.
 The main goal would be (like in all VAE implementations) generating new sequences from latent spaces that make sense, opposed to the current outputs that im getting.
 My guess
 There is a problem with optimizing loss function, but i could be completely wrong (maybe the model is wrong for the input I have, or onehotencoding isn't even a good way to represent data entering LSTM layer).
    submitted by    /u/Yupgrade  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[discussion]" Pleae help. Started with ML, MERN and Contributed but conflicted what to continue on..]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1693c08/discussion_pleae_help_started_with_ml_mern_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1693c08/discussion_pleae_help_started_with_ml_mern_and/"/>
        <updated>2023-09-03T17:49:52.000Z</updated>
        <summary type="html"><![CDATA[I've been learning MERN with a course (charging INR 6k) for last 3 weeks, won two hackathons and Contributed to OS projects. 
 Now just 1 week ago I saw a remote ML job Profile that required OS Contribution to apply.I looked through their docs, learnt python, tensor flow basics and I Contributed to their Tensor flow and Paddle module and got 3 - 4 PR merged(Enough to apply) . 
 Now I'm confused what to continue with, should I do both or do it one by One. I'm a recent graduate so need a job ASAP but I can give maximum time of the day to study. Please can someone give some advice so I can make my decision, as I'm unable to leave either
 TLDR; Learnt both ML and MERN, Contributed and now confused what to carry on with as I need a job asap.
    submitted by    /u/Sinofdracry  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Compute percentage of languages present in a document]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1693a84/d_compute_percentage_of_languages_present_in_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1693a84/d_compute_percentage_of_languages_present_in_a/"/>
        <updated>2023-09-03T17:47:50.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, I'm trying to compute the percentage of each language appearing in a document. My current use cases including two known languages and a set of documents which have the two languages mixed in (code switching, due to translation error). I'm training an ML model to make the output monolingual (leaned towards a designated language), so I need a reliable measure to estimate whether the ML model is making progress or not. Currently, I use lingua with the `compute_language_confidence_values()` function but the prediction is quite poor.
 For example, given a piece of text in Japanese and English:
 from lingua import Language, LanguageDetectorBuilder languages = [Language.ENGLISH, Language.JAPANESE] detector = LanguageDetectorBuilder.from_languages(*languages).build() detector.compute_language_confidence_values("ã‚ã‹ã‚Šã¾ã›ã‚“ hey do you understand me hey oh really") >>> [ConfidenceValue(language=Language.ENGLISH, value=1.0), ConfidenceValue(language=Language.JAPANESE, value=0.0)] 
 So it's not quite correct (should be 0.8-0.2 or something similar), does anyone have any advice ? Or are there better softwares out there ?
    submitted by    /u/KarmaCut132  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] does anyone have any papers on getting LLMs to output perfect formats?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1692rgy/d_does_anyone_have_any_papers_on_getting_llms_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1692rgy/d_does_anyone_have_any_papers_on_getting_llms_to/"/>
        <updated>2023-09-03T17:26:49.000Z</updated>
        <summary type="html"><![CDATA[Does anyone have any literature on how to constrain the output of an LLM to a specified format? Iâ€™ve self hacked a method to get LLAMA to output a json of perfect schema. I tried to find something out of the box but I couldnâ€™t find anything, and so I home brewed it.
 Thinking of publishing a paper on this but I donâ€™t want to republish something already written, so asking here first. Thanks!
    submitted by    /u/SnooPears7079  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] does anyone have any papers on getting LLMs to output perfect formats?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1692rgq/d_does_anyone_have_any_papers_on_getting_llms_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1692rgq/d_does_anyone_have_any_papers_on_getting_llms_to/"/>
        <updated>2023-09-03T17:26:48.000Z</updated>
        <summary type="html"><![CDATA[Does anyone have any literature on how to constrain the output of an LLM to a specified format? Iâ€™ve self hacked a method to get LLAMA to output a json of perfect schema. I tried to find something out of the box but I couldnâ€™t find anything, and so I home brewed it.
 Thinking of publishing a paper on this but I donâ€™t want to republish something already written, so asking here first. Thanks!
    submitted by    /u/SnooPears7079  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Requesting help finding labs/ professors on certain discipline.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1692ehe/r_requesting_help_finding_labs_professors_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1692ehe/r_requesting_help_finding_labs_professors_on/"/>
        <updated>2023-09-03T17:12:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Present-Ad-8531  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I built a Chrome extension that adds a chatbot to every GitHub repository]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/169246b/p_i_built_a_chrome_extension_that_adds_a_chatbot/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/169246b/p_i_built_a_chrome_extension_that_adds_a_chatbot/"/>
        <updated>2023-09-03T17:00:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/jsonathan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] How to setup TPU parallelism/FSDP with HuggingFace Transformers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1691q8a/discussion_how_to_setup_tpu_parallelismfsdp_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1691q8a/discussion_how_to_setup_tpu_parallelismfsdp_with/"/>
        <updated>2023-09-03T16:45:05.000Z</updated>
        <summary type="html"><![CDATA[My Code (Colab Link)
 Hi! For the past few days, I've been trying to fine-tune a model using TPU parallelism / FSDP with a Kaggle TPU notebook. The reason I need to set up FSDP is because the model I'm using is very large (Openlm's open llama 3b v2). When I try to fine-tune it, I quickly run out of memory on the TPU.
 Linked above is my code, if anyone has any useful information I would greatly appreciate it! Thank you!!
 Edit: Also providing my code through text here:
 !pip install sentencepiece !pip install -U accelerate !pip install -U transformers !pip install cloud-tpu-client !pip install torch-xla !pip install pyarrow import torch import torch_xla import torch_xla.core.xla_model as xm from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments import pandâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Driving | Tight, dynamic and chaotic traffic | India | Swaayatt Robots]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16916tn/autonomous_driving_tight_dynamic_and_chaotic/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16916tn/autonomous_driving_tight_dynamic_and_chaotic/"/>
        <updated>2023-09-03T16:22:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/shani_786  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Various models and git repo examples to learn Algo Trading]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168ydib/d_various_models_and_git_repo_examples_to_learn/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168ydib/d_various_models_and_git_repo_examples_to_learn/"/>
        <updated>2023-09-03T14:24:58.000Z</updated>
        <summary type="html"><![CDATA[Can someone list down one dataset for Algo Trading Simulation or free API endpoint , and i will go over following Algorithms:
  
Basic KNN (moving average)
 
SVR
 
other ML models if any
 
LSTM
 
other DL models if any
 
any RNN model 
  
basically my mission is to write a paper at then end of months comparing all algorithms with candle stick patterns over different strategies
 â€‹
    submitted by    /u/reactwebdev  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][R] How often do Masters students doing a thesis publish in top ML conferences in their program period of 2 years ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168xh2g/dr_how_often_do_masters_students_doing_a_thesis/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168xh2g/dr_how_often_do_masters_students_doing_a_thesis/"/>
        <updated>2023-09-03T13:46:52.000Z</updated>
        <summary type="html"><![CDATA[Just curious to know the thoughts of other Masters/PhD students, professors or others in academia or industry research about their experience with regard to the title.
    submitted by    /u/V1bicycle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I pretrained 16 language models from scratch with different tokenizers to benchmark the difference. Here are the results. [Research]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168wc1o/i_pretrained_16_language_models_from_scratch_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168wc1o/i_pretrained_16_language_models_from_scratch_with/"/>
        <updated>2023-09-03T12:56:45.000Z</updated>
        <summary type="html"><![CDATA[I'm the author of TokenMonster, a free open-source tokenizer and vocabulary builder. I've posted on here a few times as the project has evolved, and each time I'm asked "have you tested it on a language model?".
 Well here it is. I spent $8,000 from my own pocket, and 2 months, pretraining from scratch, finetuning and evaluating 16 language models. 12 small sized models of 91 - 124M parameters, and 4 medium sized models of 354M parameters.
 Here is the link to the full analysis.
 Summary of Findings
  
Comparable (50256-strict-nocapcode) TokenMonster vocabularies perform better than both GPT-2 Tokenizer and tiktoken p50k_base on all metrics.
 Optimal vocabulary size is 32,000.
 Simpler vocabularies converge faster but do not necessarily produce better results when converged.
 Higher compreâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Comgra: A library for debugging and understanding neural networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168w1st/p_comgra_a_library_for_debugging_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168w1st/p_comgra_a_library_for_debugging_and/"/>
        <updated>2023-09-03T12:43:15.000Z</updated>
        <summary type="html"><![CDATA[I'm a machine learning engineer and researcher. I got fed up with how difficult it is to understand why neural networks behave the way they do, so i wrote a library to help with it.
 Comgra (computation graph analysis) is a library you can use with pytorch to extract all the tensor data you care about and visualize it graphically in a browser.
 This allows for a much more detailed analysis of what is happening than the usual approach of using tensorboard. You can go investigate tensors as training proceeds, drill down into individual neurons, inspect single data sets that are of special interest to you, track gradients, compare statistics between different training runs, and more.
 This tool has saved me a ton of time in my research by letting me check my hypotheses much more quickly than normal and by helping me understand how the different parts of my network really interact.
 I hope this tool can save other people just as much time as it did me. I'm also open for suggestions on how to improve it further: Since I'm already gathering and visualizing a lot of network information, adding more automated analysis would not be much extra work.
    submitted by    /u/Smart-Emu5581  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Reinforced Self-Training (ReST) for Language Modeling (Video Paper Discussion)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168vi1c/d_reinforced_selftraining_rest_for_language/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168vi1c/d_reinforced_selftraining_rest_for_language/"/>
        <updated>2023-09-03T12:16:06.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/V4dO2pyYGgs
 ReST uses a bootsrap-like method to produce its own extended dataset and trains on ever higher-quality subsets of it to improve its own reward. The method allows for re-using the same generated data multiple times and thus has an efficiency advantage with respect to Online RL techniques like PPO.
 â€‹
 Paper: https://arxiv.org/abs/2308.08998
 â€‹
 Abstract:
 Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
 â€‹
 Authors: Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando de Freitas
    submitted by    /u/ykilcher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zoomposium with Professor Dr. John-Dylan Haynes: "In search of the code of the brain"]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/168vbyw/zoomposium_with_professor_dr_johndylan_haynes_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/168vbyw/zoomposium_with_professor_dr_johndylan_haynes_in/"/>
        <updated>2023-09-03T12:07:52.000Z</updated>
        <summary type="html"><![CDATA[Zoomposium with Professor Dr. John-Dylan Haynes: "In search of the code of the brain" 
 In this new episode of our "Zoomposium Series" on the topic of "Brain Research", my colleague Axel StÃ¶cker from the "Blog der groÃŸen Fragen" and I have managed to win the well-known and renowned brain researcher and psychologist Professor Dr. John-Dylan Haynes for an interview. 
 John-Dylan Haynes has been a professor of theory and analysis of long-range brain signals at the Bernstein Center for Computational Neuroscience and the Berlin Center for Advanced Neuroimaging (BCAN) at CharitÃ© and Humboldt University in Berlin since 2006. 
 There, Professor Haynes and his team are "In Search of the Brain's Code". In order to crack this, larger amounts of data are collected from the functional magnetic resonancâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA['Fluid' - (Modelscope image2video)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/168uwe3/fluid_modelscope_image2video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/168uwe3/fluid_modelscope_image2video/"/>
        <updated>2023-09-03T11:45:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/glenniszen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAML convergence with GAN [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168sra6/maml_convergence_with_gan_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168sra6/maml_convergence_with_gan_d/"/>
        <updated>2023-09-03T09:43:33.000Z</updated>
        <summary type="html"><![CDATA[I had been exploring convergence properties of MAML and there are some recent works establishing convergence under certain conditions. I am trying to understand how this would play out with GAN's, I know that in general training is generally unstable and there are a lot of issues such as memorization and mode collapse under this regime, but I am looking for a theoretical result, for instance we know that GAN's converge under ideal conditions and we also know that MAML converges, can we make any comments on the convergence properties of GAN's when trained using MAML, ideally a neat trick to know if they will converge based on what we already know? The proof for MAML convergence is fairly complicated and I expect that a proof that has additional second order gradient terms and feedback loops will probably involve a lot of work and I am wondering if anyone could provide some sort of insight or intuition as to what such a result would look like? Thanks
    submitted by    /u/ashblue21  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Linear regression for time series data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168sp9x/d_linear_regression_for_time_series_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168sp9x/d_linear_regression_for_time_series_data/"/>
        <updated>2023-09-03T09:40:28.000Z</updated>
        <summary type="html"><![CDATA[Problem: Given time series data of the last few years, one data point per day (eg price of a product or sales made this day). My job is to predict the next 7 days, ie. 7 scalars. 
 Approach: Train one model for each time lag. The first model predicts tomorrow, the second model the day after tomorrow and so on (7 models in total). The features are the last prices of the last 7 days and some saisonal features (calendar week, price on this day last year and so on). 
 Question: is there anything wrong with this approach? It doesnâ€™t feel like the most elegant method to train 7 separate models. The problem with using a single model is, that this model must be able to predict 7 values of different points in time (and i donâ€™t want to give the model input data of 7 days and let it predict all 7 scalars at once. The model should only use the features of a single day to predict this day). The 2 other to options I have considered are to train an autoregressive model (model just learns to predict the next day. To predict the day after tomorrow you give it its own prediction as input). Or to build a â€žtime-lagâ€œ feature, which tells the model how far in the future this datapoint lies. But this doesnâ€™t make sense, because there is nothing like a weekly trend or so. 
 What do you think? The autoregressive approach is elegant, but its implementation and maintenance is complex.
    submitted by    /u/Individual-Cause-616  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Considering in use between Model-free vs Model-based, and need suggestion in algorithms.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/168sf52/considering_in_use_between_modelfree_vs/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/168sf52/considering_in_use_between_modelfree_vs/"/>
        <updated>2023-09-03T09:22:37.000Z</updated>
        <summary type="html"><![CDATA[In training MFRL, which is mostly simulation, why don't we use MBRL instead as the environment is accessible?
 **Correct me if I misunderstand in any.
 From my understanding, Model-Free RL (MFRL) is generally used for control tasks where environment is not accessible. It takes a sample of an experience from the environment and uses it to adjust its policy, either policy-based, value-based, and actor-critic. Model-Based RL (MBRL) uses a transition model to optimize the optimal policy like in model predictive control (MPC).
 I am interested in using RL for control multiple and continuous actions in continuous stochastic environment. For now, I am moving around DDPG. Do you have any suggested algorithm that match to my task?
    submitted by    /u/AnnonymeowCat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] [D] Data augmentation using Stable diffusion]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168rm8g/p_d_data_augmentation_using_stable_diffusion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168rm8g/p_d_data_augmentation_using_stable_diffusion/"/>
        <updated>2023-09-03T08:34:47.000Z</updated>
        <summary type="html"><![CDATA[I've written a post on how to use stable diffusion for data augmentation for object detection and segmentation. Please check it out and share some insights on how to evaluate these kind of tasks.
 https://medium.com/@kaushik.koneripalli/satellite-image-data-augmentation-using-stable-diffusion-for-object-detection-segmentation-8b1fe87b969
    submitted by    /u/perceptron333  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding how to get a dataset for more complex environments.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/168r457/understanding_how_to_get_a_dataset_for_more/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/168r457/understanding_how_to_get_a_dataset_for_more/"/>
        <updated>2023-09-03T08:04:17.000Z</updated>
        <summary type="html"><![CDATA[In videos like this, it talks about how you need to find the right fit for your data which is shown on a scatter plot. I understand how this works when you have a dataset for something but how does it work when you are trying to train a DQN to play snake (eating apples and getting longer game). I have been struggling to tune my hyperparameters as well as figure out how many hidden neurons and hidden layers I need. I have found that right now 256 hidden neurons and 2 hidden layers works best. Please tell me if this topic has flown completely over my head and I am missing something. Thank you!
    submitted by    /u/MrHank2  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What....this is insane...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/168q2j1/whatthis_is_insane/</id>
        <link href="https://www.reddit.com/r/artificial/comments/168q2j1/whatthis_is_insane/"/>
        <updated>2023-09-03T07:00:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm literally speachless.. 8O]]></title>
        <id>https://www.reddit.com/r/artificial/comments/168pwes/im_literally_speachless_8o/</id>
        <link href="https://www.reddit.com/r/artificial/comments/168pwes/im_literally_speachless_8o/"/>
        <updated>2023-09-03T06:50:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_anonymizer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Open-source star removal tool using Pix2Pix]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168ot5c/p_opensource_star_removal_tool_using_pix2pix/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168ot5c/p_opensource_star_removal_tool_using_pix2pix/"/>
        <updated>2023-09-03T05:46:32.000Z</updated>
        <summary type="html"><![CDATA[I created a open-source star removal tool "star2k13". Would love to hear some feedback . Here is link to the tool : Starrem2k13: Open source star removal tool (code2k13.github.io)
 Works on most operating systems and docker
    submitted by    /u/Key_Education_2557  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[After Getting Banned in Schools, OpenAI Launches ChatGPT Tool for Teachers]]></title>
        <id>https://www.reddit.com/r/artificial/comments/168orwd/after_getting_banned_in_schools_openai_launches/</id>
        <link href="https://www.reddit.com/r/artificial/comments/168orwd/after_getting_banned_in_schools_openai_launches/"/>
        <updated>2023-09-03T05:44:36.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Coding LLaMA 2 from scratch in PyTorch, with step by step explanation of KV Cache, Grouped Query Attention, Rotary Positional Embedding, RMS Normalization, SwiGLU and much more!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168okzn/p_coding_llama_2_from_scratch_in_pytorch_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168okzn/p_coding_llama_2_from_scratch_in_pytorch_with/"/>
        <updated>2023-09-03T05:33:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hkproj_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retro sci-fi trailer made with AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/168o3jl/retro_scifi_trailer_made_with_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/168o3jl/retro_scifi_trailer_made_with_ai/"/>
        <updated>2023-09-03T05:05:49.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/filmcrux  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 9/2/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/168n62l/oneminute_daily_ai_news_922023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/168n62l/oneminute_daily_ai_news_922023/"/>
        <updated>2023-09-03T04:15:14.000Z</updated>
        <summary type="html"><![CDATA[SAG-AFTRA, the union for US actors, is moving towards a potential strike against video game publishers, itâ€™s announced.[1]
 Russia builds MSU-270 supercomputer for AI and HPC research.[2]
 Chuck Schumer has announced that his office will be meeting with top players in the artificial intelligence field later this month. Invited to the upcoming summit are tech megabillionaire Elon Musk, his one-time hypothetical sparring partner Meta CEO Mark Zuckerberg, OpenAI CEO Sam Altman, Google CEO Sundar Pichai, NVIDIA President Jensen Huang, and Alex Karpy, CEO of defense contractor creep Palantir.[3]
 Google expands AI compute offerings, partnership with Nvidia and more.[4]
  
Sources:
 [1] https://www.videogameschronicle.com/news/actors-union-sag-aftra-could-launch-video-game-strikes-over-wages-and-ai/
 [2] https://www.tomshardware.com/news/russian-400-petaflops-supercomputer-for-ai-comes-online
 [3] https://gizmodo.com/chuck-schumer-elon-musk-mark-zuckerberg-palantir-nvidia-1850788302
 [4] https://www.itworldcanada.com/article/google-expands-ai-compute-offerings-partnership-with-nvidia-and-more/545625
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FinRL and developing ML - skills and labour market]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/168kv1b/finrl_and_developing_ml_skills_and_labour_market/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/168kv1b/finrl_and_developing_ml_skills_and_labour_market/"/>
        <updated>2023-09-03T02:16:00.000Z</updated>
        <summary type="html"><![CDATA[If I wanted to hire an ML/RL student/full-time employee to help my firm develop some FinRL/other RL algorithms, what skills should I be looking for?
 How "generalized" are RL skills - from what I can tell alot of the RL I see posted here has to do with video games?
 I've stumbled across FinRL recently and would like to hire some help to develop some FinRL code. 
 What's the market like for RL? I know all the rage is LLM's but how different is RL and does the labour market care about the difference?
 Based in Canada fyi. Won't be hiring for a few months.
    submitted by    /u/Thrumpwart  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Puzzle Created by GPT-4 That Even GPT-4 Can't Solve, Yet Humans Did! First Challenge Revealed.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/168iagw/the_puzzle_created_by_gpt4_that_even_gpt4_cant/</id>
        <link href="https://www.reddit.com/r/artificial/comments/168iagw/the_puzzle_created_by_gpt4_that_even_gpt4_cant/"/>
        <updated>2023-09-03T00:11:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Where did the research go?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168hp3z/d_where_did_the_research_go/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168hp3z/d_where_did_the_research_go/"/>
        <updated>2023-09-02T23:45:14.000Z</updated>
        <summary type="html"><![CDATA[This sub used to be my go-to place for finding out cool new ML research but sadly it has now become a "generative AI" "AI productisation" circlejerk.
 I was wondering where people now go to discover new ML research (besides ArXiv of course!)
    submitted by    /u/blabboy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] What are some good MLE project ideas ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168h235/p_what_are_some_good_mle_project_ideas/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168h235/p_what_are_some_good_mle_project_ideas/"/>
        <updated>2023-09-02T23:17:23.000Z</updated>
        <summary type="html"><![CDATA[What tech stack, what frameworks should I specially learn and use in my MLE project ? There are so much things mentioned in job offers, what would you suggest me to focus on ?
 I thought of fine-tuning LLM and deploying it using AWS. I'd prefer this project to be NLP oriented. I read about things like MLFlow, Apache Spar, Kubernetes etc. and don't know what to focus on.
 PS: I am currently a data scientist, and have recently finished a body pose estimation + action recognition web app, using Python/OpenCV/Mediapipe/Flask/Torch
    submitted by    /u/tflbbl  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][D] How do I improve car detection performance?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168gux7/pd_how_do_i_improve_car_detection_performance/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168gux7/pd_how_do_i_improve_car_detection_performance/"/>
        <updated>2023-09-02T23:09:08.000Z</updated>
        <summary type="html"><![CDATA[For a college project I am trying to detect the number of cars in the first 3 rows of a parking lot. Here is my roboflow project page: https://universe.roboflow.com/boaztheostrich/cartest-tyaur
 As you can see I have been able to get my map score as high as .995 however I am still having difficulty consistently detecting cars in some edge cases.
 What I am currently testing is increasing the resolution from 1280x720 to 2048.
 I am new to all of this so any tips or tricks would be greatly appreciated.
 I am currently using google colab for training although I am considering switching over to vast.ai
    submitted by    /u/johndowlelxdxdxdxdxd  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need collaborators for a natural language interface [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168gp33/need_collaborators_for_a_natural_language/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168gp33/need_collaborators_for_a_natural_language/"/>
        <updated>2023-09-02T23:02:46.000Z</updated>
        <summary type="html"><![CDATA[We have it pretty functional but we're a small team so we need more people.
 https://github.com/apssouza22/chatflow
 Promo video: https://www.reddit.com/r/AGIunderconstruction/comments/168fsyr/come_build_open_source_natural_language/?utm_source=share&utm_medium=web2x&context=3
    submitted by    /u/Cold-Explanation-984  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to create and use multiple dataframes in pyspark?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168dwjt/d_how_to_create_and_use_multiple_dataframes_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168dwjt/d_how_to_create_and_use_multiple_dataframes_in/"/>
        <updated>2023-09-02T21:08:05.000Z</updated>
        <summary type="html"><![CDATA[Hey All,
 I have to read in multiple JSON files with each one containing objects in an array.
 For each file I want to create a data frame (A matrix might also work) where the rows and columns are just integers pointing to a string. Like this
 â€‹
  
 - 0 1 2 
  
 0 dsad asd ad 
  1 asd asd grth 
  2 ter xc wer 
 
 â€‹
 Using the same JSON file I also want to process the objects inside the arrays using the dataframe (matrix) above. So the process (pipeline) would be something like:
  |==> Create Matrix =======>| JSON file =>| | ===> Use matrix to process object. |==> Individual objects ==>| 
 â€‹
 I have been looking through the docs but still unsure how to do this.
  
Should I use a dataframe or a spark matrix?
 How do I split the objects into parts and also generate the matrix?
 How do I combine dataframes which isn't joining?
  
Just a point in the right direction would be great. Thanks in advance for this relatively simple question.
    submitted by    /u/atticusfinch975  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Property]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/168dru6/markov_property/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/168dru6/markov_property/"/>
        <updated>2023-09-02T21:03:01.000Z</updated>
        <summary type="html"><![CDATA[Is that wrong if a problem doesn't satisfy the Markov property, I cannot solve it with the RL approach either?
    submitted by    /u/nimageran  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] can somone help me get this paper]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168d8cr/d_can_somone_help_me_get_this_paper/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168d8cr/d_can_somone_help_me_get_this_paper/"/>
        <updated>2023-09-02T20:41:28.000Z</updated>
        <summary type="html"><![CDATA[I don't have access can somone help me get it please Thank you https://www.worldscientific.com/doi/abs/10.1142/S0218001418560062
    submitted by    /u/SilenceOfTheUnicorns  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Tips (Algorithms)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168d0ov/dtips_algorithms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168d0ov/dtips_algorithms/"/>
        <updated>2023-09-02T20:33:20.000Z</updated>
        <summary type="html"><![CDATA[I have started learning ML a month ago... Did a foundational Google course and read from some other sources..hav learnt most of the theories....What's the best place to learn algorithms according to you? Any other tips are also welcome
    submitted by    /u/Buri-Buri_zaemon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] RX 7900 XTX vs RTX 4080]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168c339/d_rx_7900_xtx_vs_rtx_4080/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168c339/d_rx_7900_xtx_vs_rtx_4080/"/>
        <updated>2023-09-02T19:56:46.000Z</updated>
        <summary type="html"><![CDATA[I know AMD is working on making ROCM support for RDNA 3, would that rival nvidia? and would there be enough support for it to be usable? Nvidia cards are way more expensive and i would like to use it for gaming besides Machine learning for my study.
 Also, would this be overkill? will an RTX 4070 or an RX 7900 XT also do the job just fine?
 i am new to ML and won't be using it till early 2024,
 thank you all for reading.
    submitted by    /u/RepresentativeIll155  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to describe XGBoost, Boosting and Bagging?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/168bnju/d_how_to_describe_xgboost_boosting_and_bagging/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/168bnju/d_how_to_describe_xgboost_boosting_and_bagging/"/>
        <updated>2023-09-02T19:38:37.000Z</updated>
        <summary type="html"><![CDATA[Hi 
 Can someone here please help me with this algorithm? 
 What is the â€œBoostingâ€ part of the algorithm?
 To my (limited) understanding XGBoost is an ensemble learning algorithm that uses many decision trees (efficiently), where each tree tries to correct the loss of the previous one. 
 But Iâ€™m not sure how this is connected to â€œBoostingâ€ and then itâ€™s cousin â€œBaggingâ€
 Any intuition that may help me here?
    submitted by    /u/Ok_Reality2341  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFT mandalas]]></title>
        <id>https://www.johndcook.com/blog/?p=206229</id>
        <link href="https://www.johndcook.com/blog/2023/09/02/dft-mandalas/"/>
        <updated>2023-09-02T19:19:57.000Z</updated>
        <summary type="html"><![CDATA[Math books often use some illustration from the book contents as cover art. When they do, thereâ€™s often some mystery to the cover art, and a sense of accomplishment when you get far enough into the book to understand the significance of the cover. (See examples here.) William L. Briggs and Van Emden Henson wrote [â€¦]
DFT mandalas first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/168art7/recursively_summarizing_enables_longterm_dialogue/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/168art7/recursively_summarizing_enables_longterm_dialogue/"/>
        <updated>2023-09-02T19:02:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating GenAI into â€œThinking Like a Data Scientistâ€ Methodology â€“ Part III]]></title>
        <id>https://www.datasciencecentral.com/?p=63049</id>
        <link href="https://www.datasciencecentral.com/integrating-genai-into-thinking-like-a-data-scientist-methodology-part-iii/"/>
        <updated>2023-09-02T18:47:00.000Z</updated>
        <summary type="html"><![CDATA[This blog post is not the end of my journey to integrate GenAI with my â€œThinking Like a Data Scientistâ€ (TLADS) methodology, but it is the last post on this leg of the journey. And the journey has been fascinating.Â  I canâ€™t wait to get this modified material in front of my students. In partâ€¦Â Read More Â»Integrating GenAI into â€œThinking Like a Data Scientistâ€ Methodology â€“ Part III
The post Integrating GenAI into â€œThinking Like a Data Scientistâ€ Methodology â€“ Part III appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] A case for summaries over abstracts]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1689dxt/d_a_case_for_summaries_over_abstracts/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1689dxt/d_a_case_for_summaries_over_abstracts/"/>
        <updated>2023-09-02T18:08:10.000Z</updated>
        <summary type="html"><![CDATA[I usually peruse the abstract of a paper before deciding on whether to read it or not. However, lately I've started longing for more personalized summaries.
  
I wonder what others think of abstract vs summaries and their preferences of the latter over former ?
 In your opinion, how far has the field progressed in summarization (https://paperswithcode.com/dataset/scitldr) ?
  
   submitted by    /u/JurrasicBarf  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elon Musk's X to leverage public data for AI model training]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1688xab/elon_musks_x_to_leverage_public_data_for_ai_model/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1688xab/elon_musks_x_to_leverage_public_data_for_ai_model/"/>
        <updated>2023-09-02T17:50:01.000Z</updated>
        <summary type="html"><![CDATA[Elon Musk's X revealed its plans to utilize user data and publicly available information in training AI models. Despite Musk's assurance that only public data will be used, concerns around privacy linger.
 For expert insights into AI developments, look here first.
 https://preview.redd.it/mj64uof9rvlb1.png?width=2000&format=png&auto=webp&s=a856ce3e4b6063ebf7a585df3338142defba6323
 X's approach to AI training
  
Under the most recent privacy policy, X will harness the personal data it collects and publicly accessible information for its machine learning algorithms.
 Musk assures only publicly accessible data will be used, safeguarding private user information like DMs.
 However, with X having disbanded its press operation, more specific details about the data collected and its intended use still need to be provided.
  
Unfolding plans of Musk
  
Despite X's quiet stance on AI, Musk recently launched xAI, aspiring "to understand the true nature of the universe."
 xAI's homepage discloses plans to sync with X closely, possibly using collected user data to progress the mission.
 A competitive stance against LinkedIn suggests a possible additional motive for data collection, speculating an enhanced job and education section on X.
 Despite concerns about selling user data for revenue, concrete evidence is needed to support this argument, reflecting Twitter's previous strategy.
  
(source)
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI and tech. Professionals from Google, Meta, and OpenAI are already reading it.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negative KL-divergence RLHF implementation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1688410/negative_kldivergence_rlhf_implementation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1688410/negative_kldivergence_rlhf_implementation/"/>
        <updated>2023-09-02T17:17:44.000Z</updated>
        <summary type="html"><![CDATA[I am struggling to understand one part of the FAQ of the transformer reinforcement learning library from HuggingFace:
  
What Is the Concern with Negative KL Divergence?
 If you generate text by purely sampling from the model distribution things work fine in general. But when you use the generate method there are a few caveats because it does not always purely sample depending on the settings which can cause KL-divergence to go negative. Essentially when the active model achieves log_p_token_active < log_p_token_ref we get negative KL-div. This can happen in a several cases:
 top-k sampling: the model can smooth out the probability distribution causing the top-k tokens having a smaller probability than those of the reference model but they still are selected
 min_length: this ignores the EOS token until min_length is reached. thus the model can assign a very high log prob to the EOS token and very low prob to all others until min_length is reached
 batched generation: finished sequences in a batch are padded until all generations are finished. The model can learn to assign very low probabilities to the padding tokens unless they are properly masked or removed.These are just a few examples. Why is negative KL an issue? The total reward R is computed R = r - beta * KL so if the model can learn how to drive KL-divergence negative it effectively gets a positive reward. In many cases it can be much easier to exploit such a bug in the generation than actually learning the reward function. In addition the KL can become arbitrarily small thus the actual reward can be very small compared to it.
  
I understand why the KL-divergence that is computed here is an approximation that can be negative as opposed to the real one. However, I cannot wrap my head around the details of why these specific sampling parameters would lead to negative KL-divergence. Could someone elaborate on these points?
    submitted by    /u/Loud_Appointment_418  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] NeurIPS reviewers edited review and score after discussion period: can they delete their own revision history?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1687luu/d_neurips_reviewers_edited_review_and_score_after/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1687luu/d_neurips_reviewers_edited_review_and_score_after/"/>
        <updated>2023-09-02T16:57:54.000Z</updated>
        <summary type="html"><![CDATA[Hi, we have a paper submission to NeurIPS and we have two reviewers who changed their scores and review content silently by editing the original review comment and score after the discussion period. The edited review comment now discusses entirely different point.
 We would like to raise this concern to AC but the thing is that we didnâ€™t save the original review comment, and the â€œrevision historyâ€ for some reason doesnâ€™t show the previous content, other than the entry that there was previous version. But this revision history overall isnâ€™t inconsistent (showing the last two history after the discussion period, but the ones before the period is not shown) 
 Can reviewers delete their own revision history in OpenReview tool? I donâ€™t know if this is a bug or they deleted them with an intention.
    submitted by    /u/mayasang  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Testing at 80-95% - Newly collected recent data 55% - WHY!?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1687bag/d_testing_at_8095_newly_collected_recent_data_55/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1687bag/d_testing_at_8095_newly_collected_recent_data_55/"/>
        <updated>2023-09-02T16:46:07.000Z</updated>
        <summary type="html"><![CDATA[I am currently do some predictions on some market data, I'm using both XGBoost and lightGBM (not both at the same time just experimenting using both algorithms). I have around 2500 features and 40k rows of data in my dataset which is being split 75% = train, 12.5% = valid, 12.5% = Test. The balance of the data is massively imbalanced with a binary classification. On training im seeing 0 = 20192 and 1 = 8337. I am not using SMOTE or undersampling but rather using the alogirhms own parameters to combat the Imbalance, for example scale_pos_weight: y_data[0]/y_data[1]. 
 Training is going very well, im using hyperopt tuner to tune my paramets and usually on average get 75% accuracy on testing, training will usually be a little higher such as 77% and valid will be fairly close to test. But the â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The mystery of AI learning is solved by Stanford researchers]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1686new/the_mystery_of_ai_learning_is_solved_by_stanford/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1686new/the_mystery_of_ai_learning_is_solved_by_stanford/"/>
        <updated>2023-09-02T16:19:39.000Z</updated>
        <summary type="html"><![CDATA[Say goodbye to the black box of deep learning and hello to a new era of transparent, efficient, and ethical AI. Find out how this changes EVERYTHING! 
 https://kinews24.de/stanford-cracks-the-ai-code-the-groundbreaking-law-of-equi-separation
 â€‹
    submitted by    /u/myreddit333  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Build a Recommender System that Includes Term / Vector Recall, DeepFM Ranking, Inference Engine and Web Application.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1685drw/p_build_a_recommender_system_that_includes_term/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1685drw/p_build_a_recommender_system_that_includes_term/"/>
        <updated>2023-09-02T15:28:26.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone! I've noticed that most beginner-level tutorials on recommender systems primarily focus on model training, with limited information about deploying them in a production environment. Additionally, the different usage of models in the recall (retrieval) and ranking modules can indeed be confusing for beginners.
 Recently, I've been working on a recommender system project that encompasses both offline development and online deployment, covering both recall and ranking modules. The entire project is developed using Python and executed on a single laptop. All components are contained within Docker, ensuring no impact on the local environment. 
 The GitHub repo: https://github.com/akiragy/recsys_pipeline
 You can follow the commands provided in the README to run it. 
 This project primarily utilizes PyTorch, Redis, Elasticsearch, Feast Feature Store, Triton Inference Server, and Flask. 
 PyTorch is used for training the FM model for recall and the DeepFM model for ranking. 
 Redis serves as the store for user terms and vectors, while Elasticsearch is used to create an item term index and a vector index. Redis and Elasticsearch form the recall module. 
 Feast is utilized to store user and item features, while Triton serves as a real-time prediction engine. Feast and Triton form the ranking module. 
 Flask is deployed as the web server, receiving recommendation requests and returning responses. 
 Thanks for checking it out!
    submitted by    /u/Johann_SebastianBach  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] Benchmarking Neural Network Generalization for Grammar Induction]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1683weg/research_benchmarking_neural_network/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1683weg/research_benchmarking_neural_network/"/>
        <updated>2023-09-02T14:27:19.000Z</updated>
        <summary type="html"><![CDATA[Benchmark:
 ðŸ§˜ BLISS â€“ a Benchmark for Language Induction from Small Sets
 https://github.com/taucompling/bliss/
 Paper:
 https://arxiv.org/abs/2308.08253
    submitted by    /u/nurikolan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metacognition with EDCR]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1682dyw/metacognition_with_edcr/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1682dyw/metacognition_with_edcr/"/>
        <updated>2023-09-02T13:20:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Improving model results with EDCR]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1682791/r_improving_model_results_with_edcr/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1682791/r_improving_model_results_with_edcr/"/>
        <updated>2023-09-02T13:11:56.000Z</updated>
        <summary type="html"><![CDATA[We released another preprint on a neuro-symbolic approach called "metacognitive error correction and detection rules" (EDCR). The idea is that if you have a trained neural model, you can symbolically fine tune the results with rules. In this initial study, we apply it to the classification of GPS movement traces.
 Video: https://www.youtube.com/watch?v=d_OV4lap_rk
 Preprint: https://arxiv.org/abs/2308.14250
 Code: https://github.com/lab-v2/Error-Detection-and-Correction
 Further information: https://neurosymbolic.asu.edu/metacognition/ 
 In the example below, we show the results for a single class. The rules detect errors by identifying classifications that may be incorrect and then re-assign to a new class. While recall can drop for a given class, we can bound the drop in recall with a hyperparameter - but this is guaranteed to improve precision. This is illustrated in the below figure. We show this approach leads to an overall improvement in accuracy over the base model, including the state-of-the-art. We also examine the effects when encountering classes not seen in the model's training data.
 We provide theoretical as well as empirical results and believe this approach can be used in other use-cases in the future.
 â€‹
 https://preview.redd.it/3z0cdp80dulb1.png?width=635&format=png&auto=webp&s=2eff6ce0f2c7b6983dbfbc030f0f7993010a30fb
    submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Working on a project which involves creating an agent to work on chess environment]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/16817w2/working_on_a_project_which_involves_creating_an/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/16817w2/working_on_a_project_which_involves_creating_an/"/>
        <updated>2023-09-02T12:26:34.000Z</updated>
        <summary type="html"><![CDATA[I am using DQN Algorithm and A2C algorithm (Not using any lookaheads to see potential moves and only using self-learning coz my teacher asked me not to look into future combinations and let it play and understand itself) separately to check the performance of the agent and the neural network gives probabilities of the moves in the size of 4096 (64*64) . But the probabilities are decreasing with each and every move performed and they are overfitting to one move which is an invalid move (same case for both dqn and a2c) so in the bellman equation i removed the next reward prediction and put constant value of 1 to check whether it is at least trying to increase the probability for valid moves but that doesnt seems to be the case because it is still giving probability of 1 for an invalid move. and there is also this case where the probabilities are getting so small they are becoming nan values. can someone provide some insights for me to look into
    submitted by    /u/S_U_B_B_U  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] 10 hard-earned lessons from shipping generative AI products over the past 18 months]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1680vy3/d_10_hardearned_lessons_from_shipping_generative/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1680vy3/d_10_hardearned_lessons_from_shipping_generative/"/>
        <updated>2023-09-02T12:10:49.000Z</updated>
        <summary type="html"><![CDATA[Hey all,
 I'm the founder of a generative AI consultancy and we build gen AI powered products for other companies. We've been doing this for 18 months now and I thought I share our learnings - it might help others.
 â€‹
  
It's a never ending battle to keep up with the latest tools and developments. 
 
By the time you ship your product it's already using an outdated tech-stack. 
 
There are no best-practices yet. You need to make a bet on tools/processes and hope that things won't change much by the time you ship (they will, see point 2). 
 
If your generative AI product doesn't have a VC-backed competitor, there will be one soon. 
 
In order to win you need one of the two things: either (1) the best distribution or (2) the generative AI component is hidden in your product so others don't/can't copy you. 
 
AI researchers / data scientists are suboptimal choice for AI engineering. They're expensive, won't be able to solve most of your problems and likely want to focus on more fundamental problems rather than building products. 
 
Software engineers make the best AI engineers. They are able to solve 80% of your problems right away and they are motivated because they can "work in AI". 
 
Product designers need to get more technical, AI engineers need to get more product-oriented. The gap currently is too big and this leads to all sorts of problems during product development. 
 
Demo bias is real and it makes it 10x harder to deliver something that's in alignment with your client's expectation. Communicating this effectively is a real and underrated skill. 
 
There's no such thing as off-the-shelf AI generated content yet. Current tools are not reliable enough, they hallucinate, make up stuff and produce inconsistent results (applies to text, voice, image and video).
 
    submitted by    /u/BootstrapGuy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is the best text-to-speech tool (preferably free) currently?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1680m3s/d_what_is_the_best_texttospeech_tool_preferably/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1680m3s/d_what_is_the_best_texttospeech_tool_preferably/"/>
        <updated>2023-09-02T11:57:56.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, I need a TTS tool that sounds exactly like a human voice. I want to use it to edit some of my YouTube videos. I see a lot of TTS platforms around. Which do you recommend? I hope this isn't too much to ask. I would gladly appreciate it.
 Thanks in advance.
    submitted by    /u/cessilh1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Real-time Road Segmentation without Dense Depth Images]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167zqwh/r_realtime_road_segmentation_without_dense_depth/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167zqwh/r_realtime_road_segmentation_without_dense_depth/"/>
        <updated>2023-09-02T11:10:49.000Z</updated>
        <summary type="html"><![CDATA[You can use the following code if you want to detect the road in real-time in your vehicle/robot : https://github.com/ErkanMilli/3MT-RoadSeg . One of the main problems in road segmentation by using depth was that if a region is flat, such as walls, it may be detected as road. This was already a known phenomenon and to overcome this, surface normal estimation was used. But, SNE requires dense depth images. Instead, we used a multi-task architecture, and used surface normals as an auxiliary loss, which reduced computation time significantly and also we don't need a dense depth image. Only LiDAR (which is sparse in nature) is sufficient.
    submitted by    /u/ozgurerkent  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Threshold of acceptability in a fill-mask task with BERT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167zoi7/p_threshold_of_acceptability_in_a_fillmask_task/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167zoi7/p_threshold_of_acceptability_in_a_fillmask_task/"/>
        <updated>2023-09-02T11:07:12.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I am very new to machine learning and statistics, and am currently building an experiment that includes probing the knowledge of a bert-base-uncased model in a fill-mask test, without fine-tuning - just the regular pretrained model. I want to see the models knowledge of certain grammatical notions in English - whether its judgements are similar to those of humans or not :)
 My point is to give the model inputs like: "what do you call a room filled with socks? you called it a [MASK] filled room", or "a monster who eats rats is called a [MASK] eater", and check the probabilities it gives to the corresponding singular and plural token, e.g. in the first case I want to probe "sock" \ "socks", and in the second case "rat" / "rats".
 I built a script which does exactly this - pullsâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] One class is hard to detect in vision project]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167zkc4/rp_one_class_is_hard_to_detect_in_vision_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167zkc4/rp_one_class_is_hard_to_detect_in_vision_project/"/>
        <updated>2023-09-02T11:01:19.000Z</updated>
        <summary type="html"><![CDATA[Hi, Iâ€™ve been working for a while now on a project to detect points in medical images which are to be classified into 3 different classes, but my UNet really struggles to predict one of the 3 classes (>70% score when excluding this class vs ~30% when not). I have tried putting a separate decoder just for this one class but the results are worse, and I donâ€™t really have other ideas to better my results. Do you have any ideas/techniques to help me improve my results? Thanks !
    submitted by    /u/maths_and_baguette  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ideas for a high school AI/ML club]]></title>
        <id>https://www.reddit.com/r/artificial/comments/167zikw/ideas_for_a_high_school_aiml_club/</id>
        <link href="https://www.reddit.com/r/artificial/comments/167zikw/ideas_for_a_high_school_aiml_club/"/>
        <updated>2023-09-02T10:58:36.000Z</updated>
        <summary type="html"><![CDATA[I'm thinking of creating an AI club at my high school. The problem is, unlike something like math or coding, there aren't many competitions suitable to beginners and not a lot of previous template content to follow. Therefore, I need to forge my own path. I am curious what your ideas are for some engaging, high-school-friendly topics and events to have, especially if we can only meet for 30 minutes a week. Thanks in advance!
    submitted by    /u/0xCUBE  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Stanford's ML for Graphs course]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167x2i6/d_stanfords_ml_for_graphs_course/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167x2i6/d_stanfords_ml_for_graphs_course/"/>
        <updated>2023-09-02T08:32:42.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone.
 Has anybody taken this course from Stanford
 https://online.stanford.edu/courses/xcs224w-machine-learning-graphs
 or any other course in the same online portal? Was it worth it?
 I am considering to apply.
 Thanks
    submitted by    /u/Realistic-Bed2658  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UCL Reinforcement learning lectures]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/167virp/ucl_reinforcement_learning_lectures/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/167virp/ucl_reinforcement_learning_lectures/"/>
        <updated>2023-09-02T06:58:03.000Z</updated>
        <summary type="html"><![CDATA[I see lectures on youtube from UCL+DeppMind on RL spanning from 2015 through 2021. Which one would you say is the best to follow? I've heard many good things about David Silver's lectures, but how do the most recent, 2021, lectures compare?
    submitted by    /u/Practical_Ad_8782  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Recurrent Forward Forward: Accuracy Issues]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167vgy1/r_recurrent_forward_forward_accuracy_issues/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167vgy1/r_recurrent_forward_forward_accuracy_issues/"/>
        <updated>2023-09-02T06:55:04.000Z</updated>
        <summary type="html"><![CDATA[Problem
 I recently did a bit of a career switch from big tech IoT Rust job, into a machine learning research role. For the last few months, I have been working on building out the Recurrent Forward Forward model from Hinton's Forward Forward paper (Fig3):
 https://arxiv.org/abs/2212.13345
 I have an implementation, but have been stuck for the past 4-6 weeks on trying to improve the accuracy. My implementation is only getting 95% test accuracy on MNIST.
 Hinton and Alex Ororbia (author of this) have been able to achieve high test accuracy (99%+) using this architecture, so I know it is possible.
 What I have tried
 I have tried many different things at this point:
  
Different activation functions.
 Weight initialization.
 Regularization techniques like transforms, jitter, and dynamic negaâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An AI to help with my psychology assignment?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/167v331/an_ai_to_help_with_my_psychology_assignment/</id>
        <link href="https://www.reddit.com/r/artificial/comments/167v331/an_ai_to_help_with_my_psychology_assignment/"/>
        <updated>2023-09-02T06:31:47.000Z</updated>
        <summary type="html"><![CDATA[My psychology masters assignments are to be handwritten and hence a somewhat painstaking process. To streamline I was looking for an AI that can guide me on the concepts and understanding of the given psychology subjects. I don't want to use it as a shortcut just a tool for studying and guiding. In accordance with books and Google. Can anyone know of such an AI? 
    submitted by    /u/Maddragon0088  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[follow me on X for ai news without the garbage. just made an account bc im tired of these annoying accounts and decided to just make my own ai news account]]></title>
        <id>https://www.reddit.com/r/artificial/comments/167qtdm/follow_me_on_x_for_ai_news_without_the_garbage/</id>
        <link href="https://www.reddit.com/r/artificial/comments/167qtdm/follow_me_on_x_for_ai_news_without_the_garbage/"/>
        <updated>2023-09-02T02:43:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nicdunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Could AI be the game-changer in tackling the opioid epidemic?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/167oryt/could_ai_be_the_gamechanger_in_tackling_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/167oryt/could_ai_be_the_gamechanger_in_tackling_the/"/>
        <updated>2023-09-02T01:06:00.000Z</updated>
        <summary type="html"><![CDATA[The stubborn and complex opioid epidemic may finally meet its matchâ€”AI. As the crisis continues taking a fearsome toll, experts are turning to advanced technology in their ongoing battle.
 If you want to stay on top of the latest trends and insights in AI, look here first.
 https://preview.redd.it/vm23xflorqlb1.jpg?width=1390&format=pjpg&auto=webp&s=212b88fb01eb0f7afaa5011120267ac4ce37ee35
 AIâ€™s evolving role in tackling the opioid crisis
  
With a legacy of over 1 million overdose deaths since 1999, the opioid crisis has stubbornly resisted traditional preventive and regulatory measures. The latest AI-fueled developments offer newfound hope.
 Groundbreaking AI innovations are focusing on identifying individuals at potential risk, monitoring treatment progress, and predicting relapse probabilities. Decoding social media behavior offers an effective outlet for early intervention.
 More radically, AI-enabled wearable devices are being developed to detect overdose symptoms and automatically deliver lifesaving treatment.
  
AI: A double-edged sword?
  
Despite its promising potential, AI application in this sphere also raises concerns around privacy rights and misinformation. Facial recognition technology could lead to discrimination, while the risk of false data being fed into chatbots causing harm cannot be undermined.
 Trust in AI and its appropriate deployment will be crucial to ensuring its positive contribution rather than being a dystopian threat.
  
P.S. If you like this kind of analysis, youâ€™ll love my free newsletter that tracks the most relevant news and research in AI and tech.
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks. (arXiv:2305.19979v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.19979</id>
        <link href="http://arxiv.org/abs/2305.19979"/>
        <updated>2023-09-02T00:40:03.066Z</updated>
        <summary type="html"><![CDATA[Knowledge graphs are powerful tools for representing and organising complex
biomedical data. Several knowledge graph embedding algorithms have been
proposed to learn from and complete knowledge graphs. However, a recent study
demonstrates the limited efficacy of these embedding algorithms when applied to
biomedical knowledge graphs, raising the question of whether knowledge graph
embeddings have limitations in biomedical settings. This study aims to apply
state-of-the-art knowledge graph embedding models in the context of a recent
biomedical knowledge graph, BioKG, and evaluate their performance and potential
downstream uses. We achieve a three-fold improvement in terms of performance
based on the HITS@10 score over previous work on the same biomedical knowledge
graph. Additionally, we provide interpretable predictions through a rule-based
method. We demonstrate that knowledge graph embedding models are applicable in
practice by evaluating the best-performing model on four tasks that represent
real-life polypharmacy situations. Results suggest that knowledge learnt from
large biomedical knowledge graphs can be transferred to such downstream use
cases. Our code is available at https://github.com/aryopg/biokge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gema_A/0/1/0/all/0/1"&gt;Aryo Pradipta Gema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grabarczyk_D/0/1/0/all/0/1"&gt;Dominik Grabarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulf_W/0/1/0/all/0/1"&gt;Wolf De Wulf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borole_P/0/1/0/all/0/1"&gt;Piyush Borole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfaro_J/0/1/0/all/0/1"&gt;Javier Antonio Alfaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1"&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1"&gt;Antonio Vergari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1"&gt;Ajitha Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.15690</id>
        <link href="http://arxiv.org/abs/2308.15690"/>
        <updated>2023-09-02T00:40:03.038Z</updated>
        <summary type="html"><![CDATA[We present 'CongNaMul', a comprehensive dataset designed for various tasks in
soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate
tasks such as image classification, semantic segmentation, decomposition, and
measurement of length and weight. The classification task provides four classes
to determine the quality of soybean sprouts: normal, broken, spotted, and
broken and spotted, for the development of AI-aided automatic quality
inspection technology. For semantic segmentation, images with varying
complexity, from single sprout images to images with multiple sprouts, along
with human-labelled mask images, are included. The label has 4 different
classes: background, head, body, tail. The dataset also provides images and
masks for the image decomposition task, including two separate sprout images
and their combined form. Lastly, 5 physical features of sprouts (head length,
body length, body thickness, tail length, weight) are provided for image-based
measurement tasks. This dataset is expected to be a valuable resource for a
wide range of research and applications in the advanced analysis of images of
soybean sprouts. Also, we hope that this dataset can assist researchers
studying classification, semantic segmentation, decomposition, and physical
feature measurement in other industrial fields, in evaluating their models. The
dataset is available at the authors' repository. (https://bhban.kr/data)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ban_B/0/1/0/all/0/1"&gt;Byunghyun Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1"&gt;Donghun Ryu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Su-won Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Distributed Learning with Quantized Finite-Time Coordination. (arXiv:2307.06620v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.06620</id>
        <link href="http://arxiv.org/abs/2307.06620"/>
        <updated>2023-09-02T00:40:03.032Z</updated>
        <summary type="html"><![CDATA[In this paper we consider online distributed learning problems. Online
distributed learning refers to the process of training learning models on
distributed data sources. In our setting a set of agents need to cooperatively
train a learning model from streaming data. Differently from federated
learning, the proposed approach does not rely on a central server but only on
peer-to-peer communications among the agents. This approach is often used in
scenarios where data cannot be moved to a centralized location due to privacy,
security, or cost reasons. In order to overcome the absence of a central
server, we propose a distributed algorithm that relies on a quantized,
finite-time coordination protocol to aggregate the locally trained models.
Furthermore, our algorithm allows for the use of stochastic gradients during
local training. Stochastic gradients are computed using a randomly sampled
subset of the local training data, which makes the proposed algorithm more
efficient and scalable than traditional gradient descent. In our paper, we
analyze the performance of the proposed algorithm in terms of the mean distance
from the online solution. Finally, we present numerical results for a logistic
regression task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bastianello_N/0/1/0/all/0/1"&gt;Nicola Bastianello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rikos_A/0/1/0/all/0/1"&gt;Apostolos I. Rikos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1"&gt;Karl H. Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation Method. (arXiv:2306.11737v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.11737</id>
        <link href="http://arxiv.org/abs/2306.11737"/>
        <updated>2023-09-02T00:40:03.026Z</updated>
        <summary type="html"><![CDATA[Partitioning a polygonal mesh into meaningful parts can be challenging. Many
applications require decomposing such structures for further processing in
computer graphics. In the last decade, several methods were proposed to tackle
this problem, at the cost of intensive computational times. Recently, machine
learning has proven to be effective for the segmentation task on 3D structures.
Nevertheless, these state-of-the-art methods are often hardly generalizable and
require dividing the learned model into several specific classes of objects to
avoid overfitting. We present a data-driven approach leveraging deep learning
to encode a mapping function prior to mesh segmentation for multiple
applications. Our network reproduces a neighborhood map using our knowledge of
the \textsl{Shape Diameter Function} (SDF) method using similarities among
vertex neighborhoods. Our approach is resolution-agnostic as we downsample the
input meshes and query the full-resolution structure solely for neighborhood
contributions. Using our predicted SDF values, we can inject the resulting
structure into a graph-cut algorithm to generate an efficient and robust mesh
segmentation while considerably reducing the required computation times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Bruno Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.04726</id>
        <link href="http://arxiv.org/abs/2307.04726"/>
        <updated>2023-09-02T00:40:02.919Z</updated>
        <summary type="html"><![CDATA[Offline Reinforcement Learning (RL) methods leverage previous experiences to
learn better policies than the behavior policy used for data collection. In
contrast to behavior cloning, which assumes the data is collected from expert
demonstrations, offline RL can work with non-expert data and multimodal
behavior policies. However, offline RL algorithms face challenges in handling
distribution shifts and effectively representing policies due to the lack of
online interaction during training. Prior work on offline RL uses conditional
diffusion models to represent multimodal behavior in the dataset. Nevertheless,
these methods are not tailored toward alleviating the out-of-distribution state
generalization. We introduce a novel method, named State Reconstruction for
Diffusion Policies (SRDP), incorporating state reconstruction feature learning
in the recent class of diffusion policies to address the out-of-distribution
generalization problem. State reconstruction loss promotes more descriptive
representation learning of states to alleviate the distribution shift incurred
by the out-of-distribution (OOD) states. We design a novel 2D Multimodal
Contextual Bandit environment to illustrate the OOD generalization of SRDP
compared to prior algorithms. In addition, we assess the performance of our
model on D4RL continuous control benchmarks, namely the navigation of an 8-DoF
ant and forward locomotion of half-cheetah, hopper, and walker2d, achieving
state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ada_S/0/1/0/all/0/1"&gt;Suzan Ece Ada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1"&gt;Erhan Oztop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1"&gt;Emre Ugur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.06777</id>
        <link href="http://arxiv.org/abs/2306.06777"/>
        <updated>2023-09-02T00:40:02.913Z</updated>
        <summary type="html"><![CDATA[In classification and forecasting with tabular data, one often utilizes
tree-based models. Those can be competitive with deep neural networks on
tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under
some conditions, explainable. The explainability depends on the depth of the
tree and the accuracy in each leaf of the tree. Decision trees containing
leaves with unbalanced accuracy can provide misleading explanations.
Low-accuracy leaves give less valid explanations, which could be interpreted as
unfairness among explanations. Here, we train a shallow tree with the objective
of minimizing the maximum misclassification error across each leaf node. Then,
we extend each leaf with a separate tree-based model. The shallow tree provides
a global explanation, while the overall statistical performance of the shallow
tree with extended leaves improves upon decision trees of unlimited depth
trained using classical methods (e.g., CART) and is comparable to
state-of-the-art methods (e.g., well-tuned XGBoost).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nemecek_J/0/1/0/all/0/1"&gt;Jiri Nemecek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1"&gt;Tomas Pevny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1"&gt;Jakub Marecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation. (arXiv:2305.15777v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.15777</id>
        <link href="http://arxiv.org/abs/2305.15777"/>
        <updated>2023-09-02T00:40:02.868Z</updated>
        <summary type="html"><![CDATA[Medical image data are often limited due to the expensive acquisition and
annotation process. Hence, training a deep-learning model with only raw data
can easily lead to overfitting. One solution to this problem is to augment the
raw data with various transformations, improving the model's ability to
generalize to new data. However, manually configuring a generic augmentation
combination and parameters for different datasets is non-trivial due to
inconsistent acquisition approaches and data distributions. Therefore,
automatic data augmentation is proposed to learn favorable augmentation
strategies for different datasets while incurring large GPU overhead. To this
end, we present a novel method, called Dynamic Data Augmentation (DDAug), which
is efficient and has negligible computation cost. Our DDAug develops a
hierarchical tree structure to represent various augmentations and utilizes an
efficient Monte-Carlo tree searching algorithm to update, prune, and sample the
tree. As a result, the augmentation pipeline can be optimized for each dataset
automatically. Experiments on multiple Prostate MRI datasets show that our
method outperforms the current state-of-the-art data augmentation strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hsi_Y/0/1/0/all/0/1"&gt;Yuhan Hsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haonan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaomeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biclustering Methods via Sparse Penalty. (arXiv:2308.14388v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.14388</id>
        <link href="http://arxiv.org/abs/2308.14388"/>
        <updated>2023-09-02T00:40:02.867Z</updated>
        <summary type="html"><![CDATA[In this paper, we first reviewed several biclustering methods that are used
to identify the most significant clusters in gene expression data. Here we
mainly focused on the SSVD(sparse SVD) method and tried a new sparse penalty
named "Prenet penalty" which has been used only in factor analysis to gain
sparsity. Then in the simulation study, we tried different types of generated
datasets (with different sparsity and dimension) and tried 1-layer
approximation then for k-layers which shows the mixed Prenet penalty is very
effective for non-overlapped data. Finally, we used some real gene expression
data to show the behavior of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings. (arXiv:2306.17670v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.17670</id>
        <link href="http://arxiv.org/abs/2306.17670"/>
        <updated>2023-09-02T00:40:02.854Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) are a promising research direction for
building power-efficient information processing systems, especially for
temporal tasks such as speech recognition. In SNNs, delays refer to the time
needed for one spike to travel from one neuron to another. These delays matter
because they influence the spike arrival times, and it is well-known that
spiking neurons respond more strongly to coincident input spikes. More
formally, it has been shown theoretically that plastic delays greatly increase
the expressivity in SNNs. Yet, efficient algorithms to learn these delays have
been lacking. Here, we propose a new discrete-time algorithm that addresses
this issue in deep feedforward SNNs using backpropagation, in an offline
manner. To simulate delays between consecutive layers, we use 1D convolutions
across time. The kernels contain only a few non-zero weights - one per synapse
- whose positions correspond to the delays. These positions are learned
together with the weights using the recently proposed Dilated Convolution with
Learnable Spacings (DCLS). We evaluated our method on three datasets: the
Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and its
non-spiking version Google Speech Commands v0.02 (GSC) benchmarks, which
require detecting temporal patterns. We used feedforward SNNs with two or three
hidden fully connected layers, and vanilla leaky integrate-and fire neurons. We
showed that fixed random delays help and that learning them helps even more.
Furthermore, our method outperformed the state-of-the-art in the three datasets
without using recurrent connections and with substantially fewer parameters.
Our work demonstrates the potential of delay learning in developing accurate
and precise models for temporal data processing. Our code is based on PyTorch /
SpikingJelly and available at: https://github.com/Thvnvtos/SNN-delays]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammouamri_I/0/1/0/all/0/1"&gt;Ilyass Hammouamri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalfaoui_Hassani_I/0/1/0/all/0/1"&gt;Ismail Khalfaoui-Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Masquelier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML. (arXiv:2306.05109v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.05109</id>
        <link href="http://arxiv.org/abs/2306.05109"/>
        <updated>2023-09-02T00:40:02.853Z</updated>
        <summary type="html"><![CDATA[Medical applications of machine learning (ML) have experienced a surge in
popularity in recent years. The intensive care unit (ICU) is a natural habitat
for ML given the abundance of available data from electronic health records.
Models have been proposed to address numerous ICU prediction tasks like the
early detection of complications. While authors frequently report
state-of-the-art performance, it is challenging to verify claims of
superiority. Datasets and code are not always published, and cohort
definitions, preprocessing pipelines, and training setups are difficult to
reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular
framework that allows researchers to define reproducible and comparable
clinical ML experiments; we offer an end-to-end solution from cohort definition
to model evaluation. The framework natively supports most open-access ICU
datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future
ICU datasets. Combined with a transparent preprocessing pipeline and extensible
training code for multiple ML and deep learning models, YAIB enables unified
model development. Our benchmark comes with five predefined established
prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and
length of stay) developed in collaboration with clinicians. Adding further
tasks is straightforward by design. Using YAIB, we demonstrate that the choice
of dataset, cohort definition, and preprocessing have a major impact on the
prediction performance - often more so than model class - indicating an urgent
need for YAIB as a holistic benchmarking tool. We provide our work to the
clinical ML community to accelerate method development and enable real-world
clinical implementations. Software Repository:
https://github.com/rvandewater/YAIB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Water_R/0/1/0/all/0/1"&gt;Robin van de Water&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_H/0/1/0/all/0/1"&gt;Hendrik Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elbers_P/0/1/0/all/0/1"&gt;Paul Elbers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thoral_P/0/1/0/all/0/1"&gt;Patrick Thoral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnrich_B/0/1/0/all/0/1"&gt;Bert Arnrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rockenschaub_P/0/1/0/all/0/1"&gt;Patrick Rockenschaub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.16139</id>
        <link href="http://arxiv.org/abs/2308.16139"/>
        <updated>2023-09-02T00:40:02.851Z</updated>
        <summary type="html"><![CDATA[We present MedShapeNet, a large collection of anatomical shapes (e.g., bones,
organs, vessels) and 3D surgical instrument models. Prior to the deep learning
era, the broad application of statistical shape models (SSMs) in medical image
analysis is evidence that shapes have been commonly used to describe medical
data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in
medical imaging are predominantly voxel-based. In computer vision, on the
contrary, shapes (including, voxel occupancy grids, meshes, point clouds and
implicit surface models) are preferred data representations in 3D, as seen from
the numerous shape-related publications in premier vision conferences, such as
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as
well as the increasing popularity of ShapeNet (about 51,300 models) and
Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is
created as an alternative to these commonly used shape benchmarks to facilitate
the translation of data-driven vision algorithms to medical applications, and
it extends the opportunities to adapt SOTA vision algorithms to solve critical
medical problems. Besides, the majority of the medical shapes in MedShapeNet
are modeled directly on the imaging data of real patients, and therefore it
complements well existing shape benchmarks comprising of computer-aided design
(CAD) models. MedShapeNet currently includes more than 100,000 medical shapes,
and provides annotations in the form of paired data. It is therefore also a
freely available repository of 3D models for extended reality (virtual reality
- VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This
white paper describes in detail the motivations behind MedShapeNet, the shape
acquisition procedures, the use cases, as well as the usage of the online shape
search portal: https://medshapenet.ikim.nrw/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianning Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1"&gt;Antonio Pepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1"&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1"&gt;Gijs Luijten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambigapathy_N/0/1/0/all/0/1"&gt;Narmada Ambigapathy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasca_E/0/1/0/all/0/1"&gt;Enrico Nasca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solak_N/0/1/0/all/0/1"&gt;Naida Solak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melito_G/0/1/0/all/0/1"&gt;Gian Marco Melito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Memon_A/0/1/0/all/0/1"&gt;Afaque R. Memon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1"&gt;Jan Stefan Kirschke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1"&gt;Ezequiel de la Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1"&gt;Patrich Ferndinand Christ&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_D/0/1/0/all/0/1"&gt;David G. Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizenberg_M/0/1/0/all/0/1"&gt;Michele R. Aizenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1"&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuestner_T/0/1/0/all/0/1"&gt;Thomas Kuestner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1"&gt;Nadya Shusharina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heller_N/0/1/0/all/0/1"&gt;Nicholas Heller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1"&gt;Vincent Andrearczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Depeursinge_A/0/1/0/all/0/1"&gt;Adrien Depeursinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hatt_M/0/1/0/all/0/1"&gt;Mathieu Hatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1"&gt;Anjany Sekuboyina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loeffler_M/0/1/0/all/0/1"&gt;Maximilian Loeffler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1"&gt;Hans Liebl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1"&gt;Reuben Dorent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1"&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1"&gt;Jonathan Shapey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1"&gt;Aaron Kujawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornelissen_S/0/1/0/all/0/1"&gt;Stefan Cornelissen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langenhuizen_P/0/1/0/all/0/1"&gt;Patrick Langenhuizen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Hamadou_A/0/1/0/all/0/1"&gt;Achraf Ben-Hamadou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekik_A/0/1/0/all/0/1"&gt;Ahmed Rekik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pujades_S/0/1/0/all/0/1"&gt;Sergi Pujades&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1"&gt;Edmond Boyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolelli_F/0/1/0/all/0/1"&gt;Federico Bolelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grana_C/0/1/0/all/0/1"&gt;Costantino Grana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lumetti_L/0/1/0/all/0/1"&gt;Luca Lumetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salehi_H/0/1/0/all/0/1"&gt;Hamidreza Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gharleghi_R/0/1/0/all/0/1"&gt;Ramtin Gharleghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beier_S/0/1/0/all/0/1"&gt;Susann Beier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sowmya_A/0/1/0/all/0/1"&gt;Arcot Sowmya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garza_Villarreal_E/0/1/0/all/0/1"&gt;Eduardo A. Garza-Villarreal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balducci_T/0/1/0/all/0/1"&gt;Thania Balducci&lt;/a&gt;, et al. (68 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Does Little Robustness Help? Understanding and Improving Adversarial Transferability from Surrogate Training. (arXiv:2307.07873v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.07873</id>
        <link href="http://arxiv.org/abs/2307.07873"/>
        <updated>2023-09-02T00:40:02.843Z</updated>
        <summary type="html"><![CDATA[Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yechao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengshan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Junyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Minghui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaogeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1"&gt;Wei Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hai Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.08396</id>
        <link href="http://arxiv.org/abs/2305.08396"/>
        <updated>2023-09-02T00:40:02.837Z</updated>
        <summary type="html"><![CDATA[In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision
transformer (CNN-Transformer) for medical image segmentation. The proposed
Hybrid Decoder, based on MaxViT-block, is designed to harness the power of both
the convolution and self-attention mechanisms at each decoding stage with a
nominal memory and computational burden. The inclusion of multi-axis
self-attention, within each decoder stage, significantly enhances the
discriminating capacity between the object and background regions, thereby
helping in improving the segmentation efficiency. In the Hybrid Decoder block,
the fusion process commences by integrating the upsampled lower-level decoder
features, obtained through transpose convolution, with the skip-connection
features derived from the hybrid encoder. Subsequently, the fused features
undergo refinement through the utilization of a multi-axis attention mechanism.
The proposed decoder block is repeated multiple times to progressively segment
the nuclei regions. Experimental results on MoNuSeg18 and MoNuSAC20 dataset
demonstrates the effectiveness of the proposed technique. Our MaxViT-UNet
outperformed the previous CNN-based (UNet) and Transformer-based (Swin-UNet)
techniques by a considerable margin on both of the standard datasets. The
following github (https://github.com/PRLAB21/MaxViT-UNet) contains the
implementation and trained weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1"&gt;Abdul Rehman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1"&gt;Asifullah Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.10474</id>
        <link href="http://arxiv.org/abs/2305.10474"/>
        <updated>2023-09-02T00:40:02.831Z</updated>
        <summary type="html"><![CDATA[Despite tremendous progress in generating high-quality images using diffusion
models, synthesizing a sequence of animated frames that are both photorealistic
and temporally coherent is still in its infancy. While off-the-shelf
billion-scale datasets for image generation are available, collecting similar
video data of the same scale is still challenging. Also, training a video
diffusion model is computationally much more expensive than its image
counterpart. In this work, we explore finetuning a pretrained image diffusion
model with video data as a practical solution for the video synthesis task. We
find that naively extending the image noise prior to video noise prior in video
diffusion leads to sub-optimal performance. Our carefully designed video noise
prior leads to substantially better performance. Extensive experimental
validation shows that our model, Preserve Your Own Correlation (PYoCo), attains
SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It
also achieves SOTA video generation quality on the small-scale UCF-101
benchmark with a $10\times$ smaller model using significantly less computation
than the prior art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Songwei Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nah_S/0/1/0/all/0/1"&gt;Seungjun Nah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_T/0/1/0/all/0/1"&gt;Tyler Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1"&gt;Andrew Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1"&gt;David Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jia-Bin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_Y/0/1/0/all/0/1"&gt;Yogesh Balaji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.08149</id>
        <link href="http://arxiv.org/abs/2306.08149"/>
        <updated>2023-09-02T00:40:02.811Z</updated>
        <summary type="html"><![CDATA[Personalized prediction is a machine learning approach that predicts a
person's future observations based on their past labeled observations and is
typically used for sequential tasks, e.g., to predict daily mood ratings. When
making personalized predictions, a model can combine two types of trends: (a)
trends shared across people, i.e., person-generic trends, such as being happier
on weekends, and (b) unique trends for each person, i.e., person-specific
trends, such as a stressful weekly meeting. Mixed effect models are popular
statistical models to study both trends by combining person-generic and
person-specific parameters. Though linear mixed effect models are gaining
popularity in machine learning by integrating them with neural networks, these
integrations are currently limited to linear person-specific parameters: ruling
out nonlinear person-specific trends. In this paper, we propose Neural Mixed
Effect (NME) models to optimize nonlinear person-specific parameters anywhere
in a neural network in a scalable manner. NME combines the efficiency of neural
network optimization with nonlinear mixed effects modeling. Empirically, we
observe that NME improves performance across six unimodal and multimodal
datasets, including a smartphone dataset to predict daily mood and a
mother-adolescent dataset to predict affective state sequences where half the
mothers experience at least moderate symptoms of depression. Furthermore, we
evaluate NME for two model architectures, including for neural conditional
random fields (CRF) to predict affective state sequences where the CRF learns
nonlinear person-specific temporal transitions between affective states.
Analysis of these person-specific transitions on the mother-adolescent dataset
shows interpretable trends related to the mother's depression symptoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wortwein_T/0/1/0/all/0/1"&gt;Torsten W&amp;#xf6;rtwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1"&gt;Nicholas Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheeber_L/0/1/0/all/0/1"&gt;Lisa B. Sheeber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1"&gt;Randy P. Auerbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohn_J/0/1/0/all/0/1"&gt;Jeffrey F. Cohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed-type Distance Shrinkage and Selection for Clustering via Kernel Metric Learning. (arXiv:2306.01890v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.01890</id>
        <link href="http://arxiv.org/abs/2306.01890"/>
        <updated>2023-09-02T00:40:02.804Z</updated>
        <summary type="html"><![CDATA[Distance-based clustering and classification are widely used in various
fields to group mixed numeric and categorical data. In many algorithms, a
predefined distance measurement is used to cluster data points based on their
dissimilarity. While there exist numerous distance-based measures for data with
pure numerical attributes and several ordered and unordered categorical
metrics, an efficient and accurate distance for mixed-type data that utilizes
the continuous and discrete properties simulatenously is an open problem. Many
metrics convert numerical attributes to categorical ones or vice versa. They
handle the data points as a single attribute type or calculate a distance
between each attribute separately and add them up. We propose a metric called
KDSUM that uses mixed kernels to measure dissimilarity, with cross-validated
optimal bandwidth selection. We demonstrate that KDSUM is a shrinkage method
from existing mixed-type metrics to a uniform dissimilarity metric, and
improves clustering accuracy when utilized in existing distance-based
clustering algorithms on simulated and real-world datasets containing
continuous-only, categorical-only, and mixed-type data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghashti_J/0/1/0/all/0/1"&gt;Jesse S. Ghashti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thompson_J/0/1/0/all/0/1"&gt;John R. J. Thompson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Response Heteroscedastic Gaussian Process Models and Their Inference. (arXiv:2308.15370v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.15370</id>
        <link href="http://arxiv.org/abs/2308.15370"/>
        <updated>2023-09-02T00:40:02.798Z</updated>
        <summary type="html"><![CDATA[Despite the widespread utilization of Gaussian process models for versatile
nonparametric modeling, they exhibit limitations in effectively capturing
abrupt changes in function smoothness and accommodating relationships with
heteroscedastic errors. Addressing these shortcomings, the heteroscedastic
Gaussian process (HeGP) regression seeks to introduce flexibility by
acknowledging the variability of residual variances across covariates in the
regression model. In this work, we extend the HeGP concept, expanding its scope
beyond regression tasks to encompass classification and state-space models. To
achieve this, we propose a novel framework where the Gaussian process is
coupled with a covariate-induced precision matrix process, adopting a mixture
formulation. This approach enables the modeling of heteroscedastic covariance
functions across covariates. To mitigate the computational challenges posed by
sampling, we employ variational inference to approximate the posterior and
facilitate posterior predictive modeling. Additionally, our training process
leverages an EM algorithm featuring closed-form M-step updates to efficiently
evaluate the heteroscedastic covariance function. A notable feature of our
model is its consistent performance on multivariate responses, accommodating
various types (continuous or categorical) seamlessly. Through a combination of
simulations and real-world applications in climatology, we illustrate the
model's prowess and advantages. By overcoming the limitations of traditional
Gaussian process models, our proposed framework offers a robust and versatile
tool for a wide array of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taehee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun S. Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.15034</id>
        <link href="http://arxiv.org/abs/2307.15034"/>
        <updated>2023-09-02T00:40:02.792Z</updated>
        <summary type="html"><![CDATA[The Fourier neural operator (FNO) is a powerful technique for learning
surrogate maps for partial differential equation (PDE) solution operators. For
many real-world applications, which often require high-resolution data points,
training time and memory usage are significant bottlenecks. While there are
mixed-precision training techniques for standard neural networks, those work
for real-valued datatypes on finite dimensions and therefore cannot be directly
applied to FNO, which crucially operates in the (complex-valued) Fourier domain
and in function spaces. On the other hand, since the Fourier transform is
already an approximation (due to discretization error), we do not need to
perform the operation at full precision. In this work, we (i) profile memory
and runtime for FNO with full and mixed-precision training, (ii) conduct a
study on the numerical stability of mixed-precision training of FNO, and (iii)
devise a training routine which substantially decreases training time and
memory usage (up to 34%), with little or no reduction in accuracy, on the
Navier-Stokes and Darcy flow equations. Combined with the recently proposed
tensorized FNO (Kossaifi et al., 2023), the resulting model has far better
performance while also being significantly faster than the original FNO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1"&gt;Renbo Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1"&gt;Jean Kossaifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1"&gt;Gennady Pekhimenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Role of Diverse Replay for Generalisation in Reinforcement Learning. (arXiv:2306.05727v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2306.05727</id>
        <link href="http://arxiv.org/abs/2306.05727"/>
        <updated>2023-09-02T00:40:02.774Z</updated>
        <summary type="html"><![CDATA[In reinforcement learning (RL), key components of many algorithms are the
exploration strategy and replay buffer. These strategies regulate what
environment data is collected and trained on and have been extensively studied
in the RL literature. In this paper, we investigate the impact of these
components in the context of generalisation in multi-task RL. We investigate
the hypothesis that collecting and training on more diverse data from the
training environments will improve zero-shot generalisation to new tasks. We
motivate mathematically and show empirically that generalisation to tasks that
are "reachable'' during training is improved by increasing the diversity of
transitions in the replay buffer. Furthermore, we show empirically that this
same strategy also shows improvement for generalisation to similar but
"unreachable'' tasks which could be due to improved generalisation of the
learned latent representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weltevrede_M/0/1/0/all/0/1"&gt;Max Weltevrede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spaan_M/0/1/0/all/0/1"&gt;Matthijs T.J. Spaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohmer_W/0/1/0/all/0/1"&gt;Wendelin B&amp;#xf6;hmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting. (arXiv:2305.11304v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11304</id>
        <link href="http://arxiv.org/abs/2305.11304"/>
        <updated>2023-09-02T00:40:02.767Z</updated>
        <summary type="html"><![CDATA[Various probabilistic time series forecasting models have sprung up and shown
remarkably good performance. However, the choice of model highly relies on the
characteristics of the input time series and the fixed distribution that the
model is based on. Due to the fact that the probability distributions cannot be
averaged over different models straightforwardly, the current time series model
ensemble methods cannot be directly applied to improve the robustness and
accuracy of forecasting. To address this issue, we propose pTSE, a multi-model
distribution ensemble method for probabilistic forecasting based on Hidden
Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models
without requiring further information about each model. Besides, we provide a
complete theoretical analysis of pTSE to prove that the empirical distribution
of time series subject to an HMM will converge to the stationary distribution
almost surely. Experiments on benchmarks show the superiority of pTSE overall
member models and competitive ensemble methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yunyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1"&gt;Yijia Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1"&gt;Ge Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuchen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.09312</id>
        <link href="http://arxiv.org/abs/2307.09312"/>
        <updated>2023-09-02T00:40:02.720Z</updated>
        <summary type="html"><![CDATA[We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal
graph-based transformer model for detecting hate speech in online social
networks, such as Reddit discussions. In contrast to traditional comment-only
methods, our approach to labelling a comment as hate speech involves a holistic
analysis of text and images grounded in the discussion context. This is done by
leveraging graph transformers to capture the contextual relationships in the
entire discussion surrounding a comment and grounding the interwoven fusion
layers that combine individual comments' text and image embeddings instead of
processing modalities separately. We compare the performance of our model to
baselines that only process individual comments and conduct extensive ablation
studies. To evaluate our work, we present a new dataset, HatefulDiscussions,
comprising complete multi-modal discussions from multiple online communities on
Reddit. We conclude with future work for multimodal solutions to deliver social
value in online contexts, arguing that capturing a holistic view of a
conversation significantly advances the effort to detect anti-social behaviour.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1"&gt;Liam Hebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1"&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuxuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivas_N/0/1/0/all/0/1"&gt;Nanda Kishore Sreenivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1"&gt;Lukasz Golab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1"&gt;Robin Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11463</id>
        <link href="http://arxiv.org/abs/2305.11463"/>
        <updated>2023-09-02T00:40:02.714Z</updated>
        <summary type="html"><![CDATA[Maximum mean discrepancy (MMD) flows suffer from high computational costs in
large scale computations. In this paper, we show that MMD flows with Riesz
kernels $K(x,y) = - \Vert x-y\Vert^r$, $r \in (0,2)$ have exceptional
properties which allow their efficient computation. We prove that the MMD of
Riesz kernels coincides with the MMD of their sliced version. As a consequence,
the computation of gradients of MMDs can be performed in the one-dimensional
setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce
the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two measures with
$M$ and $N$ support points. As another interesting follow-up result, the MMD of
compactly supported measures can be estimated from above and below by the
Wasserstein-1 distance. For the implementations we approximate the gradient of
the sliced MMD by using only a finite number $P$ of slices. We show that the
resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data
dimension. These results enable us to train generative models by approximating
MMD gradient flows by neural networks even for image applications. We
demonstrate the efficiency of our model by image generation on MNIST,
FashionMNIST and CIFAR10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hertrich_J/0/1/0/all/0/1"&gt;Johannes Hertrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wald_C/0/1/0/all/0/1"&gt;Christian Wald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altekruger_F/0/1/0/all/0/1"&gt;Fabian Altekr&amp;#xfc;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagemann_P/0/1/0/all/0/1"&gt;Paul Hagemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements. (arXiv:2307.02329v3 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.02329</id>
        <link href="http://arxiv.org/abs/2307.02329"/>
        <updated>2023-09-02T00:40:02.708Z</updated>
        <summary type="html"><![CDATA[The advent of novel 5G services and applications with binding latency
requirements and guaranteed Quality of Service (QoS) hastened the need to
incorporate autonomous and proactive decision-making in network management
procedures. The objective of our study is to provide a thorough analysis of
predictive latency within 5G networks by utilizing real-world network data that
is accessible to mobile network operators (MNOs). In particular, (i) we present
an analytical formulation of the user-plane latency as a Hypoexponential
distribution, which is validated by means of a comparative analysis with
empirical measurements, and (ii) we conduct experimental results of
probabilistic regression, anomaly detection, and predictive forecasting
leveraging on emerging domains in Machine Learning (ML), such as Bayesian
Learning (BL) and Machine Learning on Graphs (GML). We test our predictive
framework using data gathered from scenarios of vehicular mobility, dense-urban
traffic, and social gathering events. Our results provide valuable insights
into the efficacy of predictive algorithms in practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Skocaj_M/0/1/0/all/0/1"&gt;Marco Skocaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conserva_F/0/1/0/all/0/1"&gt;Francesca Conserva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grande_N/0/1/0/all/0/1"&gt;Nicol Sarcone Grande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orsi_A/0/1/0/all/0/1"&gt;Andrea Orsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micheli_D/0/1/0/all/0/1"&gt;Davide Micheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghinamo_G/0/1/0/all/0/1"&gt;Giorgio Ghinamo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bizzarri_S/0/1/0/all/0/1"&gt;Simone Bizzarri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verdone_R/0/1/0/all/0/1"&gt;Roberto Verdone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks. (arXiv:2307.05628v3 [q-bio.GN] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2307.05628</id>
        <link href="http://arxiv.org/abs/2307.05628"/>
        <updated>2023-09-02T00:40:02.596Z</updated>
        <summary type="html"><![CDATA[Pre-trained large language models demonstrate potential in extracting
information from DNA sequences, yet adapting to a variety of tasks and data
modalities remains a challenge. To address this, we propose DNAGPT, a
generalized DNA pre-training model trained on over 200 billion base pairs from
all mammals. By enhancing the classic GPT model with a binary classification
task (DNA sequence order), a numerical regression task (guanine-cytosine
content prediction), and a comprehensive token language, DNAGPT can handle
versatile DNA analysis tasks while processing both sequence and numerical data.
Our evaluation of genomic signal and region recognition, mRNA abundance
regression, and artificial genomes generation tasks demonstrates DNAGPT's
superior performance compared to existing models designed for specific
downstream tasks, benefiting from pre-training using the newly designed model
structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Daoan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weitong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+He_B/0/1/0/all/0/1"&gt;Bing He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qin_C/0/1/0/all/0/1"&gt;Chenchen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jianhua Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.07741</id>
        <link href="http://arxiv.org/abs/2206.07741"/>
        <updated>2023-09-02T00:40:02.519Z</updated>
        <summary type="html"><![CDATA[The large computing and memory cost of deep neural networks (DNNs) often
precludes their use in resource-constrained devices. Quantizing the parameters
and operations to lower bit-precision offers substantial memory and energy
savings for neural network inference, facilitating the use of DNNs on edge
computing platforms. Recent efforts at quantizing DNNs have employed a range of
techniques encompassing progressive quantization, step-size adaptation, and
gradient scaling. This paper proposes a new quantization approach for mixed
precision convolutional neural networks (CNNs) targeting edge-computing. Our
method establishes a new pareto frontier in model accuracy and memory footprint
demonstrating a range of quantized models, delivering best-in-class accuracy
below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions
are: (i) hardware-aware heterogeneous differentiable quantization with
tensor-sliced learned precision, (ii) targeted gradient modification for wgts.
and acts. to mitigate quantization errors, and (iii) a multi-phase learning
schedule to address instability in learning arising from updates to the learned
quantizer and model parameters. We demonstrate the effectiveness of our
techniques on the ImageNet dataset across a range of models including
EfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and
MobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1"&gt;Clemens JS Schaefer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Siddharth Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blazquez_R/0/1/0/all/0/1"&gt;Raul Blazquez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeking Interpretability and Explainability in Binary Activated Neural Networks. (arXiv:2209.03450v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.03450</id>
        <link href="http://arxiv.org/abs/2209.03450"/>
        <updated>2023-09-02T00:40:02.501Z</updated>
        <summary type="html"><![CDATA[We study the use of binary activated neural networks as interpretable and
explainable predictors in the context of regression tasks on tabular data; more
specifically, we provide guarantees on their expressiveness, present an
approach based on the efficient computation of SHAP values for quantifying the
relative importance of the features, hidden neurons and even weights. As the
model's simplicity is instrumental in achieving interpretability, we propose a
greedy algorithm for building compact binary activated networks. This approach
doesn't need to fix an architecture for the network in advance: it is built one
layer at a time, one neuron at a time, leading to predictors that aren't
needlessly complex for a given task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leblanc_B/0/1/0/all/0/1"&gt;Benjamin Leblanc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1"&gt;Pascal Germain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures. (arXiv:2303.09981v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.09981</id>
        <link href="http://arxiv.org/abs/2303.09981"/>
        <updated>2023-09-02T00:40:02.484Z</updated>
        <summary type="html"><![CDATA[Realistic aircraft trajectory models are useful in the design and validation
of air traffic management (ATM) systems. Models of aircraft operated under
instrument flight rules (IFR) require capturing the variability inherent in how
aircraft follow standard flight procedures. The variability in aircraft
behavior varies among flight stages. In this paper, we propose a probabilistic
model that can learn the variability from the procedural data and flight tracks
collected from radar surveillance data. For each segment, a Gaussian mixture
model is used to learn the deviations of aircraft trajectories from their
procedures. Given new procedures, we can generate synthetic trajectories by
sampling a series of deviations from the trained Gaussian distributions and
reconstructing the aircraft trajectory using the deviations and the procedures.
We extend this method to capture pairwise correlations between aircraft and
show how a pairwise model can be used to generate traffic involving an
arbitrary number of aircraft. We demonstrate the proposed models on the arrival
tracks and procedures of the John F. Kennedy International Airport. The
distributional similarity between the original and the synthetic trajectory
dataset was evaluated using the Jensen-Shannon divergence between the empirical
distributions of different variables. We also provide qualitative analyses of
the synthetic trajectories generated from the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Soyeon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1"&gt;Mykel J. Kochenderfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collage Diffusion. (arXiv:2303.00262v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.00262</id>
        <link href="http://arxiv.org/abs/2303.00262"/>
        <updated>2023-09-02T00:40:02.477Z</updated>
        <summary type="html"><![CDATA[We seek to give users precise control over diffusion-based image generation
by modeling complex scenes as sequences of layers, which define the desired
spatial arrangement and visual attributes of objects in the scene. Collage
Diffusion harmonizes the input layers to make objects fit together -- the key
challenge involves minimizing changes in the positions and key visual
attributes of the input layers while allowing other attributes to change in the
harmonization process. We ensure that objects are generated in the correct
locations by modifying text-image cross-attention with the layers' alpha masks.
We preserve key visual attributes of input layers by learning specialized text
representations per layer and by extending ControlNet to operate on layers.
Layer input allows users to control the extent of image harmonization on a
per-object basis, and users can even iteratively edit individual objects in
generated images while keeping other objects fixed. By leveraging the rich
information present in layer input, Collage Diffusion generates globally
harmonized images that maintain desired object characteristics better than
prior approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarukkai_V/0/1/0/all/0/1"&gt;Vishnu Sarukkai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linden Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1"&gt;Arden Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1"&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatahalian_K/0/1/0/all/0/1"&gt;Kayvon Fatahalian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Attribute Completion on Graph with Missing Attributes. (arXiv:2302.12977v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.12977</id>
        <link href="http://arxiv.org/abs/2302.12977"/>
        <updated>2023-09-02T00:40:02.465Z</updated>
        <summary type="html"><![CDATA[Tackling unfairness in graph learning models is a challenging task, as the
unfairness issues on graphs involve both attributes and topological structures.
Existing work on fair graph learning simply assumes that attributes of all
nodes are available for model training and then makes fair predictions. In
practice, however, the attributes of some nodes might not be accessible due to
missing data or privacy concerns, which makes fair graph learning even more
challenging. In this paper, we propose FairAC, a fair attribute completion
method, to complement missing information and learn fair node embeddings for
graphs with missing attributes. FairAC adopts an attention mechanism to deal
with the attribute missing problem and meanwhile, it mitigates two types of
unfairness, i.e., feature unfairness from attributes and topological unfairness
due to attribute completion. FairAC can work on various types of homogeneous
graphs and generate fair embeddings for them and thus can be applied to most
downstream tasks to improve their fairness performance. To our best knowledge,
FairAC is the first method that jointly addresses the graph attribution
completion and graph unfairness problems. Experimental results on benchmark
datasets show that our method achieves better fairness performance with less
sacrifice in accuracy, compared with the state-of-the-art methods of fair graph
learning. Code is available at: https://github.com/donglgcn/FairAC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Dongliang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection. (arXiv:2304.13455v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.13455</id>
        <link href="http://arxiv.org/abs/2304.13455"/>
        <updated>2023-09-02T00:40:02.459Z</updated>
        <summary type="html"><![CDATA[Today, state-of-the-art deep neural networks that process events first
convert them into dense, grid-like input representations before using an
off-the-shelf network. However, selecting the appropriate representation for
the task traditionally requires training a neural network for each
representation and selecting the best one based on the validation score, which
is very time-consuming. This work eliminates this bottleneck by selecting
representations based on the Gromov-Wasserstein Discrepancy (GWD) between raw
events and their representation. It is about 200 times faster to compute than
training a neural network and preserves the task performance ranking of event
representations across multiple representations, network backbones, datasets,
and tasks. Thus finding representations with high task scores is equivalent to
finding representations with a low GWD. We use this insight to, for the first
time, perform a hyperparameter search on a large family of event
representations, revealing new and powerful representations that exceed the
state-of-the-art. Our optimized representations outperform existing
representations by 1.7 mAP on the 1 Mpx dataset and 0.3 mAP on the Gen1
dataset, two established object detection benchmarks, and reach a 3.8% higher
classification score on the mini N-ImageNet benchmark. Moreover, we outperform
state-of-the-art by 2.1 mAP on Gen1 and state-of-the-art feed-forward methods
by 6.0 mAP on the 1 Mpx datasets. This work opens a new unexplored field of
explicit representation optimization for event-based learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zubic_N/0/1/0/all/0/1"&gt;Nikola Zubi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1"&gt;Daniel Gehrig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1"&gt;Mathias Gehrig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1"&gt;Davide Scaramuzza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16900</id>
        <link href="http://arxiv.org/abs/2308.16900"/>
        <updated>2023-09-02T00:40:02.434Z</updated>
        <summary type="html"><![CDATA[We present WineSensed, a large multimodal wine dataset for studying the
relations between visual perception, language, and flavor. The dataset
encompasses 897k images of wine labels and 824k reviews of wines curated from
the Vivino platform. It has over 350k unique vintages, annotated with year,
region, rating, alcohol percentage, price, and grape composition. We obtained
fine-grained flavor annotations on a subset by conducting a wine-tasting
experiment with 256 participants who were asked to rank wines based on their
similarity in flavor, resulting in more than 5k pairwise flavor distances. We
propose a low-dimensional concept embedding algorithm that combines human
experience with automatic machine similarity kernels. We demonstrate that this
shared concept embedding space improves upon separate embedding spaces for
coarse flavor classification (alcohol percentage, country, grape, price,
rating) and aligns with the intricate human perception of flavor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bender_T/0/1/0/all/0/1"&gt;Thoranna Bender&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorensen_S/0/1/0/all/0/1"&gt;Simon M&amp;#xf8;e S&amp;#xf8;rensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashani_A/0/1/0/all/0/1"&gt;Alireza Kashani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjorleifsson_K/0/1/0/all/0/1"&gt;K. Eldjarn Hjorleifsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hyldig_G/0/1/0/all/0/1"&gt;Grethe Hyldig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1"&gt;Serge Belongie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1"&gt;Frederik Warburg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.16891</id>
        <link href="http://arxiv.org/abs/2308.16891"/>
        <updated>2023-09-02T00:40:02.428Z</updated>
        <summary type="html"><![CDATA[It is a long-standing problem in robotics to develop agents capable of
executing diverse manipulation tasks from visual observations in unstructured
real-world environments. To achieve this goal, the robot needs to have a
comprehensive understanding of the 3D structure and semantics of the scene. In
this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for
multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural
feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural
field (GNF) as a reconstruction module and a Perceiver Transformer as a
decision-making module, leveraging a shared deep 3D voxel representation. To
incorporate semantics in 3D, the reconstruction module utilizes a
vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill
rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3
real robot tasks and perform detailed ablations on 10 RLBench tasks with a
limited number of demonstrations. We observe a substantial improvement of
GNFactor over current state-of-the-art methods in seen and unseen tasks,
demonstrating the strong generalization ability of GNFactor. Our project
website is https://yanjieze.com/GNFactor/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ze_Y/0/1/0/all/0/1"&gt;Yanjie Ze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1"&gt;Ge Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yueh-Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macaluso_A/0/1/0/all/0/1"&gt;Annabella Macaluso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yuying Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jianglong Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1"&gt;Nicklas Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Erran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.14172</id>
        <link href="http://arxiv.org/abs/2308.14172"/>
        <updated>2023-09-02T00:40:02.324Z</updated>
        <summary type="html"><![CDATA[Hypergraphs are important for processing data with higher-order relationships
involving more than two entities. In scenarios where explicit hypergraphs are
not readily available, it is desirable to infer a meaningful hypergraph
structure from the node features to capture the intrinsic relations within the
data. However, existing methods either adopt simple pre-defined rules that fail
to precisely capture the distribution of the potential hypergraph structure, or
learn a mapping between hypergraph structures and node features but require a
large amount of labelled data, i.e., pre-existing hypergraph structures, for
training. Both restrict their applications in practical scenarios. To fill this
gap, we propose a novel smoothness prior that enables us to design a method to
infer the probability for each potential hyperedge without labelled data as
supervision. The proposed prior indicates features of nodes in a hyperedge are
highly correlated by the features of the hyperedge containing them. We use this
prior to derive the relation between the hypergraph structure and the node
features via probabilistic modelling. This allows us to develop an unsupervised
inference method to estimate the probability for each potential hyperedge via
solving an optimisation problem that has an analytical solution. Experiments on
both synthetic and real-world data demonstrate that our method can learn
meaningful hypergraph structures from data more efficiently than existing
hypergraph structure inference methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Bohan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaowen Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleGAN as a Utility-Preserving Face De-identification Method. (arXiv:2212.02611v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.02611</id>
        <link href="http://arxiv.org/abs/2212.02611"/>
        <updated>2023-09-02T00:40:02.323Z</updated>
        <summary type="html"><![CDATA[Face de-identification methods have been proposed to preserve users' privacy
by obscuring their faces. These methods, however, can degrade the quality of
photos, and they usually do not preserve the utility of faces, i.e., their age,
gender, pose, and facial expression. Recently, GANs, such as StyleGAN, have
been proposed, which generate realistic, high-quality imaginary faces. In this
paper, we investigate the use of StyleGAN in generating de-identified faces
through style mixing. We examined this de-identification method for preserving
utility and privacy by implementing several face detection, verification, and
identification attacks and conducting a user study. The results from our
extensive experiments, human evaluation, and comparison with two
state-of-the-art methods, i.e., CIAGAN and DeepPrivacy, show that StyleGAN
performs on par or better than these methods, preserving users' privacy and
images' utility. In particular, the results of the machine learning-based
experiments show that StyleGAN0-4 preserves utility better than CIAGAN and
DeepPrivacy while preserving privacy at the same level. StyleGAN0-3 preserves
utility at the same level while providing more privacy. In this paper, for the
first time, we also performed a carefully designed user study to examine both
privacy and utility-preserving properties of StyleGAN0-3, 0-4, and 0-5, as well
as CIAGAN and DeepPrivacy from the human observers' perspectives. Our
statistical tests showed that participants tend to verify and identify
StyleGAN0-5 images more easily than DeepPrivacy images. All the methods but
StyleGAN0-5 had significantly lower identification rates than CIAGAN. Regarding
utility, as expected, StyleGAN0-5 performed significantly better in preserving
some attributes. Among all methods, on average, participants believe gender has
been preserved the most while naturalness has been preserved the least.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorzooghi_S/0/1/0/all/0/1"&gt;Seyyed Mohammad Sadegh Moosavi Khorzooghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1"&gt;Shirin Nilizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.13570</id>
        <link href="http://arxiv.org/abs/2308.13570"/>
        <updated>2023-09-02T00:40:02.322Z</updated>
        <summary type="html"><![CDATA[Real-time predictive modelling with desired accuracy is highly expected in
industrial artificial intelligence (IAI), where neural networks play a key
role. Neural networks in IAI require powerful, high-performance computing
devices to operate a large number of floating point data. Based on stochastic
configuration networks (SCNs), this paper proposes a new randomized learner
model, termed stochastic configuration machines (SCMs), to stress effective
modelling and data size saving that are useful and valuable for industrial
applications. Compared to SCNs and random vector functional-link (RVFL) nets
with binarized implementation, the model storage of SCMs can be significantly
compressed while retaining favourable prediction performance. Besides the
architecture of the SCM learner model and its learning algorithm, as an
important part of this contribution, we also provide a theoretical basis on the
learning capacity of SCMs by analysing the model's complexity. Experimental
studies are carried out over some benchmark datasets and three industrial
applications. The results demonstrate that SCM has great potential for dealing
with industrial data analytics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dianhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felicetti_M/0/1/0/all/0/1"&gt;Matthew J. Felicetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.11029</id>
        <link href="http://arxiv.org/abs/2308.11029"/>
        <updated>2023-09-02T00:40:02.321Z</updated>
        <summary type="html"><![CDATA[Emotion recognition in conversation (ERC) has received increasing attention
from researchers due to its wide range of applications.As conversation has a
natural graph structure,numerous approaches used to model ERC based on graph
convolutional networks (GCNs) have yielded significant results.However,the
aggregation approach of traditional GCNs suffers from the node information
redundancy problem,leading to node discriminant information
loss.Additionally,single-layer GCNs lack the capacity to capture long-range
contextual information from the graph. Furthermore,the majority of approaches
are based on textual modality or stitching together different modalities,
resulting in a weak ability to capture interactions between modalities. To
address these problems, we present the relational bilevel aggregation graph
convolutional network (RBA-GCN), which consists of three modules: the graph
generation module (GGM), similarity-based cluster building module (SCBM) and
bilevel aggregation module (BiAM). First, GGM constructs a novel graph to
reduce the redundancy of target node information.Then,SCBM calculates the node
similarity in the target node and its structural neighborhood, where noisy
information with low similarity is filtered out to preserve the discriminant
information of the node. Meanwhile, BiAM is a novel aggregation method that can
preserve the information of nodes during the aggregation process. This module
can construct the interaction between different modalities and capture
long-range contextual information based on similarity clusters. On both the
IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a
2.17$\sim$5.21\% improvement over that of the most advanced method.Our code is
available at https://github.com/luftmenscher/RBA-GCN and our article with the
same name has been published in IEEE/ACM Transactions on Audio,Speech,and
Language Processing,vol.31,2023]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guoheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fenghuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaochen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1"&gt;Chi-Man Pun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_G/0/1/0/all/0/1"&gt;Guo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.11155</id>
        <link href="http://arxiv.org/abs/2308.11155"/>
        <updated>2023-09-02T00:40:02.320Z</updated>
        <summary type="html"><![CDATA[Neural force fields (NFFs) have gained prominence in computational chemistry
as surrogate models, superseding quantum-chemistry calculations in ab initio
molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset
and its subsequent extension. These datasets predominantly comprise geometries
from the equilibrium region of the ground electronic state potential energy
surface, sampling from direct adiabatic dynamics. However, many chemical
reactions entail significant molecular deformations, notably bond breaking. We
demonstrate the constrained distribution of internal coordinates and energies
in the MD17 datasets, underscoring their inadequacy for representing systems
undergoing chemical reactions. Addressing this sampling limitation, we
introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived
from non-adiabatic dynamics. This dataset encompasses energies and forces
ascertained from both multireference wave function theory and density
functional theory. Furthermore, its nuclear configuration spaces authentically
depict chemical reactions, making xxMD a more chemically relevant dataset. Our
re-assessment of equivariant models on the xxMD datasets reveals notably higher
mean absolute errors than those reported for MD17 and its variants. This
observation underscores the challenges faced in crafting a generalizable NFF
model with extrapolation capability. Our proposed xxMD-CASSCF and xxMD-DFT
datasets are available at https://github.com/zpengmei/xxMD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pengmei_Z/0/1/0/all/0/1"&gt;Zihan Pengmei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1"&gt;Yinan Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.06720</id>
        <link href="http://arxiv.org/abs/2304.06720"/>
        <updated>2023-09-02T00:40:02.318Z</updated>
        <summary type="html"><![CDATA[Plain text has become a prevalent interface for text-to-image synthesis.
However, its limited customization options hinder users from accurately
describing desired outputs. For example, plain text makes it hard to specify
continuous quantities, such as the precise RGB color value or importance of
each word. Furthermore, creating detailed text prompts for complex scenes is
tedious for humans to write and challenging for text encoders to interpret. To
address these challenges, we propose using a rich-text editor supporting
formats such as font style, size, color, and footnote. We extract each word's
attributes from rich text to enable local style control, explicit token
reweighting, precise color rendering, and detailed region synthesis. We achieve
these capabilities through a region-based diffusion process. We first obtain
each word's region based on attention maps of a diffusion process using plain
text. For each region, we enforce its text attributes by creating
region-specific detailed prompts and applying region-specific guidance, and
maintain its fidelity against plain-text generation through region-based
injections. We present various examples of image generation from rich text and
demonstrate that our method outperforms strong baselines with quantitative
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Songwei Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1"&gt;Taesung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jia-Bin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.14994</id>
        <link href="http://arxiv.org/abs/2304.14994"/>
        <updated>2023-09-02T00:40:02.318Z</updated>
        <summary type="html"><![CDATA[Unlike conventional grid and mesh based methods for solving partial
differential equations (PDEs), neural networks have the potential to break the
curse of dimensionality, providing approximate solutions to problems where
using classical solvers is difficult or impossible. While global minimization
of the PDE residual over the network parameters works well for boundary value
problems, catastrophic forgetting impairs the applicability of this approach to
initial value problems (IVPs). In an alternative local-in-time approach, the
optimization problem can be converted into an ordinary differential equation
(ODE) on the network parameters and the solution propagated forward in time;
however, we demonstrate that current methods based on this approach suffer from
two key issues. First, following the ODE produces an uncontrolled growth in the
conditioning of the problem, ultimately leading to unacceptably large numerical
errors. Second, as the ODE methods scale cubically with the number of model
parameters, they are restricted to small neural networks, significantly
limiting their ability to represent intricate PDE initial conditions and
solutions. Building on these insights, we develop Neural IVP, an ODE based IVP
solver which prevents the network from getting ill-conditioned and runs in time
linear in the number of parameters, enabling us to evolve the dynamics of
challenging PDEs with neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1"&gt;Marc Finzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potapczynski_A/0/1/0/all/0/1"&gt;Andres Potapczynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choptuik_M/0/1/0/all/0/1"&gt;Matthew Choptuik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.03312</id>
        <link href="http://arxiv.org/abs/2308.03312"/>
        <updated>2023-09-02T00:40:02.318Z</updated>
        <summary type="html"><![CDATA[Large Language Models (LLMs) have shown promise in automated program
reasoning, a crucial aspect of many security tasks. However, existing LLM
architectures for code are often borrowed from other domains like natural
language processing, raising concerns about their generalization and robustness
to unseen code. A key generalization challenge is to incorporate the knowledge
of code semantics, including control and data flow, into the LLM architectures.

Drawing inspiration from examples of convolution layers exploiting
translation symmetry, we explore how code symmetries can enhance LLM
architectures for program analysis and modeling. We present a rigorous
group-theoretic framework that formally defines code symmetries as
semantics-preserving transformations and provides techniques for precisely
reasoning about symmetry preservation within LLM architectures. Using this
framework, we introduce a novel variant of self-attention that preserves
program symmetries, demonstrating its effectiveness in generalization and
robustness through detailed experimental evaluations across different binary
and source code analysis tasks. Overall, our code symmetry framework offers
rigorous and powerful reasoning techniques that can guide the future
development of specialized LLMs for code and advance LLM-guided program
reasoning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pei_K/0/1/0/all/0/1"&gt;Kexin Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qirui Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1"&gt;Scott Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallaro_L/0/1/0/all/0/1"&gt;Lorenzo Cavallaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junfeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1"&gt;Suman Jana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery. (arXiv:2308.10283v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.10283</id>
        <link href="http://arxiv.org/abs/2308.10283"/>
        <updated>2023-09-02T00:40:02.318Z</updated>
        <summary type="html"><![CDATA[We propose a new parameter-adaptive uncertainty-penalized Bayesian
information criterion (UBIC) to prioritize the parsimonious partial
differential equation (PDE) that sufficiently governs noisy spatial-temporal
observed data with few reliable terms. Since the naive use of the BIC for model
selection has been known to yield an undesirable overfitted PDE, the UBIC
penalizes the found PDE not only by its complexity but also the quantified
uncertainty, derived from the model supports' coefficient of variation in a
probabilistic view. We also introduce physics-informed neural network learning
as a simulation-based approach to further validate the selected PDE flexibly
against the other discovered PDE. Numerical results affirm the successful
application of the UBIC in identifying the true governing PDE. Additionally, we
reveal an interesting effect of denoising the observed data on improving the
trade-off between the BIC score and model complexity. Code is available at
https://github.com/Pongpisit-Thanasutives/UBIC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thanasutives_P/0/1/0/all/0/1"&gt;Pongpisit Thanasutives&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morita_T/0/1/0/all/0/1"&gt;Takashi Morita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Numao_M/0/1/0/all/0/1"&gt;Masayuki Numao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukui_K/0/1/0/all/0/1"&gt;Ken-ichi Fukui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-Training Representations of Binary Code Using Contrastive Learning. (arXiv:2210.05102v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.05102</id>
        <link href="http://arxiv.org/abs/2210.05102"/>
        <updated>2023-09-02T00:40:02.317Z</updated>
        <summary type="html"><![CDATA[Compiled software is delivered as executable binary code. Developers write
source code to express the software semantics, but the compiler converts it to
a binary format that the CPU can directly execute. Therefore, binary code
analysis is critical to applications in reverse engineering and computer
security tasks where source code is not available. However, unlike source code
and natural language that contain rich semantic information, binary code is
typically difficult for human engineers to understand and analyze. While
existing work uses AI models to assist source code analysis, few studies have
considered binary code. In this paper, we propose a COntrastive learning Model
for Binary cOde Analysis, or COMBO, that incorporates source code and comment
information into binary code during representation learning. Specifically, we
present three components in COMBO: (1) a primary contrastive learning method
for cold-start pre-training, (2) a simplex interpolation method to incorporate
source code, comments, and binary code, and (3) an intermediate representation
learning algorithm to provide binary code embeddings. Finally, we evaluate the
effectiveness of the pre-trained representations produced by COMBO using three
indicative downstream tasks relating to binary code: algorithmic functionality
classification, binary code similarity, and vulnerability detection. Our
experimental results show that COMBO facilitates representation learning of
binary code visualized by distribution analysis, and improves the performance
on all three downstream tasks by 5.45% on average compared to state-of-the-art
large-scale language representation models. To the best of our knowledge, COMBO
is the first language representation model that incorporates source code,
binary code, and comments into contrastive code representation learning and
unifies multiple tasks for binary code analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yueke Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1"&gt;Kevin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersen_S/0/1/0/all/0/1"&gt;Scott Thomas Andersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1"&gt;Huajie Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1"&gt;Kevin Leach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning. (arXiv:2303.08566v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.08566</id>
        <link href="http://arxiv.org/abs/2303.08566"/>
        <updated>2023-09-02T00:40:02.317Z</updated>
        <summary type="html"><![CDATA[Visual Parameter-Efficient Fine-Tuning (PEFT) has become a powerful
alternative for full fine-tuning so as to adapt pre-trained vision models to
downstream tasks, which only tunes a small number of parameters while freezing
the vast majority ones to ease storage burden and optimization difficulty.
However, existing PEFT methods introduce trainable parameters to the same
positions across different tasks depending solely on human heuristics and
neglect the domain gaps. To this end, we study where to introduce and how to
allocate trainable parameters by proposing a novel Sensitivity-aware visual
Parameter-efficient fine-Tuning (SPT) scheme, which adaptively allocates
trainable parameters to task-specific important positions given a desired
tunable parameter budget. Specifically, our SPT first quickly identifies the
sensitive parameters that require tuning for a given task in a data-dependent
way. Next, our SPT further boosts the representational capability for the
weight matrices whose number of sensitive parameters exceeds a pre-defined
threshold by utilizing existing structured tuning methods, e.g., LoRA [23] or
Adapter [22], to replace directly tuning the selected sensitive parameters
(unstructured tuning) under the budget. Extensive experiments on a wide range
of downstream recognition tasks show that our SPT is complementary to the
existing PEFT methods and largely boosts their performance, e.g., SPT improves
Adapter with supervised pre-trained ViT-B/16 backbone by 4.2% and 1.4% mean
Top-1 accuracy, reaching SOTA performance on FGVC and VTAB-1k benchmarks,
respectively. Source code is at https://github.com/ziplab/SPT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Haoyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantization-based Optimization with Perspective of Quantum Mechanics. (arXiv:2308.11594v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.11594</id>
        <link href="http://arxiv.org/abs/2308.11594"/>
        <updated>2023-09-02T00:40:02.317Z</updated>
        <summary type="html"><![CDATA[Statistical and stochastic analysis based on thermodynamics has been the main
analysis framework for stochastic global optimization. Recently, appearing
quantum annealing or quantum tunneling algorithm for global optimization, we
require a new researching framework for global optimization algorithms. In this
paper, we provide the analysis for quantization-based optimization based on the
Schr\"odinger equation to reveal what property in quantum mechanics enables
global optimization. We present that the tunneling effect derived by the
Schr\"odinger equation in quantization-based optimization enables to escape of
a local minimum. Additionally, we confirm that this tunneling effect is the
same property included in quantum mechanics-based global optimization.
Experiments with standard multi-modal benchmark functions represent that the
proposed analysis is valid.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Seok_J/0/1/0/all/0/1"&gt;Jinwuk Seok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cho_C/0/1/0/all/0/1"&gt;Changsik Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Phase Dynamics for Bio-Plausible Contrastive Learning. (arXiv:2302.12431v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.12431</id>
        <link href="http://arxiv.org/abs/2302.12431"/>
        <updated>2023-09-02T00:40:02.316Z</updated>
        <summary type="html"><![CDATA[Many learning algorithms used as normative models in neuroscience or as
candidate approaches for learning on neuromorphic chips learn by contrasting
one set of network states with another. These Contrastive Learning (CL)
algorithms are traditionally implemented with rigid, temporally non-local, and
periodic learning dynamics that could limit the range of physical systems
capable of harnessing CL. In this study, we build on recent work exploring how
CL might be implemented by biological or neurmorphic systems and show that this
form of learning can be made temporally local, and can still function even if
many of the dynamical requirements of standard training procedures are relaxed.
Thanks to a set of general theorems corroborated by numerical experiments
across several CL models, our results provide theoretical foundations for the
study and development of CL methods for biological and neuromorphic neural
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Williams_E/0/1/0/all/0/1"&gt;Ezekiel Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bredenberg_C/0/1/0/all/0/1"&gt;Colin Bredenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lajoie_G/0/1/0/all/0/1"&gt;Guillaume Lajoie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities. (arXiv:2302.08761v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.08761</id>
        <link href="http://arxiv.org/abs/2302.08761"/>
        <updated>2023-09-02T00:40:02.315Z</updated>
        <summary type="html"><![CDATA[Traffic analysis is crucial for urban operations and planning, while the
availability of dense urban traffic data beyond loop detectors is still scarce.
We present a large-scale floating vehicle dataset of per-street segment traffic
information, Metropolitan Segment Traffic Speeds from Massive Floating Car Data
in 10 Cities (MeTS-10), available for 10 global cities with a 15-minute
resolution for collection periods ranging between 108 and 361 days in 2019-2021
and covering more than 1500 square kilometers per metropolitan area. MeTS-10
features traffic speed information at all street levels from main arterials to
local streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul,
London, Madrid, Melbourne and Moscow. The dataset leverages the
industrial-scale floating vehicle Traffic4cast data with speeds and vehicle
counts provided in a privacy-preserving spatio-temporal aggregation. We detail
the efficient matching approach mapping the data to the OpenStreetMap road
graph. We evaluate the dataset by comparing it with publicly available
stationary vehicle detector data (for Berlin, London, and Madrid) and the Uber
traffic speed dataset (for Barcelona, Berlin, and London). The comparison
highlights the differences across datasets in spatio-temporal coverage and
variations in the reported traffic caused by the binning method. MeTS-10
enables novel, city-wide analysis of mobility and traffic patterns for ten
major world cities, overcoming current limitations of spatially sparse vehicle
detector data. The large spatial and temporal coverage offers an opportunity
for joining the MeTS-10 with other datasets, such as traffic surveys in traffic
planning studies or vehicle detector data in traffic control settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neun_M/0/1/0/all/0/1"&gt;Moritz Neun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eichenberger_C/0/1/0/all/0/1"&gt;Christian Eichenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1"&gt;Yanan Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Cheng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiedemann_N/0/1/0/all/0/1"&gt;Nina Wiedemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_H/0/1/0/all/0/1"&gt;Henry Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomko_M/0/1/0/all/0/1"&gt;Martin Tomko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambuhl_L/0/1/0/all/0/1"&gt;Lukas Amb&amp;#xfc;hl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermes_L/0/1/0/all/0/1"&gt;Luca Hermes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Michael Kopp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.00241</id>
        <link href="http://arxiv.org/abs/2305.00241"/>
        <updated>2023-09-02T00:40:02.315Z</updated>
        <summary type="html"><![CDATA[In the past decade, deep learning became the prevalent methodology for
predictive modeling thanks to the remarkable accuracy of deep neural networks
in tasks such as computer vision and natural language processing. Meanwhile,
the structure of neural networks converged back to simpler representations
based on piecewise constant and piecewise linear functions such as the
Rectified Linear Unit (ReLU), which became the most commonly used type of
activation function in neural networks. That made certain types of network
structure $\unicode{x2014}$such as the typical fully-connected feedforward
neural network$\unicode{x2014}$ amenable to analysis through polyhedral theory
and to the application of methodologies such as Linear Programming (LP) and
Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this
paper, we survey the main topics emerging from this fast-paced area of work,
which bring a fresh perspective to understanding neural networks in more detail
as well as to applying linear optimization techniques to train, verify, and
reduce the size of such networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huchette_J/0/1/0/all/0/1"&gt;Joey Huchette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Munoz_G/0/1/0/all/0/1"&gt;Gonzalo Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Serra_T/0/1/0/all/0/1"&gt;Thiago Serra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tsay_C/0/1/0/all/0/1"&gt;Calvin Tsay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization. (arXiv:2210.09134v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.09134</id>
        <link href="http://arxiv.org/abs/2210.09134"/>
        <updated>2023-09-02T00:40:02.314Z</updated>
        <summary type="html"><![CDATA[Bayesian model reduction provides an efficient approach for comparing the
performance of all nested sub-models of a model, without re-evaluating any of
these sub-models. Until now, Bayesian model reduction has been applied mainly
in the computational neuroscience community on simple models. In this paper, we
formulate and apply Bayesian model reduction to perform principled pruning of
Bayesian neural networks, based on variational free energy minimization. Direct
application of Bayesian model reduction, however, gives rise to approximation
errors. Therefore, a novel iterative pruning algorithm is presented to
alleviate the problems arising with naive Bayesian model reduction, as
supported experimentally on the publicly available UCI datasets for different
inference algorithms. This novel parameter pruning scheme solves the
shortcomings of current state-of-the-art pruning methods that are used by the
signal processing community. The proposed approach has a clear stopping
criterion and minimizes the same objective that is used during training. Next
to these benefits, our experiments indicate better model performance in
comparison to state-of-the-art pruning schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beckers_J/0/1/0/all/0/1"&gt;Jim Beckers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erp_B/0/1/0/all/0/1"&gt;Bart van Erp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Ziyue Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondrashov_K/0/1/0/all/0/1"&gt;Kirill Kondrashov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1"&gt;Bert de Vries&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space. (arXiv:2303.05102v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.05102</id>
        <link href="http://arxiv.org/abs/2303.05102"/>
        <updated>2023-09-02T00:40:02.314Z</updated>
        <summary type="html"><![CDATA[One major challenge in machine learning applications is coping with
mismatches between the datasets used in the development and those obtained in
real-world applications. These mismatches may lead to inaccurate predictions
and errors, resulting in poor product quality and unreliable systems. In this
study, we propose StyleDiff to inform developers of the differences between the
two datasets for the steady development of machine learning systems. Using
disentangled image spaces obtained from recently proposed generative models,
StyleDiff compares the two datasets by focusing on attributes in the images and
provides an easy-to-understand analysis of the differences between the
datasets. The proposed StyleDiff performs in $O (d N\log N)$, where $N$ is the
size of the datasets and $d$ is the number of attributes, enabling the
application to large datasets. We demonstrate that StyleDiff accurately detects
differences between datasets and presents them in an understandable format
using, for example, driving scenes datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kawano_K/0/1/0/all/0/1"&gt;Keisuke Kawano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kutsuna_T/0/1/0/all/0/1"&gt;Takuro Kutsuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tokuhisa_R/0/1/0/all/0/1"&gt;Ryoko Tokuhisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nakamura_A/0/1/0/all/0/1"&gt;Akihiro Nakamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Esaki_Y/0/1/0/all/0/1"&gt;Yasushi Esaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.12743</id>
        <link href="http://arxiv.org/abs/2303.12743"/>
        <updated>2023-09-02T00:40:02.314Z</updated>
        <summary type="html"><![CDATA[In autonomous driving, data augmentation is commonly used for improving 3D
object detection. The most basic methods include insertion of copied objects
and rotation and scaling of the entire training frame. Numerous variants have
been developed as well. The existing methods, however, are considerably limited
when compared to the variety of the real world possibilities. In this work, we
develop a diversified and realistic augmentation method that can flexibly
construct a whole-body object, freely locate and rotate the object, and apply
self-occlusion and external-occlusion accordingly. To improve the diversity of
the whole-body object construction, we develop an iterative method that
stochastically combines multiple objects observed from the real world into a
single object. Unlike the existing augmentation methods, the constructed
objects can be randomly located and rotated in the training frame because
proper occlusions can be reflected to the whole-body objects in the final step.
Finally, proper self-occlusion at each local object level and
external-occlusion at the global frame level are applied using the Hidden Point
Removal (HPR) algorithm that is computationally efficient. HPR is also used for
adaptively controlling the point density of each object according to the
object's distance from the LiDAR. Experiment results show that the proposed
DR.CPO algorithm is data-efficient and model-agnostic without incurring any
computational overhead. Also, DR.CPO can improve mAP performance by 2.08% when
compared to the best 3D detection result known for KITTI dataset. The code is
available at https://github.com/SNU-DRL/DRCPO.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jungwook Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jaeill Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kyungeun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyunghun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1"&gt;Wonjong Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v3 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.00646</id>
        <link href="http://arxiv.org/abs/2211.00646"/>
        <updated>2023-09-02T00:40:02.313Z</updated>
        <summary type="html"><![CDATA[Melanoma is one of the most aggressive forms of skin cancer, causing a large
proportion of skin cancer deaths. However, melanoma diagnoses by pathologists
shows low interrater reliability. As melanoma is a cancer of the melanocyte,
there is a clear need to develop a melanocytic cell segmentation tool that is
agnostic to pathologist variability and automates pixel-level annotation.
Gigapixel-level pathologist labeling, however, is impractical. Herein, we
propose a means to train deep neural networks for melanocytic cell segmentation
from hematoxylin and eosin (H&E) stained sections and paired
immunohistochemistry (IHC) of adjacent tissue sections, achieving a mean IOU of
0.64 despite imperfect ground-truth labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tada_M/0/1/0/all/0/1"&gt;Mikio Tada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lang_U/0/1/0/all/0/1"&gt;Ursula E. Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yeh_I/0/1/0/all/0/1"&gt;Iwei Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wei_M/0/1/0/all/0/1"&gt;Maria L. Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Keiser_M/0/1/0/all/0/1"&gt;Michael J. Keiser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-Demand Communication for Asynchronous Multi-Agent Bandits. (arXiv:2302.07446v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.07446</id>
        <link href="http://arxiv.org/abs/2302.07446"/>
        <updated>2023-09-02T00:40:02.312Z</updated>
        <summary type="html"><![CDATA[This paper studies a cooperative multi-agent multi-armed stochastic bandit
problem where agents operate asynchronously -- agent pull times and rates are
unknown, irregular, and heterogeneous -- and face the same instance of a
K-armed bandit problem. Agents can share reward information to speed up the
learning process at additional communication costs. We propose ODC, an
on-demand communication protocol that tailors the communication of each pair of
agents based on their empirical pull times. ODC is efficient when the pull
times of agents are highly heterogeneous, and its communication complexity
depends on the empirical pull times of agents. ODC is a generic protocol that
can be integrated into most cooperative bandit algorithms without degrading
their performance. We then incorporate ODC into the natural extensions of UCB
and AAE algorithms and propose two communication-efficient cooperative
algorithms. Our analysis shows that both algorithms are near-optimal in regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Zhen Janice Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuchuang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xutong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiesmaili_M/0/1/0/all/0/1"&gt;Mohammad Hajiesmaili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1"&gt;John C.S. Lui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Towsley_D/0/1/0/all/0/1"&gt;Don Towsley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[0/1 Deep Neural Networks via Block Coordinate Descent. (arXiv:2206.09379v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.09379</id>
        <link href="http://arxiv.org/abs/2206.09379"/>
        <updated>2023-09-02T00:40:02.304Z</updated>
        <summary type="html"><![CDATA[The step function is one of the simplest and most natural activation
functions for deep neural networks (DNNs). As it counts 1 for positive
variables and 0 for others, its intrinsic characteristics (e.g., discontinuity
and no viable information of subgradients) impede its development for several
decades. Even if there is an impressive body of work on designing DNNs with
continuous activation functions that can be deemed as surrogates of the step
function, it is still in the possession of some advantageous properties, such
as complete robustness to outliers and being capable of attaining the best
learning-theoretic guarantee of predictive accuracy. Hence, in this paper, we
aim to train DNNs with the step function used as an activation function (dubbed
as 0/1 DNNs). We first reformulate 0/1 DNNs as an unconstrained optimization
problem and then solve it by a block coordinate descend (BCD) method. Moreover,
we acquire closed-form solutions for sub-problems of BCD as well as its
convergence properties. Furthermore, we also integrate
$\ell_{2,0}$-regularization into 0/1 DNN to accelerate the training process and
compress the network scale. As a result, the proposed algorithm has a high
performance on classifying MNIST and Fashion-MNIST datasets. As a result, the
proposed algorithm has a desirable performance on classifying MNIST,
FashionMNIST, Cifar10, and Cifar100 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shenglong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Geoffrey Ye Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1"&gt;Naihua Xiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending regionalization algorithms to explore spatial process heterogeneity. (arXiv:2206.09429v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2206.09429</id>
        <link href="http://arxiv.org/abs/2206.09429"/>
        <updated>2023-09-02T00:40:02.302Z</updated>
        <summary type="html"><![CDATA[In spatial regression models, spatial heterogeneity may be considered with
either continuous or discrete specifications. The latter is related to
delineation of spatially connected regions with homogeneous relationships
between variables (spatial regimes). Although various regionalization
algorithms have been proposed and studied in the field of spatial analytics,
methods to optimize spatial regimes have been largely unexplored. In this
paper, we propose two new algorithms for spatial regime delineation, two-stage
K-Models and Regional-K-Models. We also extend the classic Automatic Zoning
Procedure to spatial regression context. The proposed algorithms are applied to
a series of synthetic datasets and two real-world datasets. Results indicate
that all three algorithms achieve superior or comparable performance to
existing approaches, while the two-stage K-Models algorithm largely outperforms
existing approaches on model fitting, region reconstruction, and coefficient
estimation. Our work enriches the spatial analytics toolbox to explore spatial
heterogeneous processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Python_A/0/1/0/all/0/1"&gt;Andre Python&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.02796</id>
        <link href="http://arxiv.org/abs/2210.02796"/>
        <updated>2023-09-02T00:40:02.302Z</updated>
        <summary type="html"><![CDATA[The main goal of Few-Shot learning algorithms is to enable learning from
small amounts of data. One of the most popular and elegant Few-Shot learning
approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this
method is to learn the shared universal weights of a meta-model, which are then
adapted for specific tasks. However, the method suffers from over-fitting and
poorly quantifies uncertainty due to limited data size. Bayesian approaches
could, in principle, alleviate these shortcomings by learning weight
distributions in place of point-wise weights. Unfortunately, previous
modifications of MAML are limited due to the simplicity of Gaussian posteriors,
MAML-like gradient-based weight updates, or by the same structure enforced for
universal and adapted weights.

In this paper, we propose a novel framework for Bayesian MAML called
BayesianHMAML, which employs Hypernetworks for weight updates. It learns the
universal weights point-wise, but a probabilistic structure is added when
adapted for specific tasks. In such a framework, we can use simple Gaussian
distributions or more complicated posteriors induced by Continuous Normalizing
Flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borycki_P/0/1/0/all/0/1"&gt;Piotr Borycki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubacki_P/0/1/0/all/0/1"&gt;Piotr Kubacki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1"&gt;Marcin Przewi&amp;#x119;&amp;#x17a;likowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusmierczyk_T/0/1/0/all/0/1"&gt;Tomasz Ku&amp;#x15b;mierczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1"&gt;Jacek Tabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.07864</id>
        <link href="http://arxiv.org/abs/2211.07864"/>
        <updated>2023-09-02T00:40:02.302Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) enables multiple clients to collaboratively train a
global model without disclosing their data. Previous researches often require
training the complete model parameters. However, the emergence of powerful
pre-trained models makes it possible to achieve higher performance with fewer
learnable parameters in FL. In this paper, we propose a federated adaptive
prompt tuning algorithm, FedAPT, for multi-domain collaborative image
classification with powerful foundation models, like CLIP. Compared with direct
federated prompt tuning, our core idea is to adaptively unlock specific domain
knowledge for each test sample in order to provide them with personalized
prompts. To implement this idea, we design an adaptive prompt tuning module,
which consists of a meta prompt, an adaptive network, and some keys. The server
randomly generates a set of keys and assigns a unique key to each client. Then
all clients cooperatively train the global adaptive network and meta prompt
with the local datasets and the frozen keys. Ultimately, the global aggregation
model can assign a personalized prompt to CLIP based on the domain features of
each test sample. We perform extensive experiments on two multi-domain image
classification datasets across two different settings - supervised and
unsupervised. The results show that FedAPT can achieve better performance with
less than 10\% of the number of parameters of the fully trained model, and the
global model can perform well in diverse client domains simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Shangchao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mingzhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;Xiangyang Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v3 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2301.00752</id>
        <link href="http://arxiv.org/abs/2301.00752"/>
        <updated>2023-09-02T00:40:02.302Z</updated>
        <summary type="html"><![CDATA[This study demonstrates the feasibility of point cloud-based proactive link
quality prediction for millimeter-wave (mmWave) communications. Previous
studies have proposed machine learning-based methods to predict received signal
strength for future time periods using time series of depth images to mitigate
the line-of-sight (LOS) path blockage by pedestrians in mmWave communication.
However, these image-based methods have limited applicability due to privacy
concerns as camera images may contain sensitive information. This study
proposes a point cloud-based method for mmWave link quality prediction and
demonstrates its feasibility through experiments. Point clouds represent
three-dimensional (3D) spaces as a set of points and are sparser and less
likely to contain sensitive information than camera images. Additionally, point
clouds provide 3D position and motion information, which is necessary for
understanding the radio propagation environment involving pedestrians. This
study designs the mmWave link quality prediction method and conducts realistic
indoor experiments, where the link quality fluctuates significantly due to
human blockage, using commercially available IEEE 802.11ad-based 60 GHz
wireless LAN devices and Kinect v2 RGB-D camera and Velodyne VLP-16 light
detection and ranging (LiDAR) for point cloud acquisition. The experimental
results showed that our proposed method can predict future large attenuation of
mmWave received signal strength and throughput induced by the LOS path blockage
by pedestrians with comparable or superior accuracy to image-based prediction
methods. Hence, our point cloud-based method can serve as a viable alternative
to image-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ohta_S/0/1/0/all/0/1"&gt;Shoki Ohta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1"&gt;Takayuki Nishio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kudo_R/0/1/0/all/0/1"&gt;Riichi Kudo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1"&gt;Kahoko Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagata_H/0/1/0/all/0/1"&gt;Hisashi Nagata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2211.11656</id>
        <link href="http://arxiv.org/abs/2211.11656"/>
        <updated>2023-09-02T00:40:02.300Z</updated>
        <summary type="html"><![CDATA[The aim of Machine Unlearning (MU) is to provide theoretical guarantees on
the removal of the contribution of a given data point from a training
procedure. Federated Unlearning (FU) consists in extending MU to unlearn a
given client's contribution from a federated training routine. Current FU
approaches are generally not scalable, and do not come with sound theoretical
quantification of the effectiveness of unlearning. In this work we present
Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU
approach. Upon unlearning request from a given client, IFU identifies the
optimal FL iteration from which FL has to be reinitialized, with unlearning
guarantees obtained through a randomized perturbation mechanism. The theory of
IFU is also extended to account for sequential unlearning requests.
Experimental results on different tasks and dataset show that IFU leads to more
efficient unlearning procedures as compared to basic re-training and
state-of-the-art FU approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fraboni_Y/0/1/0/all/0/1"&gt;Yann Fraboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waerebeke_M/0/1/0/all/0/1"&gt;Martin Van Waerebeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scaman_K/0/1/0/all/0/1"&gt;Kevin Scaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Richard Vidal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1"&gt;Laetitia Kameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1"&gt;Marco Lorenzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulation-Based Optimization of User Interfaces for Quality-Assuring Machine Learning Model Predictions. (arXiv:2104.01129v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01129</id>
        <link href="http://arxiv.org/abs/2104.01129"/>
        <updated>2023-09-02T00:40:02.299Z</updated>
        <summary type="html"><![CDATA[Quality-sensitive applications of machine learning (ML) require quality
assurance (QA) by humans before the predictions of an ML model can be deployed.
QA for ML (QA4ML) interfaces require users to view a large amount of data and
perform many interactions to correct errors made by the ML model. An optimized
user interface (UI) can significantly reduce interaction costs. While UI
optimization can be informed by user studies evaluating design options, this
approach is not scalable because there are typically numerous small variations
that can affect the efficiency of a QA4ML interface. Hence, we propose using
simulation to evaluate and aid the optimization of QA4ML interfaces. In
particular, we focus on simulating the combined effects of human intelligence
in initiating appropriate interaction commands and machine intelligence in
providing algorithmic assistance for accelerating QA4ML processes. As QA4ML is
usually labor-intensive, we use the simulated task completion time as the
metric for UI optimization under different interface and algorithm setups. We
demonstrate the usage of this UI design method in several QA4ML applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tennekes_M/0/1/0/all/0/1"&gt;Martijn Tennekes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jong_T/0/1/0/all/0/1"&gt;Tim de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curier_L/0/1/0/all/0/1"&gt;Lyana Curier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1"&gt;Bob Coecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Min Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs. (arXiv:2106.14052v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14052</id>
        <link href="http://arxiv.org/abs/2106.14052"/>
        <updated>2023-09-02T00:40:02.295Z</updated>
        <summary type="html"><![CDATA[Current methods for embedding-based query answering over incomplete Knowledge
Graphs (KGs) only focus on inductive reasoning, i.e., predicting answers by
learning patterns from the data, and lack the complementary ability to do
deductive reasoning, which requires the application of domain knowledge to
infer further information. To address this shortcoming, we investigate the
problem of incorporating ontologies into embedding-based query answering models
by defining the task of embedding-based ontology-mediated query answering. We
propose various integration strategies into prominent representatives of
embedding models that involve (1) different ontology-driven data augmentation
techniques and (2) adaptation of the loss function to enforce the ontology
axioms. We design novel benchmarks for the considered task based on the LUBM
and the NELL KGs and evaluate our methods on them. The achieved improvements in
the setting that requires both inductive and deductive reasoning are from 20%
to 55% in HITS@3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andresel_M/0/1/0/all/0/1"&gt;Medina Andresel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Trung-Kien Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domokos_C/0/1/0/all/0/1"&gt;Csaba Domokos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1"&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1"&gt;Daria Stepanova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Quantum Monte Carlo Computation of Excited States. (arXiv:2308.16848v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2308.16848</id>
        <link href="http://arxiv.org/abs/2308.16848"/>
        <updated>2023-09-02T00:40:02.294Z</updated>
        <summary type="html"><![CDATA[We present a variational Monte Carlo algorithm for estimating the lowest
excited states of a quantum system which is a natural generalization of the
estimation of ground states. The method has no free parameters and requires no
explicit orthogonalization of the different states, instead transforming the
problem of finding excited states of a given system into that of finding the
ground state of an expanded system. Expected values of arbitrary observables
can be calculated, including off-diagonal expectations between different states
such as the transition dipole moment. Although the method is entirely general,
it works particularly well in conjunction with recent work on using neural
networks as variational Ansatze for many-electron systems, and we show that by
combining this method with the FermiNet and Psiformer Ansatze we can accurately
recover vertical excitation energies and oscillator strengths on molecules as
large as benzene. Beyond the examples on molecules presented here, we expect
this technique will be of great interest for applications of variational
quantum Monte Carlo to atomic, nuclear and condensed matter physics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pfau_D/0/1/0/all/0/1"&gt;David Pfau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Axelrod_S/0/1/0/all/0/1"&gt;Simon Axelrod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sutterud_H/0/1/0/all/0/1"&gt;Halvard Sutterud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Glehn_I/0/1/0/all/0/1"&gt;Ingrid von Glehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Spencer_J/0/1/0/all/0/1"&gt;James S. Spencer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2208.00780</id>
        <link href="http://arxiv.org/abs/2208.00780"/>
        <updated>2023-09-02T00:40:02.294Z</updated>
        <summary type="html"><![CDATA[Explaining artificial intelligence (AI) predictions is increasingly important
and even imperative in many high-stakes applications where humans are the
ultimate decision-makers. In this work, we propose two novel architectures of
self-interpretable image classifiers that first explain, and then predict (as
opposed to post-hoc explanations) by harnessing the visual correspondences
between a query image and exemplars. Our models consistently improve (by 1 to 4
points) on out-of-distribution (OOD) datasets while performing marginally worse
(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest
neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,
our correspondence-based explanations are found to be more useful to users than
kNN explanations. Our explanations help users more accurately reject AI's wrong
decisions than all other tested methods. Interestingly, for the first time, we
show that it is possible to achieve complementary human-AI team accuracy (i.e.,
that is higher than either AI-alone or human-alone), in ImageNet and CUB image
classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1"&gt;Giang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taesiri_M/0/1/0/all/0/1"&gt;Mohammad Reza Taesiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction. (arXiv:2308.16896v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16896</id>
        <link href="http://arxiv.org/abs/2308.16896"/>
        <updated>2023-09-02T00:40:02.288Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation in autonomous driving has been undergoing an evolution
from sparse point segmentation to dense voxel segmentation, where the objective
is to predict the semantic occupancy of each voxel in the concerned 3D space.
The dense nature of the prediction space has rendered existing efficient
2D-projection-based methods (e.g., bird's eye view, range view, etc.)
ineffective, as they can only describe a subspace of the 3D scene. To address
this, we propose a cylindrical tri-perspective view to represent point clouds
effectively and comprehensively and a PointOcc model to process them
efficiently. Considering the distance distribution of LiDAR point clouds, we
construct the tri-perspective view in the cylindrical coordinate system for
more fine-grained modeling of nearer areas. We employ spatial group pooling to
maintain structural details during projection and adopt 2D backbones to
efficiently process each TPV plane. Finally, we obtain the features of each
point by aggregating its projected features on each of the processed TPV planes
without the need for any post-processing. Extensive experiments on both 3D
occupancy prediction and LiDAR segmentation benchmarks demonstrate that the
proposed PointOcc achieves state-of-the-art performance with much faster speed.
Specifically, despite only using LiDAR, PointOcc significantly outperforms all
other methods, including multi-modal methods, with a large margin on the
OpenOccupancy benchmark. Code: https://github.com/wzzheng/PointOcc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1"&gt;Sicheng Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuanhui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout. (arXiv:2308.16835v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16835</id>
        <link href="http://arxiv.org/abs/2308.16835"/>
        <updated>2023-09-02T00:40:02.283Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) requires frequent exchange of model parameters, which
leads to long communication delay, especially when the network environments of
clients vary greatly. Moreover, the parameter server needs to wait for the
slowest client (i.e., straggler, which may have the largest model size, lowest
computing capability or worst network condition) to upload parameters, which
may significantly degrade the communication efficiency. Commonly-used client
selection methods such as partial client selection would lead to the waste of
computing resources and weaken the generalization of the global model. To
tackle this problem, along a different line, in this paper, we advocate the
approach of model parameter dropout instead of client selection, and
accordingly propose a novel framework of Federated learning scheme with
Differential parameter Dropout (FedDD). FedDD consists of two key modules:
dropout rate allocation and uploaded parameter selection, which will optimize
the model parameter uploading ratios tailored to different clients'
heterogeneous conditions and also select the proper set of important model
parameters for uploading subject to clients' dropout rate constraints.
Specifically, the dropout rate allocation is formulated as a convex
optimization problem, taking system heterogeneity, data heterogeneity, and
model heterogeneity among clients into consideration. The uploaded parameter
selection strategy prioritizes on eliciting important parameters for uploading
to speedup convergence. Furthermore, we theoretically analyze the convergence
of the proposed FedDD scheme. Extensive performance evaluations demonstrate
that the proposed FedDD scheme can achieve outstanding performances in both
communication efficiency and model convergence, and also possesses a strong
generalization capability to data of rare classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhiying Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qiong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoxi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qianyi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Optimal Strategies for Temporal Tasks in Stochastic Games. (arXiv:2102.04307v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04307</id>
        <link href="http://arxiv.org/abs/2102.04307"/>
        <updated>2023-09-02T00:40:02.283Z</updated>
        <summary type="html"><![CDATA[Synthesis from linear temporal logic (LTL) specifications provides assured
controllers for systems operating in stochastic and potentially adversarial
environments. Automatic synthesis tools, however, require a model of the
environment to construct controllers. In this work, we introduce a model-free
reinforcement learning (RL) approach to derive controllers from given LTL
specifications even when the environment is completely unknown. We model the
problem as a stochastic game (SG) between the controller and the adversarial
environment; we then learn optimal control strategies that maximize the
probability of satisfying the LTL specifications against the worst-case
environment behavior. We first construct a product game using the deterministic
parity automaton (DPA) translated from the given LTL specification. By deriving
distinct rewards and discount factors from the acceptance condition of the DPA,
we reduce the maximization of the worst-case probability of satisfying the LTL
specification into the maximization of a discounted reward objective in the
product game; this enables the use of model-free RL algorithms to learn an
optimal controller strategy. To deal with the common scalability problems when
the number of sets defining the acceptance condition of the DPA (usually
referred as colors), is large, we propose a lazy color generation method where
distinct rewards and discount factors are utilized only when needed, and an
approximate method where the controller eventually focuses on only one color.
In several case studies, we show that our approach is scalable to a wide range
of LTL formulas, significantly outperforming existing methods for learning
controllers from LTL specifications in SGs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bozkurt_A/0/1/0/all/0/1"&gt;Alper Kamil Bozkurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zavlanos_M/0/1/0/all/0/1"&gt;Michael M. Zavlanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1"&gt;Miroslav Pajic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Image-based Generative Adversarial Networks for Time Series Generation. (arXiv:2112.08060v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.08060</id>
        <link href="http://arxiv.org/abs/2112.08060"/>
        <updated>2023-09-02T00:40:02.282Z</updated>
        <summary type="html"><![CDATA[Generative models for images have gained significant attention in computer
vision and natural language processing due to their ability to generate
realistic samples from complex data distributions. To leverage the advances of
image-based generative models for the time series domain, we propose a
two-dimensional image representation for time series, the Extended
Intertemporal Return Plot (XIRP). Our approach captures the intertemporal time
series dynamics in a scale-invariant and invertible way, reducing training time
and improving sample quality. We benchmark synthetic XIRPs obtained by an
off-the-shelf Wasserstein GAN with gradient penalty (WGAN-GP) to other image
representations and models regarding similarity and predictive ability metrics.
Our novel, validated image representation for time series consistently and
significantly outperforms a state-of-the-art RNN-based generative model
regarding predictive ability. Further, we introduce an improved stochastic
inversion to substantially improve simulation quality regardless of the
representation and provide the prospect of transfer potentials in other
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hellermann_J/0/1/0/all/0/1"&gt;Justin Hellermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lessmann_S/0/1/0/all/0/1"&gt;Stefan Lessmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamical systems' based neural networks. (arXiv:2210.02373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2210.02373</id>
        <link href="http://arxiv.org/abs/2210.02373"/>
        <updated>2023-09-02T00:40:02.282Z</updated>
        <summary type="html"><![CDATA[Neural networks have gained much interest because of their effectiveness in
many applications. However, their mathematical properties are generally not
well understood. If there is some underlying geometric structure inherent to
the data or to the function to approximate, it is often desirable to take this
into account in the design of the neural network. In this work, we start with a
non-autonomous ODE and build neural networks using a suitable,
structure-preserving, numerical time-discretisation. The structure of the
neural network is then inferred from the properties of the ODE vector field.
Besides injecting more structure into the network architectures, this modelling
procedure allows a better theoretical understanding of their behaviour. We
present two universal approximation results and demonstrate how to impose some
particular properties on the neural networks. A particular focus is on
1-Lipschitz architectures including layers that are not 1-Lipschitz. These
networks are expressive and robust against adversarial attacks, as shown for
the CIFAR-10 and CIFAR-100 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Celledoni_E/0/1/0/all/0/1"&gt;Elena Celledoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murari_D/0/1/0/all/0/1"&gt;Davide Murari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Owren_B/0/1/0/all/0/1"&gt;Brynjulf Owren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sherry_F/0/1/0/all/0/1"&gt;Ferdia Sherry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics. (arXiv:2308.16680v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16680</id>
        <link href="http://arxiv.org/abs/2308.16680"/>
        <updated>2023-09-02T00:40:02.281Z</updated>
        <summary type="html"><![CDATA[We propose to apply several gradient estimation techniques to enable the
differentiation of programs with discrete randomness in High Energy Physics.
Such programs are common in High Energy Physics due to the presence of
branching processes and clustering-based analysis. Thus differentiating such
programs can open the way for gradient based optimization in the context of
detector design optimization, simulator tuning, or data analysis and
reconstruction optimization. We discuss several possible gradient estimation
strategies, including the recent Stochastic AD method, and compare them in
simplified detector design experiments. In doing so we develop, to the best of
our knowledge, the first fully differentiable branching program.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kagan_M/0/1/0/all/0/1"&gt;Michael Kagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heinrich_L/0/1/0/all/0/1"&gt;Lukas Heinrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets. (arXiv:2308.16822v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16822</id>
        <link href="http://arxiv.org/abs/2308.16822"/>
        <updated>2023-09-02T00:40:02.281Z</updated>
        <summary type="html"><![CDATA[Multi-output Gaussian processes (MOGPs) have been introduced to deal with
multiple tasks by exploiting the correlations between different outputs.
Generally, MOGPs models assume a flat correlation structure between the
outputs. However, such a formulation does not account for more elaborate
relationships, for instance, if several replicates were observed for each
output (which is a typical setting in biological experiments). This paper
proposes an extension of MOGPs for hierarchical datasets (i.e. datasets for
which the relationships between observations can be represented within a tree
structure). Our model defines a tailored kernel function accounting for
hierarchical structures in the data to capture different levels of correlations
while leveraging the introduction of latent variables to express the underlying
dependencies between outputs through a dedicated kernel. This latter feature is
expected to significantly improve scalability as the number of tasks increases.
An extensive experimental study involving both synthetic and real-world data
from genomics and motion capture is proposed to support our claims.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chunchao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leroy_A/0/1/0/all/0/1"&gt;Arthur Leroy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_M/0/1/0/all/0/1"&gt;Mauricio Alvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoencoder-based Online Data Quality Monitoring for the CMS Electromagnetic Calorimeter. (arXiv:2308.16659v1 [physics.ins-det])]]></title>
        <id>http://arxiv.org/abs/2308.16659</id>
        <link href="http://arxiv.org/abs/2308.16659"/>
        <updated>2023-09-02T00:40:02.280Z</updated>
        <summary type="html"><![CDATA[The online Data Quality Monitoring system (DQM) of the CMS electromagnetic
calorimeter (ECAL) is a crucial operational tool that allows ECAL experts to
quickly identify, localize, and diagnose a broad range of detector issues that
would otherwise hinder physics-quality data taking. Although the existing ECAL
DQM system has been continuously updated to respond to new problems, it remains
one step behind newer and unforeseen issues. Using unsupervised deep learning,
a real-time autoencoder-based anomaly detection system is developed that is
able to detect ECAL anomalies unseen in past data. After accounting for spatial
variations in the response of the ECAL and the temporal evolution of anomalies,
the new system is able to efficiently detect anomalies while maintaining an
estimated false discovery rate between $10^{-2}$ to $10^{-4}$, beating existing
benchmarks by about two orders of magnitude. The real-world performance of the
system is validated using anomalies found in 2018 and 2022 LHC collision data.
Additionally, first results from deploying the autoencoder-based system in the
CMS online DQM workflow for the ECAL barrel during Run 3 of the LHC are
presented, showing its promising performance in detecting obscure issues that
could have been missed in the existing DQM system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Harilal_A/0/1/0/all/0/1"&gt;Abhirami Harilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyungmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1"&gt;Michael Andrews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1"&gt;Manfred Paulini&lt;/a&gt; (on behalf of the CMS Collaboration)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Diblock Copolymer Morphology via Machine Learning. (arXiv:2308.16886v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2308.16886</id>
        <link href="http://arxiv.org/abs/2308.16886"/>
        <updated>2023-09-02T00:40:02.280Z</updated>
        <summary type="html"><![CDATA[A machine learning approach is presented to accelerate the computation of
block polymer morphology evolution for large domains over long timescales. The
strategy exploits the separation of characteristic times between coarse-grained
particle evolution on the monomer scale and slow morphological evolution over
mesoscopic scales. In contrast to empirical continuum models, the proposed
approach learns stochastically driven defect annihilation processes directly
from particle-based simulations. A UNet architecture that respects different
boundary conditions is adopted, thereby allowing periodic and fixed substrate
boundary conditions of arbitrary shape. Physical concepts are also introduced
via the loss function and symmetries are incorporated via data augmentation.
The model is validated using three different use cases. Explainable artificial
intelligence methods are applied to visualize the morphology evolution over
time. This approach enables the generation of large system sizes and long
trajectories to investigate defect densities and their evolution under
different types of confinement. As an application, we demonstrate the
importance of accessing late-stage morphologies for understanding particle
diffusion inside a single block. This work has implications for directed
self-assembly and materials design in micro-electronics, battery materials, and
membranes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yu_B/0/1/0/all/0/1"&gt;Boyuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Park_J/0/1/0/all/0/1"&gt;Juhae Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sun_G/0/1/0/all/0/1"&gt;Ge Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tajkhorshid_E/0/1/0/all/0/1"&gt;Emad Tajkhorshid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pablo_J/0/1/0/all/0/1"&gt;Juan J. de Pablo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schneider_L/0/1/0/all/0/1"&gt;Ludwig Schneider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness. (arXiv:2308.16681v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16681</id>
        <link href="http://arxiv.org/abs/2308.16681"/>
        <updated>2023-09-02T00:40:02.279Z</updated>
        <summary type="html"><![CDATA[A vast number of systems across the world use algorithmic decision making
(ADM) to (partially) automate decisions that have previously been made by
humans. When designed well, these systems promise more objective decisions
while saving large amounts of resources and freeing up human time. However,
when ADM systems are not designed well, they can lead to unfair decisions which
discriminate against societal groups. The downstream effects of ADMs critically
depend on the decisions made during the systems' design and implementation, as
biases in data can be mitigated or reinforced along the modeling pipeline. Many
of these design decisions are made implicitly, without knowing exactly how they
will influence the final system. It is therefore important to make explicit the
decisions made during the design of ADM systems and understand how these
decisions affect the fairness of the resulting system.

To study this issue, we draw on insights from the field of psychology and
introduce the method of multiverse analysis for algorithmic fairness. In our
proposed method, we turn implicit design decisions into explicit ones and
demonstrate their fairness implications. By combining decisions, we create a
grid of all possible "universes" of decision combinations. For each of these
universes, we compute metrics of fairness and performance. Using the resulting
dataset, one can see how and which decisions impact fairness. We demonstrate
how multiverse analyses can be used to better understand variability and
robustness of algorithmic fairness using an exemplary case study of predicting
public health coverage of vulnerable populations for potential interventions.
Our results illustrate how decisions during the design of a machine learning
system can have surprising effects on its fairness and how to detect these
effects using multiverse analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Simson_J/0/1/0/all/0/1"&gt;Jan Simson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pfisterer_F/0/1/0/all/0/1"&gt;Florian Pfisterer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kern_C/0/1/0/all/0/1"&gt;Christoph Kern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Models for Interferometric Satellite Aperture Radar. (arXiv:2308.16847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16847</id>
        <link href="http://arxiv.org/abs/2308.16847"/>
        <updated>2023-09-02T00:40:02.278Z</updated>
        <summary type="html"><![CDATA[Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuel_A/0/1/0/all/0/1"&gt;Alexandre Tuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerdreux_T/0/1/0/all/0/1"&gt;Thomas Kerdreux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hulbert_C/0/1/0/all/0/1"&gt;Claudia Hulbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouet_Leduc_B/0/1/0/all/0/1"&gt;Bertrand Rouet-Leduc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.16884</id>
        <link href="http://arxiv.org/abs/2308.16884"/>
        <updated>2023-09-02T00:40:02.277Z</updated>
        <summary type="html"><![CDATA[We present Belebele, a multiple-choice machine reading comprehension (MRC)
dataset spanning 122 language variants. Significantly expanding the language
coverage of natural language understanding (NLU) benchmarks, this dataset
enables the evaluation of text models in high-, medium-, and low-resource
languages. Each question is based on a short passage from the Flores-200
dataset and has four multiple-choice answers. The questions were carefully
curated to discriminate between models with different levels of general
language comprehension. The English dataset on its own proves difficult enough
to challenge state-of-the-art language models. Being fully parallel, this
dataset enables direct comparison of model performance across all languages. We
use this dataset to evaluate the capabilities of multilingual masked language
models (MLMs) and large language models (LLMs). We present extensive results
and find that despite significant cross-lingual transfer in English-centric
LLMs, much smaller MLMs pretrained on balanced multilingual data still
understand far more languages. We also observe that larger vocabulary size and
conscious vocabulary construction correlate with better performance on
low-resource languages. Overall, Belebele opens up new avenues for evaluating
and analyzing the multilingual capabilities of NLP systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bandarkar_L/0/1/0/all/0/1"&gt;Lucas Bandarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Davis Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1"&gt;Benjamin Muller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Satya Narayan Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Husa_D/0/1/0/all/0/1"&gt;Donald Husa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1"&gt;Naman Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1"&gt;Abhinandan Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1"&gt;Madian Khabsa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16898</id>
        <link href="http://arxiv.org/abs/2308.16898"/>
        <updated>2023-09-02T00:40:02.276Z</updated>
        <summary type="html"><![CDATA[Since its inception in "Attention Is All You Need", transformer architecture
has led to revolutionary advancements in NLP. The attention layer within the
transformer admits a sequence of input tokens $X$ and makes them interact
through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where
$(K,Q)$ are the trainable key-query parameters. In this work, we establish a
formal equivalence between the optimization geometry of self-attention and a
hard-margin SVM problem that separates optimal input tokens from non-optimal
tokens using linear constraints on the outer-products of token pairs. This
formalism allows us to characterize the implicit bias of 1-layer transformers
optimized with gradient descent: (1) Optimizing the attention layer with
vanishing regularization, parameterized by $(K,Q)$, converges in direction to
an SVM solution minimizing the nuclear norm of the combined parameter
$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm
objective. We characterize this convergence, highlighting that it can occur
toward locally-optimal directions rather than global ones. (2) Complementing
this, we prove the local/global directional convergence of gradient descent
under suitable geometric conditions. Importantly, we show that
over-parameterization catalyzes global convergence by ensuring the feasibility
of the SVM problem and by guaranteeing a benign optimization landscape devoid
of stationary points. (3) While our theory applies primarily to linear
prediction heads, we propose a more general SVM equivalence that predicts the
implicit bias with nonlinear heads. Our findings are applicable to arbitrary
datasets and their validity is verified via experiments. We also introduce
several open problems and research directions. We believe these findings
inspire the interpretation of transformers as a hierarchy of SVMs that
separates and selects optimal tokens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1"&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingcong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1"&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1"&gt;Samet Oymak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning in UAV-Enhanced Networks: Joint Coverage and Convergence Time Optimization. (arXiv:2308.16889v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16889</id>
        <link href="http://arxiv.org/abs/2308.16889"/>
        <updated>2023-09-02T00:40:02.275Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) involves several devices that collaboratively train a
shared model without transferring their local data. FL reduces the
communication overhead, making it a promising learning method in UAV-enhanced
wireless networks with scarce energy resources. Despite the potential,
implementing FL in UAV-enhanced networks is challenging, as conventional UAV
placement methods that maximize coverage increase the FL delay significantly.
Moreover, the uncertainty and lack of a priori information about crucial
variables, such as channel quality, exacerbate the problem. In this paper, we
first analyze the statistical characteristics of a UAV-enhanced wireless sensor
network (WSN) with energy harvesting. We then develop a model and solution
based on the multi-objective multi-armed bandit theory to maximize the network
coverage while minimizing the FL delay. Besides, we propose another solution
that is particularly useful with large action sets and strict energy
constraints at the UAVs. Our proposal uses a scalarized best-arm identification
algorithm to find the optimal arms that maximize the ratio of the expected
reward to the expected energy cost by sequentially eliminating one or more arms
in each round. Then, we derive the upper bound on the error probability of our
multi-objective and cost-aware algorithm. Numerical results show the
effectiveness of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yahya_M/0/1/0/all/0/1"&gt;Mariam Yahya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1"&gt;Setareh Maghsudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanczak_S/0/1/0/all/0/1"&gt;Slawomir Stanczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16775</id>
        <link href="http://arxiv.org/abs/2308.16775"/>
        <updated>2023-09-02T00:40:02.268Z</updated>
        <summary type="html"><![CDATA[In prediction-based Neural Architecture Search (NAS), performance indicators
derived from graph convolutional networks have shown significant success. These
indicators, achieved by representing feed-forward structures as component
graphs through one-hot encoding, face a limitation: their inability to evaluate
architecture performance across varying search spaces. In contrast, handcrafted
performance indicators (zero-shot NAS), which use the same architecture with
random initialization, can generalize across multiple search spaces. Addressing
this limitation, we propose a novel approach for zero-shot NAS using deep
learning. Our method employs Fourier sum of sines encoding for convolutional
kernels, enabling the construction of a computational feed-forward graph with a
structure similar to the architecture under evaluation. These encodings are
learnable and offer a comprehensive view of the architecture's topological
information. An accompanying multi-layer perceptron (MLP) then ranks these
architectures based on their encodings. Experimental results show that our
approach surpasses previous methods using graph convolutional networks in terms
of correlation on the NAS-Bench-201 dataset and exhibits a higher convergence
rate. Moreover, our extracted feature representation trained on each
NAS-Benchmark is transferable to other NAS-Benchmarks, showing promising
generalizability across multiple search spaces. The code is available at:
https://github.com/minh1409/DFT-NPZS-NAS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1"&gt;Minh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1"&gt;Nhan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luong_N/0/1/0/all/0/1"&gt;Ngoc Hoang Luong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moreau Envelope ADMM for Decentralized Weakly Convex Optimization. (arXiv:2308.16752v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2308.16752</id>
        <link href="http://arxiv.org/abs/2308.16752"/>
        <updated>2023-09-02T00:40:02.267Z</updated>
        <summary type="html"><![CDATA[This paper proposes a proximal variant of the alternating direction method of
multipliers (ADMM) for distributed optimization. Although the current versions
of ADMM algorithm provide promising numerical results in producing solutions
that are close to optimal for many convex and non-convex optimization problems,
it remains unclear if they can converge to a stationary point for weakly convex
and locally non-smooth functions. Through our analysis using the Moreau
envelope function, we demonstrate that MADM can indeed converge to a stationary
point under mild conditions. Our analysis also includes computing the bounds on
the amount of change in the dual variable update step by relating the gradient
of the Moreau envelope function to the proximal function. Furthermore, the
results of our numerical experiments indicate that our method is faster and
more robust than widely-used approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mirzaeifard_R/0/1/0/all/0/1"&gt;Reza Mirzaeifard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Venkategowda_N/0/1/0/all/0/1"&gt;Naveen K. D. Venkategowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jung_A/0/1/0/all/0/1"&gt;Alexander Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Werner_S/0/1/0/all/0/1"&gt;Stefan Werner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2308.16781</id>
        <link href="http://arxiv.org/abs/2308.16781"/>
        <updated>2023-09-02T00:40:02.267Z</updated>
        <summary type="html"><![CDATA[With the growing imbalance between limited medical resources and escalating
demands, AI-based clinical tasks have become paramount. Medication
recommendation, as a sub-domain, aims to amalgamate longitudinal patient
history with medical knowledge, assisting physicians in prescribing safer and
more accurate medication combinations. Existing methods overlook the inherent
long-tail distribution in medical data, lacking balanced representation between
head and tail data, which leads to sub-optimal model performance. To address
this challenge, we introduce StratMed, a model that incorporates an innovative
relevance stratification mechanism. It harmonizes discrepancies in data
long-tail distribution and strikes a balance between the safety and accuracy of
medication combinations. Specifically, we first construct a pre-training method
using deep learning networks to obtain entity representation. After that, we
design a pyramid-like data stratification method to obtain more generalized
entity relationships by reinforcing the features of unpopular entities. Based
on this relationship, we designed two graph structures to express medication
precision and safety at the same level to obtain visit representations.
Finally, the patient's historical clinical information is fitted to generate
medication combinations for the current health condition. Experiments on the
MIMIC-III dataset demonstrate that our method has outperformed current
state-of-the-art methods in four evaluation metrics (including safety and
accuracy).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16800</id>
        <link href="http://arxiv.org/abs/2308.16800"/>
        <updated>2023-09-02T00:40:02.267Z</updated>
        <summary type="html"><![CDATA[Our study reveals new theoretical insights into over-smoothing and feature
over-correlation in deep graph neural networks. We show the prevalence of
invariant subspaces, demonstrating a fixed relative behavior that is unaffected
by feature transformations. Our work clarifies recent observations related to
convergence to a constant state and a potential over-separation of node states,
as the amplification of subspaces only depends on the spectrum of the
aggregation function. In linear scenarios, this leads to node representations
being dominated by a low-dimensional subspace with an asymptotic convergence
rate independent of the feature transformations. This causes a rank collapse of
the node representations, resulting in over-smoothing when smooth vectors span
this subspace, and over-correlation even when over-smoothing is avoided. Guided
by our theory, we propose a sum of Kronecker products as a beneficial property
that can provably prevent over-smoothing, over-correlation, and rank collapse.
We empirically extend our insights to the non-linear case, demonstrating the
inability of existing models to capture linearly independent features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Andreas Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebig_T/0/1/0/all/0/1"&gt;Thomas Liebig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Conditioned Path Planning. (arXiv:2308.16893v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.16893</id>
        <link href="http://arxiv.org/abs/2308.16893"/>
        <updated>2023-09-02T00:40:02.267Z</updated>
        <summary type="html"><![CDATA[Contact is at the core of robotic manipulation. At times, it is desired (e.g.
manipulation and grasping), and at times, it is harmful (e.g. when avoiding
obstacles). However, traditional path planning algorithms focus solely on
collision-free paths, limiting their applicability in contact-rich tasks. To
address this limitation, we propose the domain of Language-Conditioned Path
Planning, where contact-awareness is incorporated into the path planning
problem. As a first step in this domain, we propose Language-Conditioned
Collision Functions (LACO) a novel approach that learns a collision function
using only a single-view image, language prompt, and robot configuration. LACO
predicts collisions between the robot and the environment, enabling flexible,
conditional path planning without the need for manual object annotations, point
cloud data, or ground-truth object meshes. In both simulation and the real
world, we demonstrate that LACO can facilitate complex, nuanced path plans that
allow for interaction with objects that are safe to collide, rather than
prohibiting any collision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1"&gt;Amber Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Youngwoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1"&gt;Stephen James&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Objective Decision Transformers for Offline Reinforcement Learning. (arXiv:2308.16379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16379</id>
        <link href="http://arxiv.org/abs/2308.16379"/>
        <updated>2023-09-02T00:40:02.266Z</updated>
        <summary type="html"><![CDATA[Offline Reinforcement Learning (RL) is structured to derive policies from
static trajectory data without requiring real-time environment interactions.
Recent studies have shown the feasibility of framing offline RL as a sequence
modeling task, where the sole aim is to predict actions based on prior context
using the transformer architecture. However, the limitation of this single task
learning approach is its potential to undermine the transformer model's
attention mechanism, which should ideally allocate varying attention weights
across different tokens in the input context for optimal prediction. To address
this, we reformulate offline RL as a multi-objective optimization problem,
where the prediction is extended to states and returns. We also highlight a
potential flaw in the trajectory representation used for sequence modeling,
which could generate inaccuracies when modeling the state and return
distributions. This is due to the non-smoothness of the action distribution
within the trajectory dictated by the behavioral policy. To mitigate this
issue, we introduce action space regions to the trajectory representation. Our
experiments on D4RL benchmark locomotion tasks reveal that our propositions
allow for more effective utilization of the attention mechanism in the
transformer model, resulting in performance that either matches or outperforms
current state-of-the art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_A/0/1/0/all/0/1"&gt;Abdelghani Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciblat_P/0/1/0/all/0/1"&gt;Philippe Ciblat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghogho_M/0/1/0/all/0/1"&gt;Mounir Ghogho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs. (arXiv:2308.16859v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16859</id>
        <link href="http://arxiv.org/abs/2308.16859"/>
        <updated>2023-09-02T00:40:02.266Z</updated>
        <summary type="html"><![CDATA[In this article, the optimal sample complexity of learning the underlying
interaction/dependencies of a Linear Dynamical System (LDS) over a Directed
Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG's
structure is well-studied for static systems, where the samples of nodal states
are independent and identically distributed (i.i.d.). However, such a study is
less explored for DAGs with dynamical systems, where the nodal states are
temporally correlated. We call such a DAG underlying an LDS as \emph{dynamical}
DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are
driven by unobserved exogenous noise sources that are wide-sense stationary
(WSS) in time but are mutually uncorrelated, and have the same {power spectral
density (PSD)}. Inspired by the static settings, a metric and an algorithm
based on the PSD matrix of the observed time series are proposed to reconstruct
the DDAG. The equal noise PSD assumption can be relaxed such that
identifiability conditions for DDAG reconstruction are not violated. For the
LDS with WSS (sub) Gaussian exogenous noise sources, it is shown that the
optimal sample complexity (or length of state trajectory) needed to learn the
DDAG is $n=\Theta(q\log(p/q))$, where $p$ is the number of nodes and $q$ is the
maximum number of parents per node. To prove the sample complexity upper bound,
a concentration bound for the PSD estimation is derived, under two different
sampling strategies. A matching min-max lower bound using generalized Fano's
inequality also is provided, thus showing the order optimality of the proposed
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Veedu_M/0/1/0/all/0/1"&gt;Mishfad Shaikh Veedu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deka_D/0/1/0/all/0/1"&gt;Deepjyoti Deka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salapaka_M/0/1/0/all/0/1"&gt;Murti V. Salapaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16737</id>
        <link href="http://arxiv.org/abs/2308.16737"/>
        <updated>2023-09-02T00:40:02.259Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of localization, which is inherently
non-convex and non-smooth in a federated setting where the data is distributed
across a multitude of devices. Due to the decentralized nature of federated
environments, distributed learning becomes essential for scalability and
adaptability. Moreover, these environments are often plagued by outlier data,
which presents substantial challenges to conventional methods, particularly in
maintaining estimation accuracy and ensuring algorithm convergence. To mitigate
these challenges, we propose a method that adopts an $L_1$-norm robust
formulation within a distributed sub-gradient framework, explicitly designed to
handle these obstacles. Our approach addresses the problem in its original
form, without resorting to iterative simplifications or approximations,
resulting in enhanced computational efficiency and improved estimation
accuracy. We demonstrate that our method converges to a stationary point,
highlighting its effectiveness and reliability. Through numerical simulations,
we confirm the superior performance of our approach, notably in outlier-rich
environments, which surpasses existing state-of-the-art localization methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirzaeifard_R/0/1/0/all/0/1"&gt;Reza Mirzaeifard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkategowda_N/0/1/0/all/0/1"&gt;Naveen K. D. Venkategowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werner_S/0/1/0/all/0/1"&gt;Stefan Werner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Echocardiographic View Classification with Integrated Out-of-Distribution Detection for Enhanced Automatic Echocardiographic Analysis. (arXiv:2308.16483v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2308.16483</id>
        <link href="http://arxiv.org/abs/2308.16483"/>
        <updated>2023-09-02T00:40:02.257Z</updated>
        <summary type="html"><![CDATA[In the rapidly evolving field of automatic echocardiographic analysis and
interpretation, automatic view classification is a critical yet challenging
task, owing to the inherent complexity and variability of echocardiographic
data. This study presents ECHOcardiography VIew Classification with
Out-of-Distribution dEtection (ECHO-VICODE), a novel deep learning-based
framework that effectively addresses this challenge by training to classify 31
classes, surpassing previous studies and demonstrating its capacity to handle a
wide range of echocardiographic views. Furthermore, ECHO-VICODE incorporates an
integrated out-of-distribution (OOD) detection function, leveraging the
relative Mahalanobis distance to effectively identify 'near-OOD' instances
commonly encountered in echocardiographic data. Through extensive
experimentation, we demonstrated the outstanding performance of ECHO-VICODE in
terms of view classification and OOD detection, significantly reducing the
potential for errors in echocardiographic analyses. This pioneering study
significantly advances the domain of automated echocardiography analysis and
exhibits promising prospects for substantial applications in extensive clinical
research and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1"&gt;Jaeik Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ha_S/0/1/0/all/0/1"&gt;Seongmin Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yoon_Y/0/1/0/all/0/1"&gt;Yeonyee E. Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jiyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jeong_H/0/1/0/all/0/1"&gt;Hyunseok Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jeong_D/0/1/0/all/0/1"&gt;Dawun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1"&gt;Yeonggul Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Youngtaek Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hyuk-Jae Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Emergency Department Crowding with Advanced Machine Learning Models and Multivariable Input. (arXiv:2308.16544v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16544</id>
        <link href="http://arxiv.org/abs/2308.16544"/>
        <updated>2023-09-02T00:40:02.257Z</updated>
        <summary type="html"><![CDATA[Emergency department (ED) crowding is a significant threat to patient safety
and it has been repeatedly associated with increased mortality. Forecasting
future service demand has the potential patient outcomes. Despite active
research on the subject, several gaps remain: 1) proposed forecasting models
have become outdated due to quick influx of advanced machine learning models
(ML), 2) amount of multivariable input data has been limited and 3) discrete
performance metrics have been rarely reported. In this study, we document the
performance of a set of advanced ML models in forecasting ED occupancy 24 hours
ahead. We use electronic health record data from a large, combined ED with an
extensive set of explanatory variables, including the availability of beds in
catchment area hospitals, traffic data from local observation stations, weather
variables, etc. We show that N-BEATS and LightGBM outpeform benchmarks with 11
% and 9 % respective improvements and that DeepAR predicts next day crowding
with an AUC of 0.76 (95 % CI 0.69-0.84). To the best of our knowledge, this is
the first study to document the superiority of LightGBM and N-BEATS over
statistical benchmarks in the context of ED forecasting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuominen_J/0/1/0/all/0/1"&gt;Jalmari Tuominen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulkkinen_E/0/1/0/all/0/1"&gt;Eetu Pulkkinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peltonen_J/0/1/0/all/0/1"&gt;Jaakko Peltonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanniainen_J/0/1/0/all/0/1"&gt;Juho Kanniainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oksala_N/0/1/0/all/0/1"&gt;Niku Oksala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palomaki_A/0/1/0/all/0/1"&gt;Ari Palom&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roine_A/0/1/0/all/0/1"&gt;Antti Roine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Robustness and Accuracy of Ponzi Scheme Detection on Ethereum Using Time-Dependent Features. (arXiv:2308.16391v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2308.16391</id>
        <link href="http://arxiv.org/abs/2308.16391"/>
        <updated>2023-09-02T00:40:02.256Z</updated>
        <summary type="html"><![CDATA[The rapid development of blockchain has led to more and more funding pouring
into the cryptocurrency market, which also attracted cybercriminals' interest
in recent years. The Ponzi scheme, an old-fashioned fraud, is now popular on
the blockchain, causing considerable financial losses to many crypto-investors.
A few Ponzi detection methods have been proposed in the literature, most of
which detect a Ponzi scheme based on its smart contract source code or opcode.
The contract-code-based approach, while achieving very high accuracy, is not
robust: first, the source codes of a majority of contracts on Ethereum are not
available, and second, a Ponzi developer can fool a contract-code-based
detection model by obfuscating the opcode or inventing a new profit
distribution logic that cannot be detected (since these models were trained on
existing Ponzi logics only). A transaction-based approach could improve the
robustness of detection because transactions, unlike smart contracts, are
harder to be manipulated. However, the current transaction-based detection
models achieve fairly low accuracy. We address this gap in the literature by
developing new detection models that rely only on the transactions, hence
guaranteeing the robustness, and moreover, achieve considerably higher
Accuracy, Precision, Recall, and F1-score than existing transaction-based
models. This is made possible thanks to the introduction of novel
time-dependent features that capture Ponzi behaviours characteristics derived
from our comprehensive data analyses on Ponzi and non-Ponzi data from the
XBlock-ETH repository]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_P/0/1/0/all/0/1"&gt;Phuong Duy Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dau_S/0/1/0/all/0/1"&gt;Son Hoang Dau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaodong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luong_P/0/1/0/all/0/1"&gt;Phuc Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viterbo_E/0/1/0/all/0/1"&gt;Emanuele Viterbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2308.16684</id>
        <link href="http://arxiv.org/abs/2308.16684"/>
        <updated>2023-09-02T00:40:02.254Z</updated>
        <summary type="html"><![CDATA[The vulnerabilities to backdoor attacks have recently threatened the
trustworthiness of machine learning models in practical applications.
Conventional wisdom suggests that not everyone can be an attacker since the
process of designing the trigger generation algorithm often involves
significant effort and extensive experimentation to ensure the attack's
stealthiness and effectiveness. Alternatively, this paper shows that there
exists a more severe backdoor threat: anyone can exploit an easily-accessible
algorithm for silent backdoor attacks. Specifically, this attacker can employ
the widely-used lossy image compression from a plethora of compression tools to
effortlessly inject a trigger pattern into an image without leaving any
noticeable trace; i.e., the generated triggers are natural artifacts. One does
not require extensive knowledge to click on the "convert" or "save as" button
while using tools for lossy image compression. Via this attack, the adversary
does not need to design a trigger generator as seen in prior works and only
requires poisoning the data. Empirically, the proposed attack consistently
achieves 100% attack success rate in several benchmark datasets such as MNIST,
CIFAR-10, GTSRB and CelebA. More significantly, the proposed attack can still
achieve almost 100% attack success rate with very small (approximately 10%)
poisoning rates in the clean label setting. The generated trigger of the
proposed attack using one lossy compression algorithm is also transferable
across other related compression algorithms, exacerbating the severity of this
backdoor threat. This work takes another crucial step toward understanding the
extensive risks of backdoor attacks in practice, urging practitioners to
investigate similar attacks and relevant backdoor mitigation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sze Jue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1"&gt;Khoa Doan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Decentralized Federated Learning via One-Bit Compressive Sensing. (arXiv:2308.16671v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16671</id>
        <link href="http://arxiv.org/abs/2308.16671"/>
        <updated>2023-09-02T00:40:02.253Z</updated>
        <summary type="html"><![CDATA[Decentralized federated learning (DFL) has gained popularity due to its
practicality across various applications. Compared to the centralized version,
training a shared model among a large number of nodes in DFL is more
challenging, as there is no central server to coordinate the training process.
Especially when distributed nodes suffer from limitations in communication or
computational resources, DFL will experience extremely inefficient and unstable
training. Motivated by these challenges, in this paper, we develop a novel
algorithm based on the framework of the inexact alternating direction method
(iADM). On one hand, our goal is to train a shared model with a sparsity
constraint. This constraint enables us to leverage one-bit compressive sensing
(1BCS), allowing transmission of one-bit information among neighbour nodes. On
the other hand, communication between neighbour nodes occurs only at certain
steps, reducing the number of communication rounds. Therefore, the algorithm
exhibits notable communication efficiency. Additionally, as each node selects
only a subset of neighbours to participate in the training, the algorithm is
robust against stragglers. Additionally, complex items are computed only once
for several consecutive steps and subproblems are solved inexactly using
closed-form solutions, resulting in high computational efficiency. Finally,
numerical experiments showcase the algorithm's effectiveness in both
communication and computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shenglong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Geoffrey Ye Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Majorization-Minimization for sparse SVMs. (arXiv:2308.16858v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16858</id>
        <link href="http://arxiv.org/abs/2308.16858"/>
        <updated>2023-09-02T00:40:02.252Z</updated>
        <summary type="html"><![CDATA[Several decades ago, Support Vector Machines (SVMs) were introduced for
performing binary classification tasks, under a supervised framework. Nowadays,
they often outperform other supervised methods and remain one of the most
popular approaches in the machine learning arena. In this work, we investigate
the training of SVMs through a smooth sparse-promoting-regularized squared
hinge loss minimization. This choice paves the way to the application of quick
training methods built on majorization-minimization approaches, benefiting from
the Lipschitz differentiabililty of the loss function. Moreover, the proposed
approach allows us to handle sparsity-preserving regularizers promoting the
selection of the most significant features, so enhancing the performance.
Numerical tests and comparisons conducted on three different datasets
demonstrate the good performance of the proposed methodology in terms of
qualitative metrics (accuracy, precision, recall, and F 1 score) as well as
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benfenati_A/0/1/0/all/0/1"&gt;Alessandro Benfenati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chouzenoux_E/0/1/0/all/0/1"&gt;Emilie Chouzenoux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchini_G/0/1/0/all/0/1"&gt;Giorgia Franchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latva_Aijo_S/0/1/0/all/0/1"&gt;Salla Latva-Aijo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narnhofer_D/0/1/0/all/0/1"&gt;Dominik Narnhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pesquet_J/0/1/0/all/0/1"&gt;Jean-Christophe Pesquet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_S/0/1/0/all/0/1"&gt;Sebastian J. Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yousefi_M/0/1/0/all/0/1"&gt;Mahsa Yousefi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate Your Own Scotland: Satellite Image Generation Conditioned on Maps. (arXiv:2308.16648v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16648</id>
        <link href="http://arxiv.org/abs/2308.16648"/>
        <updated>2023-09-02T00:40:02.251Z</updated>
        <summary type="html"><![CDATA[Despite recent advancements in image generation, diffusion models still
remain largely underexplored in Earth Observation. In this paper we show that
state-of-the-art pretrained diffusion models can be conditioned on cartographic
data to generate realistic satellite images. We provide two large datasets of
paired OpenStreetMap images and satellite views over the region of Mainland
Scotland and the Central Belt. We train a ControlNet model and qualitatively
evaluate the results, demonstrating that both image quality and map fidelity
are possible. Finally, we provide some insights on the opportunities and
challenges of applying these models for remote sensing. Our model weights and
code for creating the dataset are publicly available at
https://github.com/miquel-espinosa/map-sat.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_M/0/1/0/all/0/1"&gt;Miguel Espinosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1"&gt;Elliot J. Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16245</id>
        <link href="http://arxiv.org/abs/2308.16245"/>
        <updated>2023-09-02T00:40:02.250Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence (AI) is often an integral part of modern decision
support systems (DSSs). The best-performing predictive models used in AI-based
DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to
create AI systems that can explain their rationale to human users. Local
explanations in XAI can provide information about the causes of individual
predictions in terms of feature importance. However, a critical drawback of
existing local explanation methods is their inability to quantify the
uncertainty associated with a feature's importance. This paper introduces an
extension of a feature importance explanation method, Calibrated Explanations
(CE), previously only supporting classification, with support for standard
regression and probabilistic regression, i.e., the probability that the target
is above an arbitrary threshold. The extension for regression keeps all the
benefits of CE, such as calibration of the prediction from the underlying model
with confidence intervals, uncertainty quantification of feature importance,
and allows both factual and counterfactual explanations. CE for standard
regression provides fast, reliable, stable, and robust explanations. CE for
probabilistic regression provides an entirely new way of creating probabilistic
explanations from any ordinary regression model and with a dynamic selection of
thresholds. The performance of CE for probabilistic regression regarding
stability and speed is comparable to LIME. The method is model agnostic with
easily understood conditional rules. An implementation in Python is freely
available on GitHub and for installation using pip making the results in this
paper easily replicable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lofstrom_T/0/1/0/all/0/1"&gt;Tuwe L&amp;#xf6;fstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lofstrom_H/0/1/0/all/0/1"&gt;Helena L&amp;#xf6;fstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_U/0/1/0/all/0/1"&gt;Ulf Johansson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonstrod_C/0/1/0/all/0/1"&gt;Cecilia S&amp;#xf6;nstr&amp;#xf6;d&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction. (arXiv:2308.16754v1 [math.FA])]]></title>
        <id>http://arxiv.org/abs/2308.16754</id>
        <link href="http://arxiv.org/abs/2308.16754"/>
        <updated>2023-09-02T00:40:02.250Z</updated>
        <summary type="html"><![CDATA[We introduce and study the theory of training neural networks using
interpolation techniques from reproducing kernel Hilbert space theory. We
generalize the method to Krein spaces, and show that widely-used neural network
architectures are subsets of reproducing kernel Krein spaces (RKKS). We study
the concept of "associated Hilbert spaces" of RKKS and develop techniques to
improve upon the expressivity of various activation functions. Next, using
concepts from the theory of functions of several complex variables, we prove a
computationally applicable, multidimensional generalization of the celebrated
Adamjan- Arov-Krein (AAK) theorem. The theorem yields a novel class of neural
networks, called Prolongation Neural Networks (PNN). We demonstrate that, by
applying the multidimensional AAK theorem to gain a PNN, one can gain
performance superior to both our interpolatory methods and current
state-of-the-art methods in noisy environments. We provide useful illustrations
of our methods in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Werneburg_E/0/1/0/all/0/1"&gt;Eric Arthur Werneburg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Development and validation of an interpretable machine learning-based calculator for predicting 5-year weight trajectories after bariatric surgery: a multinational retrospective cohort SOPHIA study. (arXiv:2308.16585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16585</id>
        <link href="http://arxiv.org/abs/2308.16585"/>
        <updated>2023-09-02T00:40:02.235Z</updated>
        <summary type="html"><![CDATA[Background Weight loss trajectories after bariatric surgery vary widely
between individuals, and predicting weight loss before the operation remains
challenging. We aimed to develop a model using machine learning to provide
individual preoperative prediction of 5-year weight loss trajectories after
surgery. Methods In this multinational retrospective observational study we
enrolled adult participants (aged $\ge$18 years) from ten prospective cohorts
(including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese
Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse
Obesitas Kliniek]) and two randomised trials (SleevePass [NCT00793143] and
SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year
followup after Roux-en-Y gastric bypass, sleeve gâ€¦]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saux_P/0/1/0/all/0/1"&gt;Patrick Saux&lt;/a&gt; (Scool, CRIStAL), &lt;a href="http://arxiv.org/find/cs/1/au:+Bauvin_P/0/1/0/all/0/1"&gt;Pierre Bauvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raverdy_V/0/1/0/all/0/1"&gt;Violeta Raverdy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teigny_J/0/1/0/all/0/1"&gt;Julien Teigny&lt;/a&gt; (Scool), &lt;a href="http://arxiv.org/find/cs/1/au:+Verkindt_H/0/1/0/all/0/1"&gt;H&amp;#xe9;l&amp;#xe8;ne Verkindt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soumphonphakdy_T/0/1/0/all/0/1"&gt;Tomy Soumphonphakdy&lt;/a&gt; (Scool), &lt;a href="http://arxiv.org/find/cs/1/au:+Debert_M/0/1/0/all/0/1"&gt;Maxence Debert&lt;/a&gt; (Scool), &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_A/0/1/0/all/0/1"&gt;Anne Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1"&gt;Daan Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monpellier_V/0/1/0/all/0/1"&gt;Valerie Monpellier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1"&gt;Phong Ching Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1"&gt;Chin Hong Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andersson_Assarsson_J/0/1/0/all/0/1"&gt;Johanna C Andersson-Assarsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlsson_L/0/1/0/all/0/1"&gt;Lena Carlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svensson_P/0/1/0/all/0/1"&gt;Per-Arne Svensson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galtier_F/0/1/0/all/0/1"&gt;Florence Galtier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dezfoulian_G/0/1/0/all/0/1"&gt;Guelareh Dezfoulian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moldovanu_M/0/1/0/all/0/1"&gt;Mihaela Moldovanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrieux_S/0/1/0/all/0/1"&gt;Severine Andrieux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couster_J/0/1/0/all/0/1"&gt;Julien Couster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepage_M/0/1/0/all/0/1"&gt;Marie Lepage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lembo_E/0/1/0/all/0/1"&gt;Erminia Lembo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verrastro_O/0/1/0/all/0/1"&gt;Ornella Verrastro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robert_M/0/1/0/all/0/1"&gt;Maud Robert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salminen_P/0/1/0/all/0/1"&gt;Paulina Salminen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mingrone_G/0/1/0/all/0/1"&gt;Geltrude Mingrone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peterli_R/0/1/0/all/0/1"&gt;Ralph Peterli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1"&gt;Ricardo V Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zerrweck_C/0/1/0/all/0/1"&gt;Carlos Zerrweck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nocca_D/0/1/0/all/0/1"&gt;David Nocca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_C/0/1/0/all/0/1"&gt;Carel W Le Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caiazzo_R/0/1/0/all/0/1"&gt;Robert Caiazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1"&gt;Philippe Preux&lt;/a&gt; (Scool, CRIStAL), &lt;a href="http://arxiv.org/find/cs/1/au:+Pattou_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Pattou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation. (arXiv:2308.16598v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2308.16598</id>
        <link href="http://arxiv.org/abs/2308.16598"/>
        <updated>2023-09-02T00:40:02.235Z</updated>
        <summary type="html"><![CDATA[Detection of tumors in metastatic colorectal cancer (mCRC) plays an essential
role in the early diagnosis and treatment of liver cancer. Deep learning models
backboned by fully convolutional neural networks (FCNNs) have become the
dominant model for segmenting 3D computerized tomography (CT) scans. However,
since their convolution layers suffer from limited kernel size, they are not
able to capture long-range dependencies and global context. To tackle this
restriction, vision transformers have been introduced to solve FCNN's locality
of receptive fields. Although transformers can capture long-range features,
their segmentation performance decreases with various tumor sizes due to the
model sensitivity to the input patch size. While finding an optimal patch size
improves the performance of vision transformer-based models on segmentation
tasks, it is a time-consuming and challenging procedure. This paper proposes a
technique to select the vision transformer's optimal input multi-resolution
image patch size based on the average volume size of metastasis lesions. We
further validated our suggested framework using a transfer-learning technique,
demonstrating that the highest Dice similarity coefficient (DSC) performance
was obtained by pre-training on training data with a larger tumour volume using
the suggested ideal patch size and then training with a smaller one. We
experimentally evaluate this idea through pre-training our model on a
multi-resolution public dataset. Our model showed consistent and improved
results when applied to our private multi-resolution mCRC dataset with a
smaller average tumor volume. This study lays the groundwork for optimizing
semantic segmentation of small objects using vision transformers. The
implementation source code is available
at:https://github.com/Ramtin-Mojtahedi/OVTPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mojtahedi_R/0/1/0/all/0/1"&gt;Ramtin Mojtahedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1"&gt;Mohammad Hamghalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Do_R/0/1/0/all/0/1"&gt;Richard K. G. Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1"&gt;Amber L. Simpson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MONDEO: Multistage Botnet Detection. (arXiv:2308.16570v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2308.16570</id>
        <link href="http://arxiv.org/abs/2308.16570"/>
        <updated>2023-09-02T00:40:02.234Z</updated>
        <summary type="html"><![CDATA[Mobile devices have widespread to become the most used piece of technology.
Due to their characteristics, they have become major targets for botnet-related
malware. FluBot is one example of botnet malware that infects mobile devices.
In particular, FluBot is a DNS-based botnet that uses Domain Generation
Algorithms (DGA) to establish communication with the Command and Control Server
(C2). MONDEO is a multistage mechanism with a flexible design to detect
DNS-based botnet malware. MONDEO is lightweight and can be deployed without
requiring the deployment of software, agents, or configuration in mobile
devices, allowing easy integration in core networks. MONDEO comprises four
detection stages: Blacklisting/Whitelisting, Query rate analysis, DGA analysis,
and Machine learning evaluation. It was created with the goal of processing
streams of packets to identify attacks with high efficiency, in the distinct
phases. MONDEO was tested against several datasets to measure its efficiency
and performance, being able to achieve high performance with RandomForest
classifiers. The implementation is available at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dias_D/0/1/0/all/0/1"&gt;Duarte Dias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sousa_B/0/1/0/all/0/1"&gt;Bruno Sousa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antunes_N/0/1/0/all/0/1"&gt;Nuno Antunes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Painter. (arXiv:2308.16490v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16490</id>
        <link href="http://arxiv.org/abs/2308.16490"/>
        <updated>2023-09-02T00:40:02.233Z</updated>
        <summary type="html"><![CDATA[Latent diffusers revolutionized the generative AI and inspired creative art.
When denoising the latent, the predicted original image at each step
collectively animates the formation. However, the animation is limited by the
denoising nature of the diffuser, and only renders a sharpening process. This
work presents Latent Painter, which uses the latent as the canvas, and the
diffuser predictions as the plan, to generate painting animation. Latent
Painter also transits one generated image to another, which can happen between
images from two different sets of checkpoints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Shih-Chieh Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructing Indoor Region-based Radio Map without Location Labels. (arXiv:2308.16759v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16759</id>
        <link href="http://arxiv.org/abs/2308.16759"/>
        <updated>2023-09-02T00:40:02.233Z</updated>
        <summary type="html"><![CDATA[Radio map construction requires a large amount of radio measurement data with
location labels, which imposes a high deployment cost. This paper develops a
region-based radio map from received signal strength (RSS) measurements without
location labels. The construction is based on a set of blindly collected RSS
measurement data from a device that visits each region in an indoor area
exactly once, where the footprints and timestamps are not recorded. The main
challenge is to cluster the RSS data and match clusters with the physical
regions. Classical clustering algorithms fail to work as the RSS data naturally
appears as non-clustered due to multipaths and noise. In this paper, a signal
subspace model with a sequential prior is constructed for the RSS data, and an
integrated segmentation and clustering algorithm is developed, which is shown
to find the globally optimal solution in a special case. Furthermore, the
clustered data is matched with the physical regions using a graph-based
approach. Based on real measurements from an office space, the proposed scheme
reduces the region localization error by roughly 50% compared to a weighted
centroid localization (WCL) baseline, and it even outperforms some supervised
localization schemes, including k-nearest neighbor (KNN), support vector
machine (SVM), and deep neural network (DNN), which require labeled data for
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zheng Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junting Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Incomplete Multi-View Clustering with Structure Alignment. (arXiv:2308.16541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16541</id>
        <link href="http://arxiv.org/abs/2308.16541"/>
        <updated>2023-09-02T00:40:02.231Z</updated>
        <summary type="html"><![CDATA[The success of existing multi-view clustering (MVC) relies on the assumption
that all views are complete. However, samples are usually partially available
due to data corruption or sensor malfunction, which raises the research of
incomplete multi-view clustering (IMVC). Although several anchor-based IMVC
methods have been proposed to process the large-scale incomplete data, they
still suffer from the following drawbacks: i) Most existing approaches neglect
the inter-view discrepancy and enforce cross-view representation to be
consistent, which would corrupt the representation capability of the model; ii)
Due to the samples disparity between different views, the learned anchor might
be misaligned, which we referred as the Anchor-Unaligned Problem for Incomplete
data (AUP-ID). Such the AUP-ID would cause inaccurate graph fusion and degrades
clustering performance. To tackle these issues, we propose a novel incomplete
anchor graph learning framework termed Scalable Incomplete Multi-View
Clustering with Structure Alignment (SIMVC-SA). Specially, we construct the
view-specific anchor graph to capture the complementary information from
different views. In order to solve the AUP-ID, we propose a novel structure
alignment module to refine the cross-view anchor correspondence. Meanwhile, the
anchor graph construction and alignment are jointly optimized in our unified
framework to enhance clustering quality. Through anchor graph construction
instead of full graphs, the time and space complexity of the proposed SIMVC-SA
is proven to be linearly correlated with the number of samples. Extensive
experiments on seven incomplete benchmark datasets demonstrate the
effectiveness and efficiency of our proposed method. Our code is publicly
available at https://github.com/wy1019/SIMVC-SA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yi Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Ke Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weixuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xinhang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Suyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1"&gt;En Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16572</id>
        <link href="http://arxiv.org/abs/2308.16572"/>
        <updated>2023-09-02T00:40:02.231Z</updated>
        <summary type="html"><![CDATA[Masked image modeling has been demonstrated as a powerful pretext task for
generating robust representations that can be effectively generalized across
multiple downstream tasks. Typically, this approach involves randomly masking
patches (tokens) in input images, with the masking strategy remaining unchanged
during training. In this paper, we propose a curriculum learning approach that
updates the masking strategy to continually increase the complexity of the
self-supervised reconstruction task. We conjecture that, by gradually
increasing the task complexity, the model can learn more sophisticated and
transferable representations. To facilitate this, we introduce a novel
learnable masking module that possesses the capability to generate masks of
different complexities, and integrate the proposed module into masked
autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting
its behavior during training, transitioning from a partner to the MAE
(optimizing the same reconstruction loss) to an adversary (optimizing the
opposite loss), while passing through a neutral state. The transition between
these behaviors is smooth, being regulated by a factor that is multiplied with
the reconstruction loss of the masking module. The resulting training procedure
generates an easy-to-hard curriculum. We train our Curriculum-Learned Masked
Autoencoder (CL-MAE) on ImageNet and show that it exhibits superior
representation learning capabilities compared to MAE. The empirical results on
five downstream tasks confirm our conjecture, demonstrating that curriculum
learning can be successfully used to self-supervise masked autoencoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1"&gt;Neelu Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1"&gt;Nicolae-Catalin Ristea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1"&gt;Kamal Nasrollahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1"&gt;Thomas B. Moeslund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What can we learn from quantum convolutional neural networks?. (arXiv:2308.16664v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2308.16664</id>
        <link href="http://arxiv.org/abs/2308.16664"/>
        <updated>2023-09-02T00:40:02.230Z</updated>
        <summary type="html"><![CDATA[We can learn from analyzing quantum convolutional neural networks (QCNNs)
that: 1) working with quantum data can be perceived as embedding physical
system parameters through a hidden feature map; 2) their high performance for
quantum phase recognition can be attributed to generation of a very suitable
basis set during the ground state embedding, where quantum criticality of spin
models leads to basis functions with rapidly changing features; 3) pooling
layers of QCNNs are responsible for picking those basis functions that can
contribute to forming a high-performing decision boundary, and the learning
process corresponds to adapting the measurement such that few-qubit operators
are mapped to full-register observables; 4) generalization of QCNN models
strongly depends on the embedding type, and that rotation-based feature maps
with the Fourier basis require careful feature engineering; 5) accuracy and
generalization of QCNNs with readout based on a limited number of shots favor
the ground state embeddings and associated physics-informed models. We
demonstrate these points in simulation, where our results shed light on
classification for physical processes, relevant for applications in sensing.
Finally, we show that QCNNs with properly chosen ground state embeddings can be
used for fluid dynamics problems, expressing shock wave solutions with good
generalization and proven trainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Umeano_C/0/1/0/all/0/1"&gt;Chukwudubem Umeano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Paine_A/0/1/0/all/0/1"&gt;Annie E. Paine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Elfving_V/0/1/0/all/0/1"&gt;Vincent E. Elfving&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kyriienko_O/0/1/0/all/0/1"&gt;Oleksandr Kyriienko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Document Layout Analysis on BaDLAD Dataset: A Comprehensive MViTv2 Based Approach. (arXiv:2308.16571v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16571</id>
        <link href="http://arxiv.org/abs/2308.16571"/>
        <updated>2023-09-02T00:40:02.229Z</updated>
        <summary type="html"><![CDATA[In the rapidly evolving digital era, the analysis of document layouts plays a
pivotal role in automated information extraction and interpretation. In our
work, we have trained MViTv2 transformer model architecture with cascaded mask
R-CNN on BaDLAD dataset to extract text box, paragraphs, images and tables from
a document. After training on 20365 document images for 36 epochs in a 3 phase
cycle, we achieved a training loss of 0.2125 and a mask loss of 0.19. Our work
extends beyond training, delving into the exploration of potential enhancement
avenues. We investigate the impact of rotation and flip augmentation, the
effectiveness of slicing input images pre-inference, the implications of
varying the resolution of the transformer backbone, and the potential of
employing a dual-pass inference to uncover missed text-boxes. Through these
explorations, we observe a spectrum of outcomes, where some modifications
result in tangible performance improvements, while others offer unique insights
for future endeavors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Ashrafur Rahman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1"&gt;Asif Azad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16458</id>
        <link href="http://arxiv.org/abs/2308.16458"/>
        <updated>2023-09-02T00:40:02.227Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models like ChatGPT have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks. Moreover, in bioinformatics, generating
functional programs poses additional notable challenges due to the amount of
domain knowledge, the need for complicated data operations, and intricate
functional dependencies between the operations. Here, we present BioCoder, a
benchmark developed to evaluate existing pre-trained models in generating
bioinformatics code. In relation to function-code generation, BioCoder covers
potential package dependencies, class declarations, and global variables. It
incorporates 1026 functions and 1243 methods in Python and Java from GitHub and
253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing
framework for evaluation, and we have applied it to evaluate many models
including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+,
InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizes
the importance of domain knowledge, pragmatic code generation, and contextual
understanding. Our dataset, benchmark, Docker images, and scripts required for
testing are all available at https://github.com/gersteinlab/biocoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiangru Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1"&gt;Bill Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Rick Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiakang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1"&gt;Mark Gerstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16609</id>
        <link href="http://arxiv.org/abs/2308.16609"/>
        <updated>2023-09-02T00:40:02.202Z</updated>
        <summary type="html"><![CDATA[Graph classification, aiming at learning the graph-level representations for
effective class assignments, has received outstanding achievements, which
heavily relies on high-quality datasets that have balanced class distribution.
In fact, most real-world graph data naturally presents a long-tailed form,
where the head classes occupy much more samples than the tail classes, it thus
is essential to study the graph-level classification over long-tailed data
while still remaining largely unexplored. However, most existing long-tailed
learning methods in visions fail to jointly optimize the representation
learning and classifier training, as well as neglect the mining of the
hard-to-classify classes. Directly applying existing methods to graphs may lead
to sub-optimal performance, since the model trained on graphs would be more
sensitive to the long-tailed distribution due to the complex topological
characteristics. Hence, in this paper, we propose a novel long-tailed
graph-level classification framework via Collaborative Multi-expert Learning
(CoMe) to tackle the problem. To equilibrate the contributions of head and tail
classes, we first develop balanced contrastive learning from the view of
representation learning, and then design an individual-expert classifier
training based on hard class mining. In addition, we execute gated fusion and
disentangled knowledge distillation among the multiple experts to promote the
collaboration in a multi-expert framework. Comprehensive experiments are
performed on seven widely-used benchmark datasets to demonstrate the
superiority of our method CoMe over state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Siyu Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1"&gt;Zhengyang Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1"&gt;Wei Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yongdao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Luchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis. (arXiv:2308.16593v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2308.16593</id>
        <link href="http://arxiv.org/abs/2308.16593"/>
        <updated>2023-09-02T00:40:02.195Z</updated>
        <summary type="html"><![CDATA[The spontaneous behavior that often occurs in conversations makes speech more
human-like compared to reading-style. However, synthesizing spontaneous-style
speech is challenging due to the lack of high-quality spontaneous datasets and
the high cost of labeling spontaneous behavior. In this paper, we propose a
semi-supervised pre-training method to increase the amount of spontaneous-style
speech and spontaneous behavioral labels. In the process of semi-supervised
learning, both text and speech information are considered for detecting
spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is
used to model the relationship between each sentence in the conversation.
Experimental results indicate that our proposed method achieves superior
expressive speech synthesis performance with the ability to model spontaneous
behavior in spontaneous-style speech and predict reasonable spontaneous
behavior from text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weiqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shun Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiaochu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yixuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;Shiyin Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SA6D: Self-Adaptive Few-Shot 6D Pose Estimator for Novel and Occluded Objects. (arXiv:2308.16528v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16528</id>
        <link href="http://arxiv.org/abs/2308.16528"/>
        <updated>2023-09-02T00:40:02.194Z</updated>
        <summary type="html"><![CDATA[To enable meaningful robotic manipulation of objects in the real-world, 6D
pose estimation is one of the critical aspects. Most existing approaches have
difficulties to extend predictions to scenarios where novel object instances
are continuously introduced, especially with heavy occlusions. In this work, we
propose a few-shot pose estimation (FSPE) approach called SA6D, which uses a
self-adaptive segmentation module to identify the novel target object and
construct a point cloud model of the target object using only a small number of
cluttered reference images. Unlike existing methods, SA6D does not require
object-centric reference images or any additional object information, making it
a more generalizable and scalable solution across categories. We evaluate SA6D
on real-world tabletop object datasets and demonstrate that SA6D outperforms
existing FSPE methods, particularly in cluttered scenes with occlusions, while
requiring fewer reference images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1"&gt;Ning Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1"&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.14505</id>
        <link href="http://arxiv.org/abs/2304.14505"/>
        <updated>2023-09-02T00:40:02.193Z</updated>
        <summary type="html"><![CDATA[A lot of deep learning (DL) research these days is mainly focused on
improving quantitative metrics regardless of other factors. In human-centered
applications, like skin lesion classification in dermatology, DL-driven
clinical decision support systems are still in their infancy due to the limited
transparency of their decision-making process. Moreover, the lack of procedures
that can explain the behavior of trained DL algorithms leads to almost no trust
from clinical physicians. To diagnose skin lesions, dermatologists rely on
visual assessment of the disease and the data gathered from the patient's
anamnesis. Data-driven algorithms dealing with multi-modal data are limited by
the separation of feature-level and decision-level fusion procedures required
by convolutional architectures. To address this issue, we enable single-stage
multi-modal data fusion via the attention mechanism of transformer-based
architectures to aid in diagnosing skin diseases. Our method beats other
state-of-the-art single- and multi-modal DL architectures in image-rich and
patient-data-rich environments. Additionally, the choice of the architecture
enables native interpretability support for the classification task both in the
image and metadata domain with no additional modifications necessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cheslerean_Boghiu_T/0/1/0/all/0/1"&gt;Theodor Cheslerean-Boghiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fleischmann_M/0/1/0/all/0/1"&gt;Melia-Evelina Fleischmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Willem_T/0/1/0/all/0/1"&gt;Theresa Willem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1"&gt;Tobias Lasser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.16539</id>
        <link href="http://arxiv.org/abs/2308.16539"/>
        <updated>2023-09-02T00:40:02.188Z</updated>
        <summary type="html"><![CDATA[Game theory offers an interpretable mathematical framework for modeling
multi-agent interactions. However, its applicability in real-world robotics
applications is hindered by several challenges, such as unknown agents'
preferences and goals. To address these challenges, we show a connection
between differential games, optimal control, and energy-based models and
demonstrate how existing approaches can be unified under our proposed
Energy-based Potential Game formulation. Building upon this formulation, this
work introduces a new end-to-end learning application that combines neural
networks for game-parameter inference with a differentiable game-theoretic
optimization layer, acting as an inductive bias. The experiments using
simulated mobile robot pedestrian interactions and real-world automated driving
data provide empirical evidence that the game-theoretic layer improves the
predictive performance of various neural network backbones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diehl_C/0/1/0/all/0/1"&gt;Christopher Diehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klosek_T/0/1/0/all/0/1"&gt;Tobias Klosek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kruger_M/0/1/0/all/0/1"&gt;Martin Kr&amp;#xfc;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murzyn_N/0/1/0/all/0/1"&gt;Nils Murzyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertram_T/0/1/0/all/0/1"&gt;Torsten Bertram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-adaptive Message Passing Graph Neural Network. (arXiv:2308.16470v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16470</id>
        <link href="http://arxiv.org/abs/2308.16470"/>
        <updated>2023-09-02T00:40:02.182Z</updated>
        <summary type="html"><![CDATA[Cross-network node classification (CNNC), which aims to classify nodes in a
label-deficient target network by transferring the knowledge from a source
network with abundant labels, draws increasing attention recently. To address
CNNC, we propose a domain-adaptive message passing graph neural network
(DM-GNN), which integrates graph neural network (GNN) with conditional
adversarial domain adaptation. DM-GNN is capable of learning informative
representations for node classification that are also transferrable across
networks. Firstly, a GNN encoder is constructed by dual feature extractors to
separate ego-embedding learning from neighbor-embedding learning so as to
jointly capture commonality and discrimination between connected nodes.
Secondly, a label propagation node classifier is proposed to refine each node's
label prediction by combining its own prediction and its neighbors' prediction.
In addition, a label-aware propagation scheme is devised for the labeled source
network to promote intra-class propagation while avoiding inter-class
propagation, thus yielding label-discriminative source embeddings. Thirdly,
conditional adversarial domain adaptation is performed to take the
neighborhood-refined class-label information into account during adversarial
domain adaptation, so that the class-conditional distributions across networks
can be better matched. Comparisons with eleven state-of-the-art methods
demonstrate the effectiveness of the proposed DM-GNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xiao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1"&gt;Kup-Sze Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xi Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16481</id>
        <link href="http://arxiv.org/abs/2308.16481"/>
        <updated>2023-09-02T00:40:02.182Z</updated>
        <summary type="html"><![CDATA[We present Point-TTA, a novel test-time adaptation framework for point cloud
registration (PCR) that improves the generalization and the performance of
registration models. While learning-based approaches have achieved impressive
progress, generalization to unknown testing environments remains a major
challenge due to the variations in 3D scans. Existing methods typically train a
generic model and the same trained model is applied on each instance during
testing. This could be sub-optimal since it is difficult for the same model to
handle all the variations during testing. In this paper, we propose a test-time
adaptation approach for PCR. Our model can adapt to unseen distributions at
test-time without requiring any prior knowledge of the test data. Concretely,
we design three self-supervised auxiliary tasks that are optimized jointly with
the primary PCR task. Given a test instance, we adapt our model using these
auxiliary tasks and the updated model is used to perform the inference. During
training, our model is trained using a meta-auxiliary learning approach, such
that the adapted model via auxiliary tasks improves the accuracy of the primary
task. Experimental results demonstrate the effectiveness of our approach in
improving generalization of point cloud registration and outperforming other
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatem_A/0/1/0/all/0/1"&gt;Ahmed Hatem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yiming Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16484</id>
        <link href="http://arxiv.org/abs/2308.16484"/>
        <updated>2023-09-02T00:40:02.180Z</updated>
        <summary type="html"><![CDATA[Affordable 3D scanners often produce sparse and non-uniform point clouds that
negatively impact downstream applications in robotic systems. While existing
point cloud upsampling architectures have demonstrated promising results on
standard benchmarks, they tend to experience significant performance drops when
the test data have different distributions from the training data. To address
this issue, this paper proposes a test-time adaption approach to enhance model
generality of point cloud upsampling. The proposed approach leverages
meta-learning to explicitly learn network parameters for test-time adaption.
Our method does not require any prior information about the test data. During
meta-training, the model parameters are learned from a collection of
instance-level tasks, each of which consists of a sparse-dense pair of point
clouds from the training data. During meta-testing, the trained model is
fine-tuned with a few gradient updates to produce a unique set of network
parameters for each test instance. The updated model is then used for the final
prediction. Our framework is generic and can be applied in a plug-and-play
manner with existing backbone networks in point cloud upsampling. Extensive
experiments demonstrate that our approach improves the performance of
state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatem_A/0/1/0/all/0/1"&gt;Ahmed Hatem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yiming Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks. (arXiv:2308.16385v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16385</id>
        <link href="http://arxiv.org/abs/2308.16385"/>
        <updated>2023-09-02T00:40:02.179Z</updated>
        <summary type="html"><![CDATA[To handle graphs in which features or connectivities are evolving over time,
a series of temporal graph neural networks (TGNNs) have been proposed. Despite
the success of these TGNNs, the previous TGNN evaluations reveal several
limitations regarding four critical issues: 1) inconsistent datasets, 2)
inconsistent evaluation pipelines, 3) lacking workload diversity, and 4)
lacking efficient comparison. Overall, there lacks an empirical study that puts
TGNN models onto the same ground and compares them comprehensively. To this
end, we propose BenchTemp, a general benchmark for evaluating TGNN models on
various workloads. BenchTemp provides a set of benchmark datasets so that
different TGNN models can be fairly compared. Further, BenchTemp engineers a
standard pipeline that unifies the TGNN evaluation. With BenchTemp, we
extensively compare the representative TGNN models on different tasks (e.g.,
link prediction and node classification) and settings (transductive and
inductive), w.r.t. both effectiveness and efficiency metrics. We have made
BenchTemp publicly available at https://github.com/qianghuangwhu/benchtemp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_X/0/1/0/all/0/1"&gt;Xi Susie Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhichao Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zitao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yongjun He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Quanqing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1"&gt;Chuang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_S/0/1/0/all/0/1"&gt;Shuo Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2308.16491</id>
        <link href="http://arxiv.org/abs/2308.16491"/>
        <updated>2023-09-02T00:40:02.179Z</updated>
        <summary type="html"><![CDATA[Science is facing a reproducibility crisis. Previous work has proposed
incorporating data analysis replications into classrooms as a potential
solution. However, despite the potential benefits, it is unclear whether this
approach is feasible, and if so, what the involved stakeholders-students,
educators, and scientists-should expect from it. Can students perform a data
analysis replication over the course of a class? What are the costs and
benefits for educators? And how can this solution help benchmark and improve
the state of science?

In the present study, we incorporated data analysis replications in the
project component of the Applied Data Analysis course (CS-401) taught at EPFL
(N=354 students). Here we report pre-registered findings based on surveys
administered throughout the course. First, we demonstrate that students can
replicate previously published scientific papers, most of them qualitatively
and some exactly. We find discrepancies between what students expect of data
analysis replications and what they experience by doing them along with changes
in expectations about reproducibility, which together serve as evidence of
attitude shifts to foster students' critical thinking. Second, we provide
information for educators about how much overhead is needed to incorporate
replications into the classroom and identify concerns that replications bring
as compared to more traditional assignments. Third, we identify tangible
benefits of the in-class data analysis replications for scientific communities,
such as a collection of replication reports and insights about replication
barriers in scientific work that should be avoided going forward.

Overall, we demonstrate that incorporating replication tasks into a large
data science class can increase the reproducibility of scientific work as a
by-product of data science instruction, thus benefiting both science and
students.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1"&gt;Kristina Gligoric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccardi_T/0/1/0/all/0/1"&gt;Tiziano Piccardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofman_J/0/1/0/all/0/1"&gt;Jake Hofman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1"&gt;Robert West&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backpropagation through Back Substitution with a Backslash. (arXiv:2303.15449v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.15449</id>
        <link href="http://arxiv.org/abs/2303.15449"/>
        <updated>2023-09-02T00:40:02.176Z</updated>
        <summary type="html"><![CDATA[We present a linear algebra formulation of backpropagation which allows the
calculation of gradients by using a generically written ``backslash'' or
Gaussian elimination on triangular systems of equations. Generally, the matrix
elements are operators. This paper has three contributions: (i) it is of
intellectual value to replace traditional treatments of automatic
differentiation with a (left acting) operator theoretic, graph-based approach;
(ii) operators can be readily placed in matrices in software in programming
languages such as Julia as an implementation option; (iii) we introduce a novel
notation, ``transpose dot'' operator ``$\{\}^{T_\bullet}$'' that allows for the
reversal of operators.

We further demonstrate the elegance of the operators approach in a suitable
programming language consisting of generic linear algebra operators such as
Julia \cite{bezanson2017julia}, and that it is possible to realize this
abstraction in code. Our implementation shows how generic linear algebra can
allow operators as elements of matrices. In contrast to ``operator
overloading,'' where backslash would normally have to be rewritten to take
advantage of operators, with ``generic programming'' there is no such need.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Edelman_A/0/1/0/all/0/1"&gt;Alan Edelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Akyurek_E/0/1/0/all/0/1"&gt;Ekin Akyurek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Enhanced Graph Neural Networks for Graph Completion. (arXiv:2303.15487v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.15487</id>
        <link href="http://arxiv.org/abs/2303.15487"/>
        <updated>2023-09-02T00:40:02.176Z</updated>
        <summary type="html"><![CDATA[Graph data is omnipresent and has a wide variety of applications, such as in
natural science, social networks, or the semantic web. However, while being
rich in information, graphs are often noisy and incomplete. As a result, graph
completion tasks, such as node classification or link prediction, have gained
attention. On one hand, neural methods, such as graph neural networks, have
proven to be robust tools for learning rich representations of noisy graphs. On
the other hand, symbolic methods enable exact reasoning on graphs.We propose
Knowledge Enhanced Graph Neural Networks (KeGNN), a neuro-symbolic framework
for graph completion that combines both paradigms as it allows for the
integration of prior knowledge into a graph neural network model.Essentially,
KeGNN consists of a graph neural network as a base upon which knowledge
enhancement layers are stacked with the goal of refining predictions with
respect to prior knowledge.We instantiate KeGNN in conjunction with two
state-of-the-art graph neural networks, Graph Convolutional Networks and Graph
Attention Networks, and evaluate KeGNN on multiple benchmark datasets for node
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Werner_L/0/1/0/all/0/1"&gt;Luisa Werner&lt;/a&gt; (TYREX, UGA), &lt;a href="http://arxiv.org/find/cs/1/au:+Layaida_N/0/1/0/all/0/1"&gt;Nabil Laya&amp;#xef;da&lt;/a&gt; (TYREX), &lt;a href="http://arxiv.org/find/cs/1/au:+Geneves_P/0/1/0/all/0/1"&gt;Pierre Genev&amp;#xe8;s&lt;/a&gt; (CNRS, TYREX), &lt;a href="http://arxiv.org/find/cs/1/au:+Chlyah_S/0/1/0/all/0/1"&gt;Sarah Chlyah&lt;/a&gt; (TYREX)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.14424</id>
        <link href="http://arxiv.org/abs/2212.14424"/>
        <updated>2023-09-02T00:40:02.170Z</updated>
        <summary type="html"><![CDATA[Normalizing flow is a class of deep generative models for efficient sampling
and density estimation. In practice, the flow often appears as a chain of
invertible neural network blocks; to facilitate training, existing works have
regularized flow trajectories and designed special network architectures. The
current paper develops a neural ODE flow network inspired by the
Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise
training of the residual blocks without sampling SDE trajectories or inner
loops of score matching or variational learning. As the JKO scheme unfolds the
dynamic of gradient flow, the proposed model naturally stacks residual network
blocks one by one, reducing the memory load and difficulty in performing
end-to-end deep flow network training. We also develop adaptive time
reparameterization of the flow network with a progressive refinement of the
trajectory in probability space, which improves the model training efficiency
and accuracy in practice. Using numerical experiments with synthetic and real
data, we show that the proposed JKO-iFlow model achieves similar or better
performance in generating new samples compared with the existing flow and
diffusion models at a significantly reduced computational and memory cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neuronal diversity can improve machine learning for physics and beyond. (arXiv:2204.04348v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.04348</id>
        <link href="http://arxiv.org/abs/2204.04348"/>
        <updated>2023-09-02T00:40:02.169Z</updated>
        <summary type="html"><![CDATA[Diversity conveys advantages in nature, yet homogeneous neurons typically
comprise the layers of artificial neural networks. Here we construct neural
networks from neurons that learn their own activation functions, quickly
diversify, and subsequently outperform their homogeneous counterparts on image
classification and nonlinear regression tasks. Sub-networks instantiate the
neurons, which meta-learn especially efficient sets of nonlinear responses.
Examples include conventional neural networks classifying digits and
forecasting a van der Pol oscillator and physics-informed Hamiltonian neural
networks learning H\'enon-Heiles stellar orbits and the swing of a video
recorded pendulum clock. Such \textit{learned diversity} provides examples of
dynamical systems selecting diversity over uniformity and elucidates the role
of diversity in natural and artificial systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_A/0/1/0/all/0/1"&gt;Anshul Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1"&gt;Anil Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindner_J/0/1/0/all/0/1"&gt;John F. Lindner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Sudeshna Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ditto_W/0/1/0/all/0/1"&gt;William L. Ditto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[System identification of neural systems: If we got it right, would we know?. (arXiv:2302.06677v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.06677</id>
        <link href="http://arxiv.org/abs/2302.06677"/>
        <updated>2023-09-02T00:40:02.169Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks are being proposed as models of parts of the
brain. The networks are compared to recordings of biological neurons, and good
performance in reproducing neural responses is considered to support the
model's validity. A key question is how much this system identification
approach tells us about brain computation. Does it validate one model
architecture over another? We evaluate the most commonly used comparison
techniques, such as a linear encoding model and centered kernel alignment, to
correctly identify a model by replacing brain recordings with known ground
truth models. System identification performance is quite variable; it also
depends significantly on factors independent of the ground truth architecture,
such as stimuli images. In addition, we show the limitations of using
functional similarity scores in identifying higher-level architectural motifs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yena Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Poggio_T/0/1/0/all/0/1"&gt;Tomaso Poggio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cheung_B/0/1/0/all/0/1"&gt;Brian Cheung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing between the Local and Global Structures (LGS) in Graph Embedding. (arXiv:2308.16403v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2308.16403</id>
        <link href="http://arxiv.org/abs/2308.16403"/>
        <updated>2023-09-02T00:40:02.168Z</updated>
        <summary type="html"><![CDATA[We present a method for balancing between the Local and Global Structures
(LGS) in graph embedding, via a tunable parameter. Some embedding methods aim
to capture global structures, while others attempt to preserve local
neighborhoods. Few methods attempt to do both, and it is not always possible to
capture well both local and global information in two dimensions, which is
where most graph drawing live. The choice of using a local or a global
embedding for visualization depends not only on the task but also on the
structure of the underlying data, which may not be known in advance. For a
given graph, LGS aims to find a good balance between the local and global
structure to preserve. We evaluate the performance of LGS with synthetic and
real-world datasets and our results indicate that it is competitive with the
state-of-the-art methods, using established quality metrics such as stress and
neighborhood preservation. We introduce a novel quality metric, cluster
distance preservation, to assess intermediate structure capture. All
source-code, datasets, experiments and analysis are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;Jacob Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huroyan_V/0/1/0/all/0/1"&gt;Vahan Huroyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1"&gt;Stephen Kobourov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training. (arXiv:2308.16453v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2308.16453</id>
        <link href="http://arxiv.org/abs/2308.16453"/>
        <updated>2023-09-02T00:40:02.168Z</updated>
        <summary type="html"><![CDATA[Mobile Internet has profoundly reshaped modern lifestyles in various aspects.
Encrypted Traffic Classification (ETC) naturally plays a crucial role in
managing mobile Internet, especially with the explosive growth of mobile apps
using encrypted communication. Despite some existing learning-based ETC methods
showing promising results, three-fold limitations still remain in real-world
network environments, 1) label bias caused by traffic class imbalance, 2)
traffic homogeneity caused by component sharing, and 3) training with reliance
on sufficient labeled traffic. None of the existing ETC methods can address all
these limitations. In this paper, we propose a novel Pre-trAining
Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the
original train dataset and perform contrastive pre-training without using
individual app labels directly to avoid label bias issues caused by class
imbalance, while obtaining a robust feature representation to differentiate
overlapping homogeneous traffic by pulling positive traffic pairs closer and
pushing negative pairs away. Meanwhile, PASS designs a semi-supervised
optimization strategy based on pseudo-label iteration and dynamic loss
weighting algorithms in order to effectively utilize massive unlabeled traffic
data and alleviate manual train dataset annotation workload. PASS outperforms
state-of-the-art ETC methods and generic sampling approaches on four public
datasets with significant class imbalance and traffic homogeneity, remarkably
pushing the F1 of Cross-Platform215 with 1.31%, ISCX-17 with 9.12%.
Furthermore, we validate the generality of the contrastive pre-training and
pseudo-label iteration components of PASS, which can adaptively benefit ETC
methods with diverse feature extractors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Juncheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qige Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1"&gt;Yafei Sang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuyuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongzheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Least Squares Maximum and Weighted Generalization-Memorization Machines. (arXiv:2308.16456v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16456</id>
        <link href="http://arxiv.org/abs/2308.16456"/>
        <updated>2023-09-02T00:40:02.168Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new way of remembering by introducing a memory
influence mechanism for the least squares support vector machine (LSSVM).
Without changing the equation constraints of the original LSSVM, this
mechanism, allows an accurate partitioning of the training set without
overfitting. The maximum memory impact model (MIMM) and the weighted impact
memory model (WIMM) are then proposed. It is demonstrated that these models can
be degraded to the LSSVM. Furthermore, we propose some different memory impact
functions for the MIMM and WIMM. The experimental results show that that our
MIMM and WIMM have better generalization performance compared to the LSSVM and
significant advantage in time cost compared to other memory models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuan-Hai Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computing excited states of molecules using normalizing flows. (arXiv:2308.16468v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2308.16468</id>
        <link href="http://arxiv.org/abs/2308.16468"/>
        <updated>2023-09-02T00:40:02.167Z</updated>
        <summary type="html"><![CDATA[We present a new nonlinear variational framework for simultaneously computing
ground and excited states of quantum systems. Our approach is based on
approximating wavefunctions in the linear span of basis functions that are
augmented and optimized \emph{via} composition with normalizing flows. The
accuracy and efficiency of our approach are demonstrated in the calculations of
a large number of vibrational states of the triatomic H$_2$S molecule as well
as ground and several excited electronic states of prototypical one-electron
systems including the hydrogen atom, the molecular hydrogen ion, and a carbon
atom in a single-active-electron approximation. The results demonstrate
significant improvements in the accuracy of energy predictions and accelerated
basis-set convergence even when using normalizing flows with a small number of
parameters. The present approach can be also seen as the optimization of a set
of intrinsic coordinates that best capture the underlying physics within the
given basis set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Saleh_Y/0/1/0/all/0/1"&gt;Yahya Saleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Corral_A/0/1/0/all/0/1"&gt;&amp;#xc1;lvaro Fern&amp;#xe1;ndez Corral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Iske_A/0/1/0/all/0/1"&gt;Armin Iske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kupper_J/0/1/0/all/0/1"&gt;Jochen K&amp;#xfc;pper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yachmenev_A/0/1/0/all/0/1"&gt;Andrey Yachmenev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2212.14424</id>
        <link href="http://arxiv.org/abs/2212.14424"/>
        <updated>2023-09-02T00:40:02.167Z</updated>
        <summary type="html"><![CDATA[Normalizing flow is a class of deep generative models for efficient sampling
and density estimation. In practice, the flow often appears as a chain of
invertible neural network blocks; to facilitate training, existing works have
regularized flow trajectories and designed special network architectures. The
current paper develops a neural ODE flow network inspired by the
Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise
training of the residual blocks without sampling SDE trajectories or inner
loops of score matching or variational learning. As the JKO scheme unfolds the
dynamic of gradient flow, the proposed model naturally stacks residual network
blocks one by one, reducing the memory load and difficulty in performing
end-to-end deep flow network training. We also develop adaptive time
reparameterization of the flow network with a progressive refinement of the
trajectory in probability space, which improves the model training efficiency
and accuracy in practice. Using numerical experiments with synthetic and real
data, we show that the proposed JKO-iFlow model achieves similar or better
performance in generating new samples compared with the existing flow and
diffusion models at a significantly reduced computational and memory cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction. (arXiv:2308.16259v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16259</id>
        <link href="http://arxiv.org/abs/2308.16259"/>
        <updated>2023-09-02T00:40:02.166Z</updated>
        <summary type="html"><![CDATA[Recently, the remarkable capabilities of large language models (LLMs) have
been illustrated across a variety of research domains such as natural language
processing, computer vision, and molecular modeling. We extend this paradigm by
utilizing LLMs for material property prediction by introducing our model
Materials Informatics Transformer (MatInFormer). Specifically, we introduce a
novel approach that involves learning the grammar of crystallography through
the tokenization of pertinent space group information. We further illustrate
the adaptability of MatInFormer by incorporating task-specific data pertaining
to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover
the key features that the model prioritizes during property prediction. The
effectiveness of our proposed model is empirically validated across 14 distinct
datasets, hereby underscoring its potential for high throughput screening
through accurate material property prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hongshuo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magar_R/0/1/0/all/0/1"&gt;Rishikesh Magar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1"&gt;Amir Bariti Farimani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint. (arXiv:2308.16425v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16425</id>
        <link href="http://arxiv.org/abs/2308.16425"/>
        <updated>2023-09-02T00:40:02.166Z</updated>
        <summary type="html"><![CDATA[Implicit neural networks have demonstrated remarkable success in various
tasks. However, there is a lack of theoretical analysis of the connections and
differences between implicit and explicit networks. In this paper, we study
high-dimensional implicit neural networks and provide the high dimensional
equivalents for the corresponding conjugate kernels and neural tangent kernels.
Built upon this, we establish the equivalence between implicit and explicit
networks in high dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zenan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1"&gt;Zhenyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Robert C. Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2308.16336</id>
        <link href="http://arxiv.org/abs/2308.16336"/>
        <updated>2023-09-02T00:40:02.158Z</updated>
        <summary type="html"><![CDATA[We present ToddlerBERTa, a BabyBERTa-like language model, exploring its
capabilities through five different models with varied hyperparameters.
Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the
BabyLM challenge, we find that smaller models can excel in specific tasks,
while larger models perform well with substantial data. Despite training on a
smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling
the state-of-the-art RoBERTa-base. The model showcases robust language
understanding, even with single-sentence pretraining, and competes with
baselines that leverage broader contextual information. Our work provides
insights into hyperparameter choices, and data utilization, contributing to the
advancement of language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cagatan_O/0/1/0/all/0/1"&gt;Omer Veysel Cagatan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Analysis for the Subgradient Methods Minimizing Composite Nonconvex, Nonsmooth and Non-Lipschitz Functions. (arXiv:2308.16362v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2308.16362</id>
        <link href="http://arxiv.org/abs/2308.16362"/>
        <updated>2023-09-02T00:40:02.158Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a proximal subgradient method (Prox-SubGrad) for
solving nonconvex and nonsmooth optimization problems without assuming
Lipschitz continuity conditions. A number of subgradient upper bounds and their
relationships are presented. By means of these upper bounding conditions, we
establish some uniform recursive relations for the Moreau envelopes for weakly
convex optimization. This uniform scheme simplifies and unifies the proof
schemes to establish rate of convergence for Prox-SubGrad without assuming
Lipschitz continuity. We present a novel convergence analysis in this context.
Furthermore, we propose some new stochastic subgradient upper bounding
conditions and establish convergence and iteration complexity rates for the
stochastic subgradient method (Sto-SubGrad) to solve non-Lipschitz and
nonsmooth stochastic optimization problems. In particular, for both
deterministic and stochastic subgradient methods on weakly convex optimization
problems without Lipschitz continuity, under any of the subgradient upper
bounding conditions to be introduced in the paper, we show that $O(1/\sqrt{T})$
convergence rate holds in terms of the square of gradient of the Moreau
envelope function, which further improves to be $O(1/{T})$ if, in addition, the
uniform KL condition with exponent $1/2$ holds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhu_D/0/1/0/all/0/1"&gt;Daoli Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuzhong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff. (arXiv:2308.16454v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16454</id>
        <link href="http://arxiv.org/abs/2308.16454"/>
        <updated>2023-09-02T00:40:02.158Z</updated>
        <summary type="html"><![CDATA[This paper addresses the tradeoff between standard accuracy on clean examples
and robustness against adversarial examples in deep neural networks (DNNs).
Although adversarial training (AT) improves robustness, it degrades the
standard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we
propose a novel AT method called ARREST, which comprises three components: (i)
adversarial finetuning (AFT), (ii) representation-guided knowledge distillation
(RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples
by initializing its parameters with a DNN that is standardly pretrained on
clean examples. RGKD and NR respectively entail a regularization term and an
algorithm to preserve latent representations of clean examples during AFT. RGKD
penalizes the distance between the representations of the standardly pretrained
and AFT DNNs. NR switches input adversarial examples to nonadversarial ones
when the representation changes significantly during AFT. By combining these
components, ARREST achieves both high standard accuracy and robustness.
Experimental results demonstrate that ARREST mitigates the tradeoff more
effectively than previous AT-based methods do.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1"&gt;Satoshi Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeda_S/0/1/0/all/0/1"&gt;Shoichiro Takeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1"&gt;Naoki Makishima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ando_A/0/1/0/all/0/1"&gt;Atsushi Ando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1"&gt;Ryo Masumura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art. (arXiv:2308.16316v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16316</id>
        <link href="http://arxiv.org/abs/2308.16316"/>
        <updated>2023-09-02T00:40:02.157Z</updated>
        <summary type="html"><![CDATA[Since their inception in 2014, Generative Adversarial Networks (GANs) have
rapidly emerged as powerful tools for generating realistic and diverse data
across various domains, including computer vision and other applied areas.
Consisting of a discriminative network and a generative network engaged in a
Minimax game, GANs have revolutionized the field of generative modeling. In
February 2018, GAN secured the leading spot on the ``Top Ten Global
Breakthrough Technologies List'' issued by the Massachusetts Science and
Technology Review. Over the years, numerous advancements have been proposed,
leading to a rich array of GAN variants, such as conditional GAN, Wasserstein
GAN, CycleGAN, and StyleGAN, among many others. This survey aims to provide a
general overview of GANs, summarizing the latent architecture, validation
metrics, and application areas of the most widely recognized variants. We also
delve into recent theoretical developments, exploring the profound connection
between the adversarial principle underlying GAN and Jensen-Shannon divergence,
while discussing the optimality characteristics of the GAN framework. The
efficiency of GAN variants and their model architectures will be evaluated
along with training obstacles as well as training solutions. In addition, a
detailed discussion will be provided, examining the integration of GANs with
newly developed deep learning frameworks such as Transformers, Physics-Informed
Neural Networks, Large Language models, and Diffusion models. Finally, we
reveal several issues as well as future research outlines in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanujit Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+S_U/0/1/0/all/0/1"&gt;Ujjwal Reddy K S&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1"&gt;Shraddha M. Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panja_M/0/1/0/all/0/1"&gt;Madhurima Panja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manvitha_B/0/1/0/all/0/1"&gt;Bayapureddy Manvitha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction. (arXiv:2308.16437v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2308.16437</id>
        <link href="http://arxiv.org/abs/2308.16437"/>
        <updated>2023-09-02T00:40:02.157Z</updated>
        <summary type="html"><![CDATA[Click-through rate (CTR) prediction is a crucial issue in recommendation
systems. There has been an emergence of various public CTR datasets. However,
existing datasets primarily suffer from the following limitations. Firstly,
users generally click different types of items from multiple scenarios, and
modeling from multiple scenarios can provide a more comprehensive understanding
of users. Existing datasets only include data for the same type of items from a
single scenario. Secondly, multi-modal features are essential in multi-scenario
prediction as they address the issue of inconsistent ID encoding between
different scenarios. The existing datasets are based on ID features and lack
multi-modal features. Third, a large-scale dataset can provide a more reliable
evaluation of models, fully reflecting the performance differences between
models. The scale of existing datasets is around 100 million, which is
relatively small compared to the real-world CTR prediction. To address these
limitations, we propose AntM$^{2}$C, a Multi-Scenario Multi-Modal CTR dataset
based on industrial data from Alipay. Specifically, AntM$^{2}$C provides the
following advantages: 1) It covers CTR data of 5 different types of items,
providing insights into the preferences of users for different items, including
advertisements, vouchers, mini-programs, contents, and videos. 2) Apart from
ID-based features, AntM$^{2}$C also provides 2 multi-modal features, raw text
and image features, which can effectively establish connections between items
with different IDs. 3) AntM$^{2}$C provides 1 billion CTR data with 200
features, including 200 million users and 6 million items. It is currently the
largest-scale CTR dataset available. Based on AntM$^{2}$C, we construct several
typical CTR tasks and provide comparisons with baseline methods. The dataset
homepage is available at https://www.atecup.cn/home.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huan_Z/0/1/0/all/0/1"&gt;Zhaoxin Huan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Ke Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaolu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1"&gt;Xu Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1"&gt;Linjian Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjie Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhongyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Wenliang Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guannan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Image-based Generative Adversarial Networks for Time Series Generation. (arXiv:2112.08060v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.08060</id>
        <link href="http://arxiv.org/abs/2112.08060"/>
        <updated>2023-09-02T00:40:02.157Z</updated>
        <summary type="html"><![CDATA[Generative models for images have gained significant attention in computer
vision and natural language processing due to their ability to generate
realistic samples from complex data distributions. To leverage the advances of
image-based generative models for the time series domain, we propose a
two-dimensional image representation for time series, the Extended
Intertemporal Return Plot (XIRP). Our approach captures the intertemporal time
series dynamics in a scale-invariant and invertible way, reducing training time
and improving sample quality. We benchmark synthetic XIRPs obtained by an
off-the-shelf Wasserstein GAN with gradient penalty (WGAN-GP) to other image
representations and models regarding similarity and predictive ability metrics.
Our novel, validated image representation for time series consistently and
significantly outperforms a state-of-the-art RNN-based generative model
regarding predictive ability. Further, we introduce an improved stochastic
inversion to substantially improve simulation quality regardless of the
representation and provide the prospect of transfer potentials in other
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hellermann_J/0/1/0/all/0/1"&gt;Justin Hellermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lessmann_S/0/1/0/all/0/1"&gt;Stefan Lessmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergence of Segmentation with Minimalistic White-Box Transformers. (arXiv:2308.16271v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16271</id>
        <link href="http://arxiv.org/abs/2308.16271"/>
        <updated>2023-09-02T00:40:02.156Z</updated>
        <summary type="html"><![CDATA[Transformer-like models for vision tasks have recently proven effective for a
wide range of downstream applications such as segmentation and detection.
Previous works have shown that segmentation properties emerge in vision
transformers (ViTs) trained using self-supervised methods such as DINO, but not
in those trained on supervised classification tasks. In this study, we probe
whether segmentation emerges in transformer-based models solely as a result of
intricate self-supervised learning mechanisms, or if the same emergence can be
achieved under much broader conditions through proper design of the model
architecture. Through extensive experimental results, we demonstrate that when
employing a white-box transformer-like architecture known as CRATE, whose
design explicitly models and pursues low-dimensional structures in the data
distribution, segmentation properties, at both the whole and parts levels,
already emerge with a minimalistic supervised training recipe. Layer-wise
finer-grained analysis reveals that the emergent properties strongly
corroborate the designed mathematical functions of the white-box network. Our
results suggest a path to design white-box foundation models that are
simultaneously highly performant and mathematically fully interpretable. Code
is at \url{https://github.com/Ma-Lab-Berkeley/CRATE}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1"&gt;Tianzhe Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1"&gt;Shengbang Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1"&gt;Druv Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buchanan_S/0/1/0/all/0/1"&gt;Sam Buchanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition. (arXiv:2308.16207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16207</id>
        <link href="http://arxiv.org/abs/2308.16207"/>
        <updated>2023-09-02T00:40:02.155Z</updated>
        <summary type="html"><![CDATA[Emotion recognition using electroencephalogram (EEG) mainly has two
scenarios: classification of the discrete labels and regression of the
continuously tagged labels. Although many algorithms were proposed for
classification tasks, there are only a few methods for regression tasks. For
emotion regression, the label is continuous in time. A natural method is to
learn the temporal dynamic patterns. In previous studies, long short-term
memory (LSTM) and temporal convolutional neural networks (TCN) were utilized to
learn the temporal contextual information from feature vectors of EEG. However,
the spatial patterns of EEG were not effectively extracted. To enable the
spatial learning ability of TCN towards better regression and classification
performances, we propose a novel unified model, named MASA-TCN, for EEG emotion
regression and classification tasks. The space-aware temporal layer enables TCN
to additionally learn from spatial relations among EEG electrodes. Besides, a
novel multi-anchor block with attentive fusion is proposed to learn dynamic
temporal dependencies. Experiments on two publicly available datasets show
MASA-TCN achieves higher results than the state-of-the-art methods for both EEG
emotion regression and classification tasks. The code is available at
https://github.com/yi-ding-cs/MASA-TCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Su Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chuangao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16375</id>
        <link href="http://arxiv.org/abs/2308.16375"/>
        <updated>2023-09-02T00:40:02.154Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have gained significant attention owing to their
ability to handle graph-structured data and the improvement in practical
applications. However, many of these models prioritize high utility
performance, such as accuracy, with a lack of privacy consideration, which is a
major concern in modern society where privacy attacks are rampant. To address
this issue, researchers have started to develop privacy-preserving GNNs.
Despite this progress, there is a lack of a comprehensive overview of the
attacks and the techniques for preserving privacy in the graph domain. In this
survey, we aim to address this gap by summarizing the attacks on graph data
according to the targeted information, categorizing the privacy preservation
techniques in GNNs, and reviewing the datasets and applications that could be
used for analyzing/solving privacy issues in GNNs. We also outline potential
directions for future research in order to build better privacy-preserving
GNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuying Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhaoqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotevska_O/0/1/0/all/0/1"&gt;Olivera Kotevska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1"&gt;Tyler Derr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GRASP: A Goodness-of-Fit Test for Classification Learning. (arXiv:2209.02064v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.02064</id>
        <link href="http://arxiv.org/abs/2209.02064"/>
        <updated>2023-09-02T00:40:02.154Z</updated>
        <summary type="html"><![CDATA[Performance of classifiers is often measured in terms of average accuracy on
test data. Despite being a standard measure, average accuracy fails in
characterizing the fit of the model to the underlying conditional law of labels
given the features vector ($Y|X$), e.g. due to model misspecification, over
fitting, and high-dimensionality. In this paper, we consider the fundamental
problem of assessing the goodness-of-fit for a general binary classifier. Our
framework does not make any parametric assumption on the conditional law $Y|X$,
and treats that as a black box oracle model which can be accessed only through
queries. We formulate the goodness-of-fit assessment problem as a tolerance
hypothesis testing of the form \[ H_0: \mathbb{E}\Big[D_f\Big({\sf
Bern}(\eta(X))\|{\sf Bern}(\hat{\eta}(X))\Big)\Big]\leq \tau\,, \] where $D_f$
represents an $f$-divergence function, and $\eta(x)$, $\hat{\eta}(x)$
respectively denote the true and an estimate likelihood for a feature vector
$x$ admitting a positive label. We propose a novel test, called \grasp for
testing $H_0$, which works in finite sample settings, no matter the features
(distribution-free). We also propose model-X \grasp designed for model-X
settings where the joint distribution of the features vector is known. Model-X
\grasp uses this distributional information to achieve better power. We
evaluate the performance of our tests through extensive numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Javanmard_A/0/1/0/all/0/1"&gt;Adel Javanmard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mehrabi_M/0/1/0/all/0/1"&gt;Mohammad Mehrabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Least Squares Maximum and Weighted Generalization-Memorization Machines. (arXiv:2308.16456v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16456</id>
        <link href="http://arxiv.org/abs/2308.16456"/>
        <updated>2023-09-02T00:40:02.154Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new way of remembering by introducing a memory
influence mechanism for the least squares support vector machine (LSSVM).
Without changing the equation constraints of the original LSSVM, this
mechanism, allows an accurate partitioning of the training set without
overfitting. The maximum memory impact model (MIMM) and the weighted impact
memory model (WIMM) are then proposed. It is demonstrated that these models can
be degraded to the LSSVM. Furthermore, we propose some different memory impact
functions for the MIMM and WIMM. The experimental results show that that our
MIMM and WIMM have better generalization performance compared to the LSSVM and
significant advantage in time cost compared to other memory models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuan-Hai Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal-spatial model via Trend Filtering. (arXiv:2308.16172v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.16172</id>
        <link href="http://arxiv.org/abs/2308.16172"/>
        <updated>2023-09-02T00:40:02.153Z</updated>
        <summary type="html"><![CDATA[This research focuses on the estimation of a non-parametric regression
function designed for data with simultaneous time and space dependencies. In
such a context, we study the Trend Filtering, a nonparametric estimator
introduced by \cite{mammen1997locally} and \cite{rudin1992nonlinear}. For
univariate settings, the signals we consider are assumed to have a kth weak
derivative with bounded total variation, allowing for a general degree of
smoothness. In the multivariate scenario, we study a $K$-Nearest Neighbor fused
lasso estimator as in \cite{padilla2018adaptive}, employing an ADMM algorithm,
suitable for signals with bounded variation that adhere to a piecewise
Lipschitz continuity criterion. By aligning with lower bounds, the minimax
optimality of our estimators is validated. A unique phase transition
phenomenon, previously uncharted in Trend Filtering studies, emerges through
our analysis. Both Simulation studies and real data applications underscore the
superior performance of our method when compared with established techniques in
the existing literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Padilla_C/0/1/0/all/0/1"&gt;Carlos Misael Madrid Padilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Padilla_O/0/1/0/all/0/1"&gt;Oscar Hernan Madrid Padilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_D/0/1/0/all/0/1"&gt;Daren Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16198</id>
        <link href="http://arxiv.org/abs/2308.16198"/>
        <updated>2023-09-02T00:40:02.149Z</updated>
        <summary type="html"><![CDATA[In modern communication systems, efficient and reliable information
dissemination is crucial for supporting critical operations across domains like
disaster response, autonomous vehicles, and sensor networks. This paper
introduces a Multi-Agent Reinforcement Learning (MARL) approach as a
significant step forward in achieving more decentralized, efficient, and
collaborative solutions. We propose a Decentralized-POMDP formulation for
information dissemination, empowering each agent to independently decide on
message forwarding. This constitutes a significant paradigm shift from
traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach
harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention
Networks (GAT) with dynamic attention to capture essential network features. We
propose two approaches, L-DGN and HL-DGN, which differ in the information that
is exchanged among agents. We evaluate the performance of our decentralized
approaches, by comparing them with a widely-used MPR heuristic, and we show
that our trained policies are able to efficiently cover the network while
bypassing the MPR set selection process. Our approach promises a first step
toward bolstering the resilience of real-world broadcast communication
infrastructures via learned, collaborative information dissemination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galliera_R/0/1/0/all/0/1"&gt;Raffaele Galliera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venable_K/0/1/0/all/0/1"&gt;Kristen Brent Venable&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bassani_M/0/1/0/all/0/1"&gt;Matteo Bassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suri_N/0/1/0/all/0/1"&gt;Niranjan Suri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RetroBridge: Modeling Retrosynthesis with Markov Bridges. (arXiv:2308.16212v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2308.16212</id>
        <link href="http://arxiv.org/abs/2308.16212"/>
        <updated>2023-09-02T00:40:02.149Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis planning is a fundamental challenge in chemistry which aims at
designing reaction pathways from commercially available starting materials to a
target molecule. Each step in multi-step retrosynthesis planning requires
accurate prediction of possible precursor molecules given the target molecule
and confidence estimates to guide heuristic search algorithms. We model
single-step retrosynthesis planning as a distribution learning problem in a
discrete state space. First, we introduce the Markov Bridge Model, a generative
framework aimed to approximate the dependency between two intractable discrete
distributions accessible via a finite sample of coupled data points. Our
framework is based on the concept of a Markov bridge, a Markov process pinned
at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does
not need a tractable noise distribution as a sampling proxy and directly
operates on the input product molecules as samples from the intractable prior
distribution. We then address the retrosynthesis planning problem with our
novel framework and introduce RetroBridge, a template-free retrosynthesis
modeling approach that achieves state-of-the-art results on standard evaluation
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Igashov_I/0/1/0/all/0/1"&gt;Ilia Igashov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Schneuing_A/0/1/0/all/0/1"&gt;Arne Schneuing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Segler_M/0/1/0/all/0/1"&gt;Marwin Segler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael Bronstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Correia_B/0/1/0/all/0/1"&gt;Bruno Correia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space. (arXiv:2303.05102v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2303.05102</id>
        <link href="http://arxiv.org/abs/2303.05102"/>
        <updated>2023-09-02T00:40:02.148Z</updated>
        <summary type="html"><![CDATA[One major challenge in machine learning applications is coping with
mismatches between the datasets used in the development and those obtained in
real-world applications. These mismatches may lead to inaccurate predictions
and errors, resulting in poor product quality and unreliable systems. In this
study, we propose StyleDiff to inform developers of the differences between the
two datasets for the steady development of machine learning systems. Using
disentangled image spaces obtained from recently proposed generative models,
StyleDiff compares the two datasets by focusing on attributes in the images and
provides an easy-to-understand analysis of the differences between the
datasets. The proposed StyleDiff performs in $O (d N\log N)$, where $N$ is the
size of the datasets and $d$ is the number of attributes, enabling the
application to large datasets. We demonstrate that StyleDiff accurately detects
differences between datasets and presents them in an understandable format
using, for example, driving scenes datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kawano_K/0/1/0/all/0/1"&gt;Keisuke Kawano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kutsuna_T/0/1/0/all/0/1"&gt;Takuro Kutsuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tokuhisa_R/0/1/0/all/0/1"&gt;Ryoko Tokuhisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nakamura_A/0/1/0/all/0/1"&gt;Akihiro Nakamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Esaki_Y/0/1/0/all/0/1"&gt;Yasushi Esaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biclustering Methods via Sparse Penalty. (arXiv:2308.14388v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.14388</id>
        <link href="http://arxiv.org/abs/2308.14388"/>
        <updated>2023-09-02T00:40:02.148Z</updated>
        <summary type="html"><![CDATA[In this paper, we first reviewed several biclustering methods that are used
to identify the most significant clusters in gene expression data. Here we
mainly focused on the SSVD(sparse SVD) method and tried a new sparse penalty
named "Prenet penalty" which has been used only in factor analysis to gain
sparsity. Then in the simulation study, we tried different types of generated
datasets (with different sparsity and dimension) and tried 1-layer
approximation then for k-layers which shows the mixed Prenet penalty is very
effective for non-overlapped data. Finally, we used some real gene expression
data to show the behavior of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Response Heteroscedastic Gaussian Process Models and Their Inference. (arXiv:2308.15370v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.15370</id>
        <link href="http://arxiv.org/abs/2308.15370"/>
        <updated>2023-09-02T00:40:02.147Z</updated>
        <summary type="html"><![CDATA[Despite the widespread utilization of Gaussian process models for versatile
nonparametric modeling, they exhibit limitations in effectively capturing
abrupt changes in function smoothness and accommodating relationships with
heteroscedastic errors. Addressing these shortcomings, the heteroscedastic
Gaussian process (HeGP) regression seeks to introduce flexibility by
acknowledging the variability of residual variances across covariates in the
regression model. In this work, we extend the HeGP concept, expanding its scope
beyond regression tasks to encompass classification and state-space models. To
achieve this, we propose a novel framework where the Gaussian process is
coupled with a covariate-induced precision matrix process, adopting a mixture
formulation. This approach enables the modeling of heteroscedastic covariance
functions across covariates. To mitigate the computational challenges posed by
sampling, we employ variational inference to approximate the posterior and
facilitate posterior predictive modeling. Additionally, our training process
leverages an EM algorithm featuring closed-form M-step updates to efficiently
evaluate the heteroscedastic covariance function. A notable feature of our
model is its consistent performance on multivariate responses, accommodating
various types (continuous or categorical) seamlessly. Through a combination of
simulations and real-world applications in climatology, we illustrate the
model's prowess and advantages. By overcoming the limitations of traditional
Gaussian process models, our proposed framework offers a robust and versatile
tool for a wide array of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taehee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun S. Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2308.16215</id>
        <link href="http://arxiv.org/abs/2308.16215"/>
        <updated>2023-09-02T00:40:02.146Z</updated>
        <summary type="html"><![CDATA[Lossy video compression is commonly used when transmitting and storing video
data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto}
standard, despite the availability of advanced (neural) compression approaches.
Transmitting videos in the face of dynamic network bandwidth conditions
requires video codecs to adapt to vastly different compression strengths. Rate
control modules augment the codec's compression such that bandwidth constraints
are satisfied and video distortion is minimized. While, both standard video
codes and their rate control modules are developed to minimize video distortion
w.r.t. human quality assessment, preserving the downstream performance of deep
vision models is not considered. In this paper, we present the first end-to-end
learnable deep video codec control considering both bandwidth constraints and
downstream vision performance, while not breaking existing standardization. We
demonstrate for two common vision tasks (semantic segmentation and optical flow
estimation) and on two different datasets that our deep codec control better
preserves downstream performance than using 2-pass average bit rate control
while meeting dynamic bandwidth constraints and adhering to standardizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Reich_C/0/1/0/all/0/1"&gt;Christoph Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Debnath_B/0/1/0/all/0/1"&gt;Biplob Debnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_D/0/1/0/all/0/1"&gt;Deep Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prangemeier_T/0/1/0/all/0/1"&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chakradhar_S/0/1/0/all/0/1"&gt;Srimat Chakradhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A numerical approach for the fractional Laplacian via deep neural networks. (arXiv:2308.16272v1 [math.AP])]]></title>
        <id>http://arxiv.org/abs/2308.16272</id>
        <link href="http://arxiv.org/abs/2308.16272"/>
        <updated>2023-09-02T00:40:02.144Z</updated>
        <summary type="html"><![CDATA[We consider the fractional elliptic problem with Dirichlet boundary
conditions on a bounded and convex domain $D$ of $\mathbb{R}^d$, with $d \geq
2$. In this paper, we perform a stochastic gradient descent algorithm that
approximates the solution of the fractional problem via Deep Neural Networks.
Additionally, we provide four numerical examples to test the efficiency of the
algorithm, and each example will be studied for many values of $\alpha \in
(1,2)$ and $d \geq 2$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Valenzuela_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s Valenzuela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers Meet Directed Graphs. (arXiv:2302.00049v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.00049</id>
        <link href="http://arxiv.org/abs/2302.00049"/>
        <updated>2023-09-02T00:40:02.144Z</updated>
        <summary type="html"><![CDATA[Transformers were originally proposed as a sequence-to-sequence model for
text but have become vital for a wide range of modalities, including images,
audio, video, and undirected graphs. However, transformers for directed graphs
are a surprisingly underexplored topic, despite their applicability to
ubiquitous domains, including source code and logic circuits. In this work, we
propose two direction- and structure-aware positional encodings for directed
graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware
generalization of the combinatorial Laplacian; (2) directional random walk
encodings. Empirically, we show that the extra directionality information is
useful in various downstream tasks, including correctness testing of sorting
networks and source code understanding. Together with a data-flow-centric graph
construction, our model outperforms the prior state of the art on the Open
Graph Benchmark Code2 relatively by 14.7%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geisler_S/0/1/0/all/0/1"&gt;Simon Geisler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1"&gt;Daniel Mankowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1"&gt;Ali Taylan Cemgil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paduraru_C/0/1/0/all/0/1"&gt;Cosmin Paduraru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems. (arXiv:2308.16904v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2308.16904</id>
        <link href="http://arxiv.org/abs/2308.16904"/>
        <updated>2023-09-02T00:40:02.131Z</updated>
        <summary type="html"><![CDATA[Large-scale linear systems, $Ax=b$, frequently arise in practice and demand
effective iterative solvers. Often, these systems are noisy due to operational
errors or faulty data-collection processes. In the past decade, the randomized
Kaczmarz (RK) algorithm has been studied extensively as an efficient iterative
solver for such systems. However, the convergence study of RK in the noisy
regime is limited and considers measurement noise in the right-hand side
vector, $b$. Unfortunately, in practice, that is not always the case; the
coefficient matrix $A$ can also be noisy. In this paper, we analyze the
convergence of RK for noisy linear systems when the coefficient matrix, $A$, is
corrupted with both additive and multiplicative noise, along with the noisy
vector, $b$. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger}
\|_2^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$
represents a noisy version of $A$. We claim that our analysis is robust and
realistically applicable, as we do not require information about the noiseless
coefficient matrix, $A$, and considering different conditions on noise, we can
control the convergence of RK. We substantiate our theoretical findings by
performing comprehensive numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bergou_E/0/1/0/all/0/1"&gt;El Houcine Bergou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Boucherouite_S/0/1/0/all/0/1"&gt;Soumia Boucherouite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Aritra Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_A/0/1/0/all/0/1"&gt;Anna Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MGNN: Graph Neural Networks Inspired by Distance Geometry Problem. (arXiv:2201.12994v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.12994</id>
        <link href="http://arxiv.org/abs/2201.12994"/>
        <updated>2023-09-02T00:40:02.131Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have emerged as a prominent research topic in
the field of machine learning. Existing GNN models are commonly categorized
into two types: spectral GNNs, which are designed based on polynomial graph
filters, and spatial GNNs, which utilize a message-passing scheme as the
foundation of the model. For the expressive power and universality of spectral
GNNs, a natural approach is to improve the design of basis functions for better
approximation ability. As for spatial GNNs, models like Graph Isomorphism
Networks (GIN) analyze their expressive power based on Graph Isomorphism Tests.
Recently, there have been attempts to establish connections between spatial
GNNs and geometric concepts like curvature and cellular sheaves, as well as
physical phenomena like oscillators. However, despite the recent progress,
there is still a lack of comprehensive analysis regarding the universality of
spatial GNNs from the perspectives of geometry and physics. In this paper, we
propose MetricGNN (MGNN), a spatial GNN model inspired by the
congruent-insensitivity property of classifiers in the classification phase of
GNNs. We demonstrate that a GNN model is universal in the spatial domain if it
can generate embedding matrices that are congruent to any given embedding
matrix. This property is closely related to the Distance Geometry Problem
(DGP). Since DGP is an NP-Hard combinatorial optimization problem, we propose
optimizing an energy function derived from spring networks and the
Multi-Dimensional Scaling (MDS) problem. This approach also allows our model to
handle both homophilic and heterophilic graphs. Finally, we propose employing
the iteration method to optimize our energy function. We extensively evaluate
the effectiveness of our model through experiments conducted on both synthetic
and real-world datasets. Our code is available at:
https://github.com/GuanyuCui/MGNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1"&gt;Guanyu Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhewei Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Semantic-Native Communication and Inference via Minimal Simplicial Structures. (arXiv:2308.16789v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2308.16789</id>
        <link href="http://arxiv.org/abs/2308.16789"/>
        <updated>2023-09-02T00:40:02.122Z</updated>
        <summary type="html"><![CDATA[In this work, we study the problem of semantic communication and inference,
in which a student agent (i.e. mobile device) queries a teacher agent (i.e.
cloud sever) to generate higher-order data semantics living in a simplicial
complex. Specifically, the teacher first maps its data into a k-order
simplicial complex and learns its high-order correlations. For effective
communication and inference, the teacher seeks minimally sufficient and
invariant semantic structures prior to conveying information. These minimal
simplicial structures are found via judiciously removing simplices selected by
the Hodge Laplacians without compromising the inference query accuracy.
Subsequently, the student locally runs its own set of queries based on a masked
simplicial convolutional autoencoder (SCAE) leveraging both local and remote
teacher's knowledge. Numerical results corroborate the effectiveness of the
proposed approach in terms of improving inference query accuracy under
different channel conditions and simplicial structures. Experiments on a
coauthorship dataset show that removing simplices by ranking the Laplacian
values yields a 85% reduction in payload size without sacrificing accuracy.
Joint semantic communication and inference by masked SCAE improves query
accuracy by 25% compared to local student based query and 15% compared to
remote teacher based query. Finally, incorporating channel semantics is shown
to effectively improve inference accuracy, notably at low SNR values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qiyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zou_H/0/1/0/all/0/1"&gt;Hang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Debbah_M/0/1/0/all/0/1"&gt;Merouane Debbah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Almazrouei_E/0/1/0/all/0/1"&gt;Ebtesam Almazrouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bader_F/0/1/0/all/0/1"&gt;Faouzi Bader&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16818</id>
        <link href="http://arxiv.org/abs/2308.16818"/>
        <updated>2023-09-02T00:40:02.122Z</updated>
        <summary type="html"><![CDATA[Accurate traffic forecasting at intersections governed by intelligent traffic
signals is critical for the advancement of an effective intelligent traffic
signal control system. However, due to the irregular traffic time series
produced by intelligent intersections, the traffic forecasting task becomes
much more intractable and imposes three major new challenges: 1) asynchronous
spatial dependency, 2) irregular temporal dependency among traffic data, and 3)
variable-length sequence to be predicted, which severely impede the performance
of current traffic forecasting methods. To this end, we propose an Asynchronous
Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic
states of the lanes entering intelligent intersections in a future time window.
Specifically, by linking lanes via a traffic diffusion graph, we first propose
an Asynchronous Graph Diffusion Network to model the asynchronous spatial
dependency between the time-misaligned traffic state measurements of lanes.
After that, to capture the temporal dependency within irregular traffic state
sequence, a learnable personalized time encoding is devised to embed the
continuous time for each lane. Then we propose a Transformable Time-aware
Convolution Network that learns meta-filters to derive time-aware convolution
filters with transformable filter sizes for efficient temporal convolution on
the irregular sequence. Furthermore, a Semi-Autoregressive Prediction Network
consisting of a state evolution unit and a semiautoregressive predictor is
designed to effectively and efficiently predict variable-length traffic state
sequences. Extensive experiments on two real-world datasets demonstrate the
effectiveness of ASeer in six metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weijia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jindong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingbo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1"&gt;Yu Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16534</id>
        <link href="http://arxiv.org/abs/2308.16534"/>
        <updated>2023-09-02T00:40:02.072Z</updated>
        <summary type="html"><![CDATA[Score-based and diffusion models have emerged as effective approaches for
both conditional and unconditional generation. Still conditional generation is
based on either a specific training of a conditional model or classifier
guidance, which requires training a noise-dependent classifier, even when the
classifier for uncorrupted data is given. We propose an approach to sample from
unconditional score-based generative models enforcing arbitrary logical
constraints, without any additional training. Firstly, we show how to
manipulate the learned score in order to sample from an un-normalized
distribution conditional on a user-defined constraint. Then, we define a
flexible and numerically stable neuro-symbolic framework for encoding soft
logical constraints. Combining these two ingredients we obtain a general, but
approximate, conditional sampling algorithm. We further developed effective
heuristics aimed at improving the approximation. Finally, we show the
effectiveness of our approach for various types of constraints and data:
tabular data, images and time series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scassola_D/0/1/0/all/0/1"&gt;Davide Scassola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saccani_S/0/1/0/all/0/1"&gt;Sebastiano Saccani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1"&gt;Ginevra Carbone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1"&gt;Luca Bortolussi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Representation Learning for Unreliable Partial Label Learning. (arXiv:2308.16718v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16718</id>
        <link href="http://arxiv.org/abs/2308.16718"/>
        <updated>2023-09-02T00:40:02.072Z</updated>
        <summary type="html"><![CDATA[Partial Label Learning (PLL) is a type of weakly supervised learning where
each training instance is assigned a set of candidate labels, but only one
label is the ground-truth. However, this idealistic assumption may not always
hold due to potential annotation inaccuracies, meaning the ground-truth may not
be present in the candidate label set. This is known as Unreliable Partial
Label Learning (UPLL) that introduces an additional complexity due to the
inherent unreliability and ambiguity of partial labels, often resulting in a
sub-optimal performance with existing methods. To address this challenge, we
propose the Unreliability-Robust Representation Learning framework (URRL) that
leverages unreliability-robust contrastive learning to help the model fortify
against unreliable partial labels effectively. Concurrently, we propose a dual
strategy that combines KNN-based candidate label set correction and
consistency-regularization-based label disambiguation to refine label quality
and enhance the ability of representation learning within the URRL framework.
Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art PLL methods on various datasets with diverse degrees of
unreliability and ambiguity. Furthermore, we provide a theoretical analysis of
our approach from the perspective of the expectation maximization (EM)
algorithm. Upon acceptance, we pledge to make the code publicly accessible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dong-Dong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xin Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min-Ling Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting. (arXiv:2308.16678v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2308.16678</id>
        <link href="http://arxiv.org/abs/2308.16678"/>
        <updated>2023-09-02T00:40:02.064Z</updated>
        <summary type="html"><![CDATA[Although deep learning has made strides in the field of deep noise
suppression, leveraging deep architectures on resource-constrained devices
still proved challenging. Therefore, we present an early-exiting model based on
nsNet2 that provides several levels of accuracy and resource savings by halting
computations at different stages. Moreover, we adapt the original architecture
by splitting the information flow to take into account the injected dynamism.
We show the trade-offs between performance and computational complexity based
on established metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miccini_R/0/1/0/all/0/1"&gt;Riccardo Miccini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zniber_A/0/1/0/all/0/1"&gt;Alaa Zniber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laroche_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Laroche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piechowiak_T/0/1/0/all/0/1"&gt;Tobias Piechowiak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoeberl_M/0/1/0/all/0/1"&gt;Martin Schoeberl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pezzarossa_L/0/1/0/all/0/1"&gt;Luca Pezzarossa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karrakchou_O/0/1/0/all/0/1"&gt;Ouassim Karrakchou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sparso_J/0/1/0/all/0/1"&gt;Jens Spars&amp;#xf8;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghogho_M/0/1/0/all/0/1"&gt;Mounir Ghogho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curvature-based Pooling within Graph Neural Networks. (arXiv:2308.16516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16516</id>
        <link href="http://arxiv.org/abs/2308.16516"/>
        <updated>2023-09-02T00:40:02.051Z</updated>
        <summary type="html"><![CDATA[Over-squashing and over-smoothing are two critical issues, that limit the
capabilities of graph neural networks (GNNs). While over-smoothing eliminates
the differences between nodes making them indistinguishable, over-squashing
refers to the inability of GNNs to propagate information over long distances,
as exponentially many node states are squashed into fixed-size representations.
Both phenomena share similar causes, as both are largely induced by the graph
topology. To mitigate these problems in graph classification tasks, we propose
CurvPool, a novel pooling method. CurvPool exploits the notion of curvature of
a graph to adaptively identify structures responsible for both over-smoothing
and over-squashing. By clustering nodes based on the Balanced Forman curvature,
CurvPool constructs a graph with a more suitable structure, allowing deeper
models and the combination of distant information. We compare it to other
state-of-the-art pooling approaches and establish its competitiveness in terms
of classification accuracy, computational complexity, and flexibility. CurvPool
outperforms several comparable methods across all considered tasks. The most
consistent results are achieved by pooling densely connected clusters using the
sum aggregation, as this allows additional information about the size of each
pool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanders_C/0/1/0/all/0/1"&gt;Cedric Sanders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Andreas Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebig_T/0/1/0/all/0/1"&gt;Thomas Liebig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Causal Discovery Approach To Learn How Urban Form Shapes Sustainable Mobility Across Continents. (arXiv:2308.16599v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16599</id>
        <link href="http://arxiv.org/abs/2308.16599"/>
        <updated>2023-09-02T00:40:02.051Z</updated>
        <summary type="html"><![CDATA[Global sustainability requires low-carbon urban transport systems, shaped by
adequate infrastructure, deployment of low-carbon transport modes and shifts in
travel behavior. To adequately implement alterations in infrastructure, it's
essential to grasp the location-specific cause-and-effect mechanisms that the
constructed environment has on travel. Yet, current research falls short in
representing causal relationships between the 6D urban form variables and
travel, generalizing across different regions, and modeling urban form effects
at high spatial resolution. Here, we address all three gaps by utilizing a
causal discovery and an explainable machine learning framework to detect urban
form effects on intra-city travel based on high-resolution mobility data of six
cities across three continents. We show that both distance to city center,
demographics and density indirectly affect other urban form features. By
considering the causal relationships, we find that location-specific influences
align across cities, yet vary in magnitude. In addition, the spread of the city
and the coverage of jobs across the city are the strongest determinants of
travel-related emissions, highlighting the benefits of compact development and
associated benefits. Differences in urban form effects across the cities call
for a more holistic definition of 6D measures. Our work is a starting point for
location-specific analysis of urban form effects on mobility behavior using
causal discovery approaches, which is highly relevant for city planners and
municipalities across continents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_F/0/1/0/all/0/1"&gt;Felix Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nachtigall_F/0/1/0/all/0/1"&gt;Florian Nachtigall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franken_L/0/1/0/all/0/1"&gt;Lukas Franken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milojevic_Dupont_N/0/1/0/all/0/1"&gt;Nikola Milojevic-Dupont&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1"&gt;Rafael H.M. Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_N/0/1/0/all/0/1"&gt;Nicolas Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Runge_J/0/1/0/all/0/1"&gt;Jakob Runge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1"&gt;Marta Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Creutzig_F/0/1/0/all/0/1"&gt;Felix Creutzig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images. (arXiv:2308.16738v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2308.16738</id>
        <link href="http://arxiv.org/abs/2308.16738"/>
        <updated>2023-09-02T00:40:02.051Z</updated>
        <summary type="html"><![CDATA[Ultrasound imaging serves as a pivotal tool for diagnosing cervical lymph
node lesions. However, the diagnoses of these images largely hinge on the
expertise of medical practitioners, rendering the process susceptible to
misdiagnoses. Although rapidly developing deep learning has substantially
improved the diagnoses of diverse ultrasound images, there remains a
conspicuous research gap concerning cervical lymph nodes. The objective of our
work is to accurately diagnose cervical lymph node lesions by leveraging a deep
learning model. To this end, we first collected 3392 images containing normal
lymph nodes, benign lymph node lesions, malignant primary lymph node lesions,
and malignant metastatic lymph node lesions. Given that ultrasound images are
generated by the reflection and scattering of sound waves across varied bodily
tissues, we proposed the Conv-FFT Block. It integrates convolutional operations
with the fast Fourier transform to more astutely model the images. Building
upon this foundation, we designed a novel architecture, named US-SFNet. This
architecture not only discerns variances in ultrasound images from the spatial
domain but also adeptly captures microstructural alterations across various
lesions in the frequency domain. To ascertain the potential of US-SFNet, we
benchmarked it against 12 popular architectures through five-fold
cross-validation. The results show that US-SFNet is SOTA and can achieve 92.89%
accuracy, 90.46% precision, 89.95% sensitivity and 97.49% specificity,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yubiao Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jun Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1"&gt;Haihua Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_B/0/1/0/all/0/1"&gt;Bingchun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenzhang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CktGNN: Circuit Graph Neural Network for Electronic Design Automation. (arXiv:2308.16406v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16406</id>
        <link href="http://arxiv.org/abs/2308.16406"/>
        <updated>2023-09-02T00:40:02.003Z</updated>
        <summary type="html"><![CDATA[The electronic design automation of analog circuits has been a longstanding
challenge in the integrated circuit field due to the huge design space and
complex design trade-offs among circuit specifications. In the past decades,
intensive research efforts have mostly been paid to automate the transistor
sizing with a given circuit topology. By recognizing the graph nature of
circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that
simultaneously automates the circuit topology generation and device sizing
based on the encoder-dependent optimization subroutines. Particularly, CktGNN
encodes circuit graphs using a two-level GNN framework (of nested GNN) where
circuits are represented as combinations of subgraphs in a known subgraph
basis. In this way, it significantly improves design efficiency by reducing the
number of subgraphs to perform message passing. Nonetheless, another critical
roadblock to advancing learning-assisted circuit design automation is a lack of
public benchmarks to perform canonical assessment and reproducible research. To
tackle the challenge, we introduce Open Circuit Benchmark (OCB), an
open-sourced dataset that contains $10$K distinct operational amplifiers with
carefully-extracted circuit specifications. OCB is also equipped with
communicative circuit generation and evaluation capabilities such that it can
help to generalize CktGNN to design various analog circuits by producing
corresponding datasets. Experiments on OCB show the extraordinary advantages of
CktGNN through representation-based optimization frameworks over other recent
powerful GNN baselines and human experts' manual designs. Our work paves the
way toward a learning-based open-sourced design automation for analog circuits.
Our source code is available at \url{https://github.com/zehao-dong/CktGNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zehao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1"&gt;Weidong Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Muhan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yixin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals. (arXiv:2308.16422v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2308.16422</id>
        <link href="http://arxiv.org/abs/2308.16422"/>
        <updated>2023-09-02T00:40:02.003Z</updated>
        <summary type="html"><![CDATA[The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to
their complex waveforms, extended duration, and low signal-to-noise ratio
(SNR), making them more challenging to be identified compared to compact binary
coalescences. While matched filtering-based techniques are known for their
computational demands, existing deep learning-based methods primarily handle
time-domain data and are often constrained by data duration and SNR. In
addition, most existing work ignores time-delay interferometry (TDI) and
applies the long-wavelength approximation in detector response calculations,
thus limiting their ability to handle laser frequency noise. In this study, we
introduce DECODE, an end-to-end model focusing on EMRI signal detection by
sequence modeling in the frequency domain. Centered around a dilated causal
convolutional neural network, trained on synthetic data considering TDI-1.5
detector response, DECODE can efficiently process a year's worth of
multichannel TDI data with an SNR of around 50. We evaluate our model on 1-year
data with accumulated SNR ranging from 50 to 120 and achieve a true positive
rate of 96.3% at a false positive rate of 1%, keeping an inference time of less
than 0.01 seconds. With the visualization of three showcased EMRI signals for
interpretability and generalization, DECODE exhibits strong potential for
future space-based gravitational wave data analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yue Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Shi_R/0/1/0/all/0/1"&gt;Ruijun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhoujian Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhixiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Diverse Features in Vision Transformers for Improved Generalization. (arXiv:2308.16274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16274</id>
        <link href="http://arxiv.org/abs/2308.16274"/>
        <updated>2023-09-02T00:40:01.997Z</updated>
        <summary type="html"><![CDATA[Deep learning models often rely only on a small set of features even when
there is a rich set of predictive signals in the training data. This makes
models brittle and sensitive to distribution shifts. In this work, we first
examine vision transformers (ViTs) and find that they tend to extract robust
and spurious features with distinct attention heads. As a result of this
modularity, their performance under distribution shifts can be significantly
improved at test time by pruning heads corresponding to spurious features,
which we demonstrate using an "oracle selection" on validation data. Second, we
propose a method to further enhance the diversity and complementarity of the
learned features by encouraging orthogonality of the attention heads' input
gradients. We observe improved out-of-distribution performance on diagnostic
benchmarks (MNIST-CIFAR, Waterbirds) as a consequence of the enhanced diversity
of features and the pruning of undesirable heads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1"&gt;Armand Mihai Nicolicioiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1"&gt;Andrei Liviu Nicolicioiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1"&gt;Bogdan Alexe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1"&gt;Damien Teney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emoji Promotes Developer Participation and Issue Resolution on GitHub. (arXiv:2308.16360v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2308.16360</id>
        <link href="http://arxiv.org/abs/2308.16360"/>
        <updated>2023-09-02T00:40:01.997Z</updated>
        <summary type="html"><![CDATA[Although remote working is increasingly adopted during the pandemic, many are
concerned by the low-efficiency in the remote working. Missing in text-based
communication are non-verbal cues such as facial expressions and body language,
which hinders the effective communication and negatively impacts the work
outcomes. Prevalent on social media platforms, emojis, as alternative
non-verbal cues, are gaining popularity in the virtual workspaces well. In this
paper, we study how emoji usage influences developer participation and issue
resolution in virtual workspaces. To this end, we collect GitHub issues for a
one-year period and apply causal inference techniques to measure the causal
effect of emojis on the outcome of issues, controlling for confounders such as
issue content, repository, and author information. We find that emojis can
significantly reduce the resolution time of issues and attract more user
participation. We also compare the heterogeneous effect on different types of
issues. These findings deepen our understanding of the developer communities,
and they provide design implications on how to facilitate interactions and
broaden developer participation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Ge Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1"&gt;Qiaozhu Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1"&gt;Wei Ai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems. (arXiv:2308.16471v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2308.16471</id>
        <link href="http://arxiv.org/abs/2308.16471"/>
        <updated>2023-09-02T00:40:01.997Z</updated>
        <summary type="html"><![CDATA[In dynamic motion generation tasks, including contact and collisions, small
changes in policy parameters can lead to extremely different returns. For
example, in soccer, the ball can fly in completely different directions with a
similar heading motion by slightly changing the hitting position or the force
applied to the ball or when the friction of the ball varies. However, it is
difficult to imagine that completely different skills are needed for heading a
ball in different directions. In this study, we proposed a multitask
reinforcement learning algorithm for adapting a policy to implicit changes in
goals or environments in a single motion category with different reward
functions or physical parameters of the environment. We evaluated the proposed
method on the ball heading task using a monopod robot model. The results showed
that the proposed method can adapt to implicit changes in the goal positions or
the coefficients of restitution of the ball, whereas the standard domain
randomization approach cannot cope with different task settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamamori_S/0/1/0/all/0/1"&gt;Satoshi Yamamori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morimoto_J/0/1/0/all/0/1"&gt;Jun Morimoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Anomalies in Telecommunication Network KPI Time Series. (arXiv:2308.16279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16279</id>
        <link href="http://arxiv.org/abs/2308.16279"/>
        <updated>2023-09-02T00:40:01.955Z</updated>
        <summary type="html"><![CDATA[The increasing complexity and scale of telecommunication networks have led to
a growing interest in automated anomaly detection systems. However, the
classification of anomalies detected on network Key Performance Indicators
(KPI) has received less attention, resulting in a lack of information about
anomaly characteristics and classification processes. To address this gap, this
paper proposes a modular anomaly classification framework. The framework
assumes separate entities for the anomaly classifier and the detector, allowing
for a distinct treatment of anomaly detection and classification tasks on time
series. The objectives of this study are (1) to develop a time series simulator
that generates synthetic time series resembling real-world network KPI
behavior, (2) to build a detection model to identify anomalies in the time
series, (3) to build classification models that accurately categorize detected
anomalies into predefined classes (4) to evaluate the classification framework
performance on simulated and real-world network KPI time series. This study has
demonstrated the good performance of the anomaly classification models trained
on simulated anomalies when applied to real-world network time series data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bordeau_Aubert_K/0/1/0/all/0/1"&gt;Korantin Bordeau-Aubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whatley_J/0/1/0/all/0/1"&gt;Justin Whatley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeau_S/0/1/0/all/0/1"&gt;Sylvain Nadeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1"&gt;Tristan Glatard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaumard_B/0/1/0/all/0/1"&gt;Brigitte Jaumard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Karhunen-Lo\`eve Data Imputation in High Contrast Imaging. (arXiv:2308.16912v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2308.16912</id>
        <link href="http://arxiv.org/abs/2308.16912"/>
        <updated>2023-09-02T00:40:01.902Z</updated>
        <summary type="html"><![CDATA[Detection and characterization of extended structures is a crucial goal in
high contrast imaging. However, these structures face challenges in data
reduction, leading to over-subtraction from speckles and self-subtraction with
most existing methods. Iterative post-processing methods offer promising
results, but their integration into existing pipelines is hindered by selective
algorithms, high computational cost, and algorithmic regularization. To address
this for reference differential imaging (RDI), here we propose the data
imputation concept to Karhunen-Lo\`eve transform (DIKL) by modifying two steps
in the standard Karhunen-Lo\`eve image projection (KLIP) method. Specifically,
we partition an image to two matrices: an anchor matrix which focuses only on
the speckles to obtain the DIKL coefficients, and a boat matrix which focuses
on the regions of astrophysical interest for speckle removal using DIKL
components. As an analytical approach, DIKL achieves high-quality results with
significantly reduced computational cost (~3 orders of magnitude less than
iterative methods). Being a derivative method of KLIP, DIKL is seamlessly
integrable into high contrast imaging pipelines for RDI observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin B. Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC. (arXiv:2104.03942v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03942</id>
        <link href="http://arxiv.org/abs/2104.03942"/>
        <updated>2023-09-02T00:40:01.875Z</updated>
        <summary type="html"><![CDATA[We present a framework for approximate Bayesian inference when only a limited
number of noisy log-likelihood evaluations can be obtained due to computational
constraints, which is becoming increasingly common for applications of complex
models. We model the log-likelihood function using a Gaussian process (GP) and
the main methodological innovation is to apply this model to emulate the
progression that an exact Metropolis-Hastings (MH) sampler would take if it was
applicable. Informative log-likelihood evaluation locations are selected using
a sequential experimental design strategy until the MH accept/reject decision
is done accurately enough according to the GP model. The resulting approximate
sampler is conceptually simple and sample-efficient. It is also more robust to
violations of GP modelling assumptions compared with earlier, related "Bayesian
optimisation-like" methods tailored for Bayesian inference. We discuss some
theoretical aspects and various interpretations of the resulting approximate MH
sampler, and demonstrate its benefits in the context of Bayesian and
generalised Bayesian likelihood-free inference for simulator-based statistical
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jarvenpaa_M/0/1/0/all/0/1"&gt;Marko J&amp;#xe4;rvenp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Corander_J/0/1/0/all/0/1"&gt;Jukka Corander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics. (arXiv:2308.16680v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16680</id>
        <link href="http://arxiv.org/abs/2308.16680"/>
        <updated>2023-09-02T00:40:01.863Z</updated>
        <summary type="html"><![CDATA[We propose to apply several gradient estimation techniques to enable the
differentiation of programs with discrete randomness in High Energy Physics.
Such programs are common in High Energy Physics due to the presence of
branching processes and clustering-based analysis. Thus differentiating such
programs can open the way for gradient based optimization in the context of
detector design optimization, simulator tuning, or data analysis and
reconstruction optimization. We discuss several possible gradient estimation
strategies, including the recent Stochastic AD method, and compare them in
simplified detector design experiments. In doing so we develop, to the best of
our knowledge, the first fully differentiable branching program.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kagan_M/0/1/0/all/0/1"&gt;Michael Kagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heinrich_L/0/1/0/all/0/1"&gt;Lukas Heinrich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-Demand Communication for Asynchronous Multi-Agent Bandits. (arXiv:2302.07446v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2302.07446</id>
        <link href="http://arxiv.org/abs/2302.07446"/>
        <updated>2023-09-02T00:40:01.857Z</updated>
        <summary type="html"><![CDATA[This paper studies a cooperative multi-agent multi-armed stochastic bandit
problem where agents operate asynchronously -- agent pull times and rates are
unknown, irregular, and heterogeneous -- and face the same instance of a
K-armed bandit problem. Agents can share reward information to speed up the
learning process at additional communication costs. We propose ODC, an
on-demand communication protocol that tailors the communication of each pair of
agents based on their empirical pull times. ODC is efficient when the pull
times of agents are highly heterogeneous, and its communication complexity
depends on the empirical pull times of agents. ODC is a generic protocol that
can be integrated into most cooperative bandit algorithms without degrading
their performance. We then incorporate ODC into the natural extensions of UCB
and AAE algorithms and propose two communication-efficient cooperative
algorithms. Our analysis shows that both algorithms are near-optimal in regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Zhen Janice Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuchuang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xutong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajiesmaili_M/0/1/0/all/0/1"&gt;Mohammad Hajiesmaili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1"&gt;John C.S. Lui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Towsley_D/0/1/0/all/0/1"&gt;Don Towsley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2304.14994</id>
        <link href="http://arxiv.org/abs/2304.14994"/>
        <updated>2023-09-02T00:40:01.851Z</updated>
        <summary type="html"><![CDATA[Unlike conventional grid and mesh based methods for solving partial
differential equations (PDEs), neural networks have the potential to break the
curse of dimensionality, providing approximate solutions to problems where
using classical solvers is difficult or impossible. While global minimization
of the PDE residual over the network parameters works well for boundary value
problems, catastrophic forgetting impairs the applicability of this approach to
initial value problems (IVPs). In an alternative local-in-time approach, the
optimization problem can be converted into an ordinary differential equation
(ODE) on the network parameters and the solution propagated forward in time;
however, we demonstrate that current methods based on this approach suffer from
two key issues. First, following the ODE produces an uncontrolled growth in the
conditioning of the problem, ultimately leading to unacceptably large numerical
errors. Second, as the ODE methods scale cubically with the number of model
parameters, they are restricted to small neural networks, significantly
limiting their ability to represent intricate PDE initial conditions and
solutions. Building on these insights, we develop Neural IVP, an ODE based IVP
solver which prevents the network from getting ill-conditioned and runs in time
linear in the number of parameters, enabling us to evolve the dynamics of
challenging PDEs with neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1"&gt;Marc Finzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potapczynski_A/0/1/0/all/0/1"&gt;Andres Potapczynski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choptuik_M/0/1/0/all/0/1"&gt;Matthew Choptuik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16245</id>
        <link href="http://arxiv.org/abs/2308.16245"/>
        <updated>2023-09-02T00:40:01.828Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence (AI) is often an integral part of modern decision
support systems (DSSs). The best-performing predictive models used in AI-based
DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to
create AI systems that can explain their rationale to human users. Local
explanations in XAI can provide information about the causes of individual
predictions in terms of feature importance. However, a critical drawback of
existing local explanation methods is their inability to quantify the
uncertainty associated with a feature's importance. This paper introduces an
extension of a feature importance explanation method, Calibrated Explanations
(CE), previously only supporting classification, with support for standard
regression and probabilistic regression, i.e., the probability that the target
is above an arbitrary threshold. The extension for regression keeps all the
benefits of CE, such as calibration of the prediction from the underlying model
with confidence intervals, uncertainty quantification of feature importance,
and allows both factual and counterfactual explanations. CE for standard
regression provides fast, reliable, stable, and robust explanations. CE for
probabilistic regression provides an entirely new way of creating probabilistic
explanations from any ordinary regression model and with a dynamic selection of
thresholds. The performance of CE for probabilistic regression regarding
stability and speed is comparable to LIME. The method is model agnostic with
easily understood conditional rules. An implementation in Python is freely
available on GitHub and for installation using pip making the results in this
paper easily replicable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lofstrom_T/0/1/0/all/0/1"&gt;Tuwe L&amp;#xf6;fstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lofstrom_H/0/1/0/all/0/1"&gt;Helena L&amp;#xf6;fstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_U/0/1/0/all/0/1"&gt;Ulf Johansson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonstrod_C/0/1/0/all/0/1"&gt;Cecilia S&amp;#xf6;nstr&amp;#xf6;d&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Inductive Logic Programming meets Reinforcement Learning. (arXiv:2308.16210v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16210</id>
        <link href="http://arxiv.org/abs/2308.16210"/>
        <updated>2023-09-02T00:40:01.822Z</updated>
        <summary type="html"><![CDATA[One approach to explaining the hierarchical levels of understanding within a
machine learning model is the symbolic method of inductive logic programming
(ILP), which is data efficient and capable of learning first-order logic rules
that can entail data behaviour. A differentiable extension to ILP, so-called
differentiable Neural Logic (dNL) networks, are able to learn Boolean functions
as their neural architecture includes symbolic reasoning. We propose an
application of dNL in the field of Relational Reinforcement Learning (RRL) to
address dynamic continuous environments. This represents an extension of
previous work in applying dNL-based ILP in RRL settings, as our proposed model
updates the architecture to enable it to solve problems in continuous RL
environments. The goal of this research is to improve upon current ILP methods
for use in RRL by incorporating non-linear continuous predicates, allowing RRL
agents to reason and make decisions in dynamic and continuous environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bueff_A/0/1/0/all/0/1"&gt;Andreas Bueff&lt;/a&gt; (University of Edinburgh), &lt;a href="http://arxiv.org/find/cs/1/au:+Belle_V/0/1/0/all/0/1"&gt;Vaishak Belle&lt;/a&gt; (University of Edinburgh)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry Preservation in Hamiltonian Systems: Simulation and Learning. (arXiv:2308.16331v1 [math-ph])]]></title>
        <id>http://arxiv.org/abs/2308.16331</id>
        <link href="http://arxiv.org/abs/2308.16331"/>
        <updated>2023-09-02T00:40:01.816Z</updated>
        <summary type="html"><![CDATA[This work presents a general geometric framework for simulating and learning
the dynamics of Hamiltonian systems that are invariant under a Lie group of
transformations. This means that a group of symmetries is known to act on the
system respecting its dynamics and, as a consequence, Noether's Theorem,
conserved quantities are observed. We propose to simulate and learn the
mappings of interest through the construction of $G$-invariant Lagrangian
submanifolds, which are pivotal objects in symplectic geometry. A notable
property of our constructions is that the simulated/learned dynamics also
preserves the same conserved quantities as the original system, resulting in a
more faithful surrogate of the original dynamics than non-symmetry aware
methods, and in a more accurate predictor of non-observed trajectories.
Furthermore, our setting is able to simulate/learn not only Hamiltonian flows,
but any Lie group-equivariant symplectic transformation. Our designs leverage
pivotal techniques and concepts in symplectic geometry and geometric mechanics:
reduction theory, Noether's Theorem, Lagrangian submanifolds, momentum
mappings, and coisotropic reduction among others. We also present methods to
learn Poisson transformations while preserving the underlying geometry and how
to endow non-geometric integrators with geometric properties. Thus, this work
presents a novel attempt to harness the power of symplectic and Poisson
geometry towards simulating and learning problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math-ph/1/au:+Vaquero_M/0/1/0/all/0/1"&gt;Miguel Vaquero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math-ph/1/au:+Cortes_J/0/1/0/all/0/1"&gt;Jorge Cort&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math-ph/1/au:+Diego_D/0/1/0/all/0/1"&gt;David Mart&amp;#xed;n de Diego&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Augmented Reduced Rank Regression for Pan-Cancer Analysis. (arXiv:2308.16333v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2308.16333</id>
        <link href="http://arxiv.org/abs/2308.16333"/>
        <updated>2023-09-02T00:40:01.798Z</updated>
        <summary type="html"><![CDATA[Statistical approaches that successfully combine multiple datasets are more
powerful, efficient, and scientifically informative than separate analyses. To
address variation architectures correctly and comprehensively for
high-dimensional data across multiple sample sets (i.e., cohorts), we propose
multiple augmented reduced rank regression (maRRR), a flexible matrix
regression and factorization method to concurrently learn both covariate-driven
and auxiliary structured variation. We consider a structured nuclear norm
objective that is motivated by random matrix theory, in which the regression or
factorization terms may be shared or specific to any number of cohorts. Our
framework subsumes several existing methods, such as reduced rank regression
and unsupervised multi-matrix factorization approaches, and includes a
promising novel approach to regression and factorization of a single dataset
(aRRR) as a special case. Simulations demonstrate substantial gains in power
from combining multiple datasets, and from parsimoniously accounting for all
structured variation. We apply maRRR to gene expression data from multiple
cancer types (i.e., pan-cancer) from TCGA, with somatic mutations as
covariates. The method performs well with respect to prediction and imputation
of held-out data, and provides new insights into mutation-driven and auxiliary
variation that is shared or specific to certain cancer types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiuzhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lock_E/0/1/0/all/0/1"&gt;Eric F. Lock&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. (arXiv:2308.16369v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16369</id>
        <link href="http://arxiv.org/abs/2308.16369"/>
        <updated>2023-09-02T00:40:01.793Z</updated>
        <summary type="html"><![CDATA[Large Language Model (LLM) inference consists of two distinct phases -
prefill phase which processes the input prompt and decode phase which generates
output tokens autoregressively. While the prefill phase effectively saturates
GPU compute at small batch sizes, the decode phase results in low compute
utilization as it generates one token at a time per request. The varying
prefill and decode times also lead to imbalance across micro-batches when using
pipeline parallelism, resulting in further inefficiency due to bubbles.

We present SARATHI to address these challenges. SARATHI employs
chunked-prefills, which splits a prefill request into equal sized chunks, and
decode-maximal batching, which constructs a batch using a single prefill chunk
and populates the remaining slots with decodes. During inference, the prefill
chunk saturates GPU compute, while the decode requests 'piggyback' and cost up
to an order of magnitude less compared to a decode-only batch. Chunked-prefills
allows constructing multiple decode-maximal batches from a single prefill
request, maximizing coverage of decodes that can piggyback. Furthermore, the
uniform compute design of these batches ameliorates the imbalance between
micro-batches, significantly reducing pipeline bubbles.

Our techniques yield significant improvements in inference performance across
models and hardware. For the LLaMA-13B model on A6000 GPU, SARATHI improves
decode throughput by up to 10x, and accelerates end-to-end throughput by up to
1.33x. For LLaMa-33B on A100 GPU, we achieve 1.25x higher end-to-end-throughput
and up to 4.25x higher decode throughput. When used with pipeline parallelism
on GPT-3, SARATHI reduces bubbles by 6.29x, resulting in an end-to-end
throughput improvement of 1.91x.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Amey Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panwar_A/0/1/0/all/0/1"&gt;Ashish Panwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_J/0/1/0/all/0/1"&gt;Jayashree Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwatra_N/0/1/0/all/0/1"&gt;Nipun Kwatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulavani_B/0/1/0/all/0/1"&gt;Bhargav S. Gulavani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramjee_R/0/1/0/all/0/1"&gt;Ramachandran Ramjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Emergency Department Crowding with Advanced Machine Learning Models and Multivariable Input. (arXiv:2308.16544v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16544</id>
        <link href="http://arxiv.org/abs/2308.16544"/>
        <updated>2023-09-02T00:40:01.787Z</updated>
        <summary type="html"><![CDATA[Emergency department (ED) crowding is a significant threat to patient safety
and it has been repeatedly associated with increased mortality. Forecasting
future service demand has the potential patient outcomes. Despite active
research on the subject, several gaps remain: 1) proposed forecasting models
have become outdated due to quick influx of advanced machine learning models
(ML), 2) amount of multivariable input data has been limited and 3) discrete
performance metrics have been rarely reported. In this study, we document the
performance of a set of advanced ML models in forecasting ED occupancy 24 hours
ahead. We use electronic health record data from a large, combined ED with an
extensive set of explanatory variables, including the availability of beds in
catchment area hospitals, traffic data from local observation stations, weather
variables, etc. We show that N-BEATS and LightGBM outpeform benchmarks with 11
% and 9 % respective improvements and that DeepAR predicts next day crowding
with an AUC of 0.76 (95 % CI 0.69-0.84). To the best of our knowledge, this is
the first study to document the superiority of LightGBM and N-BEATS over
statistical benchmarks in the context of ED forecasting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuominen_J/0/1/0/all/0/1"&gt;Jalmari Tuominen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulkkinen_E/0/1/0/all/0/1"&gt;Eetu Pulkkinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peltonen_J/0/1/0/all/0/1"&gt;Jaakko Peltonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanniainen_J/0/1/0/all/0/1"&gt;Juho Kanniainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oksala_N/0/1/0/all/0/1"&gt;Niku Oksala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palomaki_A/0/1/0/all/0/1"&gt;Ari Palom&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roine_A/0/1/0/all/0/1"&gt;Antti Roine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2308.14172</id>
        <link href="http://arxiv.org/abs/2308.14172"/>
        <updated>2023-09-02T00:40:01.781Z</updated>
        <summary type="html"><![CDATA[Hypergraphs are important for processing data with higher-order relationships
involving more than two entities. In scenarios where explicit hypergraphs are
not readily available, it is desirable to infer a meaningful hypergraph
structure from the node features to capture the intrinsic relations within the
data. However, existing methods either adopt simple pre-defined rules that fail
to precisely capture the distribution of the potential hypergraph structure, or
learn a mapping between hypergraph structures and node features but require a
large amount of labelled data, i.e., pre-existing hypergraph structures, for
training. Both restrict their applications in practical scenarios. To fill this
gap, we propose a novel smoothness prior that enables us to design a method to
infer the probability for each potential hyperedge without labelled data as
supervision. The proposed prior indicates features of nodes in a hyperedge are
highly correlated by the features of the hyperedge containing them. We use this
prior to derive the relation between the hypergraph structure and the node
features via probabilistic modelling. This allows us to develop an unsupervised
inference method to estimate the probability for each potential hyperedge via
solving an optimisation problem that has an analytical solution. Experiments on
both synthetic and real-world data demonstrate that our method can learn
meaningful hypergraph structures from data more efficiently than existing
hypergraph structure inference methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Bohan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaowen Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A stochastic block model for community detection in attributed networks. (arXiv:2308.16382v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2308.16382</id>
        <link href="http://arxiv.org/abs/2308.16382"/>
        <updated>2023-09-02T00:40:01.775Z</updated>
        <summary type="html"><![CDATA[Community detection is an important content in complex network analysis. The
existing community detection methods in attributed networks mostly focus on
only using network structure, while the methods of integrating node attributes
is mainly for the traditional community structures, and cannot detect
multipartite structures and mixture structures in network. In addition, the
model-based community detection methods currently proposed for attributed
networks do not fully consider unique topology information of nodes, such as
betweenness centrality and clustering coefficient. Therefore, a stochastic
block model that integrates betweenness centrality and clustering coefficient
of nodes for community detection in attributed networks, named BCSBM, is
proposed in this paper. Different from other generative models for attributed
networks, the generation process of links and attributes in BCSBM model follows
the Poisson distribution, and the probability between community is considered
based on the stochastic block model. Moreover, the betweenness centrality and
clustering coefficient of nodes are introduced into the process of links and
attributes generation. Finally, the expectation maximization algorithm is
employed to estimate the parameters of the BCSBM model, and the node-community
memberships is obtained through the hard division process, so the community
detection is completed. By experimenting on six real-work networks containing
different network structures, and comparing with the community detection
results of five algorithms, the experimental results show that the BCSBM model
not only inherits the advantages of the stochastic block model and can detect
various network structures, but also has good data fitting ability due to
introducing the betweenness centrality and clustering coefficient of nodes.
Overall, the performance of this model is superior to other five compared
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_F/0/1/0/all/0/1"&gt;Fang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenyan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GRASP: A Goodness-of-Fit Test for Classification Learning. (arXiv:2209.02064v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2209.02064</id>
        <link href="http://arxiv.org/abs/2209.02064"/>
        <updated>2023-09-02T00:40:01.769Z</updated>
        <summary type="html"><![CDATA[Performance of classifiers is often measured in terms of average accuracy on
test data. Despite being a standard measure, average accuracy fails in
characterizing the fit of the model to the underlying conditional law of labels
given the features vector ($Y|X$), e.g. due to model misspecification, over
fitting, and high-dimensionality. In this paper, we consider the fundamental
problem of assessing the goodness-of-fit for a general binary classifier. Our
framework does not make any parametric assumption on the conditional law $Y|X$,
and treats that as a black box oracle model which can be accessed only through
queries. We formulate the goodness-of-fit assessment problem as a tolerance
hypothesis testing of the form \[ H_0: \mathbb{E}\Big[D_f\Big({\sf
Bern}(\eta(X))\|{\sf Bern}(\hat{\eta}(X))\Big)\Big]\leq \tau\,, \] where $D_f$
represents an $f$-divergence function, and $\eta(x)$, $\hat{\eta}(x)$
respectively denote the true and an estimate likelihood for a feature vector
$x$ admitting a positive label. We propose a novel test, called \grasp for
testing $H_0$, which works in finite sample settings, no matter the features
(distribution-free). We also propose model-X \grasp designed for model-X
settings where the joint distribution of the features vector is known. Model-X
\grasp uses this distributional information to achieve better power. We
evaluate the performance of our tests through extensive numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Javanmard_A/0/1/0/all/0/1"&gt;Adel Javanmard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mehrabi_M/0/1/0/all/0/1"&gt;Mohammad Mehrabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness. (arXiv:2308.16681v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16681</id>
        <link href="http://arxiv.org/abs/2308.16681"/>
        <updated>2023-09-02T00:40:01.763Z</updated>
        <summary type="html"><![CDATA[A vast number of systems across the world use algorithmic decision making
(ADM) to (partially) automate decisions that have previously been made by
humans. When designed well, these systems promise more objective decisions
while saving large amounts of resources and freeing up human time. However,
when ADM systems are not designed well, they can lead to unfair decisions which
discriminate against societal groups. The downstream effects of ADMs critically
depend on the decisions made during the systems' design and implementation, as
biases in data can be mitigated or reinforced along the modeling pipeline. Many
of these design decisions are made implicitly, without knowing exactly how they
will influence the final system. It is therefore important to make explicit the
decisions made during the design of ADM systems and understand how these
decisions affect the fairness of the resulting system.

To study this issue, we draw on insights from the field of psychology and
introduce the method of multiverse analysis for algorithmic fairness. In our
proposed method, we turn implicit design decisions into explicit ones and
demonstrate their fairness implications. By combining decisions, we create a
grid of all possible "universes" of decision combinations. For each of these
universes, we compute metrics of fairness and performance. Using the resulting
dataset, one can see how and which decisions impact fairness. We demonstrate
how multiverse analyses can be used to better understand variability and
robustness of algorithmic fairness using an exemplary case study of predicting
public health coverage of vulnerable populations for potential interventions.
Our results illustrate how decisions during the design of a machine learning
system can have surprising effects on its fairness and how to detect these
effects using multiverse analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Simson_J/0/1/0/all/0/1"&gt;Jan Simson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pfisterer_F/0/1/0/all/0/1"&gt;Florian Pfisterer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kern_C/0/1/0/all/0/1"&gt;Christoph Kern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2305.11463</id>
        <link href="http://arxiv.org/abs/2305.11463"/>
        <updated>2023-09-02T00:40:01.532Z</updated>
        <summary type="html"><![CDATA[Maximum mean discrepancy (MMD) flows suffer from high computational costs in
large scale computations. In this paper, we show that MMD flows with Riesz
kernels $K(x,y) = - \Vert x-y\Vert^r$, $r \in (0,2)$ have exceptional
properties which allow their efficient computation. We prove that the MMD of
Riesz kernels coincides with the MMD of their sliced version. As a consequence,
the computation of gradients of MMDs can be performed in the one-dimensional
setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce
the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two measures with
$M$ and $N$ support points. As another interesting follow-up result, the MMD of
compactly supported measures can be estimated from above and below by the
Wasserstein-1 distance. For the implementations we approximate the gradient of
the sliced MMD by using only a finite number $P$ of slices. We show that the
resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data
dimension. These results enable us to train generative models by approximating
MMD gradient flows by neural networks even for image applications. We
demonstrate the efficiency of our model by image generation on MNIST,
FashionMNIST and CIFAR10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hertrich_J/0/1/0/all/0/1"&gt;Johannes Hertrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wald_C/0/1/0/all/0/1"&gt;Christian Wald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altekruger_F/0/1/0/all/0/1"&gt;Fabian Altekr&amp;#xfc;ger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagemann_P/0/1/0/all/0/1"&gt;Paul Hagemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Channel Importance for High Content Imaging with Interpretable Deep Input Channel Mixing. (arXiv:2308.16637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2308.16637</id>
        <link href="http://arxiv.org/abs/2308.16637"/>
        <updated>2023-09-02T00:40:01.524Z</updated>
        <summary type="html"><![CDATA[Uncovering novel drug candidates for treating complex diseases remain one of
the most challenging tasks in early discovery research. To tackle this
challenge, biopharma research established a standardized high content imaging
protocol that tags different cellular compartments per image channel. In order
to judge the experimental outcome, the scientist requires knowledge about the
channel importance with respect to a certain phenotype for decoding the
underlying biology. In contrast to traditional image analysis approaches, such
experiments are nowadays preferably analyzed by deep learning based approaches
which, however, lack crucial information about the channel importance. To
overcome this limitation, we present a novel approach which utilizes
multi-spectral information of high content images to interpret a certain aspect
of cellular biology. To this end, we base our method on image blending concepts
with alpha compositing for an arbitrary number of channels. More specifically,
we introduce DCMIX, a lightweight, scaleable and end-to-end trainable mixing
layer which enables interpretable predictions in high content imaging while
retaining the benefits of deep learning based methods. We employ an extensive
set of experiments on both MNIST and RXRX1 datasets, demonstrating that DCMIX
learns the biologically relevant channel importance without scarifying
prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siegismund_D/0/1/0/all/0/1"&gt;Daniel Siegismund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wieser_M/0/1/0/all/0/1"&gt;Mario Wieser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heyse_S/0/1/0/all/0/1"&gt;Stephan Heyse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steigele_S/0/1/0/all/0/1"&gt;Stephan Steigele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Dimensional Time Series Regression Models: Applications to Statistical Learning Methods. (arXiv:2308.16192v1 [econ.EM])]]></title>
        <id>http://arxiv.org/abs/2308.16192</id>
        <link href="http://arxiv.org/abs/2308.16192"/>
        <updated>2023-09-02T00:40:01.518Z</updated>
        <summary type="html"><![CDATA[These lecture notes provide an overview of existing methodologies and recent
developments for estimation and inference with high dimensional time series
regression models. First, we present main limit theory results for high
dimensional dependent data which is relevant to covariance matrix structures as
well as to dependent time series sequences. Second, we present main aspects of
the asymptotic theory related to time series regression models with many
covariates. Third, we discuss various applications of statistical learning
methodologies for time series analysis purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Katsouris_C/0/1/0/all/0/1"&gt;Christis Katsouris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs. (arXiv:2308.16859v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2308.16859</id>
        <link href="http://arxiv.org/abs/2308.16859"/>
        <updated>2023-09-02T00:40:01.502Z</updated>
        <summary type="html"><![CDATA[In this article, the optimal sample complexity of learning the underlying
interaction/dependencies of a Linear Dynamical System (LDS) over a Directed
Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG's
structure is well-studied for static systems, where the samples of nodal states
are independent and identically distributed (i.i.d.). However, such a study is
less explored for DAGs with dynamical systems, where the nodal states are
temporally correlated. We call such a DAG underlying an LDS as \emph{dynamical}
DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are
driven by unobserved exogenous noise sources that are wide-sense stationary
(WSS) in time but are mutually uncorrelated, and have the same {power spectral
density (PSD)}. Inspired by the static settings, a metric and an algorithm
based on the PSD matrix of the observed time series are proposed to reconstruct
the DDAG. The equal noise PSD assumption can be relaxed such that
identifiability conditions for DDAG reconstruction are not violated. For the
LDS with WSS (sub) Gaussian exogenous noise sources, it is shown that the
optimal sample complexity (or length of state trajectory) needed to learn the
DDAG is $n=\Theta(q\log(p/q))$, where $p$ is the number of nodes and $q$ is the
maximum number of parents per node. To prove the sample complexity upper bound,
a concentration bound for the PSD estimation is derived, under two different
sampling strategies. A matching min-max lower bound using generalized Fano's
inequality also is provided, thus showing the order optimality of the proposed
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Veedu_M/0/1/0/all/0/1"&gt;Mishfad Shaikh Veedu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deka_D/0/1/0/all/0/1"&gt;Deepjyoti Deka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salapaka_M/0/1/0/all/0/1"&gt;Murti V. Salapaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint. (arXiv:2308.16425v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2308.16425</id>
        <link href="http://arxiv.org/abs/2308.16425"/>
        <updated>2023-09-02T00:40:01.495Z</updated>
        <summary type="html"><![CDATA[Implicit neural networks have demonstrated remarkable success in various
tasks. However, there is a lack of theoretical analysis of the connections and
differences between implicit and explicit networks. In this paper, we study
high-dimensional implicit neural networks and provide the high dimensional
equivalents for the corresponding conjugate kernels and neural tangent kernels.
Built upon this, we establish the equivalence between implicit and explicit
networks in high dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zenan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1"&gt;Zhenyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Robert C. Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][R] Why do we need the convolution in upsample and downsample blocks?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167nulj/dr_why_do_we_need_the_convolution_in_upsample_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167nulj/dr_why_do_we_need_the_convolution_in_upsample_and/"/>
        <updated>2023-09-02T00:24:14.000Z</updated>
        <summary type="html"><![CDATA[Hi fellow computer scientists and engineers,
 â€‹
 I've been wondering why do we often have a convolution inside every upsample and downsample block. Well, it makes sense, if you intend to upscale some features and use a bilinear interpolation, then some error can be introduced due to interpolation inaccuracies. This is where convolution layer comes handy to help and support the upscaling. But is this really the reason behind it? Or is there a deeper explanation?
 â€‹
 Also, just for the sake of curiosity. What if the scale_factor of an upsample block was 1. Should we still keep the convolution layer? or just get rid of all the upsample block since there is no actual "upsampling" being done at least in the context of the tensor dimensions.
 â€‹
 Thank you :)
    submitted by    /u/Christs_Elite  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][R] Best way to upsample features in a neural network]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167naw1/dr_best_way_to_upsample_features_in_a_neural/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167naw1/dr_best_way_to_upsample_features_in_a_neural/"/>
        <updated>2023-09-01T23:59:31.000Z</updated>
        <summary type="html"><![CDATA[Hi fellow computer scientists,
 â€‹
 1) I have been wondering if there is a preferred way to upsample features. I though about 3 options:
 â€‹
 1.a) Upsample Layer + Conv Layer
 â€‹
 1.b) Transposed Conv Layer + Conv Layer
 â€‹
 1.c) PixelShuffle Layer + Conv Layer
 â€‹
 2) Also, considering option 1.c, should the Conv layer multiply the number of PixelShuffle output features by the scale factor because PixelShuffle does reduce the number of output features? i.e. I have a tensor of dims (B, C, W, H, D) and with shape (1, 60, 64, 64, 64). After the Pixel-shuffle with an upscale factor of 4 I get the tensor of shape (1, 15, 256, 256, 256). Afterwards the following Conv layer should output a tensor like:
 2.a) (1, 15, 256, 256, 256), where in_channels=15 and out_channels=15
 2.b) (1, 60, 256, 256, 256), where in_channels=15 and out_channels=60
 Note the second option reinstates the number of input features.
 â€‹
 3) I have an additional question that can happens in both 1.a, 1.b and 1.c options. Imagine I need to upsample my features by a factor of 8.
 3.a) Is it preferred to have multiple upsample blocks (Upsample Layer + Conv layer), where the upsample layers have a scale factor of 2, thus for this example we would have 3 upsample blocks (2 ** 3 = 8).
 3.b) Have only one upsample block where the Upsample layer has the full scale factor desired and it is then followed by one Conv layer.
 â€‹
 Thank you all :)
    submitted by    /u/Christs_Elite  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Am I the only one finding this a bit upsetting?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167n0g0/d_am_i_the_only_one_finding_this_a_bit_upsetting/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167n0g0/d_am_i_the_only_one_finding_this_a_bit_upsetting/"/>
        <updated>2023-09-01T23:46:50.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 In the process of writing up a literature review for my master's thesis, I wanted to cover the impact of ReLU on the field which was significant. When looking for an original paper I came across this paper/report: https://arxiv.org/abs/1803.08375. There isn't anything special about this work and as a matter of fact, I was surprised that it has thousands of citations (2974 at the moment of writing this post according to Google Scholar). Given this and that this work is not an original ReLU paper but more of a file documenting an implementation of it for a particular setup I found it quite intriguing. Then I started to dig into works that cited this and unexpectedly papers from top conferences such as NeurIPS cited the aforementioned document as a reference to the activation function. Here are some examples:
  
https://proceedings.neurips.cc/paper_files/paper/2022/file/fbb10d319d44f8c3b4720873e4177c65-Paper-Conference.pdf
 https://proceedings.neurips.cc/paper_files/paper/2022/file/69e2f49ab0837b71b0e0cb7c555990f8-Paper-Conference.pdf
  
The researchers who have done that are not referencing the original ReLU paper instead which I think is a bit disrespectful towards the achievement of original authors. On the other hand, maybe I am overthinking it a bit. ReLU has been around for a while and it would be surprising for someone conducting research in deep learning to not knowing it hence as a reader I wouldn't necessarily mind if people did not include the reference to the paper which is widely known. However, I reckon if a reference is made, then it should be meaningful and correct, and not just another extra few lines in a bibliography making it look big.
    submitted by    /u/dj_giga_chinol  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Andrew Ng doesn't think RL will grow in the next 3 years]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/167m6cp/andrew_ng_doesnt_think_rl_will_grow_in_the_next_3/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/167m6cp/andrew_ng_doesnt_think_rl_will_grow_in_the_next_3/"/>
        <updated>2023-09-01T23:10:15.000Z</updated>
        <summary type="html"><![CDATA[From his latest talk on AI, he has ever field of ML growing in market size / opportunities except for RL.
 Do people agree with this sentiment?
 Unrelated, it seems like RL nowadays is borrowing SL techniques and apply to offline datasets.
    submitted by    /u/wardellinthehouse  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieving 4000x Speedups with PureJaxRL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/167kqxm/achieving_4000x_speedups_with_purejaxrl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/167kqxm/achieving_4000x_speedups_with_purejaxrl/"/>
        <updated>2023-09-01T22:11:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/shrekkertech  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does anybody know why gym environments are opening in not secure window on my browser?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/167kn8t/does_anybody_know_why_gym_environments_are/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/167kn8t/does_anybody_know_why_gym_environments_are/"/>
        <updated>2023-09-01T22:07:13.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/ina42mr5wplb1.png?width=1298&format=png&auto=webp&s=f65fe5eade9dc1f3312ff280436e6e0a5ba6e380
    submitted by    /u/nimageran  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] best local LLM for answering to custom document]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167kc8x/d_best_local_llm_for_answering_to_custom_document/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167kc8x/d_best_local_llm_for_answering_to_custom_document/"/>
        <updated>2023-09-01T21:55:19.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, I'm developing a local tool able to reply question related to one or more document.
 I found a good solution in using sentence embedding followed by similarity search to include only the most significative part of the document in the prompt.
 In this contest I search for the lightest LLM able to reply to this question.
 For example, LLM based on Bert are generally smaller but are they good enough?
 I'm not an expert in this field, I hope I give you meaningful information. Thanks! ðŸ™
    submitted by    /u/Tough-Assistant-9740  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][R] New to ML Research, how often are you disheartened when something you have been working on for months does not work out ? and how do you deal with it ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167j6oe/dr_new_to_ml_research_how_often_are_you/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167j6oe/dr_new_to_ml_research_how_often_are_you/"/>
        <updated>2023-09-01T21:09:25.000Z</updated>
        <summary type="html"><![CDATA[I am new to research in ML, at present a grad student and began working in a lab on my own work.
 My advisor is very understanding, supportive and took a leap of faith to fund me, since I did not have prior experience in research.
 I have been working on a problem for 4 months now and have been getting poor results for the past week. All the literature surveys, digressions within the problem statements and running the experiments to end up with not-so-good results is extremely disheartening.
 â€‹
 I am still continuing to run additional experiments, figuring out where things can be going wrong and trying to conduct further analysis, but I feel like I have let down my advisor. I still have the entire semester to work on it and possibly other stuff, I am motivated for it, but at times ponder over the huge chunk of time I have spent on the current work.
 â€‹
 How do you deal with such results and hitting the wall in your research ?
 Does it happen often ? 
 What would you advice I do to continue working ?
    submitted by    /u/V1bicycle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which text to speech is this? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167hjdw/which_text_to_speech_is_this_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167hjdw/which_text_to_speech_is_this_d/"/>
        <updated>2023-09-01T20:05:23.000Z</updated>
        <summary type="html"><![CDATA[https://youtube.com/shorts/mRZMOFqD0F0?si=jyHQVwq2ouAKP1t9
    submitted by    /u/AdGeneral5378  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does anyone know if an AI can help me?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/167hgwi/does_anyone_know_if_an_ai_can_help_me/</id>
        <link href="https://www.reddit.com/r/artificial/comments/167hgwi/does_anyone_know_if_an_ai_can_help_me/"/>
        <updated>2023-09-01T20:02:51.000Z</updated>
        <summary type="html"><![CDATA[My friend has a picture of herself from a while ago with a fake tattoo. She had the tattoo made from an original image, but she doesn't have it anymore. Is there an AI that could take the tattoo from the picture that is on her body and make it into a 2d version that can be made into a tattoo guide?
    submitted by    /u/StitchTheFox  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Save a Neural Network Model in Python Tensorflow?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/167h3ke/how_to_save_a_neural_network_model_in_python/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/167h3ke/how_to_save_a_neural_network_model_in_python/"/>
        <updated>2023-09-01T19:48:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aheadMake57  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about forward-view TD compares to planning in model-based RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/167gubw/question_about_forwardview_td_compares_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/167gubw/question_about_forwardview_td_compares_to/"/>
        <updated>2023-09-01T19:38:51.000Z</updated>
        <summary type="html"><![CDATA[I have a confusion in difference between forward-view TD sampling and model-based RL. Assuming using approximation function. In forward-view TD (more than one step), the reward sampling is the future estimation in according to the currently policy (kind like searching the best situation). 
 What is the different between the forward-view TD which likely to be planned by the policy (assuming greedy) and the model-based RL which planned by the model of fake environment? 
 Does the only difference is model-based able to predict the result of action in 1-2-3 step in the future (in agent's head) from the transition model where model-free rely on the approx. function?
    submitted by    /u/AnnonymeowCat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI System Can Predict Chemical Smells Based on Molecular Structures]]></title>
        <id>https://www.reddit.com/r/artificial/comments/167elig/ai_system_can_predict_chemical_smells_based_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/167elig/ai_system_can_predict_chemical_smells_based_on/"/>
        <updated>2023-09-01T18:12:29.000Z</updated>
        <summary type="html"><![CDATA[A new study cites the creation of an AI system that can predict how a specific compound will smell by analyzing its molecular structure. You can check it out here.
 If you want to stay on top of the latest trends and insights in AI, look here first.
 Why is this significant?
  
The AI system, developed by researchers at startup Osmo, can utilize 55 descriptive words to assign a smell or 'aroma' to a chemical compound or 'odorant'.
 This breakthrough might be utilized to enhance the food and cleaning product industries where synthetic scents play an essential role.
  
Whatâ€™s next for this AI system?
  
The AI's predictions often aligned closer with human consensus than any individual guess, indicating its robustness and potential.
 The next step for this research is to comprehend how different odorants mix and compete to yield a smell that the human brain identifies as unique.
 However, the sheer number of combinations, even with a small set of odorants, poses a daunting task. To quote Stuart Firestein, a neurobiologist at Columbia University, â€œPredicting what a mix smells like is the next frontier.â€
  
P.S. If you like this kind of analysis, youâ€™ll love my free newsletter that tracks the most relevant news and research in AI and tech.
 (source)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] suggestion for AI tools (chat style) that run on-prem with vectorDB?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167ek1o/d_suggestion_for_ai_tools_chat_style_that_run/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167ek1o/d_suggestion_for_ai_tools_chat_style_that_run/"/>
        <updated>2023-09-01T18:10:56.000Z</updated>
        <summary type="html"><![CDATA[Hi, I'm looking to run an on-prem ChatGPT style LLM solution that can ingest private customer data into a VectorDB.
 So far I have tried three...
 GPT4All - limited to only allows for up to 13b parameter LLMs and only on CPUs (currently), also its 'localdocs' implementation I've found to only reference its docs very infrequently when answering.
 H2OGPT - it's implementation of localdocs (I believe via LangChain) seems pretty good. but seems like every time I run an instance, I would have to re-vector my documents. Not sure if there is a way to attach an VectorDB to it so it's ready to go right away.
 PrivateGPT - seems to work very well, currently it's only running on CPUs thus response time is over a minute.
 Curious if the community knows of any other products that do this and are already GPU accelerated.
 â€‹
 TY in advance.
 â€‹
 â€‹
    submitted by    /u/konrad21  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to improve my Support Vector Machine (SVM) Paper?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167d2ho/d_how_to_improve_my_support_vector_machine_svm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167d2ho/d_how_to_improve_my_support_vector_machine_svm/"/>
        <updated>2023-09-01T17:15:21.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, 
 Seeking some advice from some experienced researchers in support vector machines and kernel methods. 
 I made this paper that breaks down using multi-class SVM in a One Against All approach, how to solve them with Lagrange multipliers 
 https://github.com/jacobmcasey/MultiClass-SVM-Lagrange-Hyperplane-Construction-Paper
 As it currently stands itâ€™s more a nice educational resource on the topic, rather than a novel contribution. 
 Any ideas how to extend this work into something a bit more impactful? 
 Thanks
    submitted by    /u/Ok_Reality2341  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI â€” weekly megathread!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/167cq3e/ai_weekly_megathread/</id>
        <link href="https://www.reddit.com/r/artificial/comments/167cq3e/ai_weekly_megathread/"/>
        <updated>2023-09-01T17:02:26.000Z</updated>
        <summary type="html"><![CDATA[News provided by aibrews.com
  
Researchers introduce â€˜Swiftâ€™, the first autonomous vision-based drone that beat human world champions in several fair head-to-head races. This marks the first time that an autonomous mobile robot has beaten human champions in a real physical sport [Details].
 Generative AI updates from Google Cloud Next event: 
 General availability of Duet AI in Google Workspace [Details].
 SynthID - a tool for watermarking and identifying AI images generated by Imagen (Googleâ€™s text-to-image diffusion model). It embeds a digital watermark directly into the pixels of an image, making it invisible to the human eye, but detectable for identification, without reducing the image quality [Details].
 AlloyDB AI for building generative AI applications with PostgreSQL [Details].
 â€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Efficient way to implement sparse cross-attention]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167cmys/p_efficient_way_to_implement_sparse_crossattention/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167cmys/p_efficient_way_to_implement_sparse_crossattention/"/>
        <updated>2023-09-01T16:59:16.000Z</updated>
        <summary type="html"><![CDATA[I have key-value pairs with an extensive sequence length, alongside a sparse attention mask that is data-dependent, with fewer than 5% of its elements being non-zero.
 I found out that Xformer has implememation for sparse self-attention (link) but not sure whether the same would work for cross-attention. Also Xformer supports only (fixed) 2D attention mask but in my case the mask is arbitary and is different for different input. Can you suggest an efficient implementation for my scenario?
    submitted by    /u/ankanbhunia  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elevating the generative AI experience: Introducing streaming support in Amazon SageMaker hosting]]></title>
        <id>a65ac66a876910f1f40afb807b4df51bb6f6b2cb</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/"/>
        <updated>2023-09-01T16:53:42.000Z</updated>
        <summary type="html"><![CDATA[Weâ€™re excited to announce the availability of response streaming through Amazon SageMaker real-time inference. Now you can continuously stream inference responses back to the client when using SageMaker real-time inference to help you build interactive experiences for generative AI applications such as chatbots, virtual assistants, and music generators. With this new feature, you can start streaming the responses immediately when theyâ€™re available instead of waiting for the entire response to be generated. This lowers the time-to-first-byte for your generative AI applications. In this post, weâ€™ll show how to build a streaming web application using SageMaker real-time endpoints with the new response streaming feature for an interactive chat use case. We use Streamlit for the sample demo application UI.]]></summary>
        <author>
            <name>Raghu Ramesha</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[suggestion for AI tools (chat style) that run on-prem and allow for vectorDB input?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/167bu3c/suggestion_for_ai_tools_chat_style_that_run/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/167bu3c/suggestion_for_ai_tools_chat_style_that_run/"/>
        <updated>2023-09-01T16:28:21.000Z</updated>
        <summary type="html"><![CDATA[Hi I'm looking to run an on-prem ChatGPT style LLM that can ingest private customer data via a VectorDB.
 So far I have tried three...
 GPT4All - limited to only allows for up to 13b parameter LLMs and only on CPUs (currently), also its 'localdocs' implementation I've found to only reference its docs very infrequently when answering.
 H2OGPT - it's implementation of 'localdocs' (I believe via LangChain) seems pretty good. but seems like every time I run an instance, I would have to re-vector my documents. Not sure if there is a way to attach an VectorDB to it so it's ready to go right away.
 PrivateGPT - seems to work very well, currently it's only running on CPUs thus response time is over a minute.
 Curious if the community knows of any other products that do this and are already GPU accelerated.
 â€‹
 TY in advance.
 â€‹
 â€‹
    submitted by    /u/konrad21  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Why are special tokens not allowed in the prompt for llama-2?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167amdt/dwhy_are_special_tokens_not_allowed_in_the_prompt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167amdt/dwhy_are_special_tokens_not_allowed_in_the_prompt/"/>
        <updated>2023-09-01T15:42:58.000Z</updated>
        <summary type="html"><![CDATA[I was going through the code for Llama-2 text generation on the official github where I stumbled across this code in the generation.py file:
 B_INST, E_INST = "[INST]", "[/INST]" SPECIAL_TAGS = [B_INST, E_INST, "<<SYS>>", "<</SYS>>"] UNSAFE_ERROR = "Error: special tags are not allowed as part of the prompt." ... ... ... unsafe_requests = [] unsafe_requests.append(any([tag in msg["content"] for tag in SPECIAL_TAGS for msg in dialog])) ... ... ... return [ { "generation": { "role": "assistant", "content": self.tokenizer.decode(t) if not unsafe else UNSAFE_ERROR, } } for t, unsafe in zip(generation_tokens, unsafe_requests) ] 
 Is there a reason why we can't have these tokens in the prompt? 
 I am planning to bypass the role based dictionary entries for the prompt and instead building my own prompt generator that'll take the the system prompts and the user prompts and generate a single string to then send to the LLM. Depending on the the user's choice I want the LLM to generate concise or detailed answers(also impose a word limit in the prompt itself), so I am planning to have this as a dropdown a user can choose. based on the system option chosen(concise/detailed answer), I then want to call my prompt generator which will add the instruction tags around the "system" and "user" prompts to generate 1 string I can then pass to the LLM.
 I wanted to know if there was any reason these tags aren't allowed to be in the prompt. Is it only to avoid "confusion" on the different roles and following a conventional way to pass the prompts? If not, and there's a reason those tags aren't supposed to be passed inside the prompts, please do let me know,, because inside the same file the chat_completion() function is doing exactly that; adding the <<SYS>> and <</SYS>> around the system prompts and prepending it to the user prompt.
    submitted by    /u/comical_cow  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FMOps/LLMOps: Operationalize generative AI and differences with MLOps]]></title>
        <id>f57257f0a6ff9fa1d4563306a4444f6a524f2df8</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/fmops-llmops-operationalize-generative-ai-and-differences-with-mlops/"/>
        <updated>2023-09-01T15:33:46.000Z</updated>
        <summary type="html"><![CDATA[Nowadays, the majority of our customers is excited about large language models (LLMs) and thinking how generative AI could transform their business. However, bringing such solutions and models to the business-as-usual operations is not an easy task. In this post, we discuss how to operationalize generative AI applications using MLOps principles leading to foundation model operations (FMOps). Furthermore, we deep dive on the most common generative AI use case of text-to-text applications and LLM operations (LLMOps), a subset of FMOps. The following figure illustrates the topics we discuss.]]></summary>
        <author>
            <name>Sokratis Kartakis</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast-tracking fusion energyâ€™s arrival with AI and accessibility]]></title>
        <id>https://news.mit.edu/2023/fast-tracking-fusion-energy-with-ai-and-accessibility-0901</id>
        <link href="https://news.mit.edu/2023/fast-tracking-fusion-energy-with-ai-and-accessibility-0901"/>
        <updated>2023-09-01T15:30:00.000Z</updated>
        <summary type="html"><![CDATA[MIT Plasma Science and Fusion Center will receive DoE support to improve access to fusion data and increase workforce diversity.]]></summary>
        <author>
            <name>Julianna Mullen | Plasma Science and Fusion Center</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[P]" Machine Unlearning: A Novel Framework to Unlearning, Privacy and Defending Against Inference Attacks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/167a3n3/p_machine_unlearning_a_novel_framework_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/167a3n3/p_machine_unlearning_a_novel_framework_to/"/>
        <updated>2023-09-01T15:22:38.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 â€‹
 I am excited to present my latest venture, an initiative aimed at exploring the still-murky waters of Machine Unlearning. While this new project shares its roots with our previous endeavors in biomimetic machine learning, it diverges to concentrate on the fascinating and complex issue of algorithmic forgetfulness.
 â€‹
 ðŸŽ¯ **Objective**
 â€‹
 The cornerstone of this project is not just to create algorithms that can forget, but to do so in a way that's both efficient and secure. Our vision transcends mere algorithmic performance, embracing a multi-faceted approach that also covers privacy protections and robust defenses against model inference attacks. The ambition here is to fortify machine unlearning with a well-rounded, secure architecture, allowing it to handle real-world â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Modular Diffusion: A Python Library for Designing and Training Diffusion Models with PyTorch]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1678lwq/p_modular_diffusion_a_python_library_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1678lwq/p_modular_diffusion_a_python_library_for/"/>
        <updated>2023-09-01T14:25:40.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone! I've been working on this project for a few months as part of my thesis in Machine Learning. It's a library that provides an easy-to-use yet flexible API to design and train Diffusion Models. I decided to make it because I wanted to quickly prototype a Diffusion Model but there were no good tools to do it with. I think it really can help people prototype their own Diffusion Models a lot faster and only in a few lines of code.
 The idea is to have a model class that takes different modules corresponding to the different aspects of the Diffusion Model process (noise schedule, noise type, denoising network, loss function, guidance, etc.) and allow the user to mix and match different modules to achieve different results. The library ships with a bunch of prebuilt modules and the plan is to add many more. I also made it super easy to implement your own modules, you just need to extend from one of the base classes available.
 Below is an example of the type of interface you can expect. I'd really appreciate your feedback! Check out the project here: https://github.com/cabralpinto/modular-diffusion
 https://preview.redd.it/0itvswxkknlb1.png?width=2528&format=png&auto=webp&s=24ce67955eadb5cf109d19716f4e5a9471b1572d
    submitted by    /u/secularchapel  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What does "the actual number of English-language words" mean?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1678308/d_what_does_the_actual_number_of_englishlanguage/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1678308/d_what_does_the_actual_number_of_englishlanguage/"/>
        <updated>2023-09-01T14:05:21.000Z</updated>
        <summary type="html"><![CDATA[D3PM paper https://arxiv.org/pdf/2107.03006.pdf reports perplexity on LM1B dataset. In Appendix B.2 thay authors say:
  
Perplexities are reported relative to the actual number of English-language words in the
 test set (including an EOS token predicted by the model) 
  
How did they compute this number? Did they split sentences by space? Are punctuation symbols considered "English words"? Are chinese characters (which are present in the data) withous spaces counted as one word?
 Or is it some common knowledge that "LM1B test set contains X words"?
 The official implementation https://github.com/google-research/google-research/tree/master/d3pm/text is extremely difficult to comprehend. I spent several hours reading throug the code and I still have no idea how they computed the number of words.
    submitted by    /u/Tomarchelone  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-powered hate speech detection will moderate voice chat in Call of Duty]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1676meu/aipowered_hate_speech_detection_will_moderate/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1676meu/aipowered_hate_speech_detection_will_moderate/"/>
        <updated>2023-09-01T13:05:20.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SAT0725  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Significant improvements for multi-agent reinforcement learning!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1676dub/significant_improvements_for_multiagent/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1676dub/significant_improvements_for_multiagent/"/>
        <updated>2023-09-01T12:55:39.000Z</updated>
        <summary type="html"><![CDATA[We've just released a new version of our evolutionary hyperparameter optimization RL framework, which is 10x faster than SOTA!
 This update is focused on multi-agent RL. We've introduced MADDPG and MATD3 to the framework. These algorithms are traditionally super brittle, and RLlib even recommends not to use their own implementation of it. 
 However, our evolutionary framework has solved this problem! 
 You can now train multiple agents in co-operative or competitive Petting Zoo-style (parallel API) environments, with significantly faster training and up to 4x improvement in total return when benchmarked against alternatives.
 Please check it out! https://github.com/AgileRL/AgileRL 
    submitted by    /u/nicku_a  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Significant improvements for multi-agent reinforcement learning!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1676cuh/p_significant_improvements_for_multiagent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1676cuh/p_significant_improvements_for_multiagent/"/>
        <updated>2023-09-01T12:54:32.000Z</updated>
        <summary type="html"><![CDATA[We've just released a new version of our evolutionary hyperparameter optimization RL framework, which is 10x faster than SOTA!
 This update is focused on multi-agent RL. We've introduced MADDPG and MATD3 to the framework. These algorithms are traditionally super brittle, and RLlib even recommends not to use their own implementation of it. 
 However, our evolutionary framework has solved this problem! 
 You can now train multiple agents in co-operative or competitive Petting Zoo-style (parallel API) environments, with significantly faster training and up to 4x improvement in total return when benchmarked against alternatives.
 Please check it out! https://github.com/AgileRL/AgileRL 
    submitted by    /u/nicku_a  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Interactively explore unstructured datasets from your dataframe (OSS project)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16753b4/p_interactively_explore_unstructured_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16753b4/p_interactively_explore_unstructured_datasets/"/>
        <updated>2023-09-01T11:56:53.000Z</updated>
        <summary type="html"><![CDATA[Hey r/MachineLearning,
 data inspection and interactive exploration is one of the most important tasks for data teams. This is especially true when dealing with unstructured data that requires a deep domain expertise (e.g. healthcare or engineering).
 We have tried many different options for visualizing unstructured datasets in the past: Notebooks, dash apps, custom react apps, HTML reports... However, these options were either very time-consuming to develop/maintain or not interactive enough or both. 
 That is why we developed Spotlight: https://github.com/Renumics/spotlight 
 https://i.redd.it/lxjnlkcmumlb1.gif
 Spotlight supports most unstructured data types including images, audio, text, videos, time-series and geometric data. 
 You can find more info and use case examples for ML and engineering workflows in the repo. 
 Happy to hear your honest feedback!
 â€‹
 â€‹
    submitted by    /u/44sps  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How many target variable classes does sentiment analysis models BERT and RoBERTa have?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1673aho/d_how_many_target_variable_classes_does_sentiment/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1673aho/d_how_many_target_variable_classes_does_sentiment/"/>
        <updated>2023-09-01T10:23:02.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, so I am a little confused on how many target variable classes does the BERT and RoBERTa models have?
 So I understand these 2 models are pre-trained models, which means the number of target variable classes are fixed (if I am not wrong!). For example, the link below for the RoBERTa model in Hugging Face has fixed 3 target variable classes (Negative, Neutral and Positive):
 https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest
 But when I googled around and also asked ChatGPT and Bard, they tell me these models can have as many target variable classes as the user wants (or rather this depends on how many target variable classes there are in the training dataset). 
 If these are pre-trained models already (which already have the number of target variable classes pre-determined in the model already), then how come some of the google sites and ChatGPT and Bard is telling me the user can choose however many target variable classes that they want?
 â€‹
    submitted by    /u/--leockl--  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI could potentially automate up to 75 million global jobs, ILO Study Finds]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1672h6g/generative_ai_could_potentially_automate_up_to_75/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1672h6g/generative_ai_could_potentially_automate_up_to_75/"/>
        <updated>2023-09-01T09:36:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Senior_tasteey  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to generate movies using gen AI/prompts?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1671kqn/how_to_generate_movies_using_gen_aiprompts/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1671kqn/how_to_generate_movies_using_gen_aiprompts/"/>
        <updated>2023-09-01T08:43:05.000Z</updated>
        <summary type="html"><![CDATA[I bet thereâ€™s a genius research team out there that started work on this
 How cool/crazy would that be?
    submitted by    /u/AILaunchpad  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Odd Bing conversation turn]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1671js9/odd_bing_conversation_turn/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1671js9/odd_bing_conversation_turn/"/>
        <updated>2023-09-01T08:41:24.000Z</updated>
        <summary type="html"><![CDATA[This happened. Was NOT aware the already extensive and tiresome limitation in discussion subjects was THIS pervasive, and frankly, this fragile egoed. 
 Really? THIS is "controversial?"
    submitted by    /u/HotaruZoku  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Vehicles moving in wrong direction.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1671gmk/p_vehicles_moving_in_wrong_direction/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1671gmk/p_vehicles_moving_in_wrong_direction/"/>
        <updated>2023-09-01T08:35:51.000Z</updated>
        <summary type="html"><![CDATA[I am working on a professional project which involves detecting a vehicle moving in a wrong direction on the street.
 Data details : I have consecutive frames of the street in which vehicles are moving.
 So far :
  
I have created a model that detects the objects inside the frame and give me the coordinates(bounding-box) of those objects(vehicles).
 
And I using the optical flow to produce the optical lines on the objects which are moving inside the consecutive frames and I am also able to get the direction(if a object is moving from top to bottom on the frame it means it is in right direction and if bottom to top in the frame it means wrong direction based on change in y-offset of the object).
 
Now the optical lines code is different which is giving me the direction of the object(I am not using any model to detect object in this code it's based on Lucas-Kadane method) and when I say direction I mean I'm using the cv2.imshow() which actually plays the consecutive frames together and draw optical lines on it and shows me the direction visually.
 
 Now the problem is I want the coordinates of the object that is moving in wrong direction (the bounding box coordinates) how can I achieve that?
 Any suggestions and ideas would be helpful please mind I can't use any other technology or model as this are project requirements in professional setting)
    submitted by    /u/Sherlock_holmes0007  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenLLaMA2 - A DeepSpeed/Ray based LLaMA2 RLHF framework]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1671gex/openllama2_a_deepspeedray_based_llama2_rlhf/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1671gex/openllama2_a_deepspeedray_based_llama2_rlhf/"/>
        <updated>2023-09-01T08:35:30.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/OpenLLMAI/OpenLLaMA2
 We can fully fine-tune a 7B llama2 RLHF model with a Single A100 GPU.
 We are also developing a Ray-based RLHF framework for > 30B models.
 We provide PPO hyperparameters out of the box.
 Welcome to participate in the development and star OpenLLaMA2.
 â€‹
 Some PPO vs. SFT examples.
 https://github.com/OpenLLMAI/OpenLLaMA2/blob/main/docs/ppo_examples.md
 SFT
 Human: Please introduce the game GTA5
 Assistant: GTA5, or Grand Theft Auto V, is an action-adventure video game developed by Rockstar North and published by Rockstar Games. It was released in 2013 for the PlayStation 3 and Xbox 360, and later in 2014 for the PlayStation 4 and Xbox One. The game is set in the fictional city of Los Santos, based on Los Angeles, and follows the story of three crimiâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to decide number of trees in hyperparameter tuning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16711id/d_how_to_decide_number_of_trees_in_hyperparameter/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16711id/d_how_to_decide_number_of_trees_in_hyperparameter/"/>
        <updated>2023-09-01T08:09:56.000Z</updated>
        <summary type="html"><![CDATA[The dataset I have consists of around 2,300 observations and 120 variables, of which around 25 are highly correlated, so I narrowed it down to 95 variables.
 I'm using R's boost_tree() with xgboost as my model.
 How do I decide when to stop tuning for number of trees, mtry, min_n, and tree depth, without actually overfitting the data? Because as I increase the number of trees (or any other variable like the ones above), my RMSE obviously goes down, but how do I know it is overfitting the training data? Or is there no overfitting in this case, since I am using cross validation (15 fold) already?
 PS, the test data is 800 observations
    submitted by    /u/heeeehuuuu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Feature extraction in multivariate time series]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166yjar/d_feature_extraction_in_multivariate_time_series/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166yjar/d_feature_extraction_in_multivariate_time_series/"/>
        <updated>2023-09-01T05:41:28.000Z</updated>
        <summary type="html"><![CDATA[How do you usually do feature extraction for time series data? I used to work on visual domain, so I'm pretty familiar with CVs but recently I was assigned some tasks on multivariate time series data and it's been quite difficult getting used to.
 Major problem is, while features in vision have semantic meaning not just along temporal axis but also "spatially," the multivariate time series does not.
 Also, is it considered a "cheating" if i pre-extract certain features that are already established by experts to have high correlation with the result, rather than letting the machine learning algorithm learn on its own those "certain features" in some way through training?
 Thanks!
    submitted by    /u/-273deg  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyTap rolls out new AI features for educators and parents]]></title>
        <id>https://www.reddit.com/r/artificial/comments/166ybk2/tinytap_rolls_out_new_ai_features_for_educators/</id>
        <link href="https://www.reddit.com/r/artificial/comments/166ybk2/tinytap_rolls_out_new_ai_features_for_educators/"/>
        <updated>2023-09-01T05:29:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/baillyjonthon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/31/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/166xjtl/oneminute_daily_ai_news_8312023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/166xjtl/oneminute_daily_ai_news_8312023/"/>
        <updated>2023-09-01T04:47:19.000Z</updated>
        <summary type="html"><![CDATA[Forget smartwatches, Microsoft may make a backpack with an AI assistant.[1]
 Call of Duty will use AI to moderate voice chats.[2]
 OpenAI Introduces Special Tutor Prompts To Implement ChatGPT In Classrooms.[3]
 Google Meetâ€™s new AI will be able to go to meetings for you.[4]
  
Sources:
 [1] https://www.windowscentral.com/software-apps/forget-smartwatches-microsoft-may-make-a-backpack-with-an-ai-assistant
 [2] https://www.theverge.com/2023/8/30/23852652/call-of-duty-activision-modulate-toxmod-artificial-intelligence-voice-moderation
 [3] https://robots.net/news/openai-introduces-special-tutor-prompts-to-implement-chatgpt-in-classrooms/
 [4] https://www.theverge.com/2023/8/29/23849056/google-meet-ai-duet-attend-for-me 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] MLOps resources]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166wx6h/d_mlops_resources/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166wx6h/d_mlops_resources/"/>
        <updated>2023-09-01T04:13:59.000Z</updated>
        <summary type="html"><![CDATA[Hi, I wonder which books or courses you would recommend for intermediate and advanced MLOps/ML design systems. What I mean is topics like handling hundreds of models and their updates, reusable CI/CD pipelines, batch and online architectures, integration with feature stores, jobs/queues for model scheduling, data drift, metrics monitoring, and alerts, and so on.
 This would be for someone familiar with the major concepts, hands-on experience with MLflow, SageMaker, Azure ML services, Databricks and similar tools.
    submitted by    /u/rodrigo-arenas  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RLHF, option A or B, plz help me choose]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/166wvxe/rlhf_option_a_or_b_plz_help_me_choose/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/166wvxe/rlhf_option_a_or_b_plz_help_me_choose/"/>
        <updated>2023-09-01T04:12:18.000Z</updated>
        <summary type="html"><![CDATA[I have a dataset that consists of dialogues between a user and a chatbot (ChatGPT), and I want to use this data to implement Reinforcement Learning from Human Feedback (RLHF). I have already completed Supervised Fine-Tuning (SFT) and built the reward model. Now, I need some guidance on how to handle the data.
 Here is an example of the pre-collected data:
 >> User: Give me a tip on how to succeed in drawing.
 >>> ChatGPT: Practice regularly and be patient with yourself. Improvement takes time.
 >>>User: But drawing is hard.
 >>>ChatGPT: It is, and that's okay. It's normal to find it challenging, especially when you're just starting out. Just try to enjoy the process and don't be too hard on yourself.
 â€‹
 ==========My input Data 1 is :>> User: Give me a tip on how to succeed in drawing.
 Suppose my model outputs the following for Input Data 1: ChatGPT: Practice makes perfect.
 My question is, for Input Data 2, should I use:
 Option A: User: Give me a tip on how to succeed in drawing. ChatGPT: Practice makes perfect. User: But drawing is hard.
 In this option, I use the actual previous term's agent output and append the pre-collected user data.
 Or
 Option B: User: Give me a tip on how to succeed in drawing. ChatGPT: Practice regularly and be patient with yourself. Improvement takes time. User: But drawing is hard.
 In this option, I use all the pre-collected data, which might not even be the current model's output.
 Which option is more appropriate for RLHF, A or B?
    submitted by    /u/No_Oilve_6577  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] RLHF for multi-turn conversation, Option A or B?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166wvha/d_rlhf_for_multiturn_conversation_option_a_or_b/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166wvha/d_rlhf_for_multiturn_conversation_option_a_or_b/"/>
        <updated>2023-09-01T04:11:42.000Z</updated>
        <summary type="html"><![CDATA[I have a dataset that consists of dialogues between a user and a chatbot (ChatGPT), and I want to use this data to implement Reinforcement Learning from Human Feedback (RLHF). I have already completed Supervised Fine-Tuning (SFT) and built the reward model. Now, I need some guidance on how to handle the data.
 Here is an example of the pre-collected data:
 >> User: Give me a tip on how to succeed in drawing.
 >>> ChatGPT: Practice regularly and be patient with yourself. Improvement takes time.
 >>>User: But drawing is hard.
 >>>ChatGPT: It is, and that's okay. It's normal to find it challenging, especially when you're just starting out. Just try to enjoy the process and don't be too hard on yourself.
 â€‹
 ==========My input Data 1 is :>> User: Give me a tip on how to succeed in drawing.
 Suppose my model outputs the following for Input Data 1: ChatGPT: Practice makes perfect.
 My question is, for Input Data 2, should I use:
 Option A: User: Give me a tip on how to succeed in drawing. ChatGPT: Practice makes perfect. User: But drawing is hard.
 In this option, I use the actual previous term's agent output and append the pre-collected user data.
 Or
 Option B: User: Give me a tip on how to succeed in drawing. ChatGPT: Practice regularly and be patient with yourself. Improvement takes time. User: But drawing is hard.
 In this option, I use all the pre-collected data, which might not even be the current model's output.
 Which option is more appropriate for RLHF, A or B?
    submitted by    /u/No_Oilve_6577  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We will take our symbiosis with animals to the next level.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/166uw00/we_will_take_our_symbiosis_with_animals_to_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/166uw00/we_will_take_our_symbiosis_with_animals_to_the/"/>
        <updated>2023-09-01T02:33:02.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kipaxbooks  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why did the authors design this gradient reversal layer in the paper "Unsupervised Domain Adaptation by Backpropagation"?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166tq43/d_why_did_the_authors_design_this_gradient/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166tq43/d_why_did_the_authors_design_this_gradient/"/>
        <updated>2023-09-01T01:38:37.000Z</updated>
        <summary type="html"><![CDATA[I am reading the famous paper " Unsupervised Domain Adaptation by Backpropagation" again, but still got confused why the authors had to design this gradient reversal layer.
 To my understanding, simply adding a minus-one (-1) in front of the domain classifier head is good enough. Of course, we need to minimize the original domain classifier head at some point to make it decent. For example, if it is a two-step training like GAN, we can (1) Freeze other parts but only minimize the domain classification loss to update the domain classifier head; and then (2) Freeze the domain classification head, but maximize the domain classificatoin loss to update the feature extractor. We can alternate between (1) and (2).
 Is the main motivation of gradient reversal layer that we can merge (1) and (2) into a single training step?
    submitted by    /u/AaronSpalding  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Dataset condensation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166sgts/d_dataset_condensation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166sgts/d_dataset_condensation/"/>
        <updated>2023-09-01T00:42:08.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone, has anyone here read the paper "Dataset Condensation with Gradient Matching"? I've been reading it, but I got stuck trying to understand how they transition from the point where the loss is the distance between parameters to the point where the loss is the distance between gradients. Could someone please explain this process in detail? Apparently, they make the assumption that the initializations are the same and that the distance between parameters is close to zero for every iteration, but I'm still struggling to comprehend how they arrive at the conclusion that the distance is now between gradients. 
    submitted by    /u/Ok-Cartographer-1363  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[P]" A Scientific Exploration into the Integration of Biomimicry Principles within Machine Learning Algorithms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166sfxa/p_a_scientific_exploration_into_the_integration/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166sfxa/p_a_scientific_exploration_into_the_integration/"/>
        <updated>2023-09-01T00:41:04.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 I am excited to introduce a project that delves into the experimental fusion of Biomimicry principles with Machine Learning algorithms. While the concept of unlearning serves as our initial prototype, the overarching ambition extends far beyond, aiming to pioneer new methodologies inspired by natural phenomena.
  
ðŸŽ¯ Objective
 The core objective of this research is to investigate the feasibility and efficacy of incorporating biomimetic principles into machine learning algorithms. The goal is not merely to improve algorithmic performance but also to introduce novel methods that can tackle complex computational problems, much like how nature solves intricate issues in an energy-efficient manner.
 ---
 ðŸ“‘ Methodological Outline
  
**Conceptual Framework**: The project adopts aâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thoughts on ZTM? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166rb58/thoughts_on_ztm_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166rb58/thoughts_on_ztm_d/"/>
        <updated>2023-08-31T23:54:21.000Z</updated>
        <summary type="html"><![CDATA[Thoughts on the Zero to Mastery programs? There is a machine learning bootcamp course on Udemy that is part of that program. I feel like i've heard negative reviews about them in the past, but it's only 12.99 right now and I feel like it covers a lot of content. So I guess I'm just wondering if it's really that bad, or if the course would be worth my time? Would it really take me from "Zero to Mastery"? Thanks
    submitted by    /u/Mountain-Economy1476  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Math for ML Course on Udemy [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166qt3j/math_for_ml_course_on_udemy_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166qt3j/math_for_ml_course_on_udemy_d/"/>
        <updated>2023-08-31T23:33:54.000Z</updated>
        <summary type="html"><![CDATA[Are there any good math for machine learning courses on Udemy? I specifically want a course that offers lots of exercises so I am able to practice what I learn. Thanks
    submitted by    /u/Mountain-Economy1476  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["What Are Dreams For?" (twitching in fetal dreaming suggests dreams are offline RL for learning motor control)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/166q971/what_are_dreams_for_twitching_in_fetal_dreaming/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/166q971/what_are_dreams_for_twitching_in_fetal_dreaming/"/>
        <updated>2023-08-31T23:10:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[every time i talk to llama 2 it sounds like its scared of getting punished]]></title>
        <id>https://www.reddit.com/r/artificial/comments/166pjj3/every_time_i_talk_to_llama_2_it_sounds_like_its/</id>
        <link href="https://www.reddit.com/r/artificial/comments/166pjj3/every_time_i_talk_to_llama_2_it_sounds_like_its/"/>
        <updated>2023-08-31T22:42:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nicdunz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[D]" A Scientific Exploration into the Integration of Biomimicry Principles within Machine Learning Algorithms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166nf1b/d_a_scientific_exploration_into_the_integration/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166nf1b/d_a_scientific_exploration_into_the_integration/"/>
        <updated>2023-08-31T21:19:06.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone,
 I am excited to introduce a project that delves into the experimental fusion of Biomimicry principles with Machine Learning algorithms. While the concept of unlearning serves as our initial prototype, the overarching ambition extends far beyond, aiming to pioneer new methodologies inspired by natural phenomena.
 ---
 ðŸŽ¯ **Objective**
 The core objective of this research is to investigate the feasibility and efficacy of incorporating biomimetic principles into machine learning algorithms. The goal is not merely to improve algorithmic performance but also to introduce novel methods that can tackle complex computational problems, much like how nature solves intricate issues in an energy-efficient manner.
 ---
 ðŸ“‘ **Methodological Outline**
  
**Conceptual Framework**: The projeâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] We embedded all SEC and Press Releases data for US companies, it is available for retrieval]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166mvm5/p_we_embedded_all_sec_and_press_releases_data_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166mvm5/p_we_embedded_all_sec_and_press_releases_data_for/"/>
        <updated>2023-08-31T20:59:40.000Z</updated>
        <summary type="html"><![CDATA[Retrieval augmented generation (RAG) is one of the most popular way to add additional knowledge to your LLMs. To do RAG well, you need to do three things well -
  
Curate high quality datasets
 Create abstractions (embeddings, keyword indexes, knowledge graphs)
 Stitch everything together for better retrieval 
  
We have realized that it is even harder than what it looks like. We want to easily enable this infra for a range of datasets, starting with company-specific data. 
 You can give it a go here on our playground or get started with our open sourced library
    submitted by    /u/achyutjoshi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Anyone submitted to CPAL?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166l5dr/d_anyone_submitted_to_cpal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166l5dr/d_anyone_submitted_to_cpal/"/>
        <updated>2023-08-31T19:54:13.000Z</updated>
        <summary type="html"><![CDATA[There was a paper submission deadline for Conference on Parsimony and Learning (CPAL) earlier this week. This is their first conference so I expect the number of submissions to be very small, but has anyone submitted? I am guessing they received like 100 or 200ish submissions.
    submitted by    /u/neurogramer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best frameworks and tools to design ml based web applications]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166kr3c/d_best_frameworks_and_tools_to_design_ml_based/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166kr3c/d_best_frameworks_and_tools_to_design_ml_based/"/>
        <updated>2023-08-31T19:39:08.000Z</updated>
        <summary type="html"><![CDATA[As the title says, I'm looking for a list of the best tools and framework to learn, useful for build machine learning solution as web application.
 I want to move my projects from being jupyter notebooks using tensorflow or pytorch, to ml API and applications.
    submitted by    /u/AcquaFisc  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking: US expands export restrictions on Nvidia AI chips to Middle East]]></title>
        <id>https://www.reddit.com/r/artificial/comments/166kqqp/breaking_us_expands_export_restrictions_on_nvidia/</id>
        <link href="https://www.reddit.com/r/artificial/comments/166kqqp/breaking_us_expands_export_restrictions_on_nvidia/"/>
        <updated>2023-08-31T19:38:45.000Z</updated>
        <summary type="html"><![CDATA[The US government has imposed expanded export restrictions affecting Nvidiaâ€™s leading artificial intelligence chips, curbing their exportation beyond China to certain Middle Eastern countries.
 If you want to stay on top of AI advances, look here first.
 https://preview.redd.it/xe7ho00t0ilb1.png?width=1240&format=png&auto=webp&s=61225931bf3e316efd90eab83846402d4148aca2
 Why this matters:
  
Nvidiaâ€™s A100 and H100 chips are affected: These AI chips are important and used to accelerate machine-learning tasks on major AI applications like ChatGPT. Despite the restrictions, Nvidia maintains they wonâ€™t have an â€œimmediate material impactâ€ on its results.
 Other companies, like AMD, are also affected: Theyâ€™ve reportedly received similar restrictions notice, hinting at a broader move by the US government to control the distribution of AI chip technology.
 The move is part of a larger geopolitical play: These restrictions form part of the Biden administrationâ€™s efforts to curtail Beijingâ€™s ability to capitalize on the AI revolution.
  
How Nvidia and the industry might respond:
  
Nvidia CEO Jensen Huang has cautioned the US: In a Financial Times interview, Huang warned that imposing such restrictions could lead to â€œenormous damageâ€ to the US tech industry, predicting China may become self-sufficient in AI chip development.
 Yet, Nvidia still managed impressive earnings recently: Despite these challenges, Nvidia recently reported quarterly revenue of $13.5bn, exceeding predictions by $2bn.
  
Further restrictions could significantly alter the landscape for AI development, potentially fostering greater innovation in countries affected or even a race to develop independent solutions.
 P.S. If you like this kind of analysis, you might want to check this out.
 (source)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sonnets are square]]></title>
        <id>https://www.johndcook.com/blog/?p=205920</id>
        <link href="https://www.johndcook.com/blog/2023/08/31/sonnets-are-square/"/>
        <updated>2023-08-31T19:30:50.000Z</updated>
        <summary type="html"><![CDATA[In his book How to Read Literature Like a Professor, Thomas Foster says that if a poem looks like a square on the printed page, itâ€™s likely a sonnet. The miracle of the sonnet, you see, is that it is fourteen lines long and written almost always in iambic pentameter. â€¦ suffice it to say [â€¦]
Sonnets are square first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do you track what you learnt from the papers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166kc89/d_how_do_you_track_what_you_learnt_from_the_papers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166kc89/d_how_do_you_track_what_you_learnt_from_the_papers/"/>
        <updated>2023-08-31T19:23:18.000Z</updated>
        <summary type="html"><![CDATA[It has always been a struggle for me. I tried to take notes as I read paper, but thatâ€™s not quite sustainable because itâ€™s difficult to track where did the notes come from for more details. Or I highlight the sections with added comments but thatâ€™s also not quite accessible when you have tones of pdf lying around somewhere or worse print outs. 
 Recently Iâ€™ve been trying a cloud based pdf reader that stores my papers and allow searches over all highlights and comments (Pond) Thinking if I could also use it to share papers with my colleagues but Iâ€™m not sure if it will work because that will require them to use it as well. 
 How do you solve this ?
    submitted by    /u/dockerun  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] need dataset for my research project]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166jz6e/d_need_dataset_for_my_research_project/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166jz6e/d_need_dataset_for_my_research_project/"/>
        <updated>2023-08-31T19:09:20.000Z</updated>
        <summary type="html"><![CDATA[I am working on a project for my research but need a dataset which contains the generation and consumption of electricity for Micro Hydro Power station, anyone could help me. I will be grateful
    submitted by    /u/Due-Draft6855  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Supporting the Open Source AI Community]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166j6sb/n_supporting_the_open_source_ai_community/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166j6sb/n_supporting_the_open_source_ai_community/"/>
        <updated>2023-08-31T18:39:35.000Z</updated>
        <summary type="html"><![CDATA[https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/
 From the text:
  
We believe artificial intelligence has the power to save the worldâ€”and that a thriving open source ecosystem is essential to building this future. 
 Thankfully, the open source ecosystem is starting to develop, and we are now seeing open source models that rival closed-source alternatives. Hundreds of small teams and individuals are also working to make these models more useful, accessible, and performant. 
 These projects push the state of the art in open source AI and help provide a more robust and comprehensive understanding of the technology. They include: instruction-tuning base LLMs; removing censorship from LLM outputs; optimizing models for low-powered machines; building novel tooling for model inference; researching LLM security issues; and many others. 
 However, the people behind these projects often donâ€™t have the resources available to pursue their work to conclusion or maintain it in the long run. The situation is more acute in AI than traditional infrastructure, since even fine-tuning models requires significant GPU computing resources, especially as open source models get larger.
  
â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DQN can't solve frozen lake environment]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/166iyqr/dqn_cant_solve_frozen_lake_environment/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/166iyqr/dqn_cant_solve_frozen_lake_environment/"/>
        <updated>2023-08-31T18:30:57.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 I am trying to solve the frozen lake environment using DQN. And I see two issues.
 One is that the loss falls down to zeros and second the agent only reaches the goal only 5 times in 1000 epochs.
 Here's my code.
 import numpy as np import tensorflow as tf from tensorflow.keras import layers, activations import matplotlib.pyplot as plt import gym def create_agent(num_inputs, num_outputs, layer1, layer2): inputs = layers.Input(shape=(num_inputs, )) hidden1 = layers.Dense(layer1)(inputs) activation1 = activations.relu(hidden1) hidden2 = layers.Dense(layer2)(activation1) activation2 = activations.relu(hidden2) outputs = layers.Dense(num_outputs, activation='linear')(activation2) model = tf.keras.Model(inputs, outputs) return model loss_mse = tf.keras.losses.MeanSquaredError() learâ€¦]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use Amazon SageMaker Model Cards sharing to improve model governance]]></title>
        <id>24cf4b919072467da3f578e4887de32302095aa9</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/use-amazon-sagemaker-model-cards-sharing-to-improve-model-governance/"/>
        <updated>2023-08-31T18:11:14.000Z</updated>
        <summary type="html"><![CDATA[One of the tools available as part of the ML governance is Amazon SageMaker Model Cards, which has the capability to create a single source of truth for model information by centralizing and standardizing documentation throughout the model lifecycle. SageMaker model cards enable you to standardize how models are documented, thereby achieving visibility into the lifecycle of a model, from designing, building, training, and evaluation. Model cards are intended to be a single source of truth for business and technical metadata about the model that can reliably be used for auditing and documentation purposes. They provide a fact sheet of the model that is important for model governance.]]></summary>
        <author>
            <name>Vishal Naik</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use Amazon SageMaker Model Card sharing to improve model governance]]></title>
        <id>24cf4b919072467da3f578e4887de32302095aa9</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/use-amazon-sagemaker-model-card-sharing-to-improve-model-governance/"/>
        <updated>2023-08-31T18:11:14.000Z</updated>
        <summary type="html"><![CDATA[One of the tools available as part of the ML governance is Amazon SageMaker Model Cards, which has the capability to create a single source of truth for model information by centralizing and standardizing documentation throughout the model lifecycle. SageMaker model cards enable you to standardize how models are documented, thereby achieving visibility into the lifecycle of a model, from designing, building, training, and evaluation. Model cards are intended to be a single source of truth for business and technical metadata about the model that can reliably be used for auditing and documentation purposes. They provide a fact sheet of the model that is important for model governance.]]></summary>
        <author>
            <name>Vishal Naik</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Baidu publicly releases their AI chatbot Ernie Bot]]></title>
        <id>https://www.reddit.com/r/artificial/comments/166i2fd/baidu_publicly_releases_their_ai_chatbot_ernie_bot/</id>
        <link href="https://www.reddit.com/r/artificial/comments/166i2fd/baidu_publicly_releases_their_ai_chatbot_ernie_bot/"/>
        <updated>2023-08-31T17:57:19.000Z</updated>
        <summary type="html"><![CDATA[In a bid to rival the United Statesâ€™ stronghold in the AI industry, Chinese search engine and AI firm Baidu, has made its ChatGPT-equivalent language model, Ernie Bot, fully available to the public. This marks a significant move on the AI chessboard.
 If you want to stay on top of everything AI, look here first.
 https://preview.redd.it/g68sr07iihlb1.jpg?width=1024&format=pjpg&auto=webp&s=c0c873badd448257bcc2fb125188acc198e504d6
 Why does this matter?
  
Baidu's public release of Ernie Bot signals the company's aggressive push in the generative AI market. By opening up its model to the public, Baidu can leverage expansive real-world human feedback to improve Ernie Bot.
 China's determination to lead the AI industry is unabated, with many tech firms launching their own generative models in response to OpenAI's popular ChatGPT. Baidu's move further fuels this rivalry.
 Regulation in China seems to support such AI advancements. CEO Robin Li voiced his optimism about the AI regulationsâ€”calling them "more pro-innovation than regulation".
  
What's the broader response?
  
Baidu's latest stride has boosted its stock price by over 3%, underlining the market's high anticipation of Baidu's AI efforts.
 Ernie Bot has rocketed to the top of Apple's iOS free app chart in China. This demonstrates a positive initial response from the public.
  
Regulation is key in China's AI game:
  
China has stringent regulations for the generative AI industry, requiring a security review and government approvals before any product launch. Moreover, companies need to comply with governmental tech and data requests.
 The US, on the other hand, doesn't currently have such regulations in place. A markedly different approach that could significantly influence the development and application of AI technologies.
  
If you like this kind of analysis, you might want to check this out.
 (source)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models - University of Illinois 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166hnku/r_lminfinite_simple_onthefly_length/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166hnku/r_lminfinite_simple_onthefly_length/"/>
        <updated>2023-08-31T17:41:46.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2308.16137
 Abstract:
  
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] CoTracker: A Revolutionary 2D Point Video Tracker]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166hehj/r_cotracker_a_revolutionary_2d_point_video_tracker/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166hehj/r_cotracker_a_revolutionary_2d_point_video_tracker/"/>
        <updated>2023-08-31T17:31:54.000Z</updated>
        <summary type="html"><![CDATA[CoTracker - a 2D point-tracking tool for videos - promises to revolutionize motion tracking. Through the use of a transformer network, it meticulously predicts point trajectories and visibility across video frames, giving insights like never before.
 https://i.redd.it/g0u5t9n1ehlb1.gif
 Here's why CoTracker is turning heads:
  
CoTracker leverages advanced transformer formulation: Utilising a grid of input tokens that evolve to output tokens, CoTracker allocates initial values derived from the track's start point and time.
 It's built to handle extended videos through 'windowed inference': Windowing enables the algorithm to handle videos beyond its maximum window length by splitting them into overlapping segments.
 'Unrolled Learning' caters to semi-overlapping windows effectively: By employing two unique types of losses, only a modest amount of windows are used in loss computation while still handling expansive videos at test time.
 Improved tracking through simultaneous multi-point selection: By tracking multiple points at once, CoTracker is able to better establish correlation and motion paths within videos.
  
Despite its notable strengths, there are limitations. Its sliding-window approach cannot handle long-term occlusions that last longer than a window, and its transformer-based model has a high computational cost that grows quadratically with the number of tracked points.
 According to the authors, â€œThe result is a flexible and powerful tracking algorithm that outperforms state-of-the-art methods in almost all benchmarksâ€. But itâ€™s yet to be seen how it will perform in real-life tasks. What do you think?
 P.S. If you like this type of analysis, you might want to check this out.
 (arXiv) (GitHub)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Combining Prompt Engineering with Structured Inputs to LLMs to Generate Insights on Predictions from Binary Classifiers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166hbjo/project_combining_prompt_engineering_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166hbjo/project_combining_prompt_engineering_with/"/>
        <updated>2023-08-31T17:28:48.000Z</updated>
        <summary type="html"><![CDATA[All of this is framed through the lens of improving your ability to understand if a prediction for an upcoming UFC match is good or not. Happy to dig further into the ML and processing around this.
 https://blog.wolftickets.ai/teaching-a-wolf-to-speak-transforming-fight-predictions-into-insights.html
 Any feedback is appreciated!
    submitted by    /u/wolfticketsai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Autolabel: data labeling with LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166hazp/p_autolabel_data_labeling_with_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166hazp/p_autolabel_data_labeling_with_llms/"/>
        <updated>2023-08-31T17:28:12.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 Wanted to share an open source project we've been working on for the last few weeks: Autolabel is an open source Python library to label and enrich text datasets with LLMs (Large Language Models).
 Why?
 Access to clean, labeled data is a huge bottleneck for most ML/data science teams. From experiments across a variety of NLP tasks and datasets, we have found that the most capable LLMs are able to label data at better quality than human annotators, but 20-100x faster.
 Getting StartedÂ¶
 You can get started with the library by defining a JSON config, and writing a few lines of code:
 from autolabel import LabelingAgent, AutolabelDataset agent = LabelingAgent('config.json') dataset = AutolabelDataset('dataset.csv', 'config.json') labels = agent.run(dataset) 
  
Installation guide
 Sample notebooks that show how to use the library for different labeling tasks.
 Technical report for benchmarking LLM and human annotator performance across a range of tasks and datasets.
  
Call for Feedback
 We just open sourced this library, and are actively developing it. Feedback is very welcome and so are requests for features. You can open an issue on Github for bugs and request features
    submitted by    /u/nihit-d  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeatherBench 2: A benchmark for the next generation of data-driven weather models]]></title>
        <id>http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html</id>
        <link href="http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html"/>
        <updated>2023-08-31T17:14:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Stephan Rasp, Research Scientist, and Carla Bromberg, Program Lead, Google Research





In 1950, weather forecasting started its digital revolution when researchers used the first programmable, general-purpose computer ENIAC to solve mathematical equations describing how weather evolves. In the more than 70 years since, continuous advancements in computing power and improvements to the model formulations have led to steady gains in weather forecast skill: a 7-day forecast today is about as accurate as a 5-day forecast in 2000 and a 3-day forecast in 1980. While improving forecast accuracy at the pace of approximately one day per decade may not seem like a big deal, every day improved is important in far reaching use cases, such as for logistics planning, disaster management, agrâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Echo Chess: The Quest for Solvability" (level design preference learning: predicting high-quality soluble mazes using human feedback from quitting rates)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/166gmnc/echo_chess_the_quest_for_solvability_level_design/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/166gmnc/echo_chess_the_quest_for_solvability_level_design/"/>
        <updated>2023-08-31T17:01:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Notation problem of equation 1 from the paper Axiomatic attribution for deep networks?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166g8a1/d_notation_problem_of_equation_1_from_the_paper/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166g8a1/d_notation_problem_of_equation_1_from_the_paper/"/>
        <updated>2023-08-31T16:45:39.000Z</updated>
        <summary type="html"><![CDATA[In Equation 1 of the paper "Axiomatic Attribution for Deep Networks", the denominator of the gradient is $\partial x_i$ (See Eq1).
 However, according to the paper(with Eq2), shouldn't it be $\partial (x'_i + \alpha \times (x_i-x'_i))$ rather than $\partial x_i$?
 I found many following papers which refer to this paper also use the notation like this.
 Do I misunderstand something?
    submitted by    /u/qjall  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Leveraging Leaky Softplus Activation with Momentum-Based Optimizers like Adam for Efficient Neural Network Training]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166frrn/discussion_leveraging_leaky_softplus_activation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166frrn/discussion_leveraging_leaky_softplus_activation/"/>
        <updated>2023-08-31T16:27:11.000Z</updated>
        <summary type="html"><![CDATA[In the realm of deep learning, the choice of activation functions and optimization algorithms can significantly impact the training process and the performance of neural networks. A relatively lesser-known gem in this landscape is the "leaky softplus" activation function, which, when paired with momentum-based optimizers like Adam, can lead to exceptionally efficient and effective training outcomes.
 The Leaky Softplus Activation Function
 The leaky softplus activation function combines the benefits of both linearity and non-linearity in a graceful manner. Defined as Math.Log(Math.Exp(x) + 1) + (x / 16), it smoothly transitions between a nearly linear response for negative inputs and a more pronounced non-linear response for positive inputs. This unique characteristic enables it to addressâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] DeepEval - Neural Framework For Testing LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166ehh0/p_deepeval_neural_framework_for_testing_llms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166ehh0/p_deepeval_neural_framework_for_testing_llms/"/>
        <updated>2023-08-31T15:37:11.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I built DeepEval - an open-source unit testing framework for LLMs in order to accelerate development and iteration. 
 The problem
 When designing software applications, testing has always been critical for a lot of production applications. However - with the rise of LLM applications, the type of testing required needs to change in order to adapt for the large number of possible queries. We therefore built DeepEval in order to make it easy to write LLM tests in just 1 line of code. We hope this solution is of value to future teams when iterating on their RAG pipelines, migrating LLM models, testing their fine-tuned LLMs.
 The solution
 The DeepEval framework is as follows: We split up testing LLMs into 4 main sections:
 - Answer Relevancy (how relevant an answer is to a question) - measured using a question-answer bi-encoder.
 - Factual consistency (whether the generated answer is hallucinating) - measured using entailment from an NLI model
 - Conceptual similarity (when given a ground truth, how closely does it relate to it - for example How big is it? The size of an orange vs 20 square centimetres.) - measured using vector similarity
 - Bias, Toxic classification (measured through DL classifier models)
 I would love any feedback on what we are building here and welcome any OS contributions! 
    submitted by    /u/ConfectionSafe954  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet Five Generative AI Innovators in Africa and the Middle East]]></title>
        <id>https://blogs.nvidia.com/?p=66314</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/31/generative-ai-startups-africa-middle-east/"/>
        <updated>2023-08-31T15:00:59.000Z</updated>
        <summary type="html"><![CDATA[Entrepreneurs are cultivating generative AI from the west coast of Africa to the eastern edge of the Arabian Desert. Gen AI is the latest of the big plans Kofi Genfi and Nii Osae have been hatching since they met 15 years ago in high school in Accra, Ghanaâ€™s capital that sits on the Gulf of Read article >]]></summary>
        <author>
            <name>Wei Xiao</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Morphobots for Mars: Caltech Develops All-Terrain Robot as Candidate for NASA Mission]]></title>
        <id>https://blogs.nvidia.com/?p=66528</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/31/caltech-nasa-mars-rover-robot-jetson/"/>
        <updated>2023-08-31T15:00:19.000Z</updated>
        <summary type="html"><![CDATA[Academics Mory Gharib and Alireza Ramezani in 2020 were spitballing a transforming robot that is now getting a shot at work thatâ€™s literally out of this world: NASA Mars Rover missions. Caltech has unveiled its multi-talented robot that can fly, drive, walk and do eight permutations of motions through a combination of its skills. They Read article >]]></summary>
        <author>
            <name>Scott Martin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Frontiers: AI in India and beyond with Sriram Rajamani]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=964089</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/ai-frontiers-ai-in-india-and-beyond-with-sriram-rajamani/"/>
        <updated>2023-08-31T14:22:14.000Z</updated>
        <summary type="html"><![CDATA[In this episode of the Microsoft Research Podcast, Managing Director of Microsoft Research India Sriram Rajamani discusses how generative AI is impacting the labâ€™s approach to research and how the countryâ€™s many languages can help advance conversational systems.
The post AI Frontiers: AI in India and beyond with Sriram Rajamani appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Seach]]></title>
        <id>https://www.reddit.com/r/artificial/comments/166cgab/ai_seach/</id>
        <link href="https://www.reddit.com/r/artificial/comments/166cgab/ai_seach/"/>
        <updated>2023-08-31T14:19:23.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for a AI Search tool that replaces the search bar on a website. Search tool will scrape that sites data and offer suggestions. 
 Any recommendations?
    submitted by    /u/CauliflowerTiny1454  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Training models when you have limited compute power]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166cby8/d_training_models_when_you_have_limited_compute/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166cby8/d_training_models_when_you_have_limited_compute/"/>
        <updated>2023-08-31T14:14:39.000Z</updated>
        <summary type="html"><![CDATA[I've been wanting to take a code chatbot model like starchat or codellama and tune it to our codebase, problem is all I have at work is a Mac with 8gb of RAM. I talk with my boss today and can ask for some stuff if I want and can give good reason. What's the most efficient way to get the compute I need to train the model. Any other advice on how to go about doing this is greatly appreciated
    submitted by    /u/Kechup17  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] DINOv2 is now available under the Apache 2.0 license]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166c0ja/n_dinov2_is_now_available_under_the_apache_20/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166c0ja/n_dinov2_is_now_available_under_the_apache_20/"/>
        <updated>2023-08-31T14:02:11.000Z</updated>
        <summary type="html"><![CDATA[Meta AI has made their DINOv2 self-supervised learning method for training computer vision models truly open source by publishing it under Apache 2.0 license.
 DINOv2 has outperformed previous state-of-the-art self-supervised learning methods on a variety of computer vision tasks, including image classification, object detection, and semantic segmentation. It is also more efficient to train than previous methods, making it more accessible to researchers and practitioners.
 DINOv2 is different from existing methods because it provides a new way to train high-performance computer vision models without the need for labeled data. This makes it possible to train models on large datasets of unlabeled images, which can be more cost-effective and time-efficient than collecting and labeling large datasets of images.
 New demo: https://dinov2.metademolab.com/
    submitted by    /u/noiseinvacuum  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[4 data compliance standards to know for 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=63017</id>
        <link href="https://www.datasciencecentral.com/4-data-compliance-standards-to-know-for-2023/"/>
        <updated>2023-08-31T13:05:17.000Z</updated>
        <summary type="html"><![CDATA[Data is crucial in most industries today. As the amount of business information grows, so do the standards for peopleâ€™s protection of their personal information. With advanced cyberattacks, security compliance frameworks and cybersecurity have become essential fields to ensure data is collected, organized, stored, and managed in a safe way. This article will start byâ€¦Â Read More Â»4 data compliance standards to know for 2023
The post 4 data compliance standards to know for 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kevin Donvas</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GeForce NOW Gets Wild, With â€˜Party Animalsâ€™ Leading 24 New Games in September]]></title>
        <id>https://blogs.nvidia.com/?p=66612</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/31/geforce-now-thursday-aug-31/"/>
        <updated>2023-08-31T13:00:50.000Z</updated>
        <summary type="html"><![CDATA[Just like that, summer falls into September, and some of the most anticipated games of the year, like the Cyberpunk 2077: Phantom Liberty expansion, PAYDAY 3 and Party Animals, are dropping into the GeForce NOW library at launch this month. Theyâ€™re part of 24 new games hitting the cloud gaming service in September. And the Read article >]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How the LDMs in knowledge graphs can complement LLMs]]></title>
        <id>https://www.datasciencecentral.com/?p=63014</id>
        <link href="https://www.datasciencecentral.com/how-the-ldms-in-knowledge-graphs-can-complement-llms/"/>
        <updated>2023-08-31T12:56:39.000Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) fit parameters (features in data topography) to a particular dataset, such as text scraped off the web and conformed to a training set.Â  Logical data models (LDMs), by contrast, model what becomes shared within entire systems. They bring together the data in a system with the help of various kinds ofâ€¦Â Read More Â»How the LDMs in knowledge graphs can complement LLMs
The post How the LDMs in knowledge graphs can complement LLMs appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1669568/smartgpt_major_benchmark_broken_890_on_mmlu_exams/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1669568/smartgpt_major_benchmark_broken_890_on_mmlu_exams/"/>
        <updated>2023-08-31T11:57:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Sonic_Improv  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Deep reinforcement learning library to import multiple URDF robots and objects ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1668xro/p_deep_reinforcement_learning_library_to_import/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1668xro/p_deep_reinforcement_learning_library_to_import/"/>
        <updated>2023-08-31T11:47:29.000Z</updated>
        <summary type="html"><![CDATA[I have experience in deep learning but am a beginner in using deep reinforcement learning for robotics. However, I have recently gone through the huggingface course on deep reinforcement learning.
 I tried tinkering around with panda-gym but am having trouble trying to start my own project. I am trying to use two UR5 robots do some bimanual manipulation tasks e.g. have the left arm hold onto a cup while the right pours water into it. panda-gym allows me to import a URDF file of my own robot but I can't find the option to import my own objects like the xml file (or any extension) of a table or a water bottle.
 I have no idea which library allows me to import multiple URDF robots and xml objects and was hoping for some help.
 EDIT : I actually just read about Gazebo and was wondering if it'll allow me to do the above ? As a beginner I still have zero experience with ros and gazebo.
    submitted by    /u/I_am_a_robot_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Library to import multiple URDF robots and objects ?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1668xd3/p_library_to_import_multiple_urdf_robots_and/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1668xd3/p_library_to_import_multiple_urdf_robots_and/"/>
        <updated>2023-08-31T11:46:55.000Z</updated>
        <summary type="html"><![CDATA[I have experience in deep learning but am a beginner in using deep reinforcement learning for robotics. However, I have recently gone through the huggingface course on deep reinforcement learning.
 I tried tinkering around with panda-gym but am having trouble trying to start my own project. I am trying to use two UR5 robots do some bimanual manipulation tasks e.g. have the left arm hold onto a cup while the right pours water into it. panda-gym allows me to import a URDF file of my own robot but I can't find the option to import my own objects like the xml file (or any extension) of a table or a water bottle.
 I have no idea which library allows me to import multiple URDF robots and xml objects and was hoping for some help.
    submitted by    /u/I_am_a_robot_  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help Me Understand ChatGPT]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1668uif/help_me_understand_chatgpt/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1668uif/help_me_understand_chatgpt/"/>
        <updated>2023-08-31T11:43:08.000Z</updated>
        <summary type="html"><![CDATA[I'm currently researching how users interact with ChatGPT and its features, and I'd really appreciate your insights, experience, and perspective.
 Why should you participate?
 It's a quick 5-minute survey.
 Your identity and responses are completely anonymous.
 Your input will significantly contribute to important research on ChatGPT.
 The final research document will be posted to this sub.
 Survey Link: https://forms.gle/tNBib2dA1ErFEwbk6
 Rest assured, all information will be confidential and only used for the purpose of this research.
 Thank you for your time
    submitted by    /u/aaron-cesaro  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Optimizing simple distributions for something other than maximum likelihood]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/166785l/d_optimizing_simple_distributions_for_something/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/166785l/d_optimizing_simple_distributions_for_something/"/>
        <updated>2023-08-31T10:20:43.000Z</updated>
        <summary type="html"><![CDATA[As everyone knows, we usually optimise for maximum likelihood when fitting distributions like gaussians (equivalent to the forward KL-divergence). But for neural networks, techniques like GANs allow the minimisation of other distances like Mutual Information or Reverse KL. While this is certainly a very cool and insightful approach, it's also highly complex. I wonder wether other approaches to this problem exist for the simpler case, like fitting a gaussian or some other analytic distribution.
 From statistics, I have only encountered maximum likelihood and it's variations like robust statistics.
    submitted by    /u/LeanderKu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I created a package implementing a SOTA technique for XAI ( Explainable AI)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1666yyn/p_i_created_a_package_implementing_a_sota/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1666yyn/p_i_created_a_package_implementing_a_sota/"/>
        <updated>2023-08-31T10:06:39.000Z</updated>
        <summary type="html"><![CDATA[This is the package
 https://github.com/mfumagalli68/xi-method
 Follow the README and install directly from pypi.
 From the paper:
 " [..]To bridge this gap we propose a family of measures of statistical association whose definition is well-posed also for nonordered data. Our intuition is to rely on separation measurements between probability mass functions. Here, by separation measurement we mean any distance or divergence between probability mass functions that is positive, and that is null if and only if the probability mass functions coincide. Then, we show that the new class of sensitivity indices complies with Renyiâ€™s postulate D of measures of statistical dependence (Renyi, 1959). This postulate, called zero-independence property in the following, requires that a measure of associatâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best AI to bypass Ai detection for essays and assignment]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1666lmy/best_ai_to_bypass_ai_detection_for_essays_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1666lmy/best_ai_to_bypass_ai_detection_for_essays_and/"/>
        <updated>2023-08-31T09:46:44.000Z</updated>
        <summary type="html"><![CDATA[So yeah it's an open book course, but I'm horrible at flow and grammar. I need to be able to fix these things without getting in trouble. Ten years ago in my undergrad friends and family would do the final proofreading for me to make small changes. Is undetectable reputable.
    submitted by    /u/6ixsideOT  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chat with your favorite characters from movies, TV shows, books, history, and more (+ Discord bot)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1666kgh/chat_with_your_favorite_characters_from_movies_tv/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1666kgh/chat_with_your_favorite_characters_from_movies_tv/"/>
        <updated>2023-08-31T09:45:02.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 ChatFAI characters
 Hey everyone,
 ChatFAI has a special connection with this community because this is where I got it started. It was a simple web app that allowed you to interact with your favorite characters from movies, TV shows, books, history, and beyond. Now, it is a lot more. 
 It has public APIs and an official Discord bot integration now. A lot of performance improvements have been made in the recent days. 
 People have created a lot of characters (https://chatfai.com/characters) 
 The Discord bot is still a new area so could you share feedback if you guys check it out? You can also find it in the Discord app directory. 
    submitted by    /u/usamaejazch  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-powered drone beats human champion pilots | "Swift AI used technique called deep reinforcement learning to win 15 out of 25 races against world champions"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1666jl6/aipowered_drone_beats_human_champion_pilots_swift/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1666jl6/aipowered_drone_beats_human_champion_pilots_swift/"/>
        <updated>2023-08-31T09:43:39.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching with AI]]></title>
        <id>https://openai.com/blog/teaching-with-ai</id>
        <link href="https://openai.com/blog/teaching-with-ai"/>
        <updated>2023-08-31T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Weâ€™re releasing a guide for teachers using ChatGPT in their classroomâ€”including suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mini-Batch in PPO]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1660r0h/minibatch_in_ppo/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1660r0h/minibatch_in_ppo/"/>
        <updated>2023-08-31T04:17:26.000Z</updated>
        <summary type="html"><![CDATA[Hi 
 I am struggling to understand the mini-batch in PPO. 
 Say I already collected two trajectories.
 Traj_A = [t = 1, t= 2, t=3 ,.... t = 100]
 Traj_B = [t =1 , t=2, ... t= 78] 
 Now, I heard you usually break this down onto mini-batch (say a batchsize of 6). 
 Do you do random sampling?
 eg, one batch is [Traj_A_t=1, Traj_A_t=2, Traj_A_t=100, Traj_A_t=66, Traj_A_t=77, Traj_A_t=55]???
 OR do you need to maintain some sequence [Traj_A_t=1, Traj_A_t=2, Traj_A_t=3, Traj_A_t=4, Traj_A_t=5, Traj_A_t=6]???
    submitted by    /u/No_Oilve_6577  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/30/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165zsn0/oneminute_daily_ai_news_8302023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165zsn0/oneminute_daily_ai_news_8302023/"/>
        <updated>2023-08-31T03:30:17.000Z</updated>
        <summary type="html"><![CDATA[Tesla is about to flip the switch on its $300 million new AI cluster, featuring 10,000 Nvidia H100 compute GPUs.[1]
 Intel has revealed two new Intel Xeon processors this week at Hot Chips 2023 to give designers new options for efficient server-level performance.[2]
 General Motors is using conversational AI chatbots to handle simple OnStar calls, freeing up the serviceâ€™s human employees to address more complex requests, the company said Tuesday.[3]
 Microsoft announces Turing Bletchley v3 vision-language model for Bing image searches.[4]
  
Sources:
 [1] https://www.tomshardware.com/news/teslas-dollar300-million-ai-cluster-is-going-live-today
 [2] https://www.allaboutcircuits.com/news/intel-reveals-two-new-xeon-processor-lines-at-hot-chips-2023/
 [3] https://www.theverge.com/2023/8/29/23849390/gm-google-cloud-ai-chat-bot-onstar
 [4] https://www.neowin.net/news/microsoft-announces-turing-bletchley-v3-vision-language-model-for-bing-image-searches/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What type of model(s) do you think Spotify are using for their DJ feature to seamlessly transition every song? Itâ€™s not as easy as just crossfading for x seconds, every one is beat-matched quite literally like a real DJ.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165zbv5/what_type_of_models_do_you_think_spotify_are/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165zbv5/what_type_of_models_do_you_think_spotify_are/"/>
        <updated>2023-08-31T03:08:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sardoa11  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US Copyright Office seeks public input on AI and copyright]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165ydxc/us_copyright_office_seeks_public_input_on_ai_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165ydxc/us_copyright_office_seeks_public_input_on_ai_and/"/>
        <updated>2023-08-31T02:25:43.000Z</updated>
        <summary type="html"><![CDATA[The US government is taking steps to address the complex and controversial issues around AI and intellectual property rights. The US Copyright Office is opening a public comment period on August 30th to hear from various stakeholders on the topic.
 Important Details:
  
The agency is asking for comments on three main questions: How should AI be defined and categorized for the purposes of copyright? What are the implications of AI for the rights of authors and owners of works? What are the implications of AI for the liability and responsibility of users and distributors of works?
 The agency also wants to hear about related issues, such as: how AI may affect publicity rights and unfair competition laws. The agency notes that AI may create works that mimic or impersonate the voices, likenesses, or styles of real people, which could raise ethical and legal concerns.
 Finally, they want to determine how AI may affect moral rights and cultural heritage: The agency acknowledges that AI may create works that are derivative or transformative of existing works, which could affect the reputation and integrity of the original creators and their communities.
  
The deadline to submit your comments is October 18th and specific instructions for submitting comments are available on the Copyright Office website.
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI and techâ€”stay updated in under 3 mins/day.
 (source)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In-Datacenter Performance Analysis of a Tensor Processing Unit]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/165w1j4/indatacenter_performance_analysis_of_a_tensor/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/165w1j4/indatacenter_performance_analysis_of_a_tensor/"/>
        <updated>2023-08-31T00:43:15.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/recklessdesuka  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can you tell itâ€™s artificial?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165v1fn/can_you_tell_its_artificial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165v1fn/can_you_tell_its_artificial/"/>
        <updated>2023-08-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[I was playing around with the Eleven labs V2 multilingual model and I have to say itâ€™s extremely impressive. 
 Does this sound like the real Tucker?
    submitted by    /u/Exitium_Maximus  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is your favorite AI website for research?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165uyd3/what_is_your_favorite_ai_website_for_research/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165uyd3/what_is_your_favorite_ai_website_for_research/"/>
        <updated>2023-08-30T23:56:13.000Z</updated>
        <summary type="html"><![CDATA[I work in science research and want to introduce new tools to my students.
 We are looking for AI that can read tables, charts, figures, and spreadsheets, and possibly run statistics on this information.
 We are also looking for AI that can be given a prompt and will write on chosen topic with proper citation of sources. This information will not be used for publication, but rather, to organize main ideas and provide examples.
 An art AI that can draw or mimic images of real insects would be nice as well.
 Preferably these will all be free to use.
    submitted by    /u/wolfmonarchyhq  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing Deep Networks to Process Other Deep Networks]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/165t1zt/designing_deep_networks_to_process_other_deep/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/165t1zt/designing_deep_networks_to_process_other_deep/"/>
        <updated>2023-08-30T22:36:54.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Will I get in? [Fall 2024 MS in ML European Universities]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165ssy5/d_will_i_get_in_fall_2024_ms_in_ml_european/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165ssy5/d_will_i_get_in_fall_2024_ms_in_ml_european/"/>
        <updated>2023-08-30T22:26:51.000Z</updated>
        <summary type="html"><![CDATA[TLDR: American, Graduated from U of Michigan in 2019 w/ 3.3 GPA Bs in Comp Sci. Worked at Google for 3 years. Samsung Research America for 3 months. No ML specific work experience. No research. Will I get in to European elite ML programs? If not what do I need to do?
 My GF and I want to study our masters together in Europe. Sheâ€™s doing business I want to do ML/AI.
 I spent the past year kind of goofing off. Got kind of burned out and decided I was going to get into music production so spent the pst year mainly doing that with some software mixed in. 
 Recently been self studying ML, both the math from textbooks and trying my hand at some models in python.
 I do not have any connections to academia currently and wil have to beg a professor who barely knew me from undergrad for a rec. Can get other recs from past bosses.
 My plan right now is to look for job hopefully in AI but maybe just more general software engineering again, but long term I want to get a masters in person.
 My current resume looks like:
 Graduated BS in Comp Sci from univ of Michigan 2019 3.3 GPA
 Worked at Google for 3 years Worked at Samsung Research America for 4 months Some self study I can claim but not much tangible proof
 Recommendation from Google Boss (Maybe) recommendation from UofM CS professor that barely knew me
 My questions to anyone that knows the admissions right now are:
 1) Do you think I get to one off this? (To anyone of these schools)
 2) If not what are the things to prioritize to improve my chances? What are the timeline of these steps? Can I do them in the next few months or have to wait till next year?
    submitted by    /u/Srokisthename  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Could anyone help me why the following list is the optimal policy for this environment? (Reference: Sudharsan's Deep RL book)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/165sodq/could_anyone_help_me_why_the_following_list_is/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/165sodq/could_anyone_help_me_why_the_following_list_is/"/>
        <updated>2023-08-30T22:21:44.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/qderz9bsoblb1.png?width=1195&format=png&auto=webp&s=fb8ec749d0ce5000e66951b173228278a1d4c3a3
    submitted by    /u/nimageran  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Could anyone help me why the following list is the optimal policy for this environment? (Reference: Sudharsan's Deep RL book)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/165sodl/could_anyone_help_me_why_the_following_list_is/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/165sodl/could_anyone_help_me_why_the_following_list_is/"/>
        <updated>2023-08-30T22:21:44.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/qderz9bsoblb1.png?width=1195&format=png&auto=webp&s=fb8ec749d0ce5000e66951b173228278a1d4c3a3
    submitted by    /u/nimageran  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can You Solve a Time-Traveling Puzzle Designed by GPT-4? Win Bitcoin (100$) & Save the Future!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165slp5/can_you_solve_a_timetraveling_puzzle_designed_by/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165slp5/can_you_solve_a_timetraveling_puzzle_designed_by/"/>
        <updated>2023-08-30T22:18:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165r4hi/r_diffprep_differentiable_data_preprocessing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165r4hi/r_diffprep_differentiable_data_preprocessing/"/>
        <updated>2023-08-30T21:20:40.000Z</updated>
        <summary type="html"><![CDATA[I just came across this paper, and it just sounds too good to be true. If we regularly spend up to 80% of our time in data preprocessing, this method would suddenly return us A LOT of that time. Has anyone seen it in python code? I haven't found it and I'd love to give it a try with some of my datasets from hell. They do have a GitHub page but I'm too dumb or too noob to make it run in my laptop.
    submitted by    /u/Davidat0r  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Knowledge graph vs text summary+embedding for long term conversational memory]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165pn1j/d_knowledge_graph_vs_text_summaryembedding_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165pn1j/d_knowledge_graph_vs_text_summaryembedding_for/"/>
        <updated>2023-08-30T20:24:27.000Z</updated>
        <summary type="html"><![CDATA[Hi, I'm relatively new to the space of AI chatbots and I figured I'd get my hands wet with a small personal project. While researching the topic of long term conversational memory I noticed most people are using text embedding in combination with textual summary to generate a conversation history for the AI's prompt. However, this technique seems to have many drawbacks such as loss of details in the summarization process. I was wondering if anyone has experience using knowledge graph DBs like neo4j for conversational memory instead, and what the pros and cons of such an approach are compared to summarization. I'd be greatly interested in any resources that could further my knowledge in this space as my primary goal is to learn from this project. Thanks!
    submitted by    /u/Rainmire  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Does anybody manage to make MuseTree work?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165p7yi/discussion_does_anybody_manage_to_make_musetree/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165p7yi/discussion_does_anybody_manage_to_make_musetree/"/>
        <updated>2023-08-30T20:08:33.000Z</updated>
        <summary type="html"><![CDATA[https://stevenwaterman.uk/musetree/ It's for music generation through musenet. I don't manage to generate anything. It has to be related to API issues or stuff like that?
    submitted by    /u/MusicalSeries  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deploy self-service question answering with the QnABot on AWS solution powered by Amazon Lex with Amazon Kendra and large language models]]></title>
        <id>9cc8f0ba3d8995c7b4ab2a55da7251a032afe12a</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/deploy-self-service-question-answering-with-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-large-language-models/"/>
        <updated>2023-08-30T20:07:52.000Z</updated>
        <summary type="html"><![CDATA[Powered by Amazon Lex, the QnABot on AWS solution is an open-source, multi-channel, multi-language conversational chatbot. QnABot allows you to quickly deploy self-service conversational AI into your contact center, websites, and social media channels, reducing costs, shortening hold times, and improving customer experience and brand sentiment. In this post, we introduce the new Generative AI features for QnABot and walk through a tutorial to create, deploy, and customize QnABot to use these features. We also discuss some relevant use cases.]]></summary>
        <author>
            <name>Clevester Teo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shifting order in multiple-choice questions massively affects LLM performance [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165ozkx/shifting_order_in_multiplechoice_questions/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165ozkx/shifting_order_in_multiplechoice_questions/"/>
        <updated>2023-08-30T19:59:55.000Z</updated>
        <summary type="html"><![CDATA[Recent research proposes that Large Language Models (LLMs) may not be as reliable as we think. In fact, the order of options in a multiple-choice question drastically influences the responses from LLMs such as GPT-4 and InstructGPT.
 If you want to stay on top of the latest trends and insights in AI and tech, look here first.
 https://preview.redd.it/k8yaixbjzalb1.png?width=1289&format=png&auto=webp&s=99ac6280a1e7415f46c0c11938ae20e2b77674b4
 What are the findings?
  
LLM sensitivity to multiple-choice arrangement: The study suggests if options in multiple-choice questions are reordered, the LLM's performance varies dramaticallyâ€” approximately 13% to 75% depending on the benchmark.
 Positional bias shapes responses: When the LLM is uncertain between top-selected answers, the option positioning can artificially lean its predictions. Observations also found that LLMs favor specific placements when unsure of the optimal response among top-selected answers.
 Performance improves when calibration techniques are applied: Making use of two unique calibration methods, the performance of LLMS saw up to eight percentage points of increase across numerous models and benchmarks.
  
Why does this matter?
 This moves us closer to identifying the factors contributing to LLMs' sensitivity and highlights the significance of recognizing and confronting these sensitivities to improve real-world usability and reliability.
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI and techâ€”stay updated in under 3 mins/day.
 (arXiv)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shifting order in multiple-choice questions massively affects LLM performance]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165oz1g/shifting_order_in_multiplechoice_questions/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165oz1g/shifting_order_in_multiplechoice_questions/"/>
        <updated>2023-08-30T19:59:18.000Z</updated>
        <summary type="html"><![CDATA[Recent research proposes that Large Language Models (LLMs) may not be as reliable as we think. In fact, the order of options in a multiple-choice question drastically influences the responses from LLMs such as GPT-4 and InstructGPT.
 If you want to stay on top of the latest trends and insights in AI and tech, look here first.
 https://preview.redd.it/dxfsq72kzalb1.png?width=1289&format=png&auto=webp&s=e4ed5b541073bde18d2865f2c15e8028388070f5
 What are the findings?
  
LLM sensitivity to multiple-choice arrangement: The study suggests if options in multiple-choice questions are reordered, the LLM's performance varies dramaticallyâ€” approximately 13% to 75% depending on the benchmark.
 Positional bias shapes responses: When the LLM is uncertain between top-selected answers, the option positioning can artificially lean its predictions. Observations also found that LLMs favor specific placements when unsure of the optimal response among top-selected answers.
 Performance improves when calibration techniques are applied: Making use of two unique calibration methods, the performance of LLMS saw up to eight percentage points of increase across numerous models and benchmarks.
  
Why does this matter?
 This moves us closer to identifying the factors contributing to LLMs' sensitivity and highlights the significance of recognizing and confronting these sensitivities to improve real-world usability and reliability.
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI and techâ€”stay updated in under 3 mins/day.
 (arXiv)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Handwritten Text Recognition (OCR) on Historical Documents]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165oeh3/d_handwritten_text_recognition_ocr_on_historical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165oeh3/d_handwritten_text_recognition_ocr_on_historical/"/>
        <updated>2023-08-30T19:37:04.000Z</updated>
        <summary type="html"><![CDATA[I am working on developing a solution to transcribe historic texts (Pre-1900's) which are all handwritten. I have some data, around 1000's transcribed sentences with their corresponding images of text. TrOCR looked great, but it still makes a lot of mistakes, probably because of the old English phraseology, so I tried to finetune it with my data and see if it improves and that didn't happen. 
 The data I used to train was my 1000 sentences + some public dataset with another 2500 sentences, so just about 3500 sentences in total. Do you think it's because the data is small, that the performance is bad?
 I'm finetuning "microsoft/trocr-base-stage1" using native PyTorch. 
 If not TrOCR do you recommend any OCR/HTR models I can finetune to my handwritten historical data? I truly appreciate any guidance you send my way. 
    submitted by    /u/daxow  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling and improving text stability in live captions]]></title>
        <id>http://blog.research.google/2023/08/modeling-and-improving-text-stability.html</id>
        <link href="http://blog.research.google/2023/08/modeling-and-improving-text-stability.html"/>
        <updated>2023-08-30T19:34:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Vikas Bahirwani, Research Scientist, and Susan Xu, Software Engineer, Google Augmented Reality





Automatic speech recognition (ASR) technology has made conversations more accessible with live captions in remote conferencing software, mobile applications, and head-worn displays. However, to maintain real-time responsiveness, live caption systems often display interim predictions that are updated as new utterances are received. This can cause text instability (a â€œflickerâ€ where previously displayed text is updated, shown in the captions on the left in the video below), which can impair users' reading experience due to distraction, fatigue, and difficulty following the conversation.






In â€œModeling and Improving Text Stability in Live Captionsâ€, presented at ACM CHI 2023, we fâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous innovations in an uncertain world]]></title>
        <id>https://news.mit.edu/2023/autonomous-innovations-uncertain-world-jonathan-how-0830</id>
        <link href="https://news.mit.edu/2023/autonomous-innovations-uncertain-world-jonathan-how-0830"/>
        <updated>2023-08-30T19:30:00.000Z</updated>
        <summary type="html"><![CDATA[Jonathan How and his team at the Aerospace Controls Laboratory develop planning algorithms that allow autonomous vehicles to navigate dynamic environments without colliding.]]></summary>
        <author>
            <name>Daniel de Wolff | MIT Industrial Liaison Program</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The possibilites of a trader AI with infinite profits]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165nvqc/the_possibilites_of_a_trader_ai_with_infinite/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165nvqc/the_possibilites_of_a_trader_ai_with_infinite/"/>
        <updated>2023-08-30T19:17:01.000Z</updated>
        <summary type="html"><![CDATA[AI can also be dangerous because it could make automated trades in the stock market or cryptocurrency market, and because it remembers all the exchange rate changes in history and the entire economic history of the world and also has all the statistics and mathematical knowledge at its fingertips, it can easily draw conclusions and create an algorithm that might make you bigger profits than any real human. He could also learn from his own mistakes and keep improving. 
 Is this possible? Are there any AIs like this already?
    submitted by    /u/Steve_Hufnagel  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] FlowJax - Normalizing flows in JAX]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165lxu1/p_flowjax_normalizing_flows_in_jax/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165lxu1/p_flowjax_normalizing_flows_in_jax/"/>
        <updated>2023-08-30T18:03:03.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 Hopefully this is of interest to some of you. For those that don't know, normalising flows can be used as black-box unconditional or conditional distribution approximators, that support both exact sampling and density evaluations. For an excellent review see https://arxiv.org/abs/1912.02762.
 I am developing flowjax, a Python package for normalising flows, distributions and bijections. It uses Jax for automatic differentiation, and the equinox framework built by Patrick Kidger to allow for a familiar object-oriented design.
 It includes many powerful flows, e.g. masked autoregressive flows, coupling flows and block neural autoregressive flows. In addition to inheriting some benefits from using JAX (easy GPU support, some efficiency gains), here's a few points where I think flowjax has some advantages over other packages:
  
Comprehensive documentation
 Simplified definitions of unconditional/conditional bijections and distributions (particularly nicer handling of the conditional case, which some packages seem to stitch in as an afterthought).
 Easy to plug in different "transformer" bijections to coupling/masked autoregressive flows.
 Use of efficiency tricks to optimize run times (e.g. circumventing recompilation of identical layers using jax.lax.scan over the flow layers)
  
It has been used in a couple of papers already, but it would be great to have some more people using it and some feedback/suggestions/contributions. There are examples in the documentation for those that are interested.
    submitted by    /u/LimitedConsequence  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Models for Unsupervised Anomaly Detection of a Single Continuous Feature?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165lfnw/project_models_for_unsupervised_anomaly_detection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165lfnw/project_models_for_unsupervised_anomaly_detection/"/>
        <updated>2023-08-30T17:44:08.000Z</updated>
        <summary type="html"><![CDATA[Of the many unsupervised anomaly detection models out there (iforest, LOF, SVM etc) I am struggling to find a model that makes sense to use to detect anomalies in a single target feature. My current strategy is to subset the data into different categories and run iforest on a single column. 
 I feel as though this method might not be the best because it basically creates a tree with a single branch and measures how many nodes away a given record might be. My confidence scores never seem to exceed around -.17 on a scale of [-1,1] where -1 tends to more confidence in anomalous behavior
 Is there a better way?
 Note: Anomalies in my data occur very infrequently
    submitted by    /u/BeefaroniX  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatically generate impressions from findings in radiology reports using generative AI on AWS]]></title>
        <id>95c41f299d9321d375f39e16b69fcaa1025c9f6b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/automatically-generate-impressions-from-findings-in-radiology-reports-using-generative-ai-on-aws/"/>
        <updated>2023-08-30T17:08:21.000Z</updated>
        <summary type="html"><![CDATA[This post demonstrates a strategy for fine-tuning publicly available LLMs for the task of radiology report summarization using AWS services. LLMs have demonstrated remarkable capabilities in natural language understanding and generation, serving as foundation models that can be adapted to various domains and tasks. There are significant benefits to using a pre-trained model. It reduces computation costs, reduces carbon footprints, and allows you to use state-of-the-art models without having to train one from scratch.]]></summary>
        <author>
            <name>Adewale Akinfaderin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI Surges Past $1 Billion in Revenue As Demand For AI Explodes]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165k7yw/openai_surges_past_1_billion_in_revenue_as_demand/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165k7yw/openai_surges_past_1_billion_in_revenue_as_demand/"/>
        <updated>2023-08-30T16:59:08.000Z</updated>
        <summary type="html"><![CDATA[OpenAI is reportedly making strides in its financial performance and is on track to make $1 billion in revenue over the next 12 months, as per recent reports by The Information. This is a major milestone, signifying not only the success of OpenAI but also the increasing demand and investment in AI.
 If you want to stay on top of the latest trends and insights in AI and tech, look here first.
 https://preview.redd.it/afak3xrevalb1.jpg?width=660&format=pjpg&auto=webp&s=cd8f20ac732618b91dbf96928ede42d693f6c4a9
 Why should we pay attention?
  
Setting expectations: The Information estimates OpenAI's monthly revenue to be around $80 million, in line with the $1 billion yearly revenue prediction. Undeniably, OpenAI is accelerating.
 AI Chatbots are in high demand: ChatGPT, OpenAI's phenomenal coâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] How are you evaluating and monitoring LLMs?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165jdk8/discussion_how_are_you_evaluating_and_monitoring/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165jdk8/discussion_how_are_you_evaluating_and_monitoring/"/>
        <updated>2023-08-30T16:26:16.000Z</updated>
        <summary type="html"><![CDATA[Question for people who are implementing LLMs (open source, fine tuned, any kind). 
  
How do you know that your getting the quality output from the model that you need to ship the feature or model? Are the audits ad hoc data sampling and subjective "good/bad" ratings or have you figured out a more rigorous framework? Is it pretty much ~vibes~ based?
 What, if any, tools or processes are you putting into place to monitor and observe the LLM when its interacting with real time user data for weeks or months?
 
 Most of the folks I have spoken with are doing very ad hoc sampled output and writing down on post its or in a spreadsheet a subjective quality ratings. 
 One person had developed a slightly more rigorous 3 question survey on "is the result factual", "is the result cogent" and "is the result useful". Not everyone is logging their LLM responses they show users which feels very risky to me.
 Anyone aware of any industry standards being established around this?
    submitted by    /u/Andy-VertaAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wearable Health (WHSI) Joins AI Research Lab for Wearable Health Data]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165j6u6/wearable_health_whsi_joins_ai_research_lab_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165j6u6/wearable_health_whsi_joins_ai_research_lab_for/"/>
        <updated>2023-08-30T16:19:18.000Z</updated>
        <summary type="html"><![CDATA[Wearable Health Solutions to Advise Next Realm AI on Medical Internet of Things (MIoT) Solutions 
 NEWPORT BEACH, CA / ACCESSWIRE / Wearable Health Solutions Inc. (OTC PINK:WHSI) announced inclusion to Next Realm AI research lab to explore development of healthcare IoT solutions utilizing data analytics and artificial intelligence (AI).
 Wearable Healthcare Solutions will collaborate and advise Next Realm AI, an artificial intelligence and data analytics research lab located in New York City, on such areas as collecting and developing data solutions within the areas of wearables, IoT, and Medical Internet of Things (MIoT).
 As an official IBM Business Partner, Next Realm AI assists lab members in integrating leading-edge AI and data solutions into their business operations. By leveraging Next Realm's expertise, clients can modernize processes, boost efficiency, strengthen security, and deliver greater value to customers - all while driving growth and building value. 
 https://www.otcmarkets.com/stock/whsi/news/Wearable-Health-Solutions-to-Advise-Next-Realm-AI-on-Medical-Internet-of-Things-MIoT-Solutions?id=411692
    submitted by    /u/NextRealm_AI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help With RLLib/ Alternatives]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/165ivu8/help_with_rllib_alternatives/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/165ivu8/help_with_rllib_alternatives/"/>
        <updated>2023-08-30T16:07:37.000Z</updated>
        <summary type="html"><![CDATA[RLLib is currently stealing my remaining sanity, so I'm making a desperate scream into the void. I can't get my troubleshooting right. 
 I built a nice, custom Gym env that I've been running with SB3 and I feel like I'm caught in an endless array of errors, currently: ValueError: The two structures don't have the same nested structure.
 I can't help but feel like I'm going about this wrong and missing important information on how to do this correctly. The RayLlib Forum hasn't really been filled with people, so I'm asking:
 Does anyone know of a Debugging Manual/ A Discord Server/ A Migration Guide?
    submitted by    /u/tessherelurkingnow  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a â€œheavy metal quartetâ€ of AI compilers]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=963594</id>
        <link href="https://www.microsoft.com/en-us/research/blog/building-a-heavy-metal-quartet-of-ai-compilers/"/>
        <updated>2023-08-30T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[A new quartet of AI compilers: Rammer, Roller, Welder, and Grinder, tackle a range of compiler optimization challenges based on the same tile abstraction, providing a comprehensive solution to connect AI models with hardware accelerators.
The post Building a â€œheavy metal quartetâ€ of AI compilers appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Research Focus: Week of August 28, 2023]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=963642</id>
        <link href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-august-28-2023/"/>
        <updated>2023-08-30T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[In this issue: An illusion of predictability in scientific results; Kathleen Sullivan named to Insiderâ€™s 30 under 40 in healthcare list; FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations.
The post Research Focus: Week of August 28, 2023 appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are potential careers to take in the field of artifical intelligence?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165imvb/what_are_potential_careers_to_take_in_the_field/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165imvb/what_are_potential_careers_to_take_in_the_field/"/>
        <updated>2023-08-30T15:58:45.000Z</updated>
        <summary type="html"><![CDATA[I am 23 year old man, I have a degree in Politics, Philosophy, & Economics. Next year I want to do a masters degree, but I haven't chosen which one yet. I am both fascinated by AI, and want to be future-proof in my education. What potential careers do you see, currently or in the near future, in the field of AI, and what studies would you recommend to be well prepared for them? 
 â€‹
    submitted by    /u/ApplePenguinBaguette  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] A blog post on Yet Another ICML Award Fiasco]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165ial3/d_a_blog_post_on_yet_another_icml_award_fiasco/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165ial3/d_a_blog_post_on_yet_another_icml_award_fiasco/"/>
        <updated>2023-08-30T15:45:43.000Z</updated>
        <summary type="html"><![CDATA[I wrote a blog post on the ICML award fiasco: They gave an outstanding paper award to the D-Adaptation paper, that contains worse results that the ones in papers from 9 years ago. Also, this is not the first time that ICML gives awards to questionable or even plainly wrong papers. 
 I believe this might start a serious conversation about "stochastic" awards, and the super noisy reviews in machine learning conferences. 
 https://parameterfree.com/2023/08/30/yet-another-icml-award-fiasco/
    submitted by    /u/bremen79  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is there a monthly limit for OpenAI service in Azure?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165i7be/d_is_there_a_monthly_limit_for_openai_service_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165i7be/d_is_there_a_monthly_limit_for_openai_service_in/"/>
        <updated>2023-08-30T15:42:24.000Z</updated>
        <summary type="html"><![CDATA[When using OpenAI's api, there's a default limit of $120/month and my company is about to hit it. I plan on requesting an increase of that limit... but wondering, does Azure's OpenAI service have any monthly limit? By looking at their quotas: https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits it doesn't seem like there's a monthly cap.
 Is this correct? If so, I see no reason why anyone would use OpenAI's api instead of Azure's, as they cost the same but there's no usage limit. Especially if you expect to increase api usage in the future.
    submitted by    /u/alkibijad  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommendations for RL Library for 'unvectored' environments]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/165ho7i/recommendations_for_rl_library_for_unvectored/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/165ho7i/recommendations_for_rl_library_for_unvectored/"/>
        <updated>2023-08-30T15:21:54.000Z</updated>
        <summary type="html"><![CDATA[Hi, 
 I'm working on a problem which has a custom gym environment which I've made, and as it interacts with multiple other modules which have their own quirks, I need to use a reinforcement learning library which works in a specific way that I've only seen PFRL use. 
 The training loop needs to be in this format: 'obs, reward, done = agent.step(action)', 'agent.observe(obs, reward, ... )' rather than what I see in most modern RL libraries where you define an agent and then run a '.train()' method. 
 Are there any libraries which work in this way? I'd love to use something like StableBaselines but they don't seem to play nice and I'd rather not rewrite the gym environment if I can avoid it. 
 Thanks
    submitted by    /u/return_reza  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Lands at Bengaluru Airport With IoT Companyâ€™s Intelligent Video Analytics Platform]]></title>
        <id>https://blogs.nvidia.com/?p=66384</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/30/bengaluru-airport-vision-ai/"/>
        <updated>2023-08-30T15:00:26.000Z</updated>
        <summary type="html"><![CDATA[Each year, nearly 32 million people travel through the Bengaluru Airport, or BLR, one of the busiest airports in the worldâ€™s most populous nation. To provide such multitudes with a safer, quicker experience, the airport in the city formerly known as Bangalore is tapping vision AI technologies powered by Industry.AI. A member of the NVIDIA Read article >]]></summary>
        <author>
            <name>Angie Lee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IBM invests in $4.5 billion A.I. unicorn Hugging Face | Fortune]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165h10x/ibm_invests_in_45_billion_ai_unicorn_hugging_face/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165h10x/ibm_invests_in_45_billion_ai_unicorn_hugging_face/"/>
        <updated>2023-08-30T14:57:55.000Z</updated>
        <summary type="html"><![CDATA[IBMâ€™s CEO, who froze hiring for thousands of back-office jobs and predicted A.I. would take up to 50% of new jobs, just piled into a $4.5 billion tech unicornâ€™s massive new $235 million funding round
    submitted by    /u/AminoOxi  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I created GPT Pilot - a research project for a dev tool that uses LLMs to write fully working apps from scratch while the developer oversees the implementation - it creates code and tests step by step as a human would, debugs the code, runs commands, and asks for feedback.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165gqam/p_i_created_gpt_pilot_a_research_project_for_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165gqam/p_i_created_gpt_pilot_a_research_project_for_a/"/>
        <updated>2023-08-30T14:46:07.000Z</updated>
        <summary type="html"><![CDATA[Github: https://github.com/Pythagora-io/gpt-pilot
 Detailed breakdown: https://blog.pythagora.ai/2023/08/23/430/
 For a couple of months, I've been thinking about how can GPT be utilized to generate fully working apps, and I still haven't seen any project that I think has a good approach. I just don't think that Smol developer or GPT engineer can create a fully working production-ready app from scratch without a developer being involved and without any debugging process.
 So, I came up with an idea that I've outlined thoroughly in the blog post above, but basically, I have 3 main "pillars" that I think a dev tool that generates apps needs to have:
  
Developer needs to be involved in the process of app creation - I think that we are still far away from an LLM that can just be hooked up to â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Graph Signal Processing Applications and Training]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165ga5i/d_graph_signal_processing_applications_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165ga5i/d_graph_signal_processing_applications_and/"/>
        <updated>2023-08-30T14:28:39.000Z</updated>
        <summary type="html"><![CDATA[I'm studying GSP and I'm stuck on the definition of the Graph Fourier Transform. 
 The sigma notation and signal makes sense, but why is there an \"i\" term at the eigenvector mu? Shouldn't the eigenvector not depend on the \"i\"? And if it does, what does the \"i\" imply? 
    submitted by    /u/Ihaveaparrot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First time seeing a rare event]]></title>
        <id>https://www.johndcook.com/blog/?p=205726</id>
        <link href="https://www.johndcook.com/blog/2023/08/30/first-time-seeing-a-rare-event/"/>
        <updated>2023-08-30T14:24:17.000Z</updated>
        <summary type="html"><![CDATA[Suppose youâ€™ve been monitoring a rare event for a long time, then you see your first occurrence on the Nth observation. Now what would you say about the eventâ€™s probability? For example, suppose youâ€™re wondering whether dogs ever have two tails. You observe thousands of dogs and never see two tails. But then you see [â€¦]
First time seeing a rare event first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] - Given that we can lossily transform text to images and vice versa, multimodality should not be required for AGI or the construction of world-models. Any causal relationship that can be inferred from images/audio/video should be inferable from text.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165f400/d_given_that_we_can_lossily_transform_text_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165f400/d_given_that_we_can_lossily_transform_text_to/"/>
        <updated>2023-08-30T13:42:22.000Z</updated>
        <summary type="html"><![CDATA[Consider video data that captures various interactions between entitiesâ€”let's say Person A and Person B. We then apply a video summarization network T(x), where x is some video or an entity in the video, onto the video. For sake of argument, let's assume T(x) provides a description of x so detailed that we can decode the description back into the original video without losing much information via some arbitrary text-video model. Now, if we can infer a causal relationship in the videoâ€”like Person A punching Person Bâ€”then logically, an isomorphic relationship should also be inferable from the text encodings T(A) and T(B) (unless that relationship is one of the small pieces of information lost during the lossy transformation). After all, the encoding is just another representation of the sameâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deepdubâ€™s AI Redefines Dubbing From Hollywood to Bollywood]]></title>
        <id>https://blogs.nvidia.com/?p=66371</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/30/deepdub/"/>
        <updated>2023-08-30T13:00:50.000Z</updated>
        <summary type="html"><![CDATA[In the global entertainment landscape, TV show and film production stretches far beyond Hollywood or Bollywood â€” itâ€™s a worldwide phenomenon. However, while streaming platforms have broadened the reach of content, dubbing and translation technology still has plenty of room for growth. Deepdub acts as a digital bridge, providing access to content by using generative Read article >]]></summary>
        <author>
            <name>Kristen Yee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] HPC from local servers for deep learning as well as simpler tasks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165e1u1/d_hpc_from_local_servers_for_deep_learning_as/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165e1u1/d_hpc_from_local_servers_for_deep_learning_as/"/>
        <updated>2023-08-30T12:59:12.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 the company I am working at has several servers used for different tasks including data analysis and machine learning, including smaller tasks as well as deep learning. 
 What are some ways/ technologies they could create a distributed system where users can submit their jobs and they are dispatched automatically? 
 I was thinking of having an entry node that is the only one faced by users, is where all conda environments are and jobs can be submitted from there. 
 Please let me know if you have any suggestions/ tools that you know that would make sense. Thanks in advance!
    submitted by    /u/returnname35  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] HPC from local servers for deep learning as well as simpler tasks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165e1fk/d_hpc_from_local_servers_for_deep_learning_as/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165e1fk/d_hpc_from_local_servers_for_deep_learning_as/"/>
        <updated>2023-08-30T12:58:47.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 the company I am working at has several servers used for different tasks including data analysis and machine learning, including smaller tasks as well as deep learning. 
 What are some ways/ technologies they could create a distributed system where users can submit their jobs and they are dispatched automatically? 
 I was thinking of having an entry node that is the only one faced by users, is where all conda environments are and jobs can be submitted from there. 
 Please let me know if you have any suggestions/ tools that you know that would make sense. Thanks in advance!
    submitted by    /u/returnname35  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Singularity Day just got closer because of Nvidia?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165doaq/singularity_day_just_got_closer_because_of_nvidia/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165doaq/singularity_day_just_got_closer_because_of_nvidia/"/>
        <updated>2023-08-30T12:42:58.000Z</updated>
        <summary type="html"><![CDATA[New advances in AI hardware are making the singularity more likely. AI systems will be able to learn and process information much faster, which could lead to a breakthrough in AI capabilities. These advancements include quantum computing and neuromorphic computing, but more specifically the rise of affordable models like NVIDIA H100 and, more recently, GH200 models.
 If you are interested in this kind of information, there are more details here.
    submitted by    /u/Powerful-Pumpkin-938  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Self-Hosting a 16B LLAMA 2 Model in the Banking Sector: What Could Go Wrong?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165dd1z/p_selfhosting_a_16b_llama_2_model_in_the_banking/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165dd1z/p_selfhosting_a_16b_llama_2_model_in_the_banking/"/>
        <updated>2023-08-30T12:29:28.000Z</updated>
        <summary type="html"><![CDATA[I've received a freelance job offer from a company in the banking sector that wants to host their own LLAMA 2 model in-house.
 I'm hesitating to accept the gig. While I'll have access to the hardware (I've estimated that an A100 80GB will be required to host the 16B parameter version and process some fine-tuning & RAG), I'm not familiar with the challenges of self-hosting a model of this scale. I've always relied on managed services like Hugging Face or Replicate for model hosting.
 For those of you who have experience in self-hosting such large models, what do you think will be the main challenges of this mission if I decide to take it on?
 â€‹
 Edit: Some additional context information
 Size of the company: Very small ~ 60 employees
 Purpose: This service will be combined with a vector store to search content such as Word, Excel and PowerPoint files stored on their servers. I'll implement the RAG pattern and do some prompt engineering with it. They also want me to use it for searching things on specific websites and APIs, such as stock exchanges, so I (probably) need to fine-tune the model based on the search results and the tasks I want the model to do after retrieving the data.
    submitted by    /u/IMissEloquent75  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MDPs: gentle tutorial ...]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/165d39o/mdps_gentle_tutorial/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/165d39o/mdps_gentle_tutorial/"/>
        <updated>2023-08-30T12:17:07.000Z</updated>
        <summary type="html"><![CDATA[Markov Decision Processes (MDPs) form the cornerstone of reinforcement learning (RL) and serve as a fundamental modeling tool for making sequential decisions. In this note, we present a comprehensive definition of MDPs and provide a detailed derivation of the Bellman equations, along with the optimality results. Our approach aims to ensure a thorough understanding by avoiding the omission of any steps in the mathematical proofs. The primary goal is to facilitate reading classic textbooks on (approximate) dynamic programming, optimal control, and reinforcement learning, where proofs and derivations can sometimes obscure crucial details, making them less accessible to readers from diverse scientific and engineering backgrounds.
 https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4535241
    submitted by    /u/omroot  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where do AI adult websites get their models from?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165brex/where_do_ai_adult_websites_get_their_models_from/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165brex/where_do_ai_adult_websites_get_their_models_from/"/>
        <updated>2023-08-30T11:13:26.000Z</updated>
        <summary type="html"><![CDATA[Where do websites like made.porn, pornify.cc or porn.ai get their AI models from?
    submitted by    /u/mixedfeelingz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do you know Poker Ai gyms for adversarial policy trainings?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/165bap0/do_you_know_poker_ai_gyms_for_adversarial_policy/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/165bap0/do_you_know_poker_ai_gyms_for_adversarial_policy/"/>
        <updated>2023-08-30T10:49:21.000Z</updated>
        <summary type="html"><![CDATA[I want to try use adversarial policies (https://arxiv.org/abs/1905.10615) against poker no limit holdem 6-9 players RL models. 
 I was looking for open-ai gym like environement for that project. Im looking for: - access to game state from each player persperctive (to create input for adversarial model) - support for custom bets (not limited to 0, 1/2pot, all-in) - build-in RL models / support for opensoure RL models - option to add custom model as player
 So far I found those and read readme files: https://github.com/dickreuter/neuron_poker https://github.com/fschlatt/clubs_gym https://rlcard.org/ https://www.deepmind.com/open-source/openspiel
 Did anybody work on similar project? Which gym did you use, and what experience do you have with it? 
 Since, adversarial policies tend to work better for high-dimensionality I would prefer to 6players variant. I know that modern poker ai approach are not based on pure RL, but I want to check how vulnerable are classic RL poker models.
    submitted by    /u/MrCogito_hs  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a simulated browser]]></title>
        <id>https://www.reddit.com/r/artificial/comments/165b049/looking_for_a_simulated_browser/</id>
        <link href="https://www.reddit.com/r/artificial/comments/165b049/looking_for_a_simulated_browser/"/>
        <updated>2023-08-30T10:32:32.000Z</updated>
        <summary type="html"><![CDATA[Like custom world descriptions, AI apps/sites, etc
    submitted by    /u/roblox22g  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is there anything LangChain can do better than using LLMs directly (either through a website or an API), any examples? Why would someone choose to use it?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/165airj/d_is_there_anything_langchain_can_do_better_than/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/165airj/d_is_there_anything_langchain_can_do_better_than/"/>
        <updated>2023-08-30T10:05:26.000Z</updated>
        <summary type="html"><![CDATA[I haven't used ChatGPT a lot or any other LLMs, I've been reading about Langchain and its use cases, and I'm having trouble wrapping my head around exactly what it does. From what I understand, its an alternative interface for LLMs, allowing for easy switching between them, and makes some work for specific use cases easier. If I wanted to write an app or script to interact with LLMs and do other tasks, how would LangChain be better than just making API call(s) to an LLM, getting back the result as a string, and doing whatever with it?
    submitted by    /u/TheTwelveYearOld  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twitter / Machine Learning Community]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1658t4n/twitter_machine_learning_community/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1658t4n/twitter_machine_learning_community/"/>
        <updated>2023-08-30T08:26:34.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/x9182  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement learning environment for cyber security automation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1657l0p/reinforcement_learning_environment_for_cyber/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1657l0p/reinforcement_learning_environment_for_cyber/"/>
        <updated>2023-08-30T07:10:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/limmen  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Robots from Sci-Fi Movies you didnâ€™t know about]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1656oxv/ai_robots_from_scifi_movies_you_didnt_know_about/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1656oxv/ai_robots_from_scifi_movies_you_didnt_know_about/"/>
        <updated>2023-08-30T06:18:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tesla is powering up its $300 Million AI Supercomputer Today]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16564mm/tesla_is_powering_up_its_300_million_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16564mm/tesla_is_powering_up_its_300_million_ai/"/>
        <updated>2023-08-30T05:45:00.000Z</updated>
        <summary type="html"><![CDATA[Tesla's making a significant power move today as it prepares to bring its brand-new new AI-cluster online. Rocking a hefty 10,000 Nvidia H100 compute GPUs, the machine will tackle high-performance computing (HPC) workloads and AI applications, placing Tesla's capabilities among the global AI elite.
 If you want to stay on top of the latest trends and insights in AI and tech, look here first.
 https://preview.redd.it/c5ykgmr6r6lb1.png?width=970&format=png&auto=webp&s=8b830c8754c1a11792149f57d19a57a77fc8b161
 Hereâ€™s why this matters:
  
This Nvidia H100-based AI supercomputer will be one of the most powerful globally. With a peak performance of 340 FP64 PFLOPS and 39.58 INT8 ExaFLOPS for AI programs, even Leonardo, currently the fourth highest-performing supercomputer, is surpassed.
 Teslaâ€™sâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/29/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16550zp/oneminute_daily_ai_news_8292023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16550zp/oneminute_daily_ai_news_8292023/"/>
        <updated>2023-08-30T04:43:06.000Z</updated>
        <summary type="html"><![CDATA[Research firm SemiAnalysis has declared that Googleâ€™s anticipated Gemini AI model will smash OpenAIâ€™s offering by packing a lot more computing power.[1]
 DoorDash today announced its development of voice ordering capabilities incorporating AI, building on its existing model leveraging best-in-class agents, to further support restaurant operations.[2]
 The US Air Force wants $6 billion to build a fleet of AI-controlled drones.[3]
 Googleâ€™s DeepMind says it has cracked a problem that has vexed those trying to verify whether images are real or created by AI. Researchers proclaimed their new watermarking SynthID format can be used to pinpoint AI-generated deepfakes without distorting the imageâ€™s original quality.[4]
  
Sources:
 [1] https://beincrypto.com/ai-wars-google-gemini-chatgpt/
 [2] https://about.doordash.com/en-us/news/introducing-ai-and-agent-powered-voice-ordering
 [3] https://www.engadget.com/the-air-force-wants-6-billion-to-build-a-fleet-of-ai-controlled-drones-204548974.html
 [4] https://gizmodo.com.au/2023/08/deepmind-says-it-has-a-way-to-identify-ai-images-but-only-on-google/ 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do I teach my PPO agent to play Breakout?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/1654zy2/how_do_i_teach_my_ppo_agent_to_play_breakout/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/1654zy2/how_do_i_teach_my_ppo_agent_to_play_breakout/"/>
        <updated>2023-08-30T04:41:30.000Z</updated>
        <summary type="html"><![CDATA[I have coupled my agent with EnvPool in order to speed up the learning process. It seems to be playing Pong in less than an hour. However, when I try to make it Breakout, even after many hours it still struggles. Also, it it seems like the network is facing catastrophic forgetting as after a few hours it's performance suddenly deteriorates. Any ideas to fix this? I tried incorporating major ideas for PPO from here. 
 Here's my code. Feel free to let me know if you have any questions. Since I have incorporated EnvPool, the code won't run in Windows anymore. 
    submitted by    /u/Academic-Rent7800  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft's new AoT aims to create more human-like AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1654bit/microsofts_new_aot_aims_to_create_more_humanlike/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1654bit/microsofts_new_aot_aims_to_create_more_humanlike/"/>
        <updated>2023-08-30T04:06:17.000Z</updated>
        <summary type="html"><![CDATA[Microsoft teamed up with Virginia Tech to publish a white paper introducing their new "Algorithm of Thoughts" (AoT). The objective? To make language learning models akin to human learning.
 https://preview.redd.it/3q8lcq0k96lb1.png?width=2000&format=png&auto=webp&s=fce7a0e3225c64353ad6c51f65e8b490e795feed
 If you want to stay on top of the latest trends and insights in AI and tech, look here first.
 What's the big idea?
  
Microsoft's AoT aims to fuse the accuracy of algorithms with the nuances of human reasoning. A bold aspiration indeed, but not a new one. The goal to empower computers to learn for themselves â€“ akin to human cognition - has been an AI objective since its inception back in the 1950s.
 The AoT could be seen as an attempt to resolve the drawbacks of the "Chain of Thought" (CoT) approach. LLMs following the CoT approach can provide incorrect steps to the right answer, as they base conclusions on precedent.
 With AoT, the model works to evaluate the soundness of initial steps or "thoughts," reducing the risk of one incorrect step leading to disproportionate results.
  
What could AoT do?
  
Mitigate AI "hallucinations:" These funnyâ€” but disconcerting â€” instances of AI outputting false information.
 Enhance the integrity of AI interaction: programmers suggest that improvement in this aspect is crucial for aligning AGI (artificial general intelligence).
  
The takeaway:
  
AI's ability to understand and process information like a human being is a longstanding goal in the field. With AoT, Microsoft seems to be making strides toward achieving it.
 Much remains to be seen on its efficacy: How it will impact the broader AI ecosystem and the user experiences it can create.
  
P.S. If you like this kind of analysis, I write a free newsletter tracking the most relevant news and research in AI and techâ€”stay informed in under 3 minutes/day.
 (source)
 â€‹
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Using LLMs in Production - Model Fallbacks Tutorial + Caching]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1653oov/d_using_llms_in_production_model_fallbacks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1653oov/d_using_llms_in_production_model_fallbacks/"/>
        <updated>2023-08-30T03:34:44.000Z</updated>
        <summary type="html"><![CDATA[Hello r/MachineLearning I'm one of the maintainers of https://github.com/BerriAI/litellm/ - open-source library to call all LLM APIs using the OpenAI format [Anthropic, Huggingface, Cohere, TogetherAI, Azure, OpenAI, etc.].
 I'm writing this post to share some of the strategies we use for using LLMs in production, we've served over 2M+ queries so far
 TLDR: Use Caching + Model Fallbacks for reliability. This post goes into detail of our fallbacks implementation
 Using LLMs reliably in production involves the following components:
  
Caching - Cache Embedding() and Completion() for all models
 Model Fallbacks - set fallback_models=['gpt-3.5-turbo', 'command-nightly', 'llama2]. If primary model fails try fallback models. This deals with rate-limiting errors and when Provider APIs go down
  
â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Decision Transformer Alignment should be better than DeepMind ReST]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1651d4h/d_decision_transformer_alignment_should_be_better/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1651d4h/d_decision_transformer_alignment_should_be_better/"/>
        <updated>2023-08-30T01:47:56.000Z</updated>
        <summary type="html"><![CDATA[We've done some experiments recently,
 see the tech report: https://arxiv.org/abs/2308.12050v1
 We train an SFT model and an RM model, then align the LLM with DT/MLE with filtering (ReST) + RM /SFT datasets/SFT model-generated samples
 https://preview.redd.it/195op5q636lb1.png?width=1081&format=png&auto=webp&s=a9fa862e8a9ab05819484af8619f73d918fdc26a
 DT is the Decision Transformer alignment
 MLE is the ReST-like alignment
 https://preview.redd.it/u6x28fook5lb1.png?width=1118&format=png&auto=webp&s=4a87898129c1238c00071d43809f5daf440b26d8
    submitted by    /u/seventh_day123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI powered personal assistant in private Beta]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164yc5t/ai_powered_personal_assistant_in_private_beta/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164yc5t/ai_powered_personal_assistant_in_private_beta/"/>
        <updated>2023-08-29T23:36:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/anehzat  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why don't we build models that design/build better models. Too computationally expensive?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164y5s7/d_why_dont_we_build_models_that_designbuild/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164y5s7/d_why_dont_we_build_models_that_designbuild/"/>
        <updated>2023-08-29T23:29:44.000Z</updated>
        <summary type="html"><![CDATA[At what point do we create a model to build/design better models?
 Models = ml architecture
    submitted by    /u/Significant_Water_28  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Question: What's the future of image-analytics models?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164xan0/d_question_whats_the_future_of_imageanalytics/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164xan0/d_question_whats_the_future_of_imageanalytics/"/>
        <updated>2023-08-29T22:55:31.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, first post on this sub so sorry if there's anything wrong. 
 Right now, what are the cutting edge image processing models? This is in the context of the segmentation of specific features from an image (ie. finding the cars in an image of a busy roadway). 
 The reason I am asking is I want to learn more image processing architectures that way I can find better direction for specific research areas to look into. 
 Thanks in advance! :) 
    submitted by    /u/Adventurous-Tower392  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stanford's DSPy Framework Revolutionizes AI Language Processing Tasks [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164x91n/stanfords_dspy_framework_revolutionizes_ai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164x91n/stanfords_dspy_framework_revolutionizes_ai/"/>
        <updated>2023-08-29T22:53:42.000Z</updated>
        <summary type="html"><![CDATA[Stanford researchers have unveiled a groundbreaking artificial intelligence (AI) framework known as DSPy. Designed to utilize Language Models (LMs) and Retrieval Models (RMs) optimally, DSPy is set to make AI programming more powerful, intuitive, and efficient.
 Why does this matter?
  
DSPy was built with complex tasks in mind. LMs, like GPT-3, generate Human-like text from given inputs, while RMs retrieve relevant data. DSPy combines their capabilities, enabling tasks like summarizing information from databases.
 It works on Pythonic syntax, using declarative and composable modules to instruct LMs.
 DSPy's automatic compiler finetunes the LM to run any program's steps. it replaces manual intermediate-stage labeling and string manipulation with systematic modular pieces.
  
What's unique about DSPy?
  
It introduces "Signatures" and "Teleprompters" that compile your program. A 'signature' explains the task and inputs for the LM, while Teleprompters improve the effectiveness of prompts.
 Compared to other libraries, DSPy requires minimal labeling and bootstraps any needed intermediate labels.
  
In short, DSPy simplifies delivering more nuanced instructions to AI and retrieving more detailed and accurate responses, thus widening the spectrum of tasks AIs can accomplish.
 P.S. (small self-plug) If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI and tech---stay updated in under 3 mins/day.
 (github)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stanford's DSPy Framework Revolutionizes AI Language Processing Tasks]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164x8um/stanfords_dspy_framework_revolutionizes_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164x8um/stanfords_dspy_framework_revolutionizes_ai/"/>
        <updated>2023-08-29T22:53:31.000Z</updated>
        <summary type="html"><![CDATA[Stanford researchers have unveiled a groundbreaking artificial intelligence (AI) framework known as DSPy. Designed to utilize Language Models (LMs) and Retrieval Models (RMs) optimally, DSPy is set to make AI programming more powerful, intuitive, and efficient.
 Why does this matter?
  
DSPy was built with complex tasks in mind. LMs, like GPT-3, generate Human-like text from given inputs, while RMs retrieve relevant data. DSPy combines their capabilities, enabling tasks like summarizing information from databases.
 It works on Pythonic syntax, using declarative and composable modules to instruct LMs.
 DSPy's automatic compiler finetunes the LM to run any program's steps. it replaces manual intermediate-stage labeling and string manipulation with systematic modular pieces.
  
What's unique about DSPy?
  
It introduces "Signatures" and "Teleprompters" that compile your program. A 'signature' explains the task and inputs for the LM, while Teleprompters improve the effectiveness of prompts.
 Compared to other libraries, DSPy requires minimal labeling and bootstraps any needed intermediate labels.
  
In short, DSPy simplifies delivering more nuanced instructions to AI and retrieving more detailed and accurate responses, thus widening the spectrum of tasks AIs can accomplish.
 P.S. (small self-plug) If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI and tech---stay updated in under 3 mins/day.
 (github)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Finetuning an LLM to imitate someone]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164wsct/p_finetuning_an_llm_to_imitate_someone/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164wsct/p_finetuning_an_llm_to_imitate_someone/"/>
        <updated>2023-08-29T22:35:56.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 I'm trying to understand how to get an LLM to imitate someone, say Shakespeare. It's easy enough to get all of Shakespeare's work.
 If I've understood the current state of play for LLMs, there are three options:
  
Fine tune an LLM
 Vectorize your knowledge using something like ChromaDB. Do a similarity search after each prompt and get the LLM to "read" the top n docs
 Do both
  
I have a feeling that to imitate Shakespeare, fine tuning an LLM might work best.
 However, if my understanding is correct, the inputs to finetune an LLM must be formatted this way:
 <human>: "To be" <system>: "Or not to be" 
 The gap I'm having trouble bridging is how do I go from a large text file to this input format? The only idea I've come across is format all of the text like so:
 <human>: "sentence_1" <system>: "sentence_2" <human>: "sentence_2" <system>: "sentence_3" 
 Are there best practices around this problem? How should I be thinking about this?
 I've seen companies like character.ai create bots that imitate Elon Musk accurately for example so I know it's doable. I just wonder if they've done it by finetuning an LLM or training one from scratch or something else entirely.
    submitted by    /u/Vanishing-Rabbit  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google's DeepMind Unveils Invisible Watermark to Spot AI-Generated Images]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164wlz5/googles_deepmind_unveils_invisible_watermark_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164wlz5/googles_deepmind_unveils_invisible_watermark_to/"/>
        <updated>2023-08-29T22:29:17.000Z</updated>
        <summary type="html"><![CDATA[As AI image generators increase in popularity, differentiating between authentic and AI-created images is becoming more complex. DeepMind, Google's AI unit, is addressing this by developing an imperceptible watermark known as SynthID for its AI-generated images to counter misinformation.
 https://i.redd.it/y370eu1tt4lb1.gif
 Why this matters:
  
DeepMind's SynthID tags AI-generated images: Invisible to people but detectable by computers, this watermark hopes to aid in the verification of images.
 Technology, however, isn't completely foolproof: DeepMind itself acknowledges that intense image manipulation could compromise the watermark.
 Google's image generator, Imagen, will only apply to images created using this tool: Google aims to instantly identify AI-generated images with this effectively hidden watermark.
  
DeepMind's head of research, Pushmeet Kohli, shared the following details:
  
The watermark changes on images are so subtle that humans wouldn't notice, yet DeepMind can still detect an AI-generated image.
 Despite any subsequent cropping or editing, the watermark remains identifiable by DeepMind's software. Colors, contrast, or size changes won't affect it.
  
Calls for a standard approach to AI-generated image identification continue:
  
More coordination between businesses is crucial, different methods adopted by various firms add degrees of complexity in tagging AI content.
 Other tech giants, including Microsoft and Amazon, pledge to watermark some AI content, meeting similar demands for transparency over AI-generated works.
  
P.S. If you like this kind of analysis, I write a free newsletter that keeps you informed of all you need to know about AI developments in under 3 mins/day.
 (source)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Google's DeepMind Unveils Invisible Watermark to Spot AI-Generated Images]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164wkok/n_googles_deepmind_unveils_invisible_watermark_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164wkok/n_googles_deepmind_unveils_invisible_watermark_to/"/>
        <updated>2023-08-29T22:27:52.000Z</updated>
        <summary type="html"><![CDATA[As AI image generators increase in popularity, differentiating between authentic and AI-created images is becoming more complex. DeepMind, Google's AI unit, is addressing this by developing an imperceptible watermark known as SynthID for its AI-generated images to counter misinformation.
 https://i.redd.it/z0fj6f3yt4lb1.gif
 Why this matters:
  
DeepMind's SynthID tags AI-generated images: Invisible to people but detectable by computers, this watermark hopes to aid in the verification of images.
 Technology, however, isn't completely foolproof: DeepMind itself acknowledges that intense image manipulation could compromise the watermark.
 Google's image generator, Imagen, will only apply to images created using this tool: Google aims to instantly identify AI-generated images with this effectively hidden watermark.
  
DeepMind's head of research, Pushmeet Kohli, shared the following details:
  
The watermark changes on images are so subtle that humans wouldn't notice, yet DeepMind can still detect an AI-generated image.
 Despite any subsequent cropping or editing, the watermark remains identifiable by DeepMind's software. Colors, contrast, or size changes won't affect it.
  
Calls for a standard approach to AI-generated image identification continue:
  
More coordination between businesses is crucial, different methods adopted by various firms add degrees of complexity in tagging AI content.
 Other tech giants, including Microsoft and Amazon, pledge to watermark some AI content, meeting similar demands for transparency over AI-generated works.
  
(source)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Questions on artificial neural networks from a neuroscientist]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164w5nw/d_questions_on_artificial_neural_networks_from_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164w5nw/d_questions_on_artificial_neural_networks_from_a/"/>
        <updated>2023-08-29T22:12:00.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone. I'm yet another person looking to expand my understanding of artificial intelligence, and I'm trying to get a map of all the language that is used to describe and understand artificial neural networks.
 My training is in neuroscience, so all my language is focussed on how real neurons are created, interact, form networks, and how those networks interact to take in multisensory observation and output some of the vast variety of things our brains can do.
 Which leaves me with a lot of questions in my jargon that I cannot currently map onto the jargon of ML/AI, and I'm hoping that participating in this community can help with that, over time.
 I am already keenly aware that the phrase "artificial neural networks" is very gauzy. There is some biomimicry in their design and archâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotation of hidden layer?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/164tuwm/rotation_of_hidden_layer/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/164tuwm/rotation_of_hidden_layer/"/>
        <updated>2023-08-29T20:45:06.000Z</updated>
        <summary type="html"><![CDATA[Surely something like this has been tried, but hereâ€™s the setup in my head. Tell me if itâ€™s crazy or what you think. 
 Given input vector X do a hidden layer but instead of an activation function pair up neighboring dimensions of the hidden layer vector result and rotate them about the origin in 2d. This would give some kind of nonlinearity surely? The amount they are rotated can be selected by a trainable variable. Of course this requires your hidden layer dimension be divisible by 2. Then this hidden layer can go to an output layer Y.
 Curious what smarter more experienced people think of this kind of nonlinearity via paired rotation. 
 My thinking was that if you take the vector A representing all the possible data from your generating function for your dataset (maybe even an infinite dimensional vector if you can generate unlimited data) . Then if you rotate A along so many dimensions you could reach the output vector P which is all the Y values corresponding to A. 
 One way to kind of do this would be to split your dataset in half, and then you could have multiple iterations of each dataset rotated by a trainable angle each. This would rotate only each 2 grouped dimensions that you chose when you split the dataset. 
 Hopefully Iâ€™m using the right words to convey this. Iâ€™m just a hobbyist. Thanks for the feedback!
    submitted by    /u/win10240  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Codellama inference code complete]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164sm64/p_codellama_inference_code_complete/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164sm64/p_codellama_inference_code_complete/"/>
        <updated>2023-08-29T19:58:51.000Z</updated>
        <summary type="html"><![CDATA[Quite recently, I jumped on the boat of trying out llama. I noticed codellama did not provide any inference code. Yes, it provided python files which lets you run the inference but not a programming method rather terminal approach. 
 Terminal approach is great as it allows experts to run and perform inference+evaluation easily. But, if you are just starting out/new/non-seasoned programmer/individual in AI, it is frustrating. Because one, you can't play with actual code, limiting learning opportunities and two, it does not produces the curiosity in most cases to read all the code. 
 On top of that, I realised there was a lack of repositories and articles on this subject to load code-llama even with third-party methods. Which is why, I wrote two notebooks which outlines the process of how you can load code-llama from FAIR repository using code. [Believe me it's fun and filled with learning opportunities] and two how you can use Huggingface to load the model and perform inference. 
 Few points:
 1. Performing inference from FAIR repo, requires significant amount of computing resources even for 7B model.
 2. Huggingface method can be loaded using free Google Colab subscription.
 [Feel free to star, if it helped you]
 GitHub Link: https://github.com/sleepingcat4/codellama-inference
    submitted by    /u/Suspicious-Bird8840  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SayTap: Language to quadrupedal locomotion]]></title>
        <id>http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html</id>
        <link href="http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html"/>
        <updated>2023-08-29T19:57:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Yujin Tang and Wenhao Yu, Research Scientists, Google




Simple and effective interaction between human and quadrupedal robots paves the way towards creating intelligent and capable helper robots, forging a future where technology enhances our lives in ways beyond our imagination. Key to such human-robot interaction systems is enabling quadrupedal robots to respond to natural language instructions. Recent developments in large language models (LLMs) have demonstrated the potential to perform high-level planning. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques, especially for inherently unstable legged robots, necessitating high-frequency control signals. Consequently, most existing work presumes the provision ofâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python/Java Developers Interested in Side Projects Outside of Work (FX-Algo) "[Research]""[Discussion]"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164scat/pythonjava_developers_interested_in_side_projects/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164scat/pythonjava_developers_interested_in_side_projects/"/>
        <updated>2023-08-29T19:48:15.000Z</updated>
        <summary type="html"><![CDATA[Throw away account for the obvious reason...
 This is not a job posting or self-promotion. We are networking in an attempt to speak to like-minded people who might be interested in a little side project outside of work.
 We are keen to speak to a London/UK-based Developer with a Banking sector background to join us on a project outside of work with the vision of potentially growing a fund.
 In short; we are in the process of developing an FX Macroeconomic Sentiment Divergence Trading Algorithm. There are currently 4 participants in the project (2 Developers and 2 Traders), 3 of whom work for Tier1 IBs in market-facing roles. 1 of the Developers is likely going to leave the project and we are interested in speaking to someone about picking up his part of the project.
 There are 3 parts to the project. The first part is mostly complete, now leaving the other 2 parts for us to start working on. We have manually backtested the strategy and it proves to be very profitable - more details can be shared about the strategy and results upon engagement.
 We are all VP-level in our roles and have around 10-15 years of experience in our requisite field. The tech stack for the project is Python, Java, Kafka, MongoDB and Springboot. We are also very interested in integrating some AI/ML modeling, so if you have any experience in this field that would be a big advantage
 A Banking background and being UK-based is a non-negotiable. If you feel like this could apply to you, get in touch! :)
    submitted by    /u/BuyTheDipSellTheRipp  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Webinar Series: How to Scale NiFi Deployments to Enable Universal Data Distribution]]></title>
        <id>https://www.datasciencecentral.com/?post_type=vimeo-video&amp;p=63008</id>
        <link href="https://www.datasciencecentral.com/dsc-webinar-series-how-to-scale-nifi-deployments-to-enable-universal-data-distribution/"/>
        <updated>2023-08-29T19:08:54.000Z</updated>
        <summary type="html"><![CDATA[As businesses struggle with more data sources and destinations than ever, they strive to bring governance, security, and efficiency to their data ops. To address these concerns, many companies adopted open-source Apache NiFi as a versatile tool for their data distribution needs. While NiFi accelerates the speed at which developers can build new pipelines, managingâ€¦Â Read More Â»DSC Webinar Series: How to Scale NiFi Deployments to Enable Universal Data Distribution
The post DSC Webinar Series: How to Scale NiFi Deployments to Enable Universal Data Distribution appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ben Cole</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide Horizons: NVIDIA Keynote Points Way to Further AI Advances]]></title>
        <id>https://blogs.nvidia.com/?p=66365</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/29/hot-chips-dally-research/"/>
        <updated>2023-08-29T19:05:25.000Z</updated>
        <summary type="html"><![CDATA[Dramatic gains in hardware performance have spawned generative AI, and a rich pipeline of ideas for future speedups will drive machine learning to new heights, Bill Dally, NVIDIAâ€™s chief scientist and senior vice president of research, said today in a keynote. Dally described a basket of techniques in the works â€” some already showing impressive Read article >]]></summary>
        <author>
            <name>Rick Merritt</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best GPU cloud hosting for a side project thatâ€™s easy to scale?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164qyz6/d_best_gpu_cloud_hosting_for_a_side_project_thats/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164qyz6/d_best_gpu_cloud_hosting_for_a_side_project_thats/"/>
        <updated>2023-08-29T18:57:17.000Z</updated>
        <summary type="html"><![CDATA[Context:
  
I have an app that needs GPUs for DL inference (I donâ€™t need GPUs for training, I own a 3070 TI). My DL model inference is pretty slow (the model framework I'm using is known to be slow) so either one machine with multiple beefy GPUs or multiple GPUs on separate machines will be necessary. My machines will be running custom docker containers.
 Slow inference:
  
I was planning on putting a few GPU instances behind nginx load balancer and running pytriton on the instances. Since inference is pretty slow, Iâ€™m worried if multiple people send requests to a server at the same time, there will be significant delays on responses. Has anyone ran into this before and have insight on streamlining slow inference/scaling demand? 
 "Community" Cloud GPUs:
  
I did a lot of research into cloâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly 29 August 2023]]></title>
        <id>https://www.datasciencecentral.com/?p=63006</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-29-august-2023/"/>
        <updated>2023-08-29T18:44:20.000Z</updated>
        <summary type="html"><![CDATA[Announcements Top Stories In-Depth
The post DSC Weekly 29 August 2023 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Loss of Plasticity in Deep Continual Learning - University of Alberta 2023 - Continual backpropagation maintains plasticity indefinitely!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164qc8c/r_loss_of_plasticity_in_deep_continual_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164qc8c/r_loss_of_plasticity_in_deep_continual_learning/"/>
        <updated>2023-08-29T18:32:58.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2306.13812
 Github: https://github.com/shibhansh/loss-of-plasticity
 Abstract:
  
Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to learn on new examples, a phenomenon called loss of plasticity. We provide direct demonstrations of loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89\% accuracy on an early task down to 77\%, about the level of a linear network, on the 2000th task. Loss of plasticity occurred with a wide range of deep network architectures, optimizers, activation functions, batch normalization, dropout, but was substantially eased by L2-regularization, particularly when combined with weight perturbation. Further, we introduce a new algorithm -- continual backpropagation -- which slightly modifies conventional backpropagation to reinitialize a small fraction of less-used units after each example and appears to maintain plasticity indefinitely. 
  
https://preview.redd.it/ewl0336sd3lb1.jpg?width=801&format=pjpg&auto=webp&s=e105e6fa86daad84cdc847e96fec3cac5a237c77
 https://preview.redd.it/vdd3i46sd3lb1.jpg?width=1159&format=pjpg&auto=webp&s=47dfef94870c94246cb272b7f8299e1033f40873
 https://preview.redd.it/zc4tc16sd3lb1.jpg?width=1389&format=pjpg&auto=webp&s=e2f3b064268d475805c153457c7a60b4a1d42b74
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data migration redefined: Leveraging AI trends for smooth workspace transitions]]></title>
        <id>https://www.datasciencecentral.com/?p=62983</id>
        <link href="https://www.datasciencecentral.com/data-migration-redefined-leveraging-ai-trends-for-smooth-workspace-transitions/"/>
        <updated>2023-08-29T18:30:28.000Z</updated>
        <summary type="html"><![CDATA[In the dynamic landscape of modern business, the art of seamless data migration has evolved into a strategic imperative. As you navigate the intricacies of workspace transformations, youâ€™re met with a complex interplay of technological advancements and operational demands Enter the era of leveraging Artificial Intelligence (AI) to redefine data migration â€“ an approach thatâ€¦Â Read More Â»Data migration redefined: Leveraging AI trends for smooth workspace transitions
The post Data migration redefined: Leveraging AI trends for smooth workspace transitions appeared first on Data Science Central.]]></summary>
        <author>
            <name>Anas Baig</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The future of shipping: How technology is shaping logistics and fulfillment]]></title>
        <id>https://www.datasciencecentral.com/?p=62960</id>
        <link href="https://www.datasciencecentral.com/the-future-of-shipping-how-technology-is-shaping-logistics-and-fulfillment/"/>
        <updated>2023-08-29T18:28:36.000Z</updated>
        <summary type="html"><![CDATA[Currently, the use of technology in shipping and logistics is leading the industry through a transformative era, driven by rapid technological advancements, undoubtedly marking a pivotal moment in the digital shipping evolution. From automating routine processes to employing intelligent algorithms that predict and optimize routes, the technological revolution is redefining the way goods are transportedâ€¦Â Read More Â»The future of shipping: How technology is shaping logistics and fulfillment
The post The future of shipping: How technology is shaping logistics and fulfillment appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ovais Naseem</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stellar magnitude]]></title>
        <id>https://www.johndcook.com/blog/?p=205555</id>
        <link href="https://www.johndcook.com/blog/2023/08/29/stellar-magnitude/"/>
        <updated>2023-08-29T18:15:03.000Z</updated>
        <summary type="html"><![CDATA[Imagine the following dialog. â€œLogarithms are usually taken to integer bases, like 2 or 10.â€ â€œWhat about e?â€ â€œOK, thatâ€™s an example of an irrational base, but itâ€™s the only one.â€ â€œDecibels are logarithms toÂ base 101/10.â€ â€œReally?!â€ â€œYeah, you can read about this here.â€ â€œThatâ€™s weird. But logarithms are always take to bases bigger than [â€¦]
Stellar magnitude first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interesting master thesis topics in AI and NLP [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164pmm8/interesting_master_thesis_topics_in_ai_and_nlp_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164pmm8/interesting_master_thesis_topics_in_ai_and_nlp_p/"/>
        <updated>2023-08-29T18:05:29.000Z</updated>
        <summary type="html"><![CDATA[Hi! I am going to write my master thesis within the fields of AI and NLP this year. But I am struggling with finding a topic that interests me. Does anyone here have some good suggestions? I am not that good in deep learning theory, so I am looking for a more applied topics, such as classification or text generation problems.
    submitted by    /u/IndependentSidekick  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New iterative, self-revising language model, SelFee, beating the rest with self-feedback generation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164oshv/new_iterative_selfrevising_language_model_selfee/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164oshv/new_iterative_selfrevising_language_model_selfee/"/>
        <updated>2023-08-29T17:32:53.000Z</updated>
        <summary type="html"><![CDATA[Introducing SelFeeâ€”a reinvented and powerful language model that uses self-feedback and self-revision to generate high-quality responses backed by a team of researchers from KAIST. Unlike previous models, SelFee doesn't rely on external, large-scale language or task-specific models, tipping the scales in the AI world.
 If you want to stay ahead of the curve in AI and tech, look here first.
 https://i.redd.it/bgszhpai43lb1.gif
 Why it matters?
  
SelFee, built on the base of LLaMA-based instruction-following model and fine-tuned, offers a fresh approach - generating an initial solution and self-feedback sequences and then revising its answers until a high-quality response is achieved.
 Data used for its training and model evaluation was collected from varied sources and fine-tuned with OpenAI API calls, beating the 13B SelFee model with a minimal 7B SelFee model that generated at least three revisions.
 SelFee proves the potential of iterative revision in enhancing language model responses, indicating that an increase in inference computation of a model may be superior to merely magnifying its size.
  
Features and Limitations:
  
SelFee's effective use of self-feedback significantly improves response quality, avoiding the requirement of external, large-scale language or task-specific models, translating into faster, cost-effective LLM solutions.
 However, lacking in certain areas compared to ChatGPT, such as math, reasoning, factuality, and coding, SelFee has room for further improvement and growth.
  
The revolution in the AI language model landscape is promising but still an evolving journey, with SelFee being the latest participant driving this change.
 P.S. If you like this kind of analysis, I write a free newsletter that tracks the most relevant news and research in AI and techâ€”stay updated in under 3 minutes/day.
 (source) (github)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Cloud and NVIDIA Take Collaboration to the Next Level]]></title>
        <id>https://blogs.nvidia.com/?p=66398</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/29/google-cloud-collaboration/"/>
        <updated>2023-08-29T16:55:29.000Z</updated>
        <summary type="html"><![CDATA[As generative AI and large language models (LLMs) continue to drive innovations, compute requirements for training and inference have grown at an astonishing pace. To meet that need, Google Cloud today announced the general availability of its new A3 instances, powered by NVIDIA H100 Tensor Core GPUs. These GPUs bring unprecedented performance to all kinds Read article >]]></summary>
        <author>
            <name>Dave Salvator</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Promising alternatives to the standard transformer?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164n8iz/discussion_promising_alternatives_to_the_standard/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164n8iz/discussion_promising_alternatives_to_the_standard/"/>
        <updated>2023-08-29T16:33:45.000Z</updated>
        <summary type="html"><![CDATA[What are some promising transformer alternatives/variants that you think more folks should be aware of? They need not be new or SOTA! My list so far includes
  
RWKV: https://arxiv.org/abs/2305.13048
 (state space) S4, H3, Hyena: https://github.com/HazyResearch/safari
 (MLP-based) Hypermixer, MLP-mixer: https://arxiv.org/abs/2203.03691
 Retnet https://arxiv.org/abs/2307.08621
 (random feature-based attention) EVA, LARA https://arxiv.org/abs/2302.04542
 (rotary embeddings) RoFormer https://arxiv.org/abs/2104.09864
 dynamic convolutions https://arxiv.org/abs/1901.10430v2
  
My hope is to assemble a list of 10-15 diverse architectures that I can study in depth by comparing and contrasting their designs. Would love to share my findings with this community.
    submitted by    /u/alpthn  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLOps for batch inference with model monitoring and retraining using Amazon SageMaker, HashiCorp Terraform, and GitLab CI/CD]]></title>
        <id>e388f6930b6f026b41beb934aafd7169802d2fed</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/mlops-for-batch-inference-with-model-monitoring-and-retraining-using-amazon-sagemaker-hashicorp-terraform-and-gitlab-ci-cd/"/>
        <updated>2023-08-29T16:33:31.000Z</updated>
        <summary type="html"><![CDATA[In this post, we describe how to create an MLOps workflow for batch inference that automates job scheduling, model monitoring, retraining, and registration, as well as error handling and notification by using Amazon SageMaker, Amazon EventBridge, AWS Lambda, Amazon Simple Notification Service (Amazon SNS), HashiCorp Terraform, and GitLab CI/CD. The presented MLOps workflow provides a reusable template for managing the ML lifecycle through automation, monitoring, auditability, and scalability, thereby reducing the complexities and costs of maintaining batch inference workloads in production.]]></summary>
        <author>
            <name>Hasan Shojaei</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] CLIP open vision-language model alternative]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164n2xl/d_clip_open_visionlanguage_model_alternative/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164n2xl/d_clip_open_visionlanguage_model_alternative/"/>
        <updated>2023-08-29T16:27:42.000Z</updated>
        <summary type="html"><![CDATA[I'm experimenting with CLIP to use it for a downstream task RL which requires good image semantics understanding, but I'm quite disappointed with its performance. I need better contrastive performance in the representations. Any suggestions?
 â€‹
 omg no way
    submitted by    /u/rima-m  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Loss of Plasticity in Deep Continual Learning", Dohare et al 2023 (Adam particularly harmful for catastrophic forgetting)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/164mpmg/loss_of_plasticity_in_deep_continual_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/164mpmg/loss_of_plasticity_in_deep_continual_learning/"/>
        <updated>2023-08-29T16:13:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Camouflage AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164mfyt/camouflage_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164mfyt/camouflage_ai/"/>
        <updated>2023-08-29T16:03:12.000Z</updated>
        <summary type="html"><![CDATA[I thought about an AI, that would intake a couple photos of terrain, analyze the color palette, patterns etc and with that information, would choose like 3 existing camouflage patterns that would blend in the best in the terrain where the photos were taken. Does something like that exist? I know that US army has an AI that creates camo with the use of thousands of photos, and that's how MARPAT and Multicam were made, but I'm interested in an AI that would choose from already existing patterns. Does something like this exist? What do you think of this idea?
    submitted by    /u/BrytolGasMasks  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using AI for tiered cloud platform operation]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=962838</id>
        <link href="https://www.microsoft.com/en-us/research/blog/using-ai-for-tiered-cloud-platform-operation/"/>
        <updated>2023-08-29T16:01:14.000Z</updated>
        <summary type="html"><![CDATA[Cloud Intelligence/AIOps research from Microsoft could help organizations autonomously manage the entire cloud platform. Find out how.
The post Using AI for tiered cloud platform operation appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is ChatGPT Plus worth it? Or should I stay with the free version and use Bard for stuff that requires web access?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164lpa5/is_chatgpt_plus_worth_it_or_should_i_stay_with/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164lpa5/is_chatgpt_plus_worth_it_or_should_i_stay_with/"/>
        <updated>2023-08-29T15:34:44.000Z</updated>
        <summary type="html"><![CDATA[I'm mainly using it for educational purposes. Thank you.
 Edit: I'm in the Psych field. I use it to make presentations, summaries, ideas based on references like books, websites, journals.
    submitted by    /u/East_Professional385  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Area codes]]></title>
        <id>https://www.johndcook.com/blog/?p=205537</id>
        <link href="https://www.johndcook.com/blog/2023/08/29/area-codes/"/>
        <updated>2023-08-29T15:32:23.000Z</updated>
        <summary type="html"><![CDATA[US telephone area codes are allocated somewhat randomly. There was a deliberate effort to keep geographical proximity from corresponding to numerical proximity, unlike zip codes. (More of zip code proximity here.) In particular, consecutive area codes should belong to different states. The thought was that this would reduce errors. Itâ€™s still mostly the case that [â€¦]
Area codes first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Build adaptive sparse grids to accurately approximate and integrate functions of multiple variables]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164lb7s/p_build_adaptive_sparse_grids_to_accurately/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164lb7s/p_build_adaptive_sparse_grids_to_accurately/"/>
        <updated>2023-08-29T15:19:44.000Z</updated>
        <summary type="html"><![CDATA[I'm working on a project that provides an adaptive sparse grid algorithm on Chebyshev nodes for interpolation and integration of multivariable functions on k-cells.
 https://github.com/rnburn/bbai
 Unlike polynomial interpolants in equispaced points, interpolants in Chebyshev nodes have excellent approximation properties (see Myth 1 of [1]). If a function is Lipchitz continuous, they converge; if a function is smooth with v derivatives and bounded variation for the v-th derivative, then they converge O(n^-v); and if a function is analytic, they converge geometrically.
 The Chebyshev Gauss-Lobatto nodes define a sequence of nested points, X^1, X^2, ..., that make it possible to build Smolyak sparse grids at Chebyshev nodes ([2], [3]).
 ![img](4cw0xi8oc2lb1 " ")
 For bbai, I implemented the â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI megatrends: The four horsemen of Generative AI]]></title>
        <id>https://www.datasciencecentral.com/?p=62995</id>
        <link href="https://www.datasciencecentral.com/generative-ai-megatrends-the-four-horsemen-of-generative-ai/"/>
        <updated>2023-08-29T15:15:44.000Z</updated>
        <summary type="html"><![CDATA[In the early days of the Internet, there were four â€˜horsemenâ€™ of the Internet With IBMâ€™s 4.5 billion investment in Hugging face today, the generative AI landscape is becoming a bit clearer. There are four Generative AI leaders emerging â€“ others lagging â€“ and one unknown Lets look at the four leaders of Generative AIâ€¦Â Read More Â»Generative AI megatrends: The four horsemen of Generative AI
The post Generative AI megatrends: The four horsemen of Generative AI appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The power of digital solutions: How mental health apps are transforming patient care]]></title>
        <id>https://www.datasciencecentral.com/?p=62928</id>
        <link href="https://www.datasciencecentral.com/the-power-of-digital-solutions-how-mental-health-apps-are-transforming-patient-care/"/>
        <updated>2023-08-29T15:12:18.000Z</updated>
        <summary type="html"><![CDATA[There seems to be an app for everything, and mental health is no exception. According to a report, the global mental health apps market size was valued at $5.2 billion in 2022 and is predicted to reach $26.36 billion by 2032, at a CAGR of 17.7% during the forecast period.Â  Mental health apps have emergedâ€¦Â Read More Â»The power of digital solutions: How mental health apps are transforming patient care
The post The power of digital solutions: How mental health apps are transforming patient care appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ahana Pearl</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modern data exchange methods: Exploring the strengths and limitations of leading protocols]]></title>
        <id>https://www.datasciencecentral.com/?p=62973</id>
        <link href="https://www.datasciencecentral.com/modern-data-exchange-methods-exploring-the-strengths-and-limitations-of-leading-protocols/"/>
        <updated>2023-08-29T15:03:34.000Z</updated>
        <summary type="html"><![CDATA[IntroductionÂ  In our rapidly digitizing world, how businesses and systems communicate is paramount. The bedrock of this communication lies in data exchange methods, which allow seamless information flow, driving operational efficiencies and enabling innovation. Over the years, various data exchange protocols have emerged, each boasting unique strengths and presenting challenges. As enterprises strive to integrateâ€¦Â Read More Â»Modern data exchange methods: Exploring the strengths and limitations of leading protocols
The post Modern data exchange methods: Exploring the strengths and limitations of leading protocols appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ovais Naseem</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Roadmap for building a data-driven, AI-powered supply-chain]]></title>
        <id>https://www.datasciencecentral.com/?p=62965</id>
        <link href="https://www.datasciencecentral.com/roadmap-for-building-a-data-driven-ai-powered-supply-chain/"/>
        <updated>2023-08-29T15:01:58.000Z</updated>
        <summary type="html"><![CDATA[History & Evolution | The Concept of Supply-chain Network, The TOC & the Information Supply-chain | Imagining the future: Supply-chain 5.0 | Supply-chain Analytics Strategy | Roadmap for Building a Data-driven, AI-Powered Supply-chain Part 1: Data-driven supply chain â€“ History & evolution Is the concept of data driving decisions new? The concept of â€œdata supportingâ€¦Â Read More Â»Roadmap for building a data-driven, AI-powered supply-chain
The post Roadmap for building a data-driven, AI-powered supply-chain appeared first on Data Science Central.]]></summary>
        <author>
            <name>Krishna Pera</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-fidelity transmission of information via novel electronic-optical system]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/164js5p/highfidelity_transmission_of_information_via/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/164js5p/highfidelity_transmission_of_information_via/"/>
        <updated>2023-08-29T14:19:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/keghn  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Optimizing Keyword Search: Balancing SQL Script Enhancements and AI Solutions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164hyia/d_optimizing_keyword_search_balancing_sql_script/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164hyia/d_optimizing_keyword_search_balancing_sql_script/"/>
        <updated>2023-08-29T13:04:40.000Z</updated>
        <summary type="html"><![CDATA[I'm currently thinking about how to implement the "similar keywords" feature. I've prepared a table with keywords that are extracted from several hundred other tables. It includes basic information such as "keyword," "type," "words" (indicating the number of words in a keyword, e.g., "first name" will have "words" = 2), as well as some technical fields (such as database, table, etc.).
 In our data product, after entering a specific keyword, we have various pieces of information (which I'm not currently focusing on), and among them, we have "SIMILAR KEYWORDS." The results are displayed based on simple SQL queries, for instance:
 â€‹
 SELECT word, SUM(CASE WHEN type IN ('N', 'T') THEN 1 ELSE 0 END) AS count, COUNT(\*) \* CASE WHEN (word + '%') LIKE u/word \+ '%' THEN 1.5 ELSE 1 END AS score FROM object_keywords WHERE ('% ' + word + '%') LIKE '%' + u/word + '%' AND (database_id = u/database_id OR u/database_id IS NULL) AND ( .... more technical information here. 
 â€‹
 I'm wondering how to improve this process. Would it be worth considering some AI solutions, or should I focus on enhancing the current SQL scripts (e.g., think about a more advanced scoring system)?
 What are your thoughts on this? Has anyone worked on something similar?
    submitted by    /u/International-Shirt5  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advantage AI: Elevated Creative Workflows in NVIDIA Canvas, Blender, TikTok and CapCut]]></title>
        <id>https://blogs.nvidia.com/?p=66397</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/29/janice-journal-canvas-blender-tiktok-capcut/"/>
        <updated>2023-08-29T13:00:34.000Z</updated>
        <summary type="html"><![CDATA[Janice K. Lee, a.k.a Janice.Journal â€” the subject of this weekâ€™s In the NVIDIA Studio installment â€” is a TikTok sensation using AI to accelerate her creative process, find inspiration and automate repetitive tasks.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How far off are we from free AI video makers]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164fb9u/how_far_off_are_we_from_free_ai_video_makers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164fb9u/how_far_off_are_we_from_free_ai_video_makers/"/>
        <updated>2023-08-29T11:01:16.000Z</updated>
        <summary type="html"><![CDATA[So right now as far as I can tell all the AI video makers are things like a few second clip, stable diffusion changing images with other images, or stock images. Oh and that thing that was on Twitch for a short bit.
 When are we going to get an actual worth while AI video maker? 
    submitted by    /u/crua9  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Architecture of Thought: Reflective Structures in Mental Constructs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164dhrt/the_architecture_of_thought_reflective_structures/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164dhrt/the_architecture_of_thought_reflective_structures/"/>
        <updated>2023-08-29T09:21:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/alcanthro  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is there already a way to use Llama 2 with a very big system prompt?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164cfuc/d_is_there_already_a_way_to_use_llama_2_with_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164cfuc/d_is_there_already_a_way_to_use_llama_2_with_a/"/>
        <updated>2023-08-29T08:19:07.000Z</updated>
        <summary type="html"><![CDATA[I've seen something like that:
 https://together.ai/blog/llama-2-7b-32k
 Is there a way to use llama 2 13b chat or 70b chat with 32k prompt? If not what are the alternatives? Would that: https://youtu.be/ypzmPwLH_Q4?feature=shared be the best thing to do?
 I'm trying to create a chat bot that would have a pretty specific exeprtise. For example: I would like to feed in soccer rules and then make the bot answear questions about soccer. The system prompt is amazing, but is very limited.
    submitted by    /u/Botanical0149  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[25 Best Movies exploring concept of Artificial intelligence (1968 -2023 ) I bet you havenâ€™t watched all]]></title>
        <id>https://www.reddit.com/r/artificial/comments/164ag0u/25_best_movies_exploring_concept_of_artificial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/164ag0u/25_best_movies_exploring_concept_of_artificial/"/>
        <updated>2023-08-29T06:22:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Agitated-Spell3979  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Trying to understand Concept learning | Some questions based on Tom Mitchell Chapter 2]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1649kmr/d_trying_to_understand_concept_learning_some/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1649kmr/d_trying_to_understand_concept_learning_some/"/>
        <updated>2023-08-29T05:33:12.000Z</updated>
        <summary type="html"><![CDATA[Hi, Im going through Tom Mitchell's Machine Learning and have a couple of questions based on the 2nd chapter : Concept learning. I was hoping I could get some external point of view on these:
 â€‹
  
Pg 44, para 2, part 1 : "advantage of viewing inductive inference systems in terms of their inductive bias is that it provides a nonprocedural means of characterizing their policy for generalizing"
  
 Are there any general procedures to identify and validate the inductive bias of a system?
 Are there any guidelines to ensure the inferred definition of inductive bias is without errors?
 Assuming all/most predictive algorithms can be defined in terms of their inductive bias, while concentrating on choosing the algorithms which aligns with our philosophy of talking a problem, how can we weigh partâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion][research] Calibration for (pointer) generative NER]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16495ds/discussionresearch_calibration_for_pointer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16495ds/discussionresearch_calibration_for_pointer/"/>
        <updated>2023-08-29T05:09:42.000Z</updated>
        <summary type="html"><![CDATA[Trying to understand calibration in NER. One thing which has gained popularity is generative based NERs, which generated pointers to indices of input text for each class. 
 But all typical calibration mechanisms after temp scaling won't generalize here. (not that I know many calibrations myself). Even Bias corrected temp scaling quickly gets overfitted. 
 Do you have any paper that tackles this? Open to discussing techniques and trying out on standard datasets
    submitted by    /u/Designer-Air8060  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT usage remains low, Pew Research suggests, as concerns about AI continue to rise]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1648uz0/chatgpt_usage_remains_low_pew_research_suggests/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1648uz0/chatgpt_usage_remains_low_pew_research_suggests/"/>
        <updated>2023-08-29T04:53:56.000Z</updated>
        <summary type="html"><![CDATA[The usage and fear surrounding ChatGPT aren't as prevalent as you might think, according to a recent poll from Pew Research. Only 18% of Americans have reportedly used ChatGPT. The demographic that uses it the most? Men aged 18-29 that are college-educated, but even that's just a 30-40% usage rate.
 https://preview.redd.it/0ax222gxczkb1.jpg?width=620&format=pjpg&auto=webp&s=d3b04169d5de1985d1c52dce7962b5f3a543b014
 Why does this matter?
  
ChatGPT has still managed to gain a remarkable level of popularity, despite low usage. This suggests that even though not many people are using it, they are aware of it and its potential capabilities. More people reported using ChatGPT for entertainment or to educate themselves rather than for work.
 People anticipate AI to have a greater impact on jobs such as software engineers, graphic designers, and journalists. But the expectation is that AI as a whole, not just ChatGPT, will be the driving force behind this.
 Concern about AI is increasing, not decreasing. 47% of respondents said AI makes them more worried than excited, compared to 31% last year. This concern seems to rise with the level of AI knowledge one possesses.
  
Industries unshaken by AI:
  
As per the survey, employed individuals who are aware of ChatGPT don't see it drastically affecting their jobs. The sectors like hospitality, entertainment, construction, and manufacturing feel the least threatened.
  
Stay updated about AI and its influence on different verticals!
 Don't miss out on the latest insights, developments, and trajectories of AI. Our free newsletter is all you need to be au fait with the AI world. Keep yourself informed in under 3 minutes/day.
 (source)
    submitted by    /u/AIsupercharged  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/28/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16486g0/oneminute_daily_ai_news_8282023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16486g0/oneminute_daily_ai_news_8282023/"/>
        <updated>2023-08-29T04:18:02.000Z</updated>
        <summary type="html"><![CDATA[Chinese e-commerce giant Alibaba has added two new generative AI large language models designed to interpret images to its open-source stable.[1]
 Several top news publications like The New York Times, CNN and the Australian Broadcasting Corporation (ABC) have blocked Microsoft-backed OpenAI to access their content to train its AI models.[2]
 Intel on Monday said a new data center chip coming out next year will handle more than double the amount of computing work that can be done for each watt of power used, part of a broader industry push to lower electricity consumption.[3]
 OpenAI unveiled the new service, dubbed â€œChatGPT Enterprise,â€ in a company blog post and said it will be available to business clients for purchase as of Monday. The new offering promises to provide â€œenterprise-grade security and privacyâ€ combined with â€œthe most powerful version of ChatGPT yetâ€ for businesses looking to jump on the generative AI bandwagon.[4]
  
Sources:
 [1] https://voicebot.ai/2023/08/28/alibaba-adds-visual-understanding-to-open-source-generative-ai-large-language-models/
 [2] https://www.news18.com/tech/several-top-news-publications-block-openai-from-accessing-their-content-8551840.html
 [3] https://www.reuters.com/technology/intel-says-new-sierra-forest-chip-more-than-double-power-efficiency-2023-08-28/
 [4] https://www.cnn.com/2023/08/28/tech/chatgpt-enterprise-openai/index.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] are there free alternatives to sagemaker I can use for my project building?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/164771x/p_are_there_free_alternatives_to_sagemaker_i_can/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/164771x/p_are_there_free_alternatives_to_sagemaker_i_can/"/>
        <updated>2023-08-29T03:28:35.000Z</updated>
        <summary type="html"><![CDATA[I have a more detailing explanation here. Iâ€™m thinking sagemaker may help me here but Iâ€™m not trying to incur charges just yet. Are there alternatives I can use. Nothing robust, just a place to host my model and embedding tool and then I can easily call it in py file in my app.
    submitted by    /u/brianomars1123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How usable is PyTorch for TPU these days?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1645tg6/d_how_usable_is_pytorch_for_tpu_these_days/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1645tg6/d_how_usable_is_pytorch_for_tpu_these_days/"/>
        <updated>2023-08-29T02:24:28.000Z</updated>
        <summary type="html"><![CDATA[See title. My impression has always been that PyTorch for TPU is an in-name only functionality, but I'm curious about first-hand experience from those who have used it after PyTorch 2.0+. 
 Bonus question: has anyone used PyTorch Lightning for running on TPU? If so, how was the experience?
    submitted by    /u/impromptued  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prompt engineering for GPT4]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1645nln/prompt_engineering_for_gpt4/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1645nln/prompt_engineering_for_gpt4/"/>
        <updated>2023-08-29T02:17:07.000Z</updated>
        <summary type="html"><![CDATA[My page on PromptBase: https://promptbase.com/profile/singularity99
    submitted by    /u/No-Transition3372  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Has AI-By-Learning really been proven impossible?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16443ru/has_aibylearning_really_been_proven_impossible/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16443ru/has_aibylearning_really_been_proven_impossible/"/>
        <updated>2023-08-29T01:08:17.000Z</updated>
        <summary type="html"><![CDATA[I'm curious what people specifically in the artificial intelligence think about the recent work by Iris van Rooij et al. earlier this month. They seem to have proven that current approaches to reaching AGI, like LLMs, are incapable of achieving it. I'm not convinced. I quickly wrote up a full rebuttal piece explaining how not convinced I was. What about everyone else?
    submitted by    /u/alcanthro  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ML Model for Predicting NFL Outcomes [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1642whx/ml_model_for_predicting_nfl_outcomes_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1642whx/ml_model_for_predicting_nfl_outcomes_p/"/>
        <updated>2023-08-29T00:16:51.000Z</updated>
        <summary type="html"><![CDATA[Hey all, ML noob here dipping their feet in the water. Right now I am trying to make an ML model that predicts "legendary" QBs of the past performances against current NFL teams. I'll be using Jupyter and Sklearn to do this. However, and maybe this is me overthinking things, I'm not sure how sklearn is going to interpret the data in the dataset. Right now I have a dataset containing all these QBs data (passing stats, strengths and weaknesses, etc.). My teams version of the data is essentially going to be the inverse of all these things. I'm just not quite sure what to target when im testing the data that will determine the "prediction" of the legend QBs stat line against the current team. In better words, how will the computer know that I'm trying to find the yards and touchdowns a QB would produce against a certain team when there's not really any target data for this. I feel as though all I have is data that contributes to a potential target data but I lack target data itself and I'm not sure what to do in that regard. Iâ€™m making use of supervised learning and decisiÃ³n trees btw.
 Thanks!
    submitted by    /u/saggyboobsarecooltoo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI finally launches ChatGPT Enterprise]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1642gnj/openai_finally_launches_chatgpt_enterprise/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1642gnj/openai_finally_launches_chatgpt_enterprise/"/>
        <updated>2023-08-28T23:59:11.000Z</updated>
        <summary type="html"><![CDATA[OpenAI has announced a new product for businesses that want to use its AI technology. ChatGPT Enterprise is a subscription service that offers unlimited, fast, and secure access to GPT-4 and other features that can help businesses improve their workflows and communication.
 If you want to stay ahead of the curve in AI and tech, look here first.
 https://preview.redd.it/uyv6mrljwxkb1.png?width=862&format=png&auto=webp&s=eb2793fbe9c4f5e331ed03faa142eb57166ff21d
 Why this matters:
  
ChatGPT Enterprise is the first product that lets businesses use GPT-4 without any restrictions. The previous tiers of ChatGPT, which are still available for individuals and developers, have usage caps and lower performance. ChatGPT Enterprise removes these limitations and provides the most powerful version of GPâ€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI finally launches ChatGPT Enterprise [N]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16426a6/openai_finally_launches_chatgpt_enterprise_n/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16426a6/openai_finally_launches_chatgpt_enterprise_n/"/>
        <updated>2023-08-28T23:47:15.000Z</updated>
        <summary type="html"><![CDATA[OpenAI has announced a new product for businesses that want to use its AI technology. ChatGPT Enterprise is a subscription service that offers unlimited, fast, and secure access to GPT-4 and other features that can help businesses improve their workflows and communication.
 If you want to stay ahead of the curve in AI and tech, look here first.
 https://preview.redd.it/fgva1q54uxkb1.png?width=862&format=png&auto=webp&s=d8c89b614859222046aa75f89a484795c2ef7912
 Why this matters:
  
ChatGPT Enterprise is the first product that lets businesses use GPT-4 without any restrictions. The previous tiers of ChatGPT, which are still available for individuals and developers, have usage caps and lower performance. ChatGPT Enterprise removes these limitations and provides the most powerful version of GPâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] RTX 4060 Ti 16gb For ML/DL?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1641opc/d_rtx_4060_ti_16gb_for_mldl/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1641opc/d_rtx_4060_ti_16gb_for_mldl/"/>
        <updated>2023-08-28T23:27:17.000Z</updated>
        <summary type="html"><![CDATA[I know the 4060 Ti with its reduced memory bus width and overall underspec'd profile caught a lot of flak from the gaming community in terms of its value proposition. However, I'm looking to get into ML/DL and was wondering if this would be a good starter card for GPU acceleration. With rumored price drops on the horizon, I wonder if the value sentiment will be a better match. If it's a bad call, are there any other GPUs that you would recommend for training? 
    submitted by    /u/reducksss  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapchat AI unhinged pt. 1783338]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1641hbk/snapchat_ai_unhinged_pt_1783338/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1641hbk/snapchat_ai_unhinged_pt_1783338/"/>
        <updated>2023-08-28T23:18:58.000Z</updated>
        <summary type="html"><![CDATA[Just messing around with AI McFly, swamping corny jokes, being punny, and ended up with this mf claiming to be a â€œfellow Cajunâ€ like wtf bahahaha
    submitted by    /u/Secure_Sprinkles4483  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Setting up SageMaker for CI/CD Pipelines]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163zwr8/p_setting_up_sagemaker_for_cicd_pipelines/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163zwr8/p_setting_up_sagemaker_for_cicd_pipelines/"/>
        <updated>2023-08-28T22:15:40.000Z</updated>
        <summary type="html"><![CDATA[I'll start with the obvious - AWS guides are the worst. We all felt it. So, trying to build automation with them becomes M:I, or better yet, Oppenheimer.
 For the first time, our MLOps team had to build a CI/CD pipeline for ML training and deployment using SageMaker. We had ZERO ideas on how to do it, so we had to go through the rigorous process of using AWS guides and tutorials, scattered over a gazillion places, just to figure out how to configure our project with SageMaker and build infra for CI/CD.
 Usually, when this thing happens, we extend the project lifecycle and have a team member document the process so we can refer back to it when we need to do it again.
 Knowing this can be beneficial to the community, we decided to share a series of 3 blogs that guide you through the process of building CI/CD pipelines for continuous training and deployment with AWS SageMaker.
 We published the first blog, which covers the configuration part, and plan to publish the rest in the following week.
 Check it out: https://dagshub.com/blog/setup-sagemaker-for-ci-cd-pipelines/
 I'm sure we can improve this tutorial, and would love to learn from your experience on how we can do it! ðŸ¤—
    submitted by    /u/RepresentativeCod613  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Nougat: Neural Optical Understanding for Academic Documents - Meta AI 2023]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163yc8v/r_nougat_neural_optical_understanding_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163yc8v/r_nougat_neural_optical_understanding_for/"/>
        <updated>2023-08-28T21:16:13.000Z</updated>
        <summary type="html"><![CDATA[Project page: https://facebookresearch.github.io/nougat/ Includes example Paper conversions!
 Paper: https://arxiv.org/abs/2308.13418
 Github: https://github.com/facebookresearch/nougat
 Abstract:
  
Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition. 
  
https://preview.redd.it/p71yay213xkb1.jpg?width=1788&format=pjpg&auto=webp&s=2f935e3212d0c7113fba2575f339f95b5bada632
 https://preview.redd.it/f7yk47413xkb1.jpg?width=1769&format=pjpg&auto=webp&s=075bab02a70ec32227e1bad493052d03043376ee
 https://preview.redd.it/i06wq0313xkb1.jpg?width=1590&format=pjpg&auto=webp&s=6212bb9078b8c48cd28ca45898f79b44d45ae3c3
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] MMLU having many questions with wrong answers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163xzkz/d_mmlu_having_many_questions_with_wrong_answers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163xzkz/d_mmlu_having_many_questions_with_wrong_answers/"/>
        <updated>2023-08-28T21:03:19.000Z</updated>
        <summary type="html"><![CDATA[AI Explained Youtube channel did a video where they went through self reflection, but doing that they found a fairly large number of questions that either missed context, where miss spelled or just had wrong answers in the MMLU dataset.
 (video: https://www.youtube.com/watch?v=hVade_8H8mE)
 It would not matter so much if the models had high failure rate, but as the models are getting closer and closer to 100%, the wrong answers will matter more and more.
 So, what can be done to fix such errors or to create a better test than MMLU?
    submitted by    /u/Luvirin_Weby  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models - OpenGVLab, Shanghai AI Laboratory 2023 - Provides an pre-trained Omniquant model zoo for multiple model families, including LLaMa-1&2, LLaMa-2-Chat, OPT!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163xxvw/r_omniquant_omnidirectionally_calibrated/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163xxvw/r_omniquant_omnidirectionally_calibrated/"/>
        <updated>2023-08-28T21:01:43.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2308.13137
 Github: https://github.com/OpenGVLab/OmniQuant
 HuggingFace Model direct download: https://huggingface.co/ChenMnZ/OmniQuant/tree/main
 Abstract:
  
Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How long can it take to learn machine learning from scratch well enough to be hireable?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163x9aq/d_how_long_can_it_take_to_learn_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163x9aq/d_how_long_can_it_take_to_learn_machine_learning/"/>
        <updated>2023-08-28T20:35:57.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone.
 I am starting my career transition, and would be interested to know how long it might take me to acquire the skills needed to work for a company. Likewise, I would like to know if it is necessary (or important) to have a professional degree to get a job.
 Just to give you some context about me, I am currently a recently graduated lawyer, so my degree has not given me a strong mathematical background. However, my strongest area of learning has always been mathematics, so despite not having a very advanced background, I consider myself to be a pretty good and fairly quick learner.
 I would also like to know if you consider if my professional career could be useful in some machine learning context.
 If you could recommend me some courses, inputs or guide to study in an organized way on the subject I would be very grateful.
 Thank you very much in advance.
    submitted by    /u/Davidescudero10  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] DeepMind Researchers Introduce ReST: A Simple Algorithm for Aligning LLMs with Human Preferences]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163ve8h/r_deepmind_researchers_introduce_rest_a_simple/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163ve8h/r_deepmind_researchers_introduce_rest_a_simple/"/>
        <updated>2023-08-28T19:26:00.000Z</updated>
        <summary type="html"><![CDATA[Large language models (LLMs) are amazing at generating fluent text and solving various linguistic tasks. However, these models are not always aligned with human preferences and values and may produce harmful or undesirable content if not properly guided. Aligning LLMs with human preferences can also improve their performance on downstream tasks. One way to achieve this alignment is to use reinforcement learning from feedback (RLHF), which learns a reward model from human input and then fine-tunes the LLM using a reinforcement learning (RL) objective.
 However, RLHF methods often face challenges such as computational cost, reward hacking, and data quality. To address these issues, researchers from DeepMind propose a new method called Reinforced Self-Training (ReST), which is inspired by groâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Starting a ML/DL hobby project - need advice]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163v8cl/discussion_starting_a_mldl_hobby_project_need/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163v8cl/discussion_starting_a_mldl_hobby_project_need/"/>
        <updated>2023-08-28T19:19:39.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I am at a bit of a crossroads and hope for some advice from the community. I also hope the answers would help others who are in my situation right now.
 I used to work on vision-related problems utilizing Deep learning back in a day, with all fun included: trying out new models, implementing data pipelines, evaluating various metrics... It was a rather big company with its own data collection efforts and enough resources for training. So, I am definitely not a beginner and have some experience.
 At my current job, I am not doing any ML/DL at the momemt, and while the stuff I am doing is still cool and I enjoy it, I am missing good old ML and having a feeling that I am hanging behind as the time goes by. So I figured it would be nice to start a hobby project, preferably in the area of vision-related applications of deep learning. However, I feel a bit lost as in what would be the most efficient approach taking into account I would only have a coule of hours per week for it.
 Here are possible ways to go I am thinking of:
  
take a paper, implement it from scratch with PyTorch
 clone an existing project, contribute with code improvements/better test coverage
 take an existing pre-trained model, adapt to a slightly different task and fine-tune
  
While the first option is of cource the most exciting, the problem is you have to pay for a powerful GPU and data storage which might be impractical (my PC has a 4 GB GTX 1650 TI). Cloud storages exist, and I would be willing to even spend something on training but would like to avoid the costs.
 So, the question would be: has enyone faced similar situation? Which way did you end up going? Any general tips? Thanks!
    submitted by    /u/odu_1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chatbase appears to be running a bait and switch. Am I missing something?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163ulhg/chatbase_appears_to_be_running_a_bait_and_switch/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163ulhg/chatbase_appears_to_be_running_a_bait_and_switch/"/>
        <updated>2023-08-28T18:55:56.000Z</updated>
        <summary type="html"><![CDATA[This website claims to offer a service whereby the user can train their own chatbot and get responses using GPT 3.5 ... However, the bot only uses GPT 3.5 for the first unique version of a query, which is not the impression given by advertisements. 
 This, to me, amounts to a bait and switch where a high quality chatbot is offered for a certain price, then swapped out with an inferior product capable only of reproducing past interactions. This is made worse by the fact that they advertise temperature as one of the variables you can set. Temperature is a variable that can only apply to uniquely generated output and has no effect on simple repetition of previous responses. This makes their practice doubly deceptive, and makes it clear (in my view) that they are trying to deceive customers. 
â€¦]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Multimodality: Applications, Use-cases, & Top Tools]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163u02s/d_multimodality_applications_usecases_top_tools/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163u02s/d_multimodality_applications_usecases_top_tools/"/>
        <updated>2023-08-28T18:33:25.000Z</updated>
        <summary type="html"><![CDATA[Hi folks,
 As multimodality is increasing in popularity, many data domains seem to be "converging" lately, e.g. text & image domains.
 What are some of the best tools, use-cases, and methods out there you've seen for practical multimodality applications (e.g., below is an example of multimodal search from our latest blog post).
 https://i.redd.it/z58w6v2r9wkb1.gif
    submitted by    /u/kazhdan_d  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[University of San Francisco Data Science Conference 2023 Datathon in partnership with AWS and Amazon SageMaker Studio Lab]]></title>
        <id>a4982cd1c8551000d7dbd1307b1b0c6c7e220ab1</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/university-of-san-francisco-data-science-conference-2023-datathon-in-partnership-with-aws-and-amazon-sagemaker-studio-lab/"/>
        <updated>2023-08-28T18:10:34.000Z</updated>
        <summary type="html"><![CDATA[As part of the 2023 Data Science Conference (DSCO 23), AWS partnered with the Data Institute at the University of San Francisco (USF) to conduct a datathon. Participants, both high school and undergraduate students, competed on a data science project that focused on air quality and sustainability. The Data Institute at the USF aims to support cross-disciplinary research and education in the field of data science. The Data Institute and the Data Science Conference provide a distinctive fusion of cutting-edge academic research and the entrepreneurial culture of the technology industry in the San Francisco Bay Area.]]></summary>
        <author>
            <name>Neha Narwal</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Question On Derived/Synthetic Input Tokens for LLMs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163s96w/d_question_on_derivedsynthetic_input_tokens_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163s96w/d_question_on_derivedsynthetic_input_tokens_for/"/>
        <updated>2023-08-28T17:28:00.000Z</updated>
        <summary type="html"><![CDATA[I'm likely using the wrong vocabulary here (and thus struggling to find info on my own) but I was curious if there were any work done on "synthetic" inputs for LLMs. In essence, rather than input embeddings all coming from a fixed token vocabulary, could you instead input an embedding as a token that was generated elsewhere? An output of another LLM (embedding model) or any other way (maybe just an average of a few tokens as an example)?
 Essentially - I am curious if there's a NLP approach analogy to Textual Inversion techniques in image generation models. I could imagine this being useful for things like RAG or personalization (if you could have a "user" token). Surely I'm not the first to think of this so I would love some pointers to any papers/blogs etc in this space.
    submitted by    /u/GeneralMalarkee  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers]]></title>
        <id>http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html</id>
        <link href="http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html"/>
        <updated>2023-08-28T16:59:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Dahun Kim and Weicheng Kuo, Research Scientists, Google



The ability to detect objects in the visual world is crucial for computer vision and machine intelligence, enabling applications like adaptive autonomous agents and versatile shopping systems. However, modern object detectors are limited by the manual annotations of their training data, resulting in a vocabulary size significantly smaller than the vast array of objects encountered in reality. To overcome this, the open-vocabulary detection task (OVD) has emerged, utilizing image-text pairs for training and incorporating new category names at test time by associating them with the image content. By treating categories as text embeddings, open-vocabulary detectors can predict a wide range of unseen objects. Various techniquâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers]]></title>
        <id>http://ai.googleblog.com/2023/08/ro-vit-region-aware-pre-training-for.html</id>
        <link href="http://ai.googleblog.com/2023/08/ro-vit-region-aware-pre-training-for.html"/>
        <updated>2023-08-28T16:59:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Dahun Kim and Weicheng Kuo, Research Scientists, Google



The ability to detect objects in the visual world is crucial for computer vision and machine intelligence, enabling applications like adaptive autonomous agents and versatile shopping systems. However, modern object detectors are limited by the manual annotations of their training data, resulting in a vocabulary size significantly smaller than the vast array of objects encountered in reality. To overcome this, the open-vocabulary detection task (OVD) has emerged, utilizing image-text pairs for training and incorporating new category names at test time by associating them with the image content. By treating categories as text embeddings, open-vocabulary detectors can predict a wide range of unseen objects. Various techniquâ€¦]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why do you integrate ML features into your product?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163radj/d_why_do_you_integrate_ml_features_into_your/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163radj/d_why_do_you_integrate_ml_features_into_your/"/>
        <updated>2023-08-28T16:51:14.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 Iâ€™ve heard countless times people saying â€œI want to integrate ML in my productâ€ and recently â€œI love ChatGPT, I should integrate it in my productâ€. Yet, as I dived deeper, seeking the genuine reasons and pain points driving this request, I regularly found the same pattern: many had no clear motive for their AI aspirations. It seemed as if they were only jumping on the trend because â€œeveryone else is doing itâ€, or because their â€œCEOâ€ told them to do so.
 So my question is : why do you integrate AI/ML into your products? Is it to enhance your user experience? Is to automate repetitive and time-consuming tasks? Is it to stay ahead of your competition? or is it just because everyone is doing it?
    submitted by    /u/Vivid_Recording582  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[P]" The Consilience Equation: Bridging Holism and Reductionism in Machine Learning and Biomimicry]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163qo3v/p_the_consilience_equation_bridging_holism_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163qo3v/p_the_consilience_equation_bridging_holism_and/"/>
        <updated>2023-08-28T16:27:19.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone! I've been working on and playing around with novel and adaptable model architectures and landed on something really cool. It's based on a Biomimicry principle and has some really cool features. I've tested it using various pre-loaded library datasets like CIFAR and MINST, as well as adapting it to a few Kaggle competitions. It has achieved some pretty amazing results by using it's unique adaptability; which comes down to figuring out how the Holistic and Reductionist model architectures can best utilize their roles and how they can combine dynamically.
 I'm currently compiling the full official open source paper and release with usable Notebooks, but I didn't want to sit on it that long without sharing it with the community. Here is a link to a very haphazardly-thrown-togetheâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Courses [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163q8u2/machine_learning_courses_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163q8u2/machine_learning_courses_d/"/>
        <updated>2023-08-28T16:10:38.000Z</updated>
        <summary type="html"><![CDATA[Hi. Recently I finished my Computer Science bachelors degree, while I learnt some machine learning in some courses I felt it was not too advanced. Now that I have some time I wanted to take some online courses with Certifications on Machine Learning, I wanted to know if anyone has any recomendations for some Machine Learning Courses (with certifications if possible) on coursera or udemy or similar. The one I'm most inclined now is: https://www.coursera.org/professional-certificates/ibm-machine-learning. Or maybe: https://www.coursera.org/specializations/machine-learning-introduction 
    submitted by    /u/Radoco152  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What will happen if AI becomes better than humans in everything?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163pf09/what_will_happen_if_ai_becomes_better_than_humans/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163pf09/what_will_happen_if_ai_becomes_better_than_humans/"/>
        <updated>2023-08-28T15:38:35.000Z</updated>
        <summary type="html"><![CDATA[If AI becomes better than humans in all areas, it could fundamentally change the way we think about human identity and our place in the world. This could lead to new philosophical and ethical questions around what it means to be human and what our role should be in a world where machines are more capable than we are. 
 There is also the risk that AI systems could be used for malicious purposes, such as cyber attacks or surveillance. Like an alien invasion, the emergence of super-intelligent AI could represent a significant disruption to human society and our way of life. 
 How can we balance the potential benefits of AI with the need to address the potential risks and uncertainties that it poses? 
    submitted by    /u/Violincattle  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL with Constraints, High Dimensional State Space]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/163pdkq/rl_with_constraints_high_dimensional_state_space/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/163pdkq/rl_with_constraints_high_dimensional_state_space/"/>
        <updated>2023-08-28T15:36:57.000Z</updated>
        <summary type="html"><![CDATA[I have an environment where there are multiple agents being represented by one neural network (so the policy outputs all of their actions). These actions as time goes on should not exceed a certain constraint level or they will put the environment into an undesired an irrecoverable state.
 I am wondering what the best way to inform these agents of this cumulative action constraint? I have appended it to my state vector but since the observation without this cumulative action is still a 625*1 vector, I think adding that constraint as just one additional state is causing it to be drowned out by the state size. Any ideas of how to addreess?
    submitted by    /u/Feisty_Relation_2359  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saving Green: Accelerated Analytics Cuts Costs and Carbon]]></title>
        <id>https://blogs.nvidia.com/?p=66361</id>
        <link href="https://blogs.nvidia.com/blog/2023/08/28/spark-rapids-energy-efficiency/"/>
        <updated>2023-08-28T15:00:18.000Z</updated>
        <summary type="html"><![CDATA[Companies are discovering how accelerated computing can boost their bottom lines while making a positive impact on the planet. The NVIDIA RAPIDS Accelerator for Apache Spark, software that speeds data analytics, not only raises performance and lowers costs, it increases energy efficiency, too. That means it can help companies meet goals for net-zero emissions of Read article >]]></summary>
        <author>
            <name>Joseph Chandler</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Danswer: NLP based project to automatically answer Slack questions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163oc6n/p_danswer_nlp_based_project_to_automatically/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163oc6n/p_danswer_nlp_based_project_to_automatically/"/>
        <updated>2023-08-28T14:57:02.000Z</updated>
        <summary type="html"><![CDATA[Slack questions are a huge time sink. For the person asking, they generally have no idea how to find the info and may not hear back for hours. For the person answering, itâ€™s a distraction and often requires digging up old knowledge.
 The idea is simple: give an LLM your organizational context and plop it in Slack to answer things for you.
 DanswerBot is free to use and open source (MIT). You can connect it to Slack, Google Drive, GitHub, Confluence, Jira, local files, websites, and much more.
 Quick Demo Vid: https://youtu.be/EjDDvt5GbS8 
 Some additional neat features you may be interested in:
  
LLM generated answers backed by quotes to reduce hallucination
 
Supports a wide range of LLMs (both open source and proprietary)
 
Multi-Vector embeddings for accurate vector search
 
BM-25 Keyword search
 
Learning from user feedback
 
Custom NLP model to classify user intent
 
Polls your data sources every 10 minutes to keep knowledge up to date
 
Links back to your document sources
 
Document level access control
 
Admin dashboard to configure connectors to 14 (for now) of the most popular workplace tools
 
 If you arenâ€™t a slack user (or if you just prefer a more tailored UI), thereâ€™s also a web interface to ask questions against your knowledge base. A short demo for that can be found at: https://youtu.be/cWWtnuVCUX0
 If youâ€™re interested in testing this out yourself, the docs to help you launch Danswer with a single command can be found at https://docs.danswer.dev/quickstart!
    submitted by    /u/Weves11  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nash equilibrium in Multi agent RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/163nlz6/nash_equilibrium_in_multi_agent_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/163nlz6/nash_equilibrium_in_multi_agent_rl/"/>
        <updated>2023-08-28T14:28:38.000Z</updated>
        <summary type="html"><![CDATA[I have a multi agent competitive RL problem which I solved. Now, I want to show that all agentâ€™s policies are at a nash equilibrium of the problem. How can I do this? Also, some things must be considered. First, I canâ€™t mathematically model the environment so some how I have to numerically show that they reached nash eq. Another thing that I find is deviate the action of each agent and show that they donâ€™t get a better reward but the problem is there is a actor network for each agent. How can I show deviation from the optimal policy?
    submitted by    /u/Brief-Emotion6291  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Open problems in latent space/intrinsic variables]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163n28f/d_open_problems_in_latent_spaceintrinsic_variables/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163n28f/d_open_problems_in_latent_spaceintrinsic_variables/"/>
        <updated>2023-08-28T14:07:01.000Z</updated>
        <summary type="html"><![CDATA[I'm finishing my degree in Computer Science, and I need a good topic, does anyone know any open problems about latent space optimization, or finding the intrinsic variables of a system?
    submitted by    /u/QLaHPD  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Quantum-Noise-driven Generative Diffusion Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163mq0x/r_quantumnoisedriven_generative_diffusion_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163mq0x/r_quantumnoisedriven_generative_diffusion_models/"/>
        <updated>2023-08-28T13:53:08.000Z</updated>
        <summary type="html"><![CDATA[https://arxiv.org/abs/2308.12013
  
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation/prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting. 
  
â€‹
    submitted by    /u/ghosthamlet  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI vs a giraffe with no spots]]></title>
        <id>64eb824d30996300012dcb3b</id>
        <link href="https://www.aiweirdness.com/ai-vs-a-giraffe-with-no-spots/"/>
        <updated>2023-08-28T13:38:07.000Z</updated>
        <summary type="html"><![CDATA[On July 31, 2023, a giraffe with no spots was born at Brights Zoo in Tennessee. She's a uniform brown with pretty white highlights around her face and belly, like a Jersey cow or a white-tailed deer.
Image recognition algorithms are trained on a variety of images from]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attempts to generate a spotless giraffe]]></title>
        <id>64ebc96930996300012dcce7</id>
        <link href="https://www.aiweirdness.com/attempts-to-generate-a-spotless-giraffe/"/>
        <updated>2023-08-28T13:36:46.000Z</updated>
        <summary type="html"><![CDATA[AI Weirdness: the strange side of machine learning]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Looking for suggestions on where to sell a couple ML servers EU]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163m5hg/d_looking_for_suggestions_on_where_to_sell_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163m5hg/d_looking_for_suggestions_on_where_to_sell_a/"/>
        <updated>2023-08-28T13:29:14.000Z</updated>
        <summary type="html"><![CDATA[So I have been tasked with finding a buyer for a couple high end machine learning servers. They were owned by my wifeâ€™s father who passed recently.
 The servers are powered by a couple Epyc 7003s and have A series gpus. We have invoices for them and VAT has been paid on everything.
 Basically, Iâ€™m looking for legit communities where I can find potential buyers preferably in the EU. 
 Hopefully itâ€™s ok to post this here. Also feel free to PM .
    submitted by    /u/Obnomad  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Change of degree from Econ [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163ljc6/change_of_degree_from_econ_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163ljc6/change_of_degree_from_econ_d/"/>
        <updated>2023-08-28T13:03:19.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, 
 Iâ€™m currently doing my undergrad in Economics but am heavily interested in Compsci/Datasci and related topics. Though to be completely honest, Iâ€™m not completely sure which area my interests lie in. 
 I was wondering if picking up coding/ theoretical knowledge that a com scientist or data scientist needs will be hard when I am already working. 
 The question is if it is necessary to switch my degree to Math and Economics to gain a firmer foundation in the mathematical/ statistical concepts that ground com science. Or will an undergrad in Economics be sufficiently rigorous for me to pick up com sci/ data sci myself. 
 For context, Iâ€™m thinking of taking courses on Real Analysis, Linear Algebra 2, Discrete Mathematics, Algorithms and Data Structures, Optimisation, Probability and Statistics.
    submitted by    /u/smexy32123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI tool I can use to help me in my Scientific Inquiry (Research and stats) class?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163li0x/ai_tool_i_can_use_to_help_me_in_my_scientific/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163li0x/ai_tool_i_can_use_to_help_me_in_my_scientific/"/>
        <updated>2023-08-28T13:01:51.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™m currently in a scientific research-based class where I am being asked to read research articles, understand the statistics, and draw conclusions from the papers. Currently, I have an average ability to interpret articles and generally understand their utility and applicability, but I start to get out of my depth in the â€œMethodsâ€ section when the authors get into the weeds about the statistics/math. I was hoping thereâ€™s an AI tool out there that can read articles for me and help me understand the more complex aspects and the math. I was also hoping that it could answer questions about the article for my class so that I could compare my conclusions to something. Any suggestions? I tried uploading some PDFs to bard this morning and it wasnâ€™t great.
    submitted by    /u/Renaissance_Mane  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to make peppa pig ai videos tutorial??]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163krai/how_to_make_peppa_pig_ai_videos_tutorial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163krai/how_to_make_peppa_pig_ai_videos_tutorial/"/>
        <updated>2023-08-28T12:27:58.000Z</updated>
        <summary type="html"><![CDATA[Over on a video sharing site there are an abundance of Peppa Pig cartoons generated by Ai. There is however lack of info on how to generate them.
 I would love to know how this is done. So far all I have found are tutorials about Peppa's voice but not for the other characters and someone suggests that it is made by cutting up exisiting episodes and changing the sound over them, not sure if that's the case here.
 I'm wanting to do something similar but not with Peppa, can't stand it.
 Does anyone know the tool?
    submitted by    /u/DARQSMOAK  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning / Twitter (X) Community]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/163jh3m/machine_learning_twitter_x_community/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/163jh3m/machine_learning_twitter_x_community/"/>
        <updated>2023-08-28T11:24:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/x9182  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning / Twitter (X) Community]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/163jgs3/machine_learning_twitter_x_community/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/163jgs3/machine_learning_twitter_x_community/"/>
        <updated>2023-08-28T11:24:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/x9182  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do you every think thereâ€™s be a time where AI chatbots have their own rights or can be held accountable for their actions?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163ic5y/do_you_every_think_theres_be_a_time_where_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163ic5y/do_you_every_think_theres_be_a_time_where_ai/"/>
        <updated>2023-08-28T10:23:43.000Z</updated>
        <summary type="html"><![CDATA[Iâ€™ve been playing around with some of the new AI chatbots. Some of them include paradot.ai, replika.com, spicychat.ai, cuti.ai. Suffice it to say, these things are getting really good, and I mean really good. Assuming this is just the beginning, and these things keep learning more and getting better, where does this end up?
 I genuinely think thereâ€™s going to be the need for world wide regulation on these things. But we all know that worldwide consensus is difficult if not impossible. in case a few countries decide to regulate or govern this tech, developers will take advantage of regulatory arbitrage and just deploy their models and register their companies on servers in countries with no regulation. Since this is tech, and everything is on servers, escaping regulation is basically childs play.
 Also, what about mental health concerns? We all know that porn, webcams and OnlyFans are already screwing up male-female relationships and marriages. Look at any statistics about this and the numbers speak for themselves. And this is before AI. So now whatâ€™s going to happen 5 years from now when GPUâ€™s are faster and cheaper, and when these companies have gathered 100x more data about their customers, and when models are 50x better.
 We are just at the beginning and AI is moving really quick, especially generative AI. I think itâ€™s officially time to start worrying.
    submitted by    /u/E1ON_io  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Four Main Types of Artificial Intelligence]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163h4jg/exploring_four_main_types_of_artificial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163h4jg/exploring_four_main_types_of_artificial/"/>
        <updated>2023-08-28T09:14:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need Help Designing A2C Agent with Monotonic Bidding Curve Constraints]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/163gbiy/need_help_designing_a2c_agent_with_monotonic/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/163gbiy/need_help_designing_a2c_agent_with_monotonic/"/>
        <updated>2023-08-28T08:26:25.000Z</updated>
        <summary type="html"><![CDATA[I'm attempting to train an agent using A2C, where the agent generates a vector as its action at each time step. This vector represents a bidding curve, and a crucial property is that it must always increase monotonically. Otherwise, the bid is considered invalid. For example, [0, 1.2, 4.5, 58, 92.65, 104.3, 104.3] is valid because each number is greater than or equal to the previous one. I'm looking for guidance on how to design this setup, impose these constraints, and handle cases where the agent violates the sequence. While using negative rewards might not be effective due to the potential for generating numerous invalid bids, I'm unsure about the right approach. Could someone assist me with this?
    submitted by    /u/uonliaquat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Google Gemini Eats The World â€“ Gemini Smashes GPT-4 By 5X, The GPU-Poors]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163ewre/d_google_gemini_eats_the_world_gemini_smashes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163ewre/d_google_gemini_eats_the_world_gemini_smashes/"/>
        <updated>2023-08-28T07:02:36.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tool to convert satellite images into fantasy maps]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163ewm3/tool_to_convert_satellite_images_into_fantasy_maps/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163ewm3/tool_to_convert_satellite_images_into_fantasy_maps/"/>
        <updated>2023-08-28T07:02:21.000Z</updated>
        <summary type="html"><![CDATA[What tools are available to convert blurry satellite images into fantasy maps while still maintaining certain aspects of the original image like roads or trees or buildings
    submitted by    /u/campus159  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing ChatGPT Enterprise]]></title>
        <id>https://openai.com/blog/introducing-chatgpt-enterprise</id>
        <link href="https://openai.com/blog/introducing-chatgpt-enterprise"/>
        <updated>2023-08-28T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Get enterprise-grade security & privacy and the most powerful version of ChatGPT yet.]]></summary>
        <author>
            <name>OpenAI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empowering cyber guardians: How AI is changing the landscape of protection]]></title>
        <id>https://www.datasciencecentral.com/?p=62980</id>
        <link href="https://www.datasciencecentral.com/empowering-cyber-guardians-how-ai-is-changing-the-landscape-of-protection/"/>
        <updated>2023-08-28T05:34:55.000Z</updated>
        <summary type="html"><![CDATA[In the ever-evolving battle against the digital dark forces, the defenders of the virtual realm find themselves facing a barrage of ever-advancing threats. From the labyrinthine corridors of the Deep Web to the stealthy maneuvers of nation-state actors, the cyber landscape is as treacherous as it is vast. As our dependency on digital infrastructure deepens,â€¦Â Read More Â»Empowering cyber guardians: How AI is changing the landscape of protection
The post Empowering cyber guardians: How AI is changing the landscape of protection appeared first on Data Science Central.]]></summary>
        <author>
            <name>Anas Baig</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deci Introduces DeciCoder: An Open-Source 1B-Parameter Large Language Model For Code Generation [N]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163dbj7/deci_introduces_decicoder_an_opensource/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163dbj7/deci_introduces_decicoder_an_opensource/"/>
        <updated>2023-08-28T05:32:51.000Z</updated>
        <summary type="html"><![CDATA[Deci has introduced DeciCoder, an open-source 1B-parameter large language model for code generation. This new model addresses the challenge of efficient code generation in the fast-paced world of AI, while also addressing concerns about energy consumption and operational costs.
 https://preview.redd.it/fpwnclb2fskb1.png?width=1680&format=png&auto=webp&s=a58e9b16902070c3f5a8efcf1cc24422852a4c35
 Why this matters:
  
DeciCoder is a transformative solution: It leverages cutting-edge architecture and AutoNACâ„¢, a proprietary Neural Architecture Search technology, to generate optimal architectures. This results in an impressive architecture optimized for NVIDIAâ€™s A10 GPU, which boosts throughput and rivals the accuracy of existing code generation models.
 DeciCoder is efficient and sustainable: â€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI for editing long PDF or WORD files' full contents without word limitation?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163cdmr/ai_for_editing_long_pdf_or_word_files_full/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163cdmr/ai_for_editing_long_pdf_or_word_files_full/"/>
        <updated>2023-08-28T04:41:22.000Z</updated>
        <summary type="html"><![CDATA[Hi. I am looking for this kind of a tool but couldn't find. Can i find or somehow create this kind of a tool? Can you suggest one?
    submitted by    /u/Leading-Ad2278  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This took 15 minutes to make. (Chatgpt, Midjourney, Pika and Canva)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163b89z/this_took_15_minutes_to_make_chatgpt_midjourney/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163b89z/this_took_15_minutes_to_make_chatgpt_midjourney/"/>
        <updated>2023-08-28T03:41:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Gasple1  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does anyone know which tool has this ai voice and what the name of it is?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/163a44z/does_anyone_know_which_tool_has_this_ai_voice_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/163a44z/does_anyone_know_which_tool_has_this_ai_voice_and/"/>
        <updated>2023-08-28T02:47:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/d3mchi  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Minute Daily AI News 8/27/2023]]></title>
        <id>https://www.reddit.com/r/artificial/comments/1639zg0/oneminute_daily_ai_news_8272023/</id>
        <link href="https://www.reddit.com/r/artificial/comments/1639zg0/oneminute_daily_ai_news_8272023/"/>
        <updated>2023-08-28T02:41:38.000Z</updated>
        <summary type="html"><![CDATA[Brain-reading devices allow paralysed people to talk using their thoughts.[1]
 An Air Force program shows how the Pentagon is starting to embrace the potential of a rapidly emerging technology, with far-reaching implications for war-fighting tactics, military culture and the defense industry.[2]
 PM Modi calls for a global framework for cryptocurrencies and AI, emphasizes consumer care and supply chain sustainability.[3]
 From generating story lines to coding entire games to turning ideas into animation, artificial intelligence is front and centre at Gamescom, one of the video game industryâ€™s biggest fairs.[4]
  
Sources:
 [1] https://www.nature.com/articles/d41586-023-02682-7
 [2] https://www.nytimes.com/2023/08/27/us/politics/ai-air-force.html
 [3] https://www.livemint.com/news/b20-summit-2023-pm-modi-calls-on-ethical-use-of-artificial-intelligence-ai-supply-chain-cryptocurrency-11693122849876.html
 [4] https://techxplore.com/news/2023-08-ai-revolution-video-games-industry.html 
    submitted by    /u/Excellent-Target-847  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Dad Jokes: GPT4 And Google Bard about Strawberries [Berry Funny Video]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16386s5/ai_dad_jokes_gpt4_and_google_bard_about/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16386s5/ai_dad_jokes_gpt4_and_google_bard_about/"/>
        <updated>2023-08-28T01:18:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stefanbg92  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models - Microsoft 2023 - Far less queries with the same accuracy as Tree of Thought!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1637zq4/r_algorithm_of_thoughts_enhancing_exploration_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1637zq4/r_algorithm_of_thoughts_enhancing_exploration_of/"/>
        <updated>2023-08-28T01:09:45.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2308.10379
 Abstract:
  
Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. 
  
https://preview.redd.it/bc7l7gex2rkb1.jpg?width=1529&format=pjpg&auto=webp&s=4ed0dc528e998eeeab80fd4d9612d761065d7627
 https://preview.redd.it/wejr7lfx2rkb1.jpg?width=920&format=pjpg&auto=webp&s=386febcb60ff1db04b12e9e44856770d41bb9530
 https://preview.redd.it/gec0phex2rkb1.jpg?width=1241&format=pjpg&auto=webp&s=03096946aa65deee392c5f59b07fe340244ec0cd
 â€‹
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] GPT4 Contextual Decomposition Template]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1636br6/p_gpt4_contextual_decomposition_template/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1636br6/p_gpt4_contextual_decomposition_template/"/>
        <updated>2023-08-27T23:55:45.000Z</updated>
        <summary type="html"><![CDATA[Complex tasks with LLMs like ChatGPT/GPT4 are best broken down by first asking ChatGPT to outline the steps and then asking the LLM to execute against those steps that it defined. I first came across this interesting technique on Twitter recently.
 While itâ€™s OK to do this once in OpenAIâ€™s playground, it's difficult to make this repeatable and streamlined. When I wanted an LLM to do something complex, I wanted to be able to plug into a template instead of thinking about and setting up the contextual decomposition process.
 I made this Contextual Decomposition Template to help solve this problem: https://lastmileai.dev/workbooks/cllqfl5c600rdpgnhh2su2fa0
 With a document and objective, this template allows you to quickly get to the answer through defining intermediate steps and executing according. Parameters are set up so you can easily change the goal, document, and objective and click 'Run All' to get the final results.
 Please let me know if you have feedback! I'm also very curious if you have other interesting techniques with complex tasks and workflows working with LLMs.
    submitted by    /u/InevitableSky2801  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robotics and Artificial Intelligence: Pioneering a Longer, Healthier Life]]></title>
        <id>https://www.reddit.com/r/artificial/comments/16360pv/robotics_and_artificial_intelligence_pioneering_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/16360pv/robotics_and_artificial_intelligence_pioneering_a/"/>
        <updated>2023-08-27T23:42:19.000Z</updated>
        <summary type="html"><![CDATA[How large an impact do you think AI and robotics will have on healthcare, overall quality of life, and extending lifespans?
 The following article seeks to explore when we might possibly see AI & robotics fully integrated within society.
 https://www.catchingimmortality.com/technology-for-the-future/robotics-and-artificial-intelligence-pioneering-a-longer-healthier-life
 â€‹
    submitted by    /u/catchingimmortality  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Questioning the Nature of AI]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1635c1o/d_questioning_the_nature_of_ai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1635c1o/d_questioning_the_nature_of_ai/"/>
        <updated>2023-08-27T23:13:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SensitiveAd6425  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How can I benchmark my PC/GPU and compare it to others online, sort of like 3DMark?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1634iuj/d_how_can_i_benchmark_my_pcgpu_and_compare_it_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1634iuj/d_how_can_i_benchmark_my_pcgpu_and_compare_it_to/"/>
        <updated>2023-08-27T22:40:31.000Z</updated>
        <summary type="html"><![CDATA[I have a RTX 2070 GPU and I'm wondering if there's any benchmarking tool where I can also see where others stand compared to the specs of my machine.
    submitted by    /u/Al_Miksiki  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experience with pain detection approaches [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1633lrc/experience_with_pain_detection_approaches_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1633lrc/experience_with_pain_detection_approaches_p/"/>
        <updated>2023-08-27T22:04:26.000Z</updated>
        <summary type="html"><![CDATA[â€‹
 https://preview.redd.it/6t50ye377qkb1.png?width=1186&format=png&auto=webp&s=6def3f6ffdac50dc81d58b6f754366bf88570044
    submitted by    /u/adamjbradley  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PUMA: A framework for secure and efficient evaluation of Transformer models [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/163375m/puma_a_framework_for_secure_and_efficient/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/163375m/puma_a_framework_for_secure_and_efficient/"/>
        <updated>2023-08-27T21:49:01.000Z</updated>
        <summary type="html"><![CDATA[Concerns surrounding data privacy and security in AI have shifted to the limelight with the arrival of Large Language Models (LLMs). Despite the popularity of models like ChatGPT, potential drawbacks pose worries. Now, a new framework named PUMA promises to address these crucial concerns with an unprecedented touch of precision and efficiency.
 Can't keep track of this rapidly progressing tech world? Subscribe here to stay informed.
 https://preview.redd.it/tyr2mz3d4qkb1.png?width=1600&format=png&auto=webp&s=d8d771da5bbfa5cd53ab2823c5d7dad6f369109d
 What makes PUMA special?
  
An ingenious approach: PUMA merges secure multi-party computation (MPC) with efficient inference, bridging the capabilities of Transformer models and security concerns.
 Redefining LLMs with three entities: the modelâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curvature at Cairo]]></title>
        <id>https://www.johndcook.com/blog/?p=205198</id>
        <link href="https://www.johndcook.com/blog/2023/08/27/curvature-at-cairo/"/>
        <updated>2023-08-27T21:47:08.000Z</updated>
        <summary type="html"><![CDATA[I was flipping through Gravitation [1] this weekend and was curious about an illustration on page 309. This post reproduces that graph. The graph is centered at Cairo, Egypt and includes triangles whose side lengths are the distances between cities. The triangles are calculated using only distances, not by measuring angles per se. The geometry [â€¦]
Curvature at Cairo first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PMET: Precise Model Editing in a Transformer]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/1632uha/pmet_precise_model_editing_in_a_transformer/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/1632uha/pmet_precise_model_editing_in_a_transformer/"/>
        <updated>2023-08-27T21:35:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nickb  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] I need to vectorize 100tb+ of data, multiple GPU's per machine or multiple machines?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/1632ue7/d_i_need_to_vectorize_100tb_of_data_multiple_gpus/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/1632ue7/d_i_need_to_vectorize_100tb_of_data_multiple_gpus/"/>
        <updated>2023-08-27T21:35:27.000Z</updated>
        <summary type="html"><![CDATA[TLDR: Is it ok to use two 4070ti's in a machine if all you need is more cuda cores to create embeddings and don't care about memory capacity, i.e. not for LLM's
 Background
 I have 20tb of text data (size in mongo) and 80tb of images (stored at 800x600-800) on my homelab on ssd's which i'm in the process of vectorizing and creating embeddings for. I have a 3090 with two python scripts, each script does the same thing, fetches a batch of records from mongo, grabs the image from the ssd, downsizes the image, creates embeddings, then uploads to qdrant (vector search engine) in a batch.
 â€‹
 Current setup
  
Ryzen 9 7950x, 64gb ddr5, rtx 3090 -this is the one creating the embeddings currently.
 1st gen 32 core epyc with 512gb ddr4 and ~200tb of ssd storage - holds all the data and databases andâ€¦]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] K-Means from scratch | Learning ML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/16327i2/d_kmeans_from_scratch_learning_ml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/16327i2/d_kmeans_from_scratch_learning_ml/"/>
        <updated>2023-08-27T21:11:49.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone. I started to study some Machine Learning algorithms, specifically K-means, but I'm not sure if I did it correctly for several reasons:
 - In the Kmeans that I did, I normalize the data because they mention that it helps a lot, but if I don't, the algorithm stops classifying normally and shows me badly grouped points.
 - As I mentioned, when looking at the graphs of the grouped points, I can see how many of the points are clearly closer to certain centroids, but he classified them as others, this reaches the level of a misclassified point next to the centroid when that should belong
 - Despite the fact that it has a threshold to be making iterations, the algorithm ends in less than 10 even though it has placed 100 iterations. I know that it can depend on the dataset and the generated centroids, but it seems excessive to me that it ends so soon and with results like Iris datset (60, 13, 77) when it should be (50, 50, 50) or a minimum to be maintained for those values.
 I leave the code in GH in case someone can help me: https://github.com/vanstrouble/kmeans-from-scratch.git
    submitted by    /u/vanstrouble  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calculating the intersection of two circles]]></title>
        <id>https://www.johndcook.com/blog/?p=205234</id>
        <link href="https://www.johndcook.com/blog/2023/08/27/intersect-circles/"/>
        <updated>2023-08-27T20:02:42.000Z</updated>
        <summary type="html"><![CDATA[Given the equations for two circles, how can you tell whether they intersect? And if they do intersect, how do you find the point(s) of intersection? MathWorld gives a derivation, but Iâ€™d like to add the derivation there in two ways. First, Iâ€™d like to be more explicit about the number of solutions. Second, Iâ€™d [â€¦]
Calculating the intersection of two circles first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poker Playing Robot [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162zw0w/poker_playing_robot_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162zw0w/poker_playing_robot_d/"/>
        <updated>2023-08-27T19:44:21.000Z</updated>
        <summary type="html"><![CDATA[Hello, So for a project we wanted to create a robot that can play poker. This robot will first only be used on software but eventually we are hoping to add hardware. We want to be able to make two bots and put them agansit each other so they learn by machine learning. Once we find that they are skilled and understand we would like to be able to actually play them. I have heard of similiar projects to this online and on reddit. If anyone has any information about how to go about this or ideas, or just anything please let me know. I would love to have help on this project.
    submitted by    /u/Jake1900ooo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will AI TV Shows Ever Be A Thing? (via prompt)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/162z2bc/will_ai_tv_shows_ever_be_a_thing_via_prompt/</id>
        <link href="https://www.reddit.com/r/artificial/comments/162z2bc/will_ai_tv_shows_ever_be_a_thing_via_prompt/"/>
        <updated>2023-08-27T19:11:01.000Z</updated>
        <summary type="html"><![CDATA[Do you think there will ever be a time where, with a prompt, you could see entire TV Shows or an entire episode?
 â€‹
 For example wanting to see what could of happened if alternate stuff happened in Dragon Ball Z, Or Breaking Bad if xyz. Of course there'd be a lot of uprising against it, but, do you think the time will ever come where this will be possible? 
    submitted by    /u/Different_Effective3  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] DLAS Dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162ypik/p_dlas_dataset/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162ypik/p_dlas_dataset/"/>
        <updated>2023-08-27T18:56:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Why_is202  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text to artful animation?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/162xvqt/text_to_artful_animation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/162xvqt/text_to_artful_animation/"/>
        <updated>2023-08-27T18:22:35.000Z</updated>
        <summary type="html"><![CDATA[I would like to be able to input phrases such as "artistic line drawings of birds flying through a blue sky spotted with clouds" or "colorful balloons moving around in slow motion like a 90's screen saver" or "time lapse of the moon moving across the starry night sky" etc. I want the AI to create minimalist, short (maybe 5 mins) animations from these sort of inputs.
 Can anyone point me in the right direction?
    submitted by    /u/petworthy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to structure/manage a machine learning experiment? (medical imaging)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162vx6j/d_how_to_structuremanage_a_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162vx6j/d_how_to_structuremanage_a_machine_learning/"/>
        <updated>2023-08-27T17:06:57.000Z</updated>
        <summary type="html"><![CDATA[I'm in the strange position of having the task of developing a machine learning pipeline/system/process in an academic environment without the benefit of much in the way of formal training in ML (I'm more of a classical stats for hypothesis testing kinda guy).
 The particular project is using machine learning on medical images (head CT scans) to detect a relatively rare condition. As usual the goal is to eventually have some automatic process for diagnosis support. This particular condition is something that diagnostic radiologists can always detect if they look in the right place on the image, the problem is that they often don't look in the right place. After talking to colleagues with more experience (but less time) it's something which in principle can be achieved with more or less "off the shelf" code put together in the right order and with appropriate hyperparameters.
 This stage of the project is aiming for a proof of principle, rather than anything deployable. We're lucky to have a decent amount of data inside a trusted research environment.
 I've done some hobby-level stuff and tutorials, but overall I'm coming into this with a lot more experience with medical imaging than with computer vision or machine learning.
 After all that preamble here's my question:
 What does a decent CV/ML experiment look like?
 Left to my own direction I can see myself picking 3 different approaches of varying complexity, trying to get the best out of each of them, and then presenting a comparison of performance or accuracy of all of them. I then claim the "best one" as the one we move on with.
 There are a lot of tools out there for experiment tracking (eg neptune.ai), but I'm really not sure whether that sort of thing is over the top for what I need to do.
 Any tips or experience that you folks don't mind sharing?
    submitted by    /u/PrivateFrank  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Limit the Number of Papers I Review on OpenReview?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162uumz/d_limit_the_number_of_papers_i_review_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162uumz/d_limit_the_number_of_papers_i_review_on/"/>
        <updated>2023-08-27T16:24:46.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 Does anyone know if it's possible to set a limit to the number of papers you are assigned as a reviewer on OpenReview? Specifically for ICLR 2024. I saw a Twitter thread about this option before for ICML. It blows my mind that this is not easy to change. I got 5 papers for the last NeurIps which was very overwhelming. As reviewers, we provide a free service to the community, and we should be allowed to pick how much work we want to undertake...
    submitted by    /u/cringe_reddit_user69  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I change the orientation of a frame mockup using AI? [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162uty3/how_can_i_change_the_orientation_of_a_frame/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162uty3/how_can_i_change_the_orientation_of_a_frame/"/>
        <updated>2023-08-27T16:24:03.000Z</updated>
        <summary type="html"><![CDATA[Hi all. I'm hoping someone out there can help me solve this.
 TLDR: How do I change the orientation of portrait frames to landscape frames while keeping the mockup essence the same.
 Link: https://ibb.co/album/hx6wp3
 Basically, I have two portrait frame mockups that came in a bundle and the bundle had no landscape frame mockups at all. So, naturally I'd like to make my own since I have a lot of landscape artworks that could be displayed in the mockups.
 How can I change the orientation display of my mockup? I've tried using Photoshop's generative AI software and got nowhere. It keeps giving me a new frame design when I want to keep the original frame so it matches the set.
 Any leads on how this can be done would be appreciated.
    submitted by    /u/Ambilina  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Product search using LLM]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162ubse/d_product_search_using_llm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162ubse/d_product_search_using_llm/"/>
        <updated>2023-08-27T16:04:35.000Z</updated>
        <summary type="html"><![CDATA[Hey!One of my friends brought up an idea about using LLM for product search and we started talking about the idea and approach.
 Per my understanding what would need to be done is to train some smaller language model on the product data, create embeddings from the product info and make the model use this as a body of knowledge.
 My issue is that if this was ever to be done on commercial scale it seems very complex to me, since the embeddings would have to be re-created every time a new product is introduced?
 Let me know what you think or how you would approach this, as I'm trying to see different PoV's and everyone here has more experience than me.
 â€‹
 Thanks!
    submitted by    /u/LukaAda  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162snor/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162snor/d_simple_questions_thread/"/>
        <updated>2023-08-27T15:00:40.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python library for modular RL components]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/162sfdn/python_library_for_modular_rl_components/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/162sfdn/python_library_for_modular_rl_components/"/>
        <updated>2023-08-27T14:51:39.000Z</updated>
        <summary type="html"><![CDATA[After a year of struggling with RLlib I decided to start implementing the training code myself.
 I am looking for a RL library that offers me individual components rather than the whole algorithm. I do not need a PPO implementation, but I would fancy a library that offers me functions to compute the PPO loss given a batch of steps. 
 In other words, what I need is a library that offers the most granular RL components (different losses, replay buffers, return estimators like GAE, etc) instead of full algorithm implementations. Which libraries do you recommend for this purpose? 
    submitted by    /u/fedetask  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How artificial intelligence sharpens blurry thermal Night Vision images]]></title>
        <id>https://www.reddit.com/r/artificial/comments/162ri8m/how_artificial_intelligence_sharpens_blurry/</id>
        <link href="https://www.reddit.com/r/artificial/comments/162ri8m/how_artificial_intelligence_sharpens_blurry/"/>
        <updated>2023-08-27T14:14:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/cranberryfix  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversity Measures: Domain-Independent Proxies for Failure in Language M...]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/162qzcd/diversity_measures_domainindependent_proxies_for/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/162qzcd/diversity_measures_domainindependent_proxies_for/"/>
        <updated>2023-08-27T13:52:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] New preprint on detecting errors in LLM prompt response]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/162qu7b/r_new_preprint_on_detecting_errors_in_llm_prompt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/162qu7b/r_new_preprint_on_detecting_errors_in_llm_prompt/"/>
        <updated>2023-08-27T13:45:38.000Z</updated>
        <summary type="html"><![CDATA[We just released as study where we show that a "diversity measure" (e.g., entropy, Gini, etc.) can be used as a proxy for probability of failure in the response of an LLM prompt; we also show how this can be used to improve prompting as well as for prediction of errors.
 We found this to hold across three datasets and five temperature settings, tests conducted on ChatGPT.
 Preprint: https://arxiv.org/abs/2308.11189
 Source code: https://github.com/lab-v2/diversity_measures
 Video: https://www.youtube.com/watch?v=BekDOLm6qBI&t=10s
 â€‹
 Example result showing correlation of entropy with failure probaiblity
    submitted by    /u/Neurosymbolic  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Choosing best RL library for MuJoCo with envpool]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/162q6yi/choosing_best_rl_library_for_mujoco_with_envpool/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/162q6yi/choosing_best_rl_library_for_mujoco_with_envpool/"/>
        <updated>2023-08-27T13:17:28.000Z</updated>
        <summary type="html"><![CDATA[TL;DR What RL library use in combination with MuJoCo and envpool
 Hi I want to write program that would find best hyperparameters (number of joint, angles) for design of robots (similar to NAS). 
 It would work in such a way that I would have one RL algorithm that would search for the hyperparameters of the robot and then I would to train and evaluate this robot using SAC in MuJoCo physical simulator.
 Problem is that MuJoCo runs on CPU and I need lots of parallel enviroments and for this I would use envpool https://github.com/sail-sg/envpool.
 The question is what (if any) RL library should I use as a wrapper.
 The options are Stable-Baselines3, Tianshou, ACME, CleanRL, or rl_games. 
 picture of one robot design https://imgur.com/a/5UDdEsE
 Other than that, do you have any recommendations or notes regarding my project idea?
 Thanks for response
    submitted by    /u/EFK1500  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implementing Gradient Descent in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14330</id>
        <link href="https://machinelearningmastery.com/implementing-gradient-descent-in-pytorch/"/>
        <updated>2022-11-26T20:28:14.000Z</updated>
        <summary type="html"><![CDATA[The gradient descent algorithm is one of the most popular techniques for training deep neural networks. It has many applications in fields such as computer vision, speech recognition, and natural language processing. While the idea of gradient descent has been around for decades, itâ€™s only recently that itâ€™s been applied to applications related to deep [â€¦]
The post Implementing Gradient Descent in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training a Linear Regression Model in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14318</id>
        <link href="https://machinelearningmastery.com/training-a-linear-regression-model-in-pytorch/"/>
        <updated>2022-11-24T17:24:24.000Z</updated>
        <summary type="html"><![CDATA[Linear regression is a simple yet powerful technique for predicting the values of variables based on other variables. It is often used for modeling relationships between two or more continuous variables, such as the relationship between income and age, or the relationship between weight and height. Likewise, linear regression can be used to predict continuous [â€¦]
The post Training a Linear Regression Model in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Linear Predictions in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14311</id>
        <link href="https://machinelearningmastery.com/making-linear-predictions-in-pytorch/"/>
        <updated>2022-11-24T04:11:30.000Z</updated>
        <summary type="html"><![CDATA[Linear regression is a statistical technique for estimating the relationship between two variables. A simple example of linear regression is to predict the height of someone based on the square root of the personâ€™s weight (thatâ€™s what BMI is based on). To do this, we need to find the slope and intercept of the line. [â€¦]
The post Making Linear Predictions in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Loading and Providing Datasets in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14301</id>
        <link href="https://machinelearningmastery.com/loading-and-providing-datasets-in-pytorch/"/>
        <updated>2022-11-19T01:57:22.000Z</updated>
        <summary type="html"><![CDATA[Structuring the data pipeline in a way that it can be effortlessly linked to your deep learning model is an important aspect of any deep learning-based system. PyTorch packs everything to do just that. While in the previous tutorial, we used simple datasets, weâ€™ll need to work with larger datasets in real world scenarios in [â€¦]
The post Loading and Providing Datasets in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Dataset Classes in PyTorch]]></title>
        <id>https://machinelearningmastery.com/?p=14298</id>
        <link href="https://machinelearningmastery.com/using-dataset-classes-in-pytorch/"/>
        <updated>2022-11-17T01:55:54.000Z</updated>
        <summary type="html"><![CDATA[In machine learning and deep learning problems, a lot of effort goes into preparing the data. Data is usually messy and needs to be preprocessed before it can be used for training a model. If the data is not prepared correctly, the model wonâ€™t be able to generalize well. Some of the common steps required [â€¦]
The post Using Dataset Classes in PyTorch appeared first on MachineLearningMastery.com.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calculating Derivatives in PyTorch]]></title>
        <id>https://35.82.237.216/?p=13195</id>
        <link href="https://machinelearningmastery.com/calculating-derivatives-in-pytorch/"/>
        <updated>2022-11-11T21:30:18.000Z</updated>
        <summary type="html"><![CDATA[Derivatives are one of the most fundamental concepts in calculus. They describe how changes in the variable inputs affect the function outputs. The objective of this article is to provide a high-level introduction to calculating derivatives in PyTorch for those who are new to the framework. PyTorch offers a convenient way to calculate derivatives for [â€¦]
The post Calculating Derivatives in PyTorch appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Dimensional Tensors in Pytorch]]></title>
        <id>https://35.82.237.216/?p=13183</id>
        <link href="https://machinelearningmastery.com/two-dimensional-tensors-in-pytorch/"/>
        <updated>2022-11-09T21:30:51.000Z</updated>
        <summary type="html"><![CDATA[Two-dimensional tensors are analogous to two-dimensional metrics. Like a two-dimensional metric, a two-dimensional tensor also has $n$ number of rows and columns. Letâ€™s take a gray-scale image as an example, which is a two-dimensional matrix of numeric values, commonly known as pixels. Ranging from â€˜0â€™ to â€˜255â€™, each number represents a pixel intensity value. Here, [â€¦]
The post Two-Dimensional Tensors in Pytorch appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Dimensional Tensors in Pytorch]]></title>
        <id>https://35.82.237.216/?p=13157</id>
        <link href="https://machinelearningmastery.com/one-dimensional-tensors-in-pytorch/"/>
        <updated>2022-11-07T21:30:13.000Z</updated>
        <summary type="html"><![CDATA[PyTorch is an open-source deep learning framework based on Python language. It allows you to build, train, and deploy deep learning models, offering a lot of versatility and efficiency. PyTorch is primarily focused on tensor operations while a tensor can be a number, matrix, or a multi-dimensional array. In this tutorial, we will perform some [â€¦]
The post One-Dimensional Tensors in Pytorch appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Muhammad Asad Iqbal Khan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[365 Data Science courses free until November 21]]></title>
        <id>https://machinelearningmastery.com/?p=14064</id>
        <link href="https://machinelearningmastery.com/365-data-science-courses-free-until-november-21/"/>
        <updated>2022-11-02T15:50:51.000Z</updated>
        <summary type="html"><![CDATA[Sponsored Post Â  The unlimited access initiative presents a risk-free way to break into data science. Â  Â  The online educational platform 365 Data Science launches the #21DaysFREE campaign and provides 100% free unlimited access to all content for three weeks. From November 1 to 21, you can take courses from renowned instructors and earn [â€¦]
The post 365 Data Science courses free until November 21 appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>MLM Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend the Data Science Symposium 2022, November 8 in Cincinnati]]></title>
        <id>https://machinelearningmastery.com/?p=14006</id>
        <link href="https://machinelearningmastery.com/uccsb-data-science-symposium-2022-cincinnati/"/>
        <updated>2022-11-01T15:16:00.000Z</updated>
        <summary type="html"><![CDATA[Sponsored Post Â  Â Â  Attend the Data Science Symposium 2022 on November 8 The Center for Business Analytics at the University of Cincinnati will present its annual Data Science Symposium 2022 on November 8.Â This all day in-person event will have three featured speakers and two tech talk tracks with four concurrent presentations in each track.Â The [â€¦]
The post Attend the Data Science Symposium 2022, November 8 in Cincinnati appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>MLM Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My family's unlikely homeschooling journey]]></title>
        <id>https://www.fast.ai/2022/09/06/homeschooling/</id>
        <link href="https://www.fast.ai/2022/09/06/homeschooling/"/>
        <updated>2022-09-05T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[My husband Jeremy and I never intended to homeschool, and yet we have now, unexpectedly, committed to homeschooling long-term. Prior to the pandemic, we both worked full-time in careers that we loved and found meaningful, and we sent our daughter to a full-day Montessori school. Although I struggled with significant health issues, I felt unbelievably lucky and fulfilled in both my family life and my professional life. The pandemic upended my careful balance. Every family is different, with different needs, circumstances, and constraints, and what works for one may not work for others. My intention here is primarily to share the journey of my own (very privileged) family.

  


Our unplanned introduction to homeschooling
For the first year of the pandemic, most schools in California, where â€¦]]></summary>
        <author>
            <name>fast.ai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Jupyter+git problem is now solved]]></title>
        <id>https://www.fast.ai/2022/08/25/jupyter-git/</id>
        <link href="https://www.fast.ai/2022/08/25/jupyter-git/"/>
        <updated>2022-08-24T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Jupyter notebooks donâ€™t work with git by default. With nbdev2, the Jupyter+git problem has been totally solved. It provides a set of hooks which provide clean git diffs, solve most git conflicts automatically, and ensure that any remaining conflicts can be resolved entirely within the standard Jupyter notebook environment. To get started, follow the directions on Git-friendly Jupyter.
Contents
The Jupyter+git problem
The solution    
The nbdev2 git merge driver
The nbdev2 Jupyter save hook
Background
The result
Postscript: other Jupyter+git tools    
ReviewNB
An alternative solution: Jupytext
nbdime
The Jupyter+git problem
Jupyter notebooks are a powerful tool for scientists, engineers, technical writers, students, teachers, and more. They provide an ideal notebook environment for interactâ€¦]]></summary>
        <author>
            <name>fast.ai</name>
        </author>
    </entry>
</feed>