<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://rsohlot.github.io/rss-feed/index.html</id>
    <title>osmos::feed</title>
    <updated>2022-04-15T00:54:16.646Z</updated>
    <generator>osmosfeed 1.14.4</generator>
    <link rel="alternate" href="https://rsohlot.github.io/rss-feed/index.html"/>
    <link rel="self" href="https://rsohlot.github.io/rss-feed/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[[D] LOOCV]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u3tld1/d_loocv/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u3tld1/d_loocv/"/>
        <updated>2022-04-14T22:50:24.000Z</updated>
        <summary type="html"><![CDATA[I'm working with a small dataset (~400 labeled data). I plan to use logistic regression. Does it make sense/is it necessary to have a hold-out validation set along with doing Leave-one out cross validation (LOOCV) (E.g. leave 20% out, and train model on LOOCV)?
    submitted by    /u/yontbont1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locked-image Tuning: Adding Language Understanding to Image Models]]></title>
        <id>http://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html</id>
        <link href="http://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html"/>
        <updated>2022-04-14T21:41:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Andreas Steiner and Basil Mustafa, Research Software Engineers at Google Research, Brain team 
The ability to classify images into categories has been transformed by deep learning. It has also been significantly accelerated by transfer learning, whereby models are first pre-trained on large datasets, like ImageNet, to learn visual representations that are then transferred via fine-tuning to a new task with less data (e.g., classifying animals). Previous works such as BiT and ViT employed these methods to achieve state-of-the-art performance on a wide range of classification tasks, such as the VTAB benchmark. 
However, fine-tuning has some downsides: though pre-training is done only once, fine-tuning is necessary on every new dataset for which task-specific data is needed. Multimo…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[questions to ask an AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3rraq/questions_to_ask_an_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3rraq/questions_to_ask_an_ai/"/>
        <updated>2022-04-14T21:22:13.000Z</updated>
        <summary type="html"><![CDATA[i recently played a game called tacoma that had a focus on AI and in the game there was a guide for AI that showed 4 hypotheticals to ask an AI to check it's morality and it got me thinking how useful that would be for a real self-aware intelligence so i want to make a list of questions/hypotheticals to ask AGIs
 if you had to interview a recently created sentient AI what questions or hypotheticals would you give it to gauge it's morality, intelligence, creativity, emotion etc.?
    submitted by    /u/neonvolta  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[YouTuber Meets His Creepy Robot Double and Freaks Out]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3plf4/youtuber_meets_his_creepy_robot_double_and_freaks/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3plf4/youtuber_meets_his_creepy_robot_double_and_freaks/"/>
        <updated>2022-04-14T19:42:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/estasfuera  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPO with one worker always picking the best action?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u3pcrp/ppo_with_one_worker_always_picking_the_best_action/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u3pcrp/ppo_with_one_worker_always_picking_the_best_action/"/>
        <updated>2022-04-14T19:31:16.000Z</updated>
        <summary type="html"><![CDATA[If I use PPO with distributed workers, and one of the workers always picks the best action, would that skew the PPO algorithm? It might perform a tad slower, but would it factually introduce wrong math? Perhaps because the PPO optimization requires that all actions are taking proportional to their probabilities? Or would it (mathematically) not matter?
    submitted by    /u/tmuxed  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three from MIT awarded 2022 Paul and Daisy Soros Fellowships for New Americans]]></title>
        <id>https://news.mit.edu/2022/three-mit-students-awarded-paul-daisy-soros-fellowships-new-americans-0414</id>
        <link href="https://news.mit.edu/2022/three-mit-students-awarded-paul-daisy-soros-fellowships-new-americans-0414"/>
        <updated>2022-04-14T19:30:00.000Z</updated>
        <summary type="html"><![CDATA[Fellowship funds graduate studies for outstanding immigrants and children of immigrants.]]></summary>
        <author>
            <name>Julia Mongo | Office of Distinguished Fellowships</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What's the probability distribution of the Feature importances in an ensemble method?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u3o4jp/d_whats_the_probability_distribution_of_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u3o4jp/d_whats_the_probability_distribution_of_the/"/>
        <updated>2022-04-14T18:34:14.000Z</updated>
        <summary type="html"><![CDATA[Assuming feature importance as defined by mean decrease in impurities. I'm curious if there are any studies about their distribution. I'm thinking about using a statistical test to check if a feature is relevant or not, all I can find is using the standard deviation as a measurement of noise. Additionally I imagine if we can give the probability of one feature being more relevant than another given their feature importances
    submitted by    /u/FellowOfHorses  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Collecting Feedback for FinRL: Financial Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u3nru7/discussion_collecting_feedback_for_finrl/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u3nru7/discussion_collecting_feedback_for_finrl/"/>
        <updated>2022-04-14T18:17:49.000Z</updated>
        <summary type="html"><![CDATA[Dear all,
 As a creator of the open-source FinRL project, I would like to welcome all kinds of feedback regarding financial reinforcement learning, especially about how to improve the open-source project FinRL.
 After several years of development and maintenance, we have passed the phase of caring about #stars, now we care more about #downloads, also Wall Street's adoption.
 Appreciate your feedback and sharing!
 Previously when we exposed our message on Reddit, the community was not very supportive about open-source projects' "advertisements". Maybe it consumed public attention and raised bad feelings. Therefore, this time we created a reddit sub-channel for FinRL-related discussions, available at: https://www.reddit.com/r/AI4Finance_FinRL/
 Best,
 Yang
    submitted by    /u/Character-Meat-9176  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Collection for FinRL: Financial Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u3nhkz/feedback_collection_for_finrl_financial/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u3nhkz/feedback_collection_for_finrl_financial/"/>
        <updated>2022-04-14T18:04:11.000Z</updated>
        <summary type="html"><![CDATA[Dear all,
 As a creator of the open-source FinRL project, I would like to welcome all kinds of feedback regarding financial reinforcement learning, especially about how to improve the open-source project FinRL.
 After several years of development and maintenance, we have passed the phase of caring about #stars, now we care more about #downloads, also Wall Street's adoption.
 Appreciate your feedback and sharing!
 Previously when we exposed our message on Reddit, the community was not very supportive about open-source projects' "advertisements". Maybe it consumed public attention and raised bad feelings. Therefore, this time we created a reddit sub-channel for FinRL-related discussions, available at: https://www.reddit.com/r/AI4Finance_FinRL/
 Best,
 Yang
    submitted by    /u/Character-Meat-9176  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latest Research From Stanford Introduces ‘Domino’: A Python Tool for Identifying and Describing Underperforming Slices in Machine Learning Models]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u3n5pe/latest_research_from_stanford_introduces_domino_a/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u3n5pe/latest_research_from_stanford_introduces_domino_a/"/>
        <updated>2022-04-14T17:49:06.000Z</updated>
        <summary type="html"><![CDATA[Machine learning and Artificial Intelligence models have gained promising results in recent years. The major factor behind their success is the availability and development of vast datasets. However, regardless of how many terabytes of data you have or how skilled you are at data science, machine learning models will be useless and even dangerous if you can’t make sense of data records.
 A slice is a collection of data samples with a common feature. For example, in a picture dataset, photographs of antique vehicles make up a slice. When a model’s performance on the data samples in a slice is significantly lower than its overall performance, the slice is considered underperforming.
 Deploying models underperforming on crucial data slices could seriously harm safety and fairness. For instance, models trained to detect collapsed lungs in chest X-rays generally make predictions based on the presence of chest drains, a common therapeutic device. As a result, computer models typically fail to detect collapsed lungs in images without chest drains, a critical data slice in which inaccurate negative predictions could be catastrophic.
 Not many studies have considered underperforming slices during model evaluation. Researchers believe that knowing which slices their models underperform would help practitioners not just make better decisions regarding model deployment but also improve model robustness by upgrading the training dataset or utilizing robust optimization strategies.
 Detecting slices is challenging because the “hidden” data slices are linked by a notion that isn’t easily derived from unstructured inputs or labeled in metadata (e.g., images, video, time-series data).
 Continue reading the summary
 Paper: https://arxiv.org/pdf/2203.14960.pdf
 Article: http://ai.stanford.edu/blog/domino/
 Github: https://github.com/HazyResearch/domino
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reference request for applications of time to ai]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3melc/reference_request_for_applications_of_time_to_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3melc/reference_request_for_applications_of_time_to_ai/"/>
        <updated>2022-04-14T17:13:26.000Z</updated>
        <summary type="html"><![CDATA[Does anyone know of any AI papers, books articles etc that discuss using a sense of time to develop AI, (especially real world time)?
 I've come across papers that discuss how having a sense of time seems to play a role in animal cognition (e.g. temporal cognition), and I'm curious to what extent this has influenced the development of AI.
 Thanks in advance
    submitted by    /u/patterntheoryacc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IBM Data Science and AI Programs on Coursera Free for 30 Days]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3k4j5/ibm_data_science_and_ai_programs_on_coursera_free/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3k4j5/ibm_data_science_and_ai_programs_on_coursera_free/"/>
        <updated>2022-04-14T15:29:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/awsconsultant  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are you aware of these AI Ethical Challenges?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3ipg3/are_you_aware_of_these_ai_ethical_challenges/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3ipg3/are_you_aware_of_these_ai_ethical_challenges/"/>
        <updated>2022-04-14T14:24:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/JencyJane  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-generated easter eggs]]></title>
        <id>62563d78e262cd003d13f40f</id>
        <link href="https://www.aiweirdness.com/ai-generated-easter-eggs/"/>
        <updated>2022-04-14T13:54:58.000Z</updated>
        <summary type="html"><![CDATA[How would AI decorate an easter egg?
I've tried this before by training an image-generating model exclusively on pictures of easter eggs I decorated (they came out plain, if a bit wobbly).
I decided to see what I would get using a model based on CLIP, which has]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonus: What does the x-ray of an Easter egg look like?]]></title>
        <id>625662e4e262cd003d13f4f4</id>
        <link href="https://www.aiweirdness.com/bonus-what-does-the-x-ray-of-an-easter-egg-look-like/"/>
        <updated>2022-04-14T13:54:40.000Z</updated>
        <summary type="html"><![CDATA[AI Weirdness: the strange side of machine learning]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[R-Learning AI self-taking over processes]]></title>
        <id>https://medium.com/p/3d6da629b102</id>
        <link href="https://becominghuman.ai/r-learning-ai-self-taking-over-processes-3d6da629b102?source=rss----5e5bef33608a---4"/>
        <updated>2022-04-14T13:34:18.000Z</updated>
        <summary type="html"><![CDATA[An inside look at how REINFORCEMENT learning, without past reference, extracts “optimal” decisions through simple interaction …]]></summary>
        <author>
            <name>Riccardo Castellani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is a steady linear increase in average reward during training too good to be true? Are there any common pitfalls?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u3hk7l/is_a_steady_linear_increase_in_average_reward/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u3hk7l/is_a_steady_linear_increase_in_average_reward/"/>
        <updated>2022-04-14T13:28:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/C_BearHill  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GFN Thursday Gears Up With More Electronic Arts Games on GeForce NOW]]></title>
        <id>https://blogs.nvidia.com/?p=56571</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/14/geforce-now-thursday-april-14/"/>
        <updated>2022-04-14T13:00:55.000Z</updated>
        <summary type="html"><![CDATA[This GFN Thursday delivers more gr-EA-t games as two new titles from Electronic Arts join the GeForce NOW library. Gamers can now enjoy Need for Speed HEAT  and Plants vs. Zombies Garden Warfare 2 streaming from GeForce NOW to underpowered PCs, Macs, Chromebooks, SHIELD TV and mobile devices. It’s all part of the eight  total Read article >
The post GFN Thursday Gears Up With More Electronic Arts Games on GeForce NOW appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What fun things in ML would you give a presentation on?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u3gdx1/d_what_fun_things_in_ml_would_you_give_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u3gdx1/d_what_fun_things_in_ml_would_you_give_a/"/>
        <updated>2022-04-14T12:27:01.000Z</updated>
        <summary type="html"><![CDATA[If you had 30 minutes to present something fun and exciting to a semi-technical audience, what would you talk about on Machine Learning that would gain interest and engagement?
    submitted by    /u/aero_gsr  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic²: Can AI Be A Powerful Force For Creation? | SiGMA/AGS UAE 2022]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3fjo1/synthetic²_can_ai_be_a_powerful_force_for/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3fjo1/synthetic²_can_ai_be_a_powerful_force_for/"/>
        <updated>2022-04-14T11:41:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/thedyezwfl  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Evaluation and iteration for production models - how?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u3erzw/d_evaluation_and_iteration_for_production_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u3erzw/d_evaluation_and_iteration_for_production_models/"/>
        <updated>2022-04-14T10:57:44.000Z</updated>
        <summary type="html"><![CDATA[How do you evaluate and improve your models in production (particularly for complex modalities like text/vision/audio)? 
 Good models are hard
 In my experience from managing our CNN-based text classification & NER model at a small media analytics startup, evaluating and improving models is a mess. Our domain is fairly niche and diverse, so getting enough training data has been challenging and I mix in custom synthetics & augmentations (which can cause weird model artifacts if you're not careful). It takes a lot of time to discover tricky failure cases by either 1) observing production traffic or 2) probing manually, and then it takes even more time to get the right data to improve model behavior. 
 Are good models hard?
 What's your approach to model evaluation & targeted improvement? Are there any known best practices? I'm a bit at a loss here. As mentioned, I'm specifically interested in others who have deep models as an important part of their product or pipeline across any task or modality. More particularly:
  
How wrong is your model? How do you test it? How would you know about errors before and after it's deployed?
 How much of your time do you spend on iterating on your models? For what kind of issue? 
 Which aspects are most useful to you for improving model performance and reducing critical errors?
  
Maybe I'll take some of the more general ideas from my work and build them out into an evaluation & iteration framework. It's currently a hybrid web of synthetic, interactive/probing and classical approaches. Or maybe there is some approach/library that makes iteration easier without me having to do anything :)
    submitted by    /u/flotothemoon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google finance chief: "We automate everything that can be automated"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3eb72/google_finance_chief_we_automate_everything_that/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3eb72/google_finance_chief_we_automate_everything_that/"/>
        <updated>2022-04-14T10:28:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/much_successes  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Webinar series | Automated CV Pipelines | Instance Classification]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u3dxdq/free_webinar_series_automated_cv_pipelines/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u3dxdq/free_webinar_series_automated_cv_pipelines/"/>
        <updated>2022-04-14T10:04:39.000Z</updated>
        <summary type="html"><![CDATA[Automated CV Pipelines 3rd part is open for registration. It will be covering the methods of streamlining instance classification. If you are interested to check out, here is the link to register.
    submitted by    /u/WeekendClassic  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Trace norm in KFAC paper for regularization]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u3cgfy/d_trace_norm_in_kfac_paper_for_regularization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u3cgfy/d_trace_norm_in_kfac_paper_for_regularization/"/>
        <updated>2022-04-14T08:24:01.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I doubt that the trace norm of the Kronecker product is mistaken in the KFAC paper (https://arxiv.org/abs/1503.05671).
 Shouldn't the division in the blue mark be replaced by multiplication?
 https://preview.redd.it/quoxpzubfgt81.png?width=1241&format=png&auto=webp&s=19e7b60628302f3cb37ba42944088d89d7a7bd28
    submitted by    /u/Cautious_Proposal132  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determine Gridworld values with no probability]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u3c5tr/determine_gridworld_values_with_no_probability/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u3c5tr/determine_gridworld_values_with_no_probability/"/>
        <updated>2022-04-14T08:01:55.000Z</updated>
        <summary type="html"><![CDATA[I am learning Reinforcement learning for games following Gridworld examples. Apologies in advance if this is a basic question, very new to reinforcement learning.
 I am slightly confused in scenarios where probability of moving up, down, left and right are not provided or stated. In this scenario, I assume we assume the optimal policy and therefore, you would apply the Bellman equation as:
 V(s)=maxa(R(s,a)+γV(s′))
 Cost for any movement is 0 and an agent can choose to terminate at a numbered grid to collect a reward amount of the grid number. This is why my square closest to the reward takes in the value 8 since it will terminate with the action to the next state to collect the reward.
 Would this be the correct way to determine the value for the surrounding grid squares?
 https://preview.redd.it/s9l0ok4kbgt81.png?width=806&format=png&auto=webp&s=dfb50450001541b0569d0361fd04a73daa29f222
    submitted by    /u/Artezian  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] To what extent can Rust be used for Machine Learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u3bu3e/d_to_what_extent_can_rust_be_used_for_machine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u3bu3e/d_to_what_extent_can_rust_be_used_for_machine/"/>
        <updated>2022-04-14T07:37:40.000Z</updated>
        <summary type="html"><![CDATA[I recently saw that some parts of HuggingFace ecosystem use Rust under the hood, and HF is a large ecosystem. I've also heard from some of my friends that they had to learn Rust as a first thing in an ML company (it's their first job so they couldn't explain to me exactly why).
 My questions are:
  
What are pros and cons over Python?
 Are there any good frameworks in Rust for ML?
 Are there a decent community & documentation for Rust?
 Is learning it a fun experience?
 Is it used only for deployment?
  
 The reason I'm asking this is that I really love to learn by doing. And so, if I engaged in learning a bit of Rust for ML purposes, would I be able to create something ML-like right of the bat? It can be something as simple as MNIST classifier
  
Take note that I don't know anything about Rust, so these questions might seem noob-like. But I believe that the answers can be of help to others as well.
    submitted by    /u/Icy_Fisherman7187  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NN from Scratch: #3 Forward propagation | Kolbenkraft]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u3blzp/nn_from_scratch_3_forward_propagation_kolbenkraft/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u3blzp/nn_from_scratch_3_forward_propagation_kolbenkraft/"/>
        <updated>2022-04-14T07:21:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/cjmodi306  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How AI is Changing Digital Marketing]]></title>
        <id>https://www.datasciencecentral.com/?p=57232</id>
        <link href="https://www.datasciencecentral.com/how-ai-is-changing-digital-marketing/"/>
        <updated>2022-04-14T05:26:11.000Z</updated>
        <summary type="html"><![CDATA[What is Artificial Intelligence? Oxford Languages defines AI as the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages. For those of us working in the realm of digital marketing, the impact has become even more clear over… Read More »How AI is Changing Digital Marketing
The post How AI is Changing Digital Marketing appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ryan Solecki</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI For Compliance: What, Why, How]]></title>
        <id>https://www.datasciencecentral.com/?p=57219</id>
        <link href="https://www.datasciencecentral.com/ai-for-compliance-what-why-how/"/>
        <updated>2022-04-14T05:19:25.000Z</updated>
        <summary type="html"><![CDATA[With the constant rise and use of technology, Artificial Intelligence (AI) has become a great companion to compliance. Compliance is one of the biggest playing fields and plays a pivotal role in banking institutions. It aims to identify, diminish, and manage risks such as insider trading, spoofing attacks, exploitation of the market, front-running, and more by… Read More »AI For Compliance: What, Why, How
The post AI For Compliance: What, Why, How appeared first on Data Science Central.]]></summary>
        <author>
            <name>Edouard d'Archimbaud</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benefits of Data Governance and Compliance]]></title>
        <id>https://www.datasciencecentral.com/?p=57214</id>
        <link href="https://www.datasciencecentral.com/benefits-of-data-governance-and-compliance/"/>
        <updated>2022-04-14T05:00:39.000Z</updated>
        <summary type="html"><![CDATA[While data compliance is the practice of organizations ensuring that all sensitive data is managed and organized in a way that enables them to meet their business rules alongside legal and governmental regulations, data governance involves the process of managing organizational data’s usability, security, availability, and quality using the internally set rules and policies. Data… Read More »Benefits of Data Governance and Compliance
The post Benefits of Data Governance and Compliance appeared first on Data Science Central.]]></summary>
        <author>
            <name>Indhu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Tips for using Ensemble Learning with a small dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u394zj/d_tips_for_using_ensemble_learning_with_a_small/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u394zj/d_tips_for_using_ensemble_learning_with_a_small/"/>
        <updated>2022-04-14T04:32:22.000Z</updated>
        <summary type="html"><![CDATA[I have started to look into using an ensemble of relatively shallow MLPs to predict using a small dataset (~100 training samples). I was looking specifically at bagging (bootstrap aggregation) as a possibility of improving prediction accuracy.
 I was curious if there were any heuristics for how many models to include in a bagging ensemble? 
 Also, more generally, am I on the correct path, or is there a better direction given my situation? A different ensemble technique, or a different path all together?
 Any advice would be appreciated.
    submitted by    /u/Fritos121  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How To Write A Technical Dissertation]]></title>
        <id>https://www.datasciencecentral.com/?p=57134</id>
        <link href="https://www.datasciencecentral.com/how-to-write-a-technical-dissertation/"/>
        <updated>2022-04-14T04:32:09.000Z</updated>
        <summary type="html"><![CDATA[Technical dissertation writing sometimes seems impossible until it is done. A dissertation is among the lengthiest tasks that can take months to get completed. Thus, it exhausts students, but there is no way around it. It is worth more than about 60 credits in a thesis-based degree. Moreover, gathering proper knowledge and top guidelines about… Read More »How To Write A Technical Dissertation
The post How To Write A Technical Dissertation appeared first on Data Science Central.]]></summary>
        <author>
            <name>EdwardNick</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Data Engineers are in Greater Demand than Data Scientists]]></title>
        <id>https://www.datasciencecentral.com/?p=57178</id>
        <link href="https://www.datasciencecentral.com/why-data-engineers-are-in-greater-demand-than-data-scientists/"/>
        <updated>2022-04-14T04:19:24.000Z</updated>
        <summary type="html"><![CDATA[Globally, many think that data scientist is the best job after Harvard declared it to be one of the hottest jobs of the decade.  And since then, many have been choosing it as their career path. But the role of a data engineer is as important as the data scientist is, because if a data… Read More »Why Data Engineers are in Greater Demand than Data Scientists
The post Why Data Engineers are in Greater Demand than Data Scientists appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When to Go, and When to Explore: The Benefit of Post-Exploration in Intrinsic Motivation. (arXiv:2203.16311v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16311</id>
        <link href="http://arxiv.org/abs/2203.16311"/>
        <updated>2022-04-14T00:58:52.151Z</updated>
        <summary type="html"><![CDATA[Go-Explore achieved breakthrough performance on challenging reinforcement
learning (RL) tasks with sparse rewards. The key insight of Go-Explore was that
successful exploration requires an agent to first return to an interesting
state ('Go'), and only then explore into unknown terrain ('Explore'). We refer
to such exploration after a goal is reached as 'post-exploration'. In this
paper we present a systematic study of post-exploration, answering open
questions that the Go-Explore paper did not answer yet. First, we study the
isolated potential of post-exploration, by turning it on and off within the
same algorithm. Subsequently, we introduce new methodology to adaptively decide
when to post-explore and for how long to post-explore. Experiments on a range
of MiniGrid environments show that post-exploration indeed boosts performance
(with a bigger impact than tuning regular exploration parameters), and this
effect is further enhanced by adaptively deciding when and for how long to
post-explore. In short, our work identifies adaptive post-exploration as a
promising direction for RL exploration research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1"&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1"&gt;Mike Preuss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimators of Entropy and Information via Inference in Probabilistic Models. (arXiv:2202.12363v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.12363</id>
        <link href="http://arxiv.org/abs/2202.12363"/>
        <updated>2022-04-14T00:58:52.150Z</updated>
        <summary type="html"><![CDATA[Estimating information-theoretic quantities such as entropy and mutual
information is central to many problems in statistics and machine learning, but
challenging in high dimensions. This paper presents estimators of entropy via
inference (EEVI), which deliver upper and lower bounds on many information
quantities for arbitrary variables in a probabilistic generative model. These
estimators use importance sampling with proposal distribution families that
include amortized variational inference and sequential Monte Carlo, which can
be tailored to the target model and used to squeeze true information values
with high accuracy. We present several theoretical properties of EEVI and
demonstrate scalability and efficacy on two problems from the medical domain:
(i) in an expert system for diagnosing liver disorders, we rank medical tests
according to how informative they are about latent diseases, given a pattern of
observed symptoms and patient attributes; and (ii) in a differential equation
model of carbohydrate metabolism, we find optimal times to take blood glucose
measurements that maximize information about a diabetic patient's insulin
sensitivity, given their meal and medication schedule.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Saad_F/0/1/0/all/0/1"&gt;Feras A. Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cusumano_Towner_M/0/1/0/all/0/1"&gt;Marco Cusumano-Towner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mansinghka_V/0/1/0/all/0/1"&gt;Vikash K. Mansinghka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating permeability of 3D micro-CT images by physics-informed CNNs based on DNS. (arXiv:2109.01818v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.01818</id>
        <link href="http://arxiv.org/abs/2109.01818"/>
        <updated>2022-04-14T00:58:52.138Z</updated>
        <summary type="html"><![CDATA[In recent years, convolutional neural networks (CNNs) have experienced an
increasing interest in their ability to perform a fast approximation of
effective hydrodynamic parameters in porous media research and applications.
This paper presents a novel methodology for permeability prediction from
micro-CT scans of geological rock samples. The training data set for CNNs
dedicated to permeability prediction consists of permeability labels that are
typically generated by classical lattice Boltzmann methods (LBM) that simulate
the flow through the pore space of the segmented image data. We instead perform
direct numerical simulation (DNS) by solving the stationary Stokes equation in
an efficient and distributed-parallel manner. As such, we circumvent the
convergence issues of LBM that frequently are observed on complex pore
geometries, and therefore, improve the generality and accuracy of our training
data set. Using the DNS-computed permeabilities, a physics-informed CNN PhyCNN)
is trained by additionally providing a tailored characteristic quantity of the
pore space. More precisely, by exploiting the connection to flow problems on a
graph representation of the pore space, additional information about confined
structures is provided to the network in terms of the maximum flow value, which
is the key innovative component of our workflow. The robustness of this
approach is reflected by very high prediction accuracy, which is observed for a
variety of sandstone samples from archetypal rock formations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garttner_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xe4;rttner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alpak_F/0/1/0/all/0/1"&gt;Faruk O. Alpak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meier_A/0/1/0/all/0/1"&gt;Andreas Meier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1"&gt;Nadja Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_F/0/1/0/all/0/1"&gt;Florian Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highly efficient reliability analysis of anisotropic heterogeneous slopes: Machine Learning aided Monte Carlo method. (arXiv:2204.06098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06098</id>
        <link href="http://arxiv.org/abs/2204.06098"/>
        <updated>2022-04-14T00:58:52.137Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) algorithms are increasingly used as surrogate models to
increase the efficiency of stochastic reliability analyses in geotechnical
engineering. This paper presents a highly efficient ML aided reliability
technique that is able to accurately predict the results of a Monte Carlo (MC)
reliability study, and yet performs 500 times faster. A complete MC reliability
analysis on anisotropic heterogeneous slopes consisting of 120,000 simulated
samples is conducted in parallel to the proposed ML aided stochastic technique.
Comparing the results of the complete MC study and the proposed ML aided
technique, the expected errors of the proposed method are realistically
examined. Circumventing the time-consuming computation of factors of safety for
the training datasets, the proposed technique is more efficient than previous
methods. Different ML models, including Random Forest (RF), Support Vector
Machine (SVM) and Artificial Neural Networks (ANN) are presented, optimised and
compared. The effects of the size and type of training and testing datasets are
discussed. The expected errors of the ML predicted probability of failure are
characterised by different levels of soil heterogeneity and anisotropy. Using
only 1% of MC samples to train ML surrogate models, the proposed technique can
accurately predict the probability of failure with mean errors limited to 0.7%.
The proposed technique reduces the computational time required for our study
from 306 days to only 14 hours, providing 500 times higher efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aminpour_M/0/1/0/all/0/1"&gt;Mohammad Aminpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaie_R/0/1/0/all/0/1"&gt;Reza Alaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kardani_N/0/1/0/all/0/1"&gt;Navid Kardani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moridpour_S/0/1/0/all/0/1"&gt;Sara Moridpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nazem_M/0/1/0/all/0/1"&gt;Majidreza Nazem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v3 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.11147</id>
        <link href="http://arxiv.org/abs/2201.11147"/>
        <updated>2022-04-14T00:58:52.137Z</updated>
        <summary type="html"><![CDATA[Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1"&gt;Zhen Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaozhuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1"&gt;Haosen Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shumin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1"&gt;Jiazhang Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huajun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A streamable large-scale clinical EEG dataset for Deep Learning. (arXiv:2203.02552v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.02552</id>
        <link href="http://arxiv.org/abs/2203.02552"/>
        <updated>2022-04-14T00:58:52.137Z</updated>
        <summary type="html"><![CDATA[Deep Learning has revolutionized various fields, including Computer Vision,
Natural Language Processing, as well as Biomedical research. Within the field
of neuroscience, specifically in electrophysiological neuroimaging, researchers
are starting to explore leveraging deep learning to make predictions on their
data without extensive feature engineering. The availability of large-scale
datasets is a crucial aspect of allowing the experimentation of Deep Learning
models. We are publishing the first large-scale clinical EEG dataset that
simplifies data access and management for Deep Learning. This dataset contains
eyes-closed EEG data prepared from a collection of 1,574 juvenile participants
from the Healthy Brain Network. We demonstrate a use case integrating this
framework, and discuss why providing such neuroinformatics infrastructure to
the community is critical for future scientific discoveries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_D/0/1/0/all/0/1"&gt;Dung Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_M/0/1/0/all/0/1"&gt;Manisha Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkataraju_K/0/1/0/all/0/1"&gt;Kannan Umadevi Venkataraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milham_M/0/1/0/all/0/1"&gt;Michael Milham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delorme_A/0/1/0/all/0/1"&gt;Arnaud Delorme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Research on Intellectual Property Resource Profile and Evolution Law. (arXiv:2204.06221v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2204.06221</id>
        <link href="http://arxiv.org/abs/2204.06221"/>
        <updated>2022-04-14T00:58:52.136Z</updated>
        <summary type="html"><![CDATA[In the era of big data, intellectual property-oriented scientific and
technological resources show the trend of large data scale, high information
density and low value density, which brings severe challenges to the effective
use of intellectual property resources, and the demand for mining hidden
information in intellectual property is increasing. This makes intellectual
property-oriented science and technology resource portraits and analysis of
evolution become the current research hotspot. This paper sorts out the
construction method of intellectual property resource intellectual portrait and
its pre-work property entity extraction and entity completion from the aspects
of algorithm classification and general process, and directions for improvement
of future methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yingxia Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adjacency constraint for efficient hierarchical reinforcement learning. (arXiv:2111.00213v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.00213</id>
        <link href="http://arxiv.org/abs/2111.00213"/>
        <updated>2022-04-14T00:58:52.136Z</updated>
        <summary type="html"><![CDATA[Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is large. Searching in a large goal space poses
difficulty for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that in a deterministic Markov Decision
Process (MDP), the proposed adjacency constraint preserves the optimal
hierarchical policy, while in a stochastic MDP the adjacency constraint induces
a bounded state-value suboptimality determined by the MDP's transition
structure. We further show that this constraint can be practically implemented
by training an adjacency network that can discriminate between adjacent and
non-adjacent subgoals. Experimental results on discrete and continuous control
tasks including challenging simulated robot locomotion and manipulation tasks
show that incorporating the adjacency constraint significantly boosts the
performance of state-of-the-art goal-conditioned HRL approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianren Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shangqi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tian Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaolin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from All Vehicles. (arXiv:2203.11934v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.11934</id>
        <link href="http://arxiv.org/abs/2203.11934"/>
        <updated>2022-04-14T00:58:52.135Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a system to train driving policies from experiences
collected not just from the ego-vehicle, but all vehicles that it observes.
This system uses the behaviors of other agents to create more diverse driving
scenarios without collecting additional data. The main difficulty in learning
from other vehicles is that there is no sensor information. We use a set of
supervisory tasks to learn an intermediate representation that is invariant to
the viewpoint of the controlling vehicle. This not only provides a richer
signal at training time but also allows more complex reasoning during
inference. Learning how all vehicles drive helps predict their behavior at test
time and can avoid collisions. We evaluate this system in closed-loop driving
simulations. Our system outperforms all prior methods on the public CARLA
Leaderboard by a wide margin, improving driving score by 25 and route
completion rate by 24 points. Our method won the 2021 CARLA Autonomous Driving
challenge. Code and data are available at https://github.com/dotchen/LAV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1"&gt;Philipp Kr&amp;#xe4;henb&amp;#xfc;hl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Membership Inference Bounds for Adaptive Composition of Sampled Gaussian Mechanisms. (arXiv:2204.06106v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2204.06106</id>
        <link href="http://arxiv.org/abs/2204.06106"/>
        <updated>2022-04-14T00:58:52.134Z</updated>
        <summary type="html"><![CDATA[Given a trained model and a data sample, membership-inference (MI) attacks
predict whether the sample was in the model's training set. A common
countermeasure against MI attacks is to utilize differential privacy (DP)
during model training to mask the presence of individual examples. While this
use of DP is a principled approach to limit the efficacy of MI attacks, there
is a gap between the bounds provided by DP and the empirical performance of MI
attacks. In this paper, we derive bounds for the \textit{advantage} of an
adversary mounting a MI attack, and demonstrate tightness for the widely-used
Gaussian mechanism. We further show bounds on the \textit{confidence} of MI
attacks. Our bounds are much stronger than those obtained by DP analysis. For
example, analyzing a setting of DP-SGD with $\epsilon=4$ would obtain an upper
bound on the advantage of $\approx0.36$ based on our analyses, while getting
bound of $\approx 0.97$ using the analysis of previous work that convert
$\epsilon$ to membership inference bounds.

Finally, using our analysis, we provide MI metrics for models trained on
CIFAR10 dataset. To the best of our knowledge, our analysis provides the
state-of-the-art membership inference bounds for the privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1"&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1"&gt;Alexandre Sablayrolles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cormode_G/0/1/0/all/0/1"&gt;Graham Cormode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LDPC codes: comparing cluster graphs to factor graphs. (arXiv:2204.06350v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2204.06350</id>
        <link href="http://arxiv.org/abs/2204.06350"/>
        <updated>2022-04-14T00:58:52.134Z</updated>
        <summary type="html"><![CDATA[We present a comparison study between a cluster and factor graph
representation of LDPC codes. In probabilistic graphical models, cluster graphs
retain useful dependence between random variables during inference, which are
advantageous in terms of computational cost, convergence speed, and accuracy of
marginal probabilities. This study investigates these benefits in the context
of LDPC codes and shows that a cluster graph representation outperforms the
traditional factor graph representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toit_J/0/1/0/all/0/1"&gt;J du Toit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preez_J/0/1/0/all/0/1"&gt;J du Preez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolhuter_R/0/1/0/all/0/1"&gt;R Wolhuter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Framework for Automatic Cranial Defect Reconstruction and Implant Modeling. (arXiv:2204.06310v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2204.06310</id>
        <link href="http://arxiv.org/abs/2204.06310"/>
        <updated>2022-04-14T00:58:52.121Z</updated>
        <summary type="html"><![CDATA[The goal of this work is to propose a robust, fast, and fully automatic
method for personalized cranial defect reconstruction and implant modeling.

We propose a two-step deep learning-based method using a modified U-Net
architecture to perform the defect reconstruction, and a dedicated iterative
procedure to improve the implant geometry, followed by automatic generation of
models ready for 3-D printing. We propose a cross-case augmentation based on
imperfect image registration combining cases from different datasets. We
perform ablation studies regarding different augmentation strategies and
compare them to other state-of-the-art methods.

We evaluate the method on three datasets introduced during the AutoImplant
2021 challenge, organized jointly with the MICCAI conference. We perform the
quantitative evaluation using the Dice and boundary Dice coefficients, and the
Hausdorff distance. The average Dice coefficient, boundary Dice coefficient,
and the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm
respectively. We perform an additional qualitative evaluation by 3-D printing
and visualization in mixed reality to confirm the implant's usefulness.

We propose a complete pipeline that enables one to create the cranial implant
model ready for 3-D printing. The described method is a greatly extended
version of the method that scored 1st place in all AutoImplant 2021 challenge
tasks. We freely release the source code, that together with the open datasets,
makes the results fully reproducible. The automatic reconstruction of cranial
defects may enable manufacturing personalized implants in a significantly
shorter time, possibly allowing one to perform the 3-D printing process
directly during a given intervention. Moreover, we show the usability of the
defect reconstruction in mixed reality that may further reduce the surgery
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wodzinski_M/0/1/0/all/0/1"&gt;Marek Wodzinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Daniol_M/0/1/0/all/0/1"&gt;Mateusz Daniol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Socha_M/0/1/0/all/0/1"&gt;Miroslaw Socha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hemmerling_D/0/1/0/all/0/1"&gt;Daria Hemmerling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stanuch_M/0/1/0/all/0/1"&gt;Maciej Stanuch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Skalski_A/0/1/0/all/0/1"&gt;Andrzej Skalski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Models with Parametric Likelihood Ratios. (arXiv:2204.06340v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06340</id>
        <link href="http://arxiv.org/abs/2204.06340"/>
        <updated>2022-04-14T00:58:52.121Z</updated>
        <summary type="html"><![CDATA[As machine learning models are deployed ever more broadly, it becomes
increasingly important that they are not only able to perform well on their
training distribution, but also yield accurate predictions when confronted with
distribution shift. The Distributionally Robust Optimization (DRO) framework
proposes to address this issue by training models to minimize their expected
risk under a collection of distributions, to imitate test-time shifts. This is
most commonly achieved by instance-level re-weighting of the training objective
to emulate the likelihood ratio with possible test distributions, which allows
for estimating their empirical risk via importance sampling (assuming that they
are subpopulations of the training distribution). However, re-weighting schemes
in the literature are usually limited due to the difficulty of keeping the
optimization problem tractable and the complexity of enforcing normalization
constraints. In this paper, we show that three simple ideas -- mini-batch level
normalization, a KL penalty and simultaneous gradient updates -- allow us to
train models with DRO using a broader class of parametric likelihood ratios. In
a series of experiments on both image and text classification benchmarks, we
find that models trained with the resulting parametric adversaries are
consistently more robust to subpopulation shifts when compared to other DRO
approaches, and that the method performs reliably well with little
hyper-parameter tuning. Code to reproduce our experiments can be found at
https://github.com/pmichel31415/P-DRO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1"&gt;Paul Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1"&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning multiobjective rough terrain traversability. (arXiv:2203.16354v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16354</id>
        <link href="http://arxiv.org/abs/2203.16354"/>
        <updated>2022-04-14T00:58:52.121Z</updated>
        <summary type="html"><![CDATA[We present a method that uses high-resolution topography data of rough
terrain, and ground vehicle simulation, to predict traversability.
Traversability is expressed as three independent measures: the ability to
traverse the terrain at a target speed, energy consumption, and acceleration.
The measures are continuous and reflect different objectives for planning that
go beyond binary classification. A deep neural network is trained to predict
the traversability measures from the local heightmap and target speed. To
produce training data, we use an articulated vehicle with wheeled bogie
suspensions and procedurally generated terrains. We evaluate the model on
laser-scanned forest terrains, previously unseen by the model. The model
predicts traversability with an accuracy of 90%. Predictions rely on features
from the high-dimensional terrain data that surpass local roughness and slope
relative to the heading. Correlations show that the three traversability
measures are complementary to each other. With an inference speed 3000 times
faster than the ground truth simulation and trivially parallelizable, the model
is well suited for traversability analysis and optimal path planning over large
areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallin_E/0/1/0/all/0/1"&gt;Erik Wallin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiberg_V/0/1/0/all/0/1"&gt;Viktor Wiberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vesterlund_F/0/1/0/all/0/1"&gt;Folke Vesterlund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holmgren_J/0/1/0/all/0/1"&gt;Johan Holmgren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Persson_H/0/1/0/all/0/1"&gt;Henrik Persson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Servin_M/0/1/0/all/0/1"&gt;Martin Servin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization. (arXiv:2204.06504v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2204.06504</id>
        <link href="http://arxiv.org/abs/2204.06504"/>
        <updated>2022-04-14T00:58:52.118Z</updated>
        <summary type="html"><![CDATA[Since 2016, we have witnessed the tremendous growth of artificial
intelligence+visualization (AI+VIS) research. However, existing survey papers
on AI+VIS focus on visual analytics and information visualization, not
scientific visualization (SciVis). In this paper, we survey related deep
learning (DL) works in SciVis, specifically in the direction of DL4SciVis:
designing DL solutions for solving SciVis problems. To stay focused, we
primarily consider works that handle scalar and vector field data but exclude
mesh data. We classify and discuss these works along six dimensions: domain
setting, research task, learning type, network architecture, loss function, and
evaluation metric. The paper concludes with a discussion of the remaining gaps
to fill along the discussed dimensions and the grand challenges we need to
tackle as a community. This state-of-the-art survey guides SciVis researchers
in gaining an overview of this emerging topic and points out future directions
to grow this research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chaoli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jun Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Training of Language Models using JAX pjit and TPUv4. (arXiv:2204.06514v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06514</id>
        <link href="http://arxiv.org/abs/2204.06514"/>
        <updated>2022-04-14T00:58:52.118Z</updated>
        <summary type="html"><![CDATA[Modern large language models require distributed training strategies due to
their size. The challenges of efficiently and robustly training them are met
with rapid developments on both software and hardware frontiers. In this
technical report, we explore challenges and design decisions associated with
developing a scalable training framework, and present a quantitative analysis
of efficiency improvements coming from adopting new software and hardware
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Joanna Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perlin_K/0/1/0/all/0/1"&gt;Kuba Perlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamalakara_S/0/1/0/all/0/1"&gt;Siddhartha Rao Kamalakara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o G.M. Ara&amp;#xfa;jo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentiment Analysis of Political Tweets for Israel using Machine Learning. (arXiv:2204.06515v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.06515</id>
        <link href="http://arxiv.org/abs/2204.06515"/>
        <updated>2022-04-14T00:58:52.118Z</updated>
        <summary type="html"><![CDATA[Sentiment Analysis is a vital research topic in the field of Computer
Science. With the accelerated development of Information Technology and social
networks, a massive amount of data related to comment texts has been generated
on web applications or social media platforms like Twitter. Due to this, people
have actively started proliferating general information and the information
related to political opinions, which becomes an important reason for analyzing
public reactions. Most researchers have used social media specifics or contents
to analyze and predict public opinion concerning political events. This
research proposes an analytical study using Israeli political Twitter data to
interpret public opinion towards the Palestinian-Israeli conflict. The
attitudes of ethnic groups and opinion leaders in the form of tweets are
analyzed using Machine Learning algorithms like Support Vector Classifier
(SVC), Decision Tree (DT), and Naive Bayes (NB). Finally, a comparative
analysis is done based on experimental results from different models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangwar_A/0/1/0/all/0/1"&gt;Amisha Gangwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_T/0/1/0/all/0/1"&gt;Tanvi Mehta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Probabilistic Time Series Forecasting using Augmented Recurrent Input for Dynamic Systems. (arXiv:2106.05848v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05848</id>
        <link href="http://arxiv.org/abs/2106.05848"/>
        <updated>2022-04-14T00:58:52.117Z</updated>
        <summary type="html"><![CDATA[The demand of probabilistic time series forecasting has been recently raised
in various dynamic system scenarios, for example, system identification and
prognostic and health management of machines. To this end, we combine the
advances in both deep generative models and state space model (SSM) to come up
with a novel, data-driven deep probabilistic sequence model. Specifically, we
follow the popular encoder-decoder generative structure to build the recurrent
neural networks (RNN) assisted variational sequence model on an augmented
recurrent input space, which could induce rich stochastic sequence dependency.
Besides, in order to alleviate the inconsistency issue of the posterior between
training and predicting as well as improving the mining of dynamic patterns, we
(i) propose using a lagged hybrid output as input for the posterior at next
time step, which brings training and predicting into alignment; and (ii)
further devise a generalized auto-regressive strategy that encodes all the
historical dependencies for the posterior. Thereafter, we first investigate the
methodological characteristics of the proposed deep probabilistic sequence
model on toy cases, and then comprehensively demonstrate the superiority of our
model against existing deep probabilistic SSM models through extensive
numerical experiments on eight system identification benchmarks from various
dynamic systems. Finally, we apply our sequence model to a real-world
centrifugal compressor forecasting problem, and again verify its outstanding
performance by quantifying the time series predictive distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haitao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Changjun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaomo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xudong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuhua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaofang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COIL: Constrained Optimization in Learned Latent Space -- Learning Representations for Valid Solutions. (arXiv:2202.02163v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.02163</id>
        <link href="http://arxiv.org/abs/2202.02163"/>
        <updated>2022-04-14T00:58:52.117Z</updated>
        <summary type="html"><![CDATA[Constrained optimization problems can be difficult because their search
spaces have properties not conducive to search, e.g., multimodality,
discontinuities, or deception. To address such difficulties, considerable
research has been performed on creating novel evolutionary algorithms or
specialized genetic operators. However, if the representation that defined the
search space could be altered such that it only permitted valid solutions that
satisfied the constraints, the task of finding the optimal would be made more
feasible without any need for specialized optimization algorithms. We propose
Constrained Optimization in Latent Space (COIL), which uses a VAE to generate a
learned latent representation from a dataset comprising samples from the valid
region of the search space according to a constraint, thus enabling the
optimizer to find the objective in the new space defined by the learned
representation. Preliminary experiments show promise: compared to an identical
GA using a standard representation that cannot meet the constraints or find fit
solutions, COIL with its learned latent representation can perfectly satisfy
different types of constraints while finding high-fitness solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bentley_P/0/1/0/all/0/1"&gt;Peter J Bentley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Soo Ling Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaier_A/0/1/0/all/0/1"&gt;Adam Gaier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1"&gt;Linh Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.02194</id>
        <link href="http://arxiv.org/abs/2203.02194"/>
        <updated>2022-04-14T00:58:52.117Z</updated>
        <summary type="html"><![CDATA[In some scenarios, classifier requires detecting out-of-distribution samples
far from its training data. With desirable characteristics, reconstruction
autoencoder-based methods deal with this problem by using input reconstruction
error as a metric of novelty vs. normality. We formulate the essence of such
approach as a quadruplet domain translation with an intrinsic bias to only
query for a proxy of conditional data uncertainty. Accordingly, an improvement
direction is formalized as maximumly compressing the autoencoder's latent space
while ensuring its reconstructive power for acting as a described domain
translator. From it, strategies are introduced including semantic
reconstruction, data certainty decomposition and normalized L2 distance to
substantially improve original methods, which together establish
state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of
CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method
works without any additional data, hard-to-implement structure, time-consuming
pipeline, and even harming the classification accuracy of known classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yibo Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforced MOOCs Concept Recommendation in Heterogeneous Information Networks. (arXiv:2203.11011v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.11011</id>
        <link href="http://arxiv.org/abs/2203.11011"/>
        <updated>2022-04-14T00:58:52.117Z</updated>
        <summary type="html"><![CDATA[Massive open online courses (MOOCs), which provide a large-scale interactive
participation and open access via the web, are becoming a modish way for online
and distance education. To help users have a better study experience, many MOOC
platforms have provided the services of recommending courses to users. However,
we argue that directly recommending a course to users will ignore the expertise
levels of different users. To fill this gap, this paper studies the problem of
concept recommendation in a more fine-grained view. We propose a novel
Heterogeneous Information Networks based Concept Recommender with Reinforcement
Learning (HinCRec-RL) incorporated for concept recommendation in MOOCs.
Specifically, we first formulate the concept recommendation in MOOCs as a
reinforcement learning problem to better model the dynamic interaction among
users and knowledge concepts. In addition, to mitigate the data sparsity issue
which also exists in many other recommendation tasks, we consider a
heterogeneous information network (HIN) among users, courses, videos and
concepts, to better learn the semantic representation of users. In particular,
we use the meta-paths on HIN to guide the propagation of users' preferences and
propose a heterogeneous graph attention network to represent the meta-paths. To
validate the effectiveness of our proposed approach, we conduct comprehensive
experiments on a real-world dataset from XuetangX, a popular MOOC platform from
China. The promising results show that our proposed approach can outperform
other baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1"&gt;Jibing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yao Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wenzheng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes. (arXiv:2204.06164v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.06164</id>
        <link href="http://arxiv.org/abs/2204.06164"/>
        <updated>2022-04-14T00:58:52.116Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a dynamic cascaded encoder Automatic Speech
Recognition (ASR) model, which unifies models for different deployment
scenarios. Moreover, the model can significantly reduce model size and power
consumption without loss of quality. Namely, with the dynamic cascaded encoder
model, we explore three techniques to maximally boost the performance of each
model size: 1) Use separate decoders for each sub-model while sharing the
encoders; 2) Use funnel-pooling to improve the encoder efficiency; 3) Balance
the size of causal and non-causal encoders to improve quality and fit
deployment constraints. Overall, the proposed large-medium model has 30%
smaller size and reduces power consumption by 33%, compared to the baseline
cascaded encoder model. The triple-size model that unifies the large, medium,
and small models achieves 37% total size reduction with minimal quality loss,
while substantially reducing the engineering efforts of having separate models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shaojin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1"&gt;Tara N. Sainath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1"&gt;Yanzhang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+David_R/0/1/0/all/0/1"&gt;Robert David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botros_R/0/1/0/all/0/1"&gt;Rami Botros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Panigrahy_R/0/1/0/all/0/1"&gt;Rina Panigrahy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Dongseong Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1"&gt;Ian McGraw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prabhavalkar_R/0/1/0/all/0/1"&gt;Rohit Prabhavalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1"&gt;Trevor Strohman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Height Optimisation for Cellular-Connected UAVs using Reinforcement Learning. (arXiv:2007.13695v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13695</id>
        <link href="http://arxiv.org/abs/2007.13695"/>
        <updated>2022-04-14T00:58:52.116Z</updated>
        <summary type="html"><![CDATA[Providing reliable connectivity to cellular-connected UAV can be very
challenging; their performance highly depends on the nature of the surrounding
environment, such as density and heights of the ground BSs. On the other hand,
tall buildings might block undesired interference signals from ground BSs,
thereby improving the connectivity between the UAVs and their serving BSs. To
address the connectivity of UAVs in such environments, this paper proposes a RL
algorithm to dynamically optimise the height of a UAV as it moves through the
environment, with the goal of increasing the throughput or spectrum efficiency
that it experiences. The proposed solution is evaluated in two settings: using
a series of generated environments where we vary the number of BS and building
densities, and in a scenario using real-world data obtained from an experiment
in Dublin, Ireland. Results show that our proposed RL-based solution improves
UAVs QoS by 6% to 41%, depending on the scenario. We also conclude that, when
flying at heights higher than the buildings, building density variation has no
impact on UAV QoS. On the other hand, BS density can negatively impact UAV QoS,
with higher numbers of BSs generating more interference and deteriorating UAV
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fonseca_E/0/1/0/all/0/1"&gt;Erika Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Galkin_B/0/1/0/all/0/1"&gt;Boris Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Amer_R/0/1/0/all/0/1"&gt;Ramy Amer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+DaSilva_L/0/1/0/all/0/1"&gt;Luiz A. DaSilva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dusparic_I/0/1/0/all/0/1"&gt;Ivana Dusparic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QU-net++: Image Quality Detection Framework for Segmentation of Medical 3D Image Stacks. (arXiv:2110.14181v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.14181</id>
        <link href="http://arxiv.org/abs/2110.14181"/>
        <updated>2022-04-14T00:58:52.116Z</updated>
        <summary type="html"><![CDATA[Automated segmentation of pathological regions of interest aids medical image
diagnostics and follow-up care. However, accurate pathological segmentations
require high quality of annotated data that can be both cost and time intensive
to generate. In this work, we propose an automated two-step method that detects
a minimal image subset required to train segmentation models by evaluating the
quality of medical images from 3D image stacks using a U-net++ model. These
images that represent a lack of quality training can then be annotated and used
to fully train a U-net-based segmentation model. The proposed QU-net++ model
detects this lack of quality training based on the disagreement in
segmentations produced from the final two output layers. The proposed model
isolates around 10% of the slices per 3D image stack and can scale across
imaging modalities to segment cysts in OCT images and ground glass opacity
(GGO) in lung CT images with Dice scores in the range 0.56-0.72. Thus, the
proposed method can be applied for cost effective multi-modal pathology
segmentation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Roychowdhury_S/0/1/0/all/0/1"&gt;Sohini Roychowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why KDAC? A general activation function for knowledge discovery. (arXiv:2111.13858v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.13858</id>
        <link href="http://arxiv.org/abs/2111.13858"/>
        <updated>2022-04-14T00:58:52.116Z</updated>
        <summary type="html"><![CDATA[Deep learning oriented named entity recognition (DNER) has gradually become
the paradigm of knowledge discovery, which greatly promotes domain
intelligence. However, the current activation function of DNER fails to treat
gradient vanishing, no negative output or non-differentiable existence, which
may impede knowledge exploration caused by the omission and incomplete
representation of latent semantics. To break through the dilemma, we present a
novel activation function termed KDAC. Detailly, KDAC is an aggregation
function with multiple conversion modes. The backbone of the activation region
is the interaction between exponent and linearity, and the both ends extend
through adaptive linear divergence, which surmounts the obstacle of gradient
vanishing and no negative output. Crucially, the non-differentiable points are
alerted and eliminated by an approximate smoothing algorithm. KDAC has a series
of brilliant properties, including nonlinear, stable near-linear transformation
and derivative, as well as dynamic style, etc. We perform experiments based on
BERT-BiLSTM-CNN-CRF model on six benchmark datasets containing different domain
knowledge, such as Weibo, Clinical, E-commerce, Resume, HAZOP and People's
daily. The evaluation results show that KDAC is advanced and effective, and can
provide more generalized activation to stimulate the performance of DNER. We
hope that KDAC can be exploited as a promising activation function to devote
itself to the construction of knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhenhua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1"&gt;Dong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haozhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fanglin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aspirations and Practice of Model Documentation: Moving the Needle with Nudging and Traceability. (arXiv:2204.06425v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2204.06425</id>
        <link href="http://arxiv.org/abs/2204.06425"/>
        <updated>2022-04-14T00:58:52.115Z</updated>
        <summary type="html"><![CDATA[Machine learning models have been widely developed, released, and adopted in
numerous applications. Meanwhile, the documentation practice for machine
learning models often falls short of established practices for traditional
software components, which impedes model accountability, inadvertently abets
inappropriate or misuse of models, and may trigger negative social impact.
Recently, model cards, a template for documenting machine learning models, have
attracted notable attention, but their impact on the practice of model
documentation is unclear. In this work, we examine publicly available model
cards and other similar documentation. Our analysis reveals a substantial gap
between the suggestions made in the original model card work and the content in
actual documentation. Motivated by this observation and literature on fields
such as software documentation, interaction design, and traceability, we
further propose a set of design guidelines that aim to support the
documentation practice for machine learning models including (1) the
collocation of documentation environment with the coding environment, (2)
nudging the consideration of model card sections during model development, and
(3) documentation derived from and traced to the source. We designed a
prototype tool named DocML following those guidelines to support model
development in computational notebooks. A lab study reveals the benefit of our
tool to shift the behavior of data scientists towards documentation quality and
accountability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1"&gt;Avinash Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coursey_A/0/1/0/all/0/1"&gt;Austin Coursey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Grace Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sixian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nahar_N/0/1/0/all/0/1"&gt;Nadia Nahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shurui Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kastner_C/0/1/0/all/0/1"&gt;Christian K&amp;#xe4;stner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jin L.C. Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling Evolutionary and Stationary User Preferences for Temporal Sets Prediction. (arXiv:2204.05490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.05490</id>
        <link href="http://arxiv.org/abs/2204.05490"/>
        <updated>2022-04-14T00:58:52.115Z</updated>
        <summary type="html"><![CDATA[Given a sequence of sets, where each set is associated with a timestamp and
contains an arbitrary number of elements, the task of temporal sets prediction
aims to predict the elements in the subsequent set. Previous studies for
temporal sets prediction mainly capture each user's evolutionary preference by
learning from his/her own sequence. Although insightful, we argue that: 1) the
collaborative signals latent in different users' sequences are essential but
have not been exploited; 2) users also tend to show stationary preferences
while existing methods fail to consider. To this end, we propose an integrated
learning framework to model both the evolutionary and the stationary
preferences of users for temporal sets prediction, which first constructs a
universal sequence by chronologically arranging all the user-set interactions,
and then learns on each user-set interaction. In particular, for each user-set
interaction, we first design an evolutionary user preference modelling
component to track the user's time-evolving preference and exploit the latent
collaborative signals among different users. This component maintains a memory
bank to store memories of the related user and elements, and continuously
updates their memories based on the currently encoded messages and the past
memories. Then, we devise a stationary user preference modelling module to
discover each user's personalized characteristics according to the historical
sequence, which adaptively aggregates the previously interacted elements from
dual perspectives with the guidance of the user's and elements' embeddings.
Finally, we develop a set-batch algorithm to improve the model efficiency,
which can create time-consistent batches in advance and achieve 3.5x training
speedups on average. Experiments on real-world datasets demonstrate the
effectiveness and good interpretability of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Le Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zihang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tongyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Leilei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bowen Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1"&gt;Weifeng Lv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AHP: Learning to Negative Sample for Hyperedge Prediction. (arXiv:2204.06353v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06353</id>
        <link href="http://arxiv.org/abs/2204.06353"/>
        <updated>2022-04-14T00:58:52.114Z</updated>
        <summary type="html"><![CDATA[Hypergraphs (i.e., sets of hyperedges) naturally represent group relations
(e.g., researchers co-authoring a paper and ingredients used together in a
recipe), each of which corresponds to a hyperedge (i.e., a subset of nodes).
Predicting future or missing hyperedges bears significant implication for many
applications (e.g., collaboration and recipe recommendation). What makes
hyperedge prediction particularly challenging is the vast number of
non-hyperedge subsets, which grows exponentially with the number of nodes.
Since it is prohibitive to use all of them as negative examples for model
training, it is inevitable to sample a very small portion of them, and to this
end, heuristic sampling schemes have been employed. However, trained models
suffer from poor generalization capability for examples of different natures.
In this paper, we propose AHP, an adversarial training-based
hyperedge-prediction method. It learns to sample negative examples without
relying on any heuristic schemes. Using six real hypergraphs, we show that AHP
generalizes better to negative examples of various natures. It yields up to
28.2% higher AUROC than best existing methods and often even outperforms its
variants with sampling schemes tailored to test sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1"&gt;Hyunjin Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungwoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chanyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kijung Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Augmentation with Reinforced Labeling for Weak Supervision. (arXiv:2204.06436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06436</id>
        <link href="http://arxiv.org/abs/2204.06436"/>
        <updated>2022-04-14T00:58:52.114Z</updated>
        <summary type="html"><![CDATA[Weak supervision (WS) is an alternative to the traditional supervised
learning to address the need for ground truth. Data programming is a practical
WS approach that allows programmatic labeling data samples using labeling
functions (LFs) instead of hand-labeling each data point. However, the existing
approach fails to fully exploit the domain knowledge encoded into LFs,
especially when the LFs' coverage is low. This is due to the common data
programming pipeline that neglects to utilize data features during the
generative process. This paper proposes a new approach called reinforced
labeling (RL). Given an unlabeled dataset and a set of LFs, RL augments the
LFs' outputs to cases not covered by LFs based on similarities among samples.
Thus, RL can lead to higher labeling coverage for training an end classifier.
The experiments on several domains (classification of YouTube comments, wine
quality, and weather prediction) result in considerable gains. The new approach
produces significant performance improvement, leading up to +21 points in
accuracy and +61 points in F1 scores compared to the state-of-the-art data
programming approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solmaz_G/0/1/0/all/0/1"&gt;G&amp;#xfc;rkan Solmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cirillo_F/0/1/0/all/0/1"&gt;Flavio Cirillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maresca_F/0/1/0/all/0/1"&gt;Fabio Maresca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Anagha Gode Anil Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clinical trial site matching with improved diversity using fair policy learning. (arXiv:2204.06501v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06501</id>
        <link href="http://arxiv.org/abs/2204.06501"/>
        <updated>2022-04-14T00:58:52.114Z</updated>
        <summary type="html"><![CDATA[The ongoing pandemic has highlighted the importance of reliable and efficient
clinical trials in healthcare. Trial sites, where the trials are conducted, are
chosen mainly based on feasibility in terms of medical expertise and access to
a large group of patients. More recently, the issue of diversity and inclusion
in clinical trials is gaining importance. Different patient groups may
experience the effects of a medical drug/ treatment differently and hence need
to be included in the clinical trials. These groups could be based on
ethnicity, co-morbidities, age, or economic factors. Thus, designing a method
for trial site selection that accounts for both feasibility and diversity is a
crucial and urgent goal. In this paper, we formulate this problem as a ranking
problem with fairness constraints. Using principles of fairness in machine
learning, we learn a model that maps a clinical trial description to a ranked
list of potential trial sites. Unlike existing fairness frameworks, the group
membership of each trial site is non-binary: each trial site may have access to
patients from multiple groups. We propose fairness criteria based on
demographic parity to address such a multi-group membership scenario. We test
our method on 480 real-world clinical trials and show that our model results in
a list of potential trial sites that provides access to a diverse set of
patients while also ensuing a high number of enrolled patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_R/0/1/0/all/0/1"&gt;Rakshith S Srinivasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodorou_B/0/1/0/all/0/1"&gt;Brandon Theodorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spaeder_J/0/1/0/all/0/1"&gt;Jeffrey Spaeder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_L/0/1/0/all/0/1"&gt;Lucas Glass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling the Knowledge of Romanian BERTs Using Multiple Teachers. (arXiv:2112.12650v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.12650</id>
        <link href="http://arxiv.org/abs/2112.12650"/>
        <updated>2022-04-14T00:58:52.114Z</updated>
        <summary type="html"><![CDATA[Running large-scale pre-trained language models in computationally
constrained environments remains a challenging problem yet to be addressed,
while transfer learning from these models has become prevalent in Natural
Language Processing tasks. Several solutions, including knowledge distillation,
network quantization, or network pruning have been previously proposed;
however, these approaches focus mostly on the English language, thus widening
the gap when considering low-resource languages. In this work, we introduce
three light and fast versions of distilled BERT models for the Romanian
language: Distil-BERT-base-ro, Distil-RoBERT-base, and
DistilMulti-BERT-base-ro. The first two models resulted from the individual
distillation of knowledge from two base versions of Romanian BERTs available in
literature, while the last one was obtained by distilling their ensemble. To
our knowledge, this is the first attempt to create publicly available Romanian
distilled BERT models, which were thoroughly evaluated on five tasks:
part-of-speech tagging, named entity recognition, sentiment analysis, semantic
textual similarity, and dialect identification. Our experimental results argue
that the three distilled models offer performance comparable to their teachers,
while being twice as fast on a GPU and ~35% smaller. In addition, we further
test the similarity between the predictions of our students versus their
teachers by measuring their label and probability loyalty, together with
regression loyalty - a new metric introduced in this work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1"&gt;Andrei-Marius Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catrina_D/0/1/0/all/0/1"&gt;Darius Catrina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1"&gt;Dumitru-Clementin Cercel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1"&gt;Mihai Dasc&amp;#x103;lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rebedea_T/0/1/0/all/0/1"&gt;Traian Rebedea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1"&gt;Vasile P&amp;#x103;i&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1"&gt;Dan Tufi&amp;#x15f;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimation of stellar atmospheric parameters from LAMOST DR8 low-resolution spectra with 20$\leq$SNR$<$30. (arXiv:2204.06301v1 [astro-ph.GA])]]></title>
        <id>http://arxiv.org/abs/2204.06301</id>
        <link href="http://arxiv.org/abs/2204.06301"/>
        <updated>2022-04-14T00:58:52.113Z</updated>
        <summary type="html"><![CDATA[The accuracy of the estimated stellar atmospheric parameter decreases
evidently with the decreasing of spectral signal-to-noise ratio (SNR) and there
are a huge amount of this kind observations, especially in case of SNR$<$30.
Therefore, it is helpful to improve the parameter estimation performance for
these spectra and this work studied the ($T_\texttt{eff}, \log~g$, [Fe/H])
estimation problem for LAMOST DR8 low-resolution spectra with 20$\leq$SNR$<$30.
We proposed a data-driven method based on machine learning techniques. Firstly,
this scheme detected stellar atmospheric parameter-sensitive features from
spectra by the Least Absolute Shrinkage and Selection Operator (LASSO),
rejected ineffective data components and irrelevant data. Secondly, a
Multi-layer Perceptron (MLP) method was used to estimate stellar atmospheric
parameters from the LASSO features. Finally, the performance of the LASSO-MLP
was evaluated by computing and analyzing the consistency between its estimation
and the reference from the APOGEE (Apache Point Observatory Galactic Evolution
Experiment) high-resolution spectra. Experiments show that the Mean Absolute
Errors (MAE) of $T_\texttt{eff}, \log~g$, [Fe/H] are reduced from the LASP
(137.6 K, 0.195 dex, 0.091 dex) to LASSO-MLP (84.32 K, 0.137 dex, 0.063 dex),
which indicate evident improvements on stellar atmospheric parameter
estimation. In addition, this work estimated the stellar atmospheric parameters
for 1,162,760 low-resolution spectra with 20$\leq$SNR$<$30 from LAMOST DR8
using LASSO-MLP, and released the estimation catalog, learned model,
experimental code, trained model, training data and test data for scientific
exploration and algorithm study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangru Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Si Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Liao_C/0/1/0/all/0/1"&gt;Caixiu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Du_B/0/1/0/all/0/1"&gt;Bing Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kong_X/0/1/0/all/0/1"&gt;X. Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Li_H/0/1/0/all/0/1"&gt;Haining Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomy and Perception for Space Mining. (arXiv:2109.12109v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.12109</id>
        <link href="http://arxiv.org/abs/2109.12109"/>
        <updated>2022-04-14T00:58:52.113Z</updated>
        <summary type="html"><![CDATA[Future Moon bases will likely be constructed using resources mined from the
surface of the Moon. The difficulty of maintaining a human workforce on the
Moon and communications lag with Earth means that mining will need to be
conducted using collaborative robots with a high degree of autonomy. In this
paper, we describe our solution for Phase 2 of the NASA Space Robotics
Challenge, which provided a simulated lunar environment in which teams were
tasked to develop software systems to achieve autonomous collaborative robots
for mining on the Moon. Our 3rd place and innovation award winning solution
shows how machine learning-enabled vision could alleviate major challenges
posed by the lunar environment towards autonomous space mining, chiefly the
lack of satellite positioning systems, hazardous terrain, and delicate robot
interactions. A robust multi-robot coordinator was also developed to achieve
long-term operation and effective collaboration between robots.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1"&gt;Ragav Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hammond_R/0/1/0/all/0/1"&gt;Ravi Hammond&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bockman_J/0/1/0/all/0/1"&gt;James Bockman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arthur_A/0/1/0/all/0/1"&gt;Alec Arthur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smart_B/0/1/0/all/0/1"&gt;Brandon Smart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Craggs_D/0/1/0/all/0/1"&gt;Dustin Craggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1"&gt;Anh-Dzung Doan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rowntree_T/0/1/0/all/0/1"&gt;Thomas Rowntree&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutz_E/0/1/0/all/0/1"&gt;Elijah Schutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orenstein_A/0/1/0/all/0/1"&gt;Adrian Orenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1"&gt;Andy Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1"&gt;Tat-Jun Chin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Ian Reid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Statistical Learning View of Simple Kriging. (arXiv:2202.07365v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07365</id>
        <link href="http://arxiv.org/abs/2202.07365"/>
        <updated>2022-04-14T00:58:52.113Z</updated>
        <summary type="html"><![CDATA[In the Big Data era, with the ubiquity of geolocation sensors in particular,
massive datasets exhibiting a possibly complex spatial dependence structure are
becoming increasingly available. In this context, the standard probabilistic
theory of statistical learning does not apply directly and guarantees of the
generalization capacity of predictive rules learned from such data are left to
establish. We analyze here the simple Kriging task, the flagship problem in
Geostatistics: the values of a square integrable random field $X=\{X_s\}_{s\in
S}$, $S\subset \mathbb{R}^2$, with unknown covariance structure are to be
predicted with minimum quadratic risk, based upon observing a single
realization of the spatial process at a finite number of locations $s_1,\;
\ldots,\; s_n$ in $S$. Despite the connection of this minimization problem with
kernel ridge regression, establishing the generalization capacity of empirical
risk minimizers is far from straightforward, due to the non i.i.d. nature of
the spatial data $X_{s_1},\; \ldots,\; X_{s_n}$ involved. In this article,
nonasymptotic bounds of order $O_{\mathbb{P}}(1/n)$ are proved for the excess
risk of a plug-in predictive rule mimicking the true minimizer in the case of
isotropic stationary Gaussian processes observed at locations forming a regular
grid. These theoretical results, as well as the role played by the technical
conditions required to establish them, are illustrated by various numerical
experiments and hopefully pave the way for further developments in statistical
learning based on spatial data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Siviero_E/0/1/0/all/0/1"&gt;Emilia Siviero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chautru_E/0/1/0/all/0/1"&gt;Emilie Chautru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Clemencon_S/0/1/0/all/0/1"&gt;Stephan Cl&amp;#xe9;men&amp;#xe7;on&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Error Bounds for Multiclass Sparse Linear Classifiers. (arXiv:2204.06264v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2204.06264</id>
        <link href="http://arxiv.org/abs/2204.06264"/>
        <updated>2022-04-14T00:58:52.112Z</updated>
        <summary type="html"><![CDATA[We consider high-dimensional multiclass classification by sparse multinomial
logistic regression. Unlike binary classification, in the multiclass setup one
can think about an entire spectrum of possible notions of sparsity associated
with different structural assumptions on the regression coefficients matrix. We
propose a computationally feasible feature selection procedure based on
penalized maximum likelihood with convex penalties capturing a specific type of
sparsity at hand. In particular, we consider global sparsity, double row-wise
sparsity, and low-rank sparsity, and show that with the properly chosen tuning
parameters the derived plug-in classifiers attain the minimax generalization
error bounds (in terms of misclassification excess risk) within the
corresponding classes of multiclass sparse linear classifiers. The developed
approach is general and can be adapted to other types of sparsity as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Levy_T/0/1/0/all/0/1"&gt;Tomer Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Abramovich_F/0/1/0/all/0/1"&gt;Felix Abramovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Autoencoders (DAE). (arXiv:2202.09926v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.09926</id>
        <link href="http://arxiv.org/abs/2202.09926"/>
        <updated>2022-04-14T00:58:52.112Z</updated>
        <summary type="html"><![CDATA[Noting the importance of factorizing (or disentangling) the latent space, we
propose a novel, non-probabilistic disentangling framework for autoencoders,
based on the principles of symmetry transformations in group-theory. To the
best of our knowledge, this is the first deterministic model that is aiming to
achieve disentanglement based on autoencoders without regularizers. The
proposed model is compared to seven state-of-the-art generative models based on
autoencoders and evaluated based on five supervised disentanglement metrics.
The experimental results show that the proposed model can have better
disentanglement when variances of each features are different. We believe that
this model leads to a new field for disentanglement learning based on
autoencoders without regularizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1"&gt;Jaehoon Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiyagalingam_J/0/1/0/all/0/1"&gt;Jeyan Thiyagalingam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning on Graph: A Survey. (arXiv:2204.06127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06127</id>
        <link href="http://arxiv.org/abs/2204.06127"/>
        <updated>2022-04-14T00:58:52.109Z</updated>
        <summary type="html"><![CDATA[Graph mining tasks arise from many different application domains, ranging
from social networks, transportation, E-commerce, etc., which have been
receiving great attention from the theoretical and algorithm design communities
in recent years, and there has been some pioneering work using the hotly
researched reinforcement learning (RL) techniques to address graph data mining
tasks. However, these graph mining algorithms and RL models are dispersed in
different research areas, which makes it hard to compare different algorithms
with each other. In this survey, we provide a comprehensive overview of RL
models and graph mining and generalize these algorithms to Graph Reinforcement
Learning (GRL) as a unified formulation. We further discuss the applications of
GRL methods across various domains and summarize the method description,
open-source codes, and benchmark datasets of GRL methods. Finally, we propose
possible important directions and challenges to be solved in the future. This
is the latest work on a comprehensive survey of GRL literature, and this work
provides a global view for researchers as well as a learning resource for
researchers outside the domain. In addition, we create an online open-source
for both interested researchers who want to enter this rapidly developing
domain and experts who would like to compare GRL methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mingshuo_N/0/1/0/all/0/1"&gt;Nie Mingshuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dongming_C/0/1/0/all/0/1"&gt;Chen Dongming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dongqi_W/0/1/0/all/0/1"&gt;Wang Dongqi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Production federated keyword spotting via distillation, filtering, and joint federated-centralized training. (arXiv:2204.06322v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.06322</id>
        <link href="http://arxiv.org/abs/2204.06322"/>
        <updated>2022-04-14T00:58:52.109Z</updated>
        <summary type="html"><![CDATA[We trained a keyword spotting model using federated learning on real user
devices and observed significant improvements when the model was deployed for
inference on phones. To compensate for data domains that are missing from
on-device training caches, we employed joint federated-centralized training.
And to learn in the absence of curated labels on-device, we formulated a
confidence filtering strategy based on user-feedback signals for federated
distillation. These techniques created models that significantly improved
quality metrics in offline evaluations and user-experience metrics in live A/B
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hard_A/0/1/0/all/0/1"&gt;Andrew Hard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Partridge_K/0/1/0/all/0/1"&gt;Kurt Partridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1"&gt;Neng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Augenstein_S/0/1/0/all/0/1"&gt;Sean Augenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1"&gt;Aishanee Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyun Jin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_A/0/1/0/all/0/1"&gt;Alex Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ng_S/0/1/0/all/0/1"&gt;Sara Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_J/0/1/0/all/0/1"&gt;Jessica Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1"&gt;Ignacio Lopez Moreno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mathews_R/0/1/0/all/0/1"&gt;Rajiv Mathews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;oise Beaufays&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling and Analysis of Intermittent Federated Learning Over Cellular-Connected UAV Networks. (arXiv:2110.07077v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.07077</id>
        <link href="http://arxiv.org/abs/2110.07077"/>
        <updated>2022-04-14T00:58:52.109Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a promising distributed learning technique
particularly suitable for wireless learning scenarios since it can accomplish a
learning task without raw data transportation so as to preserve data privacy
and lower network resource consumption. However, current works on FL over
wireless networks do not profoundly study the fundamental performance of FL
over wireless networks that suffers from communication outage due to channel
impairment and network interference. To accurately exploit the performance of
FL over wireless networks, this paper proposes a novel intermittent FL model
over a cellular-connected unmanned aerial vehicle (UAV) network, which
characterizes communication outage from UAV (clients) to their server and data
heterogeneity among the datasets at UAVs. We propose an analytically tractable
framework to derive the uplink outage probability and use it to devise a
simulation-based approach so as to evaluate the performance of the proposed
intermittent FL model. Our findings reveal how the intermittent FL model is
impacted by uplink communication outage and UAV deployment. Extensive numerical
simulations are provided to show the consistency between the simulated and
analytical performances of the proposed intermittent FL model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chun-Hung Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Di-Chun Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gau_R/0/1/0/all/0/1"&gt;Rung-Hung Gau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning. (arXiv:2204.05562v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.05562</id>
        <link href="http://arxiv.org/abs/2204.05562"/>
        <updated>2022-04-14T00:58:52.109Z</updated>
        <summary type="html"><![CDATA[The incredible development of federated learning (FL) has benefited various
tasks in the domains of computer vision and natural language processing, and
the existing frameworks such as TFF and FATE has made the deployment easy in
real-world applications. However, federated graph learning (FGL), even though
graph data are prevalent, has not been well supported due to its unique
characteristics and requirements. The lack of FGL-related framework increases
the efforts for accomplishing reproducible research and deploying in real-world
applications. Motivated by such strong demand, in this paper, we first discuss
the challenges in creating an easy-to-use FGL package and accordingly present
our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified
view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo
and ModelZoo for out-of-the-box FGL capability; (3) an efficient model
auto-tuning component; and (4) off-the-shelf privacy attack and defense
abilities. We validate the effectiveness of FS-G by conducting extensive
experiments, which simultaneously gains many valuable insights about FGL for
the community. Moreover, we employ FS-G to serve the FGL application in
real-world E-commerce scenarios, where the attained improvements indicate great
potential business benefits. We publicly release FS-G, as submodules of
FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's
research and enable broad applications that would otherwise be infeasible due
to the lack of a dedicated package.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_W/0/1/0/all/0/1"&gt;Weirui Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuexiang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Liuyi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1"&gt;Bolin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Baseline Computation for Attribution Methods Based on Interpolated Inputs. (arXiv:2204.06120v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.06120</id>
        <link href="http://arxiv.org/abs/2204.06120"/>
        <updated>2022-04-14T00:58:52.108Z</updated>
        <summary type="html"><![CDATA[We discuss a way to find a well behaved baseline for attribution methods that
work by feeding a neural network with a sequence of interpolated inputs between
two given inputs. Then, we test it with our novel Riemann-Stieltjes Integrated
Gradient-weighted Class Activation Mapping (RSI-Grad-CAM) attribution method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lerma_M/0/1/0/all/0/1"&gt;Miguel Lerma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1"&gt;Mirtha Lucas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity. (arXiv:2204.06242v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06242</id>
        <link href="http://arxiv.org/abs/2204.06242"/>
        <updated>2022-04-14T00:58:52.108Z</updated>
        <summary type="html"><![CDATA[Many real-world systems are described not only by data from a single source
but via multiple data views. For example, in genomic medicine, a patient can be
described by data from different molecular layers. This raises the need for
multi-view models that are able to disentangle variation within and across data
views in an interpretable manner. Latent variable models with structured
sparsity are a commonly used tool to address this modeling task but
interpretability is cumbersome since it requires a direct inspection and
interpretation of each factor via a specialized domain expert. Here, we propose
MuVI, a novel approach for domain-informed multi-view latent variable models,
facilitating the analysis of multi-view data in an inherently explainable
manner. We demonstrate that our model (i) is able to integrate noisy domain
expertise in form of feature sets, (ii) is robust to noise in the encoded
domain knowledge, (iii) results in identifiable factors and (iv) is able to
infer interpretable and biologically meaningful axes of variation in a
real-world multi-view dataset of cancer patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qoku_A/0/1/0/all/0/1"&gt;Arber Qoku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buettner_F/0/1/0/all/0/1"&gt;Florian Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A quantum generative model for multi-dimensional time series using Hamiltonian learning. (arXiv:2204.06150v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2204.06150</id>
        <link href="http://arxiv.org/abs/2204.06150"/>
        <updated>2022-04-14T00:58:52.104Z</updated>
        <summary type="html"><![CDATA[Synthetic data generation has proven to be a promising solution for
addressing data availability issues in various domains. Even more challenging
is the generation of synthetic time series data, where one has to preserve
temporal dynamics, i.e., the generated time series must respect the original
relationships between variables across time. Recently proposed techniques such
as generative adversarial networks (GANs) and quantum-GANs lack the ability to
attend to the time series specific temporal correlations adequately. We propose
using the inherent nature of quantum computers to simulate quantum dynamics as
a technique to encode such features. We start by assuming that a given time
series can be generated by a quantum process, after which we proceed to learn
that quantum process using quantum machine learning. We then use the learned
model to generate out-of-sample time series and show that it captures unique
and complex features of the learned time series. We also study the class of
time series that can be modeled using this technique. Finally, we
experimentally demonstrate the proposed algorithm on an 11-qubit trapped-ion
quantum machine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Horowitz_H/0/1/0/all/0/1"&gt;Haim Horowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rao_P/0/1/0/all/0/1"&gt;Pooja Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Radha_S/0/1/0/all/0/1"&gt;Santosh Kumar Radha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Organization of a Latent Space structure in VAE/GAN trained by navigation data. (arXiv:2102.01852v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01852</id>
        <link href="http://arxiv.org/abs/2102.01852"/>
        <updated>2022-04-14T00:58:52.104Z</updated>
        <summary type="html"><![CDATA[We present a novel artificial cognitive mapping system using generative deep
neural networks, called variational autoencoder/generative adversarial network
(VAE/GAN), which can map input images to latent vectors and generate temporal
sequences internally. The results show that the distance of the predicted image
is reflected in the distance of the corresponding latent vector after training.
This indicates that the latent space is self-organized to reflect the proximity
structure of the dataset and may provide a mechanism through which many aspects
of cognition are spatially represented. The present study allows the network to
internally generate temporal sequences that are analogous to the hippocampal
replay/pre-play ability, where VAE produces only near-accurate replays of past
experiences, but by introducing GANs, the generated sequences are coupled with
instability and novelty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kojima_H/0/1/0/all/0/1"&gt;Hiroki Kojima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikegami_T/0/1/0/all/0/1"&gt;Takashi Ikegami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations. (arXiv:2204.06508v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.06508</id>
        <link href="http://arxiv.org/abs/2204.06508"/>
        <updated>2022-04-14T00:58:52.093Z</updated>
        <summary type="html"><![CDATA[Despite recent improvements in abstractive summarization, most current
approaches generate summaries that are not factually consistent with the source
document, severely restricting their trust and usage in real-world
applications. Recent works have shown promising improvements in factuality
error identification using text or dependency arc entailments; however, they do
not consider the entire semantic graph simultaneously. To this end, we propose
FactGraph, a method that decomposes the document and the summary into
structured meaning representations (MR), which are more suitable for factuality
evaluation. MRs describe core semantic concepts and their relations,
aggregating the main content in both document and summary in a canonical form,
and reducing data sparsity. FactGraph encodes such graphs using a graph encoder
augmented with structure-aware adapters to capture interactions among the
concepts based on the graph connectivity, along with text representations using
an adapter-based text encoder. Experiments on different benchmarks for
evaluating factuality show that FactGraph outperforms previous approaches by up
to 15%. Furthermore, FactGraph improves performance on identifying content
verifiability errors and better captures subsentence-level factual
inconsistencies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1"&gt;Leonardo F. R. Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1"&gt;Markus Dreyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Opportunities of Edge AI for Next-Generation Implantable BMIs. (arXiv:2204.02362v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.02362</id>
        <link href="http://arxiv.org/abs/2204.02362"/>
        <updated>2022-04-14T00:58:52.093Z</updated>
        <summary type="html"><![CDATA[Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaeri_M/0/1/0/all/0/1"&gt;MohammadAli Shaeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afzal_A/0/1/0/all/0/1"&gt;Arshia Afzal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoaran_M/0/1/0/all/0/1"&gt;Mahsa Shoaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the dynamics of credit history and social interaction features, and their impact on creditworthiness assessment performance. (arXiv:2204.06122v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2204.06122</id>
        <link href="http://arxiv.org/abs/2204.06122"/>
        <updated>2022-04-14T00:58:52.092Z</updated>
        <summary type="html"><![CDATA[For more than a half-century, credit risk management has used credit scoring
models in each of its well-defined stages to manage credit risk. Application
scoring is used to decide whether to grant a credit or not, while behavioral
scoring is used mainly for portfolio management and to take preventive actions
in case of default signals. In both cases, network data has recently been shown
to be valuable to increase the predictive power of these models, especially
when the borrower's historical data is scarce or not available. This study aims
to understand the creditworthiness assessment performance dynamics and how it
is influenced by the credit history, repayment behavior, and social network
features. To accomplish this, we introduced a machine learning classification
framework to analyze 97.000 individuals and companies from the moment they
obtained their first loan to 12 months afterward. Our novel and massive dataset
allow us to characterize each borrower according to their credit behavior, and
social and economic relationships. Our research shows that borrowers' history
increases performance at a decreasing rate during the first six months and then
stabilizes. The most notable effect on perfomance of social networks features
occurs at loan application; in personal scoring, this effect prevails a few
months, while in business scoring adds value throughout the study period. These
findings are of great value to improve credit risk management and optimize the
use of traditional information and alternative data sources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munoz_Cancino_R/0/1/0/all/0/1"&gt;Ricardo Mu&amp;#xf1;oz-Cancino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1"&gt;Cristi&amp;#xe1;n Bravo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rios_S/0/1/0/all/0/1"&gt;Sebasti&amp;#xe1;n A. R&amp;#xed;os&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grana_M/0/1/0/all/0/1"&gt;Manuel Gra&amp;#xf1;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Receptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation. (arXiv:2204.06439v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2204.06439</id>
        <link href="http://arxiv.org/abs/2204.06439"/>
        <updated>2022-04-14T00:58:52.092Z</updated>
        <summary type="html"><![CDATA[Speech dereverberation is often an important requirement in robust speech
processing tasks. Supervised deep learning (DL) models give state-of-the-art
performance for single-channel speech dereverberation. Temporal convolutional
networks (TCNs) are commonly used for sequence modelling in speech enhancement
tasks. A feature of TCNs is that they have a receptive field (RF) dependant on
the specific model configuration which determines the number of input frames
that can be observed to produce an individual output frame. It has been shown
that TCNs are capable of performing dereverberation of simulated speech data,
however a thorough analysis, especially with focus on the RF is yet lacking in
the literature. This paper analyses dereverberation performance depending on
the model size and the RF of TCNs. Experiments using the WHAMR corpus which is
extended to include room impulse responses (RIRs) with larger T60 values
demonstrate that a larger RF can have significant improvement in performance
when training smaller TCN models. It is also demonstrated that TCNs benefit
from a wider RF when dereverberating RIRs with larger RT60 values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ravenscroft_W/0/1/0/all/0/1"&gt;William Ravenscroft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1"&gt;Stefan Goetze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1"&gt;Thomas Hain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Diverse Solutions in Deep Reinforcement Learning by Maximizing State-Action-Based Mutual Information. (arXiv:2103.07084v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07084</id>
        <link href="http://arxiv.org/abs/2103.07084"/>
        <updated>2022-04-14T00:58:52.091Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning algorithms are typically limited to learning a single
solution for a specified task, even though diverse solutions often exist.
Recent studies showed that learning a set of diverse solutions is beneficial
because diversity enables robust few-shot adaptation. Although existing methods
learn diverse solutions by using the mutual information as unsupervised
rewards, such an approach often suffers from the bias of the gradient estimator
induced by value function approximation. In this study, we propose a novel
method that can learn diverse solutions without suffering the bias problem. In
our method, a policy conditioned on a continuous or discrete latent variable is
trained by directly maximizing the variational lower bound of the mutual
information, instead of using the mutual information as unsupervised rewards as
in previous studies. Through extensive experiments on robot locomotion tasks,
we demonstrate that the proposed method successfully learns an infinite set of
diverse solutions by learning continuous latent variables, which is more
challenging than learning a finite number of solutions. Subsequently, we show
that our method enables more effective few-shot adaptation compared with
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Osa_T/0/1/0/all/0/1"&gt;Takayuki Osa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tangkaratt_V/0/1/0/all/0/1"&gt;Voot Tangkaratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A pipeline and comparative study of 12 machine learning models for text classification. (arXiv:2204.06518v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.06518</id>
        <link href="http://arxiv.org/abs/2204.06518"/>
        <updated>2022-04-14T00:58:52.090Z</updated>
        <summary type="html"><![CDATA[Text-based communication is highly favoured as a communication method,
especially in business environments. As a result, it is often abused by sending
malicious messages, e.g., spam emails, to deceive users into relaying personal
information, including online accounts credentials or banking details. For this
reason, many machine learning methods for text classification have been
proposed and incorporated into the services of most email providers. However,
optimising text classification algorithms and finding the right tradeoff on
their aggressiveness is still a major research problem.

We present an updated survey of 12 machine learning text classifiers applied
to a public spam corpus. A new pipeline is proposed to optimise hyperparameter
selection and improve the models' performance by applying specific methods
(based on natural language processing) in the preprocessing stage.

Our study aims to provide a new methodology to investigate and optimise the
effect of different feature sizes and hyperparameters in machine learning
classifiers that are widely used in text classification problems. The
classifiers are tested and evaluated on different metrics including F-score
(accuracy), precision, recall, and run time. By analysing all these aspects, we
show how the proposed pipeline can be used to achieve a good accuracy towards
spam filtering on the Enron dataset, a widely used public email corpus.
Statistical tests and explainability techniques are applied to provide a robust
analysis of the proposed pipeline and interpret the classification outcomes of
the 12 machine learning models, also identifying words that drive the
classification results. Our analysis shows that it is possible to identify an
effective machine learning model to classify the Enron dataset with an F-score
of 94%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Occhipinti_A/0/1/0/all/0/1"&gt;Annalisa Occhipinti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_L/0/1/0/all/0/1"&gt;Louis Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angione_C/0/1/0/all/0/1"&gt;Claudio Angione&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Practical Robustness Analysis for DNNs based on PAC-Model Learning. (arXiv:2101.10102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10102</id>
        <link href="http://arxiv.org/abs/2101.10102"/>
        <updated>2022-04-14T00:58:52.090Z</updated>
        <summary type="html"><![CDATA[To analyse local robustness properties of deep neural networks (DNNs), we
present a practical framework from a model learning perspective. Based on
black-box model learning with scenario optimisation, we abstract the local
behaviour of a DNN via an affine model with the probably approximately correct
(PAC) guarantee. From the learned model, we can infer the corresponding
PAC-model robustness property. The innovation of our work is the integration of
model learning into PAC robustness analysis: that is, we construct a PAC
guarantee on the model level instead of sample distribution, which induces a
more faithful and accurate robustness evaluation. This is in contrast to
existing statistical methods without model learning. We implement our method in
a prototypical tool named DeepPAC. As a black-box method, DeepPAC is scalable
and efficient, especially when DNNs have complex structures or high-dimensional
inputs. We extensively evaluate DeepPAC, with 4 baselines (using formal
verification, statistical methods, testing and adversarial attack) and 20 DNN
models across 3 datasets, including MNIST, CIFAR-10, and ImageNet. It is shown
that DeepPAC outperforms the state-of-the-art statistical method PROVERO, and
it achieves more practical robustness analysis than the formal verification
tool ERAN. Also, its results are consistent with existing DNN testing work like
DeepGini.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Renjue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pengfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Cheng-Chao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Youcheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1"&gt;Bai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks. (arXiv:2111.05955v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.05955</id>
        <link href="http://arxiv.org/abs/2111.05955"/>
        <updated>2022-04-14T00:58:52.090Z</updated>
        <summary type="html"><![CDATA[Spiking neural networks (SNNs) have become an interesting alternative to
conventional artificial neural networks (ANN) thanks to their temporal
processing capabilities and energy efficient implementations in neuromorphic
hardware. However the challenges involved in training SNNs have limited their
performance in terms of accuracy and thus their applications. Improving
learning algorithms and neural architectures for a more accurate feature
extraction is therefore one of the current priorities in SNN research. In this
paper we present a study on the key components of modern spiking architectures.
We empirically compare different techniques in image classification datasets
taken from the best performing networks. We design a spiking version of the
successful residual network architecture and provide an in-depth study on the
possible implementations of spiking residual connections. Our results provide a
state of the art guide to SNN design, which allows to make informed choices
when trying to build the optimal visual feature extractor. Finally, our network
outperforms previous SNN architectures in CIFAR-10 (94.14%) and CIFAR-100
(74.65%) datasets and matches the state of the art in DVS-CIFAR10 (72.98%),
with less parameters than the previous state of the art and without the need
for ANN-SNN conversion. Code available at
https://github.com/VicenteAlex/Spiking_ResNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_Sola_A/0/1/0/all/0/1"&gt;Alex Vicente-Sola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manna_D/0/1/0/all/0/1"&gt;Davide L. Manna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirkland_P/0/1/0/all/0/1"&gt;Paul Kirkland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caterina_G/0/1/0/all/0/1"&gt;Gaetano Di Caterina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bihl_T/0/1/0/all/0/1"&gt;Trevor Bihl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inspection-L: A Self-Supervised GNN-Based Money Laundering Detection System for Bitcoin. (arXiv:2203.10465v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.10465</id>
        <link href="http://arxiv.org/abs/2203.10465"/>
        <updated>2022-04-14T00:58:52.090Z</updated>
        <summary type="html"><![CDATA[Criminals have become increasingly experienced in using cryptocurrencies,
such as Bitcoin, for money laundering. The use of cryptocurrencies can hide
criminal identities and transfer hundreds of millions of dollars of dirty funds
through their criminal digital wallets. However, this is considered a paradox
because cryptocurrencies are gold mines for open-source intelligence, allowing
law enforcement agencies to have more power in conducting forensic analyses.
This paper proposed Inspection-L, a graph neural network (GNN) framework based
on self-supervised Deep Graph Infomax (DGI), with supervised learning
algorithms, namely Random Forest (RF) to detect illicit transactions for AML.
To the best of our knowledge, our proposal is the first of applying
self-supervised GNNs to the problem of AML in Bitcoin. The proposed method has
been evaluated on the Elliptic dataset and shows that our approach outperforms
the baseline in terms of key classification metrics, which demonstrates the
potential of self-supervised GNN in cryptocurrency illicit transaction
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_W/0/1/0/all/0/1"&gt;Wai Weng Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Layeghy_S/0/1/0/all/0/1"&gt;Siamak Layeghy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1"&gt;Marius Portmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meaningful machine learning models and machine-learned pharmacophores from fragment screening campaigns. (arXiv:2204.06348v1 [q-bio.BM])]]></title>
        <id>http://arxiv.org/abs/2204.06348</id>
        <link href="http://arxiv.org/abs/2204.06348"/>
        <updated>2022-04-14T00:58:52.089Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) is widely used in drug discovery to train models that
predict protein-ligand binding. These models are of great value to medicinal
chemists, in particular if they provide case-specific insight into the physical
interactions that drive the binding process. In this study we derive ML models
from over 50 fragment-screening campaigns to introduce two important elements
that we believe are absent in most -- if not all -- ML studies of this type
reported to date: First, alongside the observed hits we use to train our
models, we incorporate true misses and show that these experimentally validated
negative data are of significant importance to the quality of the derived
models. Second, we provide a physically interpretable and verifiable
representation of what the ML model considers important for successful binding.
This representation is derived from a straightforward attribution procedure
that explains the prediction in terms of the (inter-)action of chemical
environments. Critically, we validate the attribution outcome on a large scale
against prior annotations made independently by expert molecular modellers. We
find good agreement between the key molecular substructures proposed by the ML
model and those assigned manually, even when the model's performance in
discriminating hits from misses is far from perfect. By projecting the
attribution onto predefined interaction prototypes (pharmacophores), we show
that ML allows us to formulate simple rules for what drives fragment binding
against a target automatically from screening data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Poelking_C/0/1/0/all/0/1"&gt;Carl Poelking&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chessari_G/0/1/0/all/0/1"&gt;Gianni Chessari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Murray_C/0/1/0/all/0/1"&gt;Christopher W. Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hall_R/0/1/0/all/0/1"&gt;Richard J. Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Colwell_L/0/1/0/all/0/1"&gt;Lucy Colwell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Verdonk_M/0/1/0/all/0/1"&gt;Marcel Verdonk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online greedy identification of linear dynamical systems. (arXiv:2204.06375v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06375</id>
        <link href="http://arxiv.org/abs/2204.06375"/>
        <updated>2022-04-14T00:58:52.089Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of exploration in an unknown environment. For
linear dynamical systems, we use an experimental design framework and introduce
an online greedy policy where the control maximizes the information of the next
step. In a setting with a limited number of experimental trials, our algorithm
has low complexity and shows experimentally competitive performances compared
to more elaborate gradient-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Blanke_M/0/1/0/all/0/1"&gt;Matthieu Blanke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lelarge_M/0/1/0/all/0/1"&gt;Marc Lelarge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convex-Concave Min-Max Stackelberg Games. (arXiv:2110.05192v4 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.05192</id>
        <link href="http://arxiv.org/abs/2110.05192"/>
        <updated>2022-04-14T00:58:52.089Z</updated>
        <summary type="html"><![CDATA[Min-max optimization problems (i.e., min-max games) have been attracting a
great deal of attention because of their applicability to a wide range of
machine learning problems. Although significant progress has been made
recently, the literature to date has focused on games with independent strategy
sets; little is known about solving games with dependent strategy sets, which
can be characterized as min-max Stackelberg games. We introduce two first-order
methods that solve a large class of convex-concave min-max Stackelberg games,
and show that our methods converge in polynomial time. Min-max Stackelberg
games were first studied by Wald, under the posthumous name of Wald's maximin
model, a variant of which is the main paradigm used in robust optimization,
which means that our methods can likewise solve many convex robust optimization
problems. We observe that the computation of competitive equilibria in Fisher
markets also comprises a min-max Stackelberg game. Further, we demonstrate the
efficacy and efficiency of our algorithms in practice by computing competitive
equilibria in Fisher markets with varying utility structures. Our experiments
suggest potential ways to extend our theoretical results, by demonstrating how
different smoothness properties can affect the convergence rate of our
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goktas_D/0/1/0/all/0/1"&gt;Denizalp Goktas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenwald_A/0/1/0/all/0/1"&gt;Amy Greenwald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Neural Network Augmented Physics-based Models for Nonlinear Filtering. (arXiv:2204.06471v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06471</id>
        <link href="http://arxiv.org/abs/2204.06471"/>
        <updated>2022-04-14T00:58:52.041Z</updated>
        <summary type="html"><![CDATA[In this paper we present a hybrid neural network augmented physics-based
modeling (APBM) framework for Bayesian nonlinear latent space estimation. The
proposed APBM strategy allows for model adaptation when new operation
conditions come into play or the physics-based model is insufficient (or
incomplete) to properly describe the latent phenomenon. One advantage of the
APBMs and our estimation procedure is the capability of maintaining the
physical interpretability of estimated states. Furthermore, we propose a
constraint filtering approach to control the neural network contributions to
the overall model. We also exploit assumed density filtering techniques and
cubature integration rules to present a flexible estimation strategy that can
easily deal with nonlinear models and high-dimensional latent spaces. Finally,
we demonstrate the efficacy of our methodology by leveraging a target tracking
scenario with nonlinear and incomplete measurement and acceleration models,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1"&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demirkaya_A/0/1/0/all/0/1"&gt;Ahmet Demirkaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunik_J/0/1/0/all/0/1"&gt;Jind&amp;#x159;ich Dun&amp;#xed;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Straka_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Straka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1"&gt;Deniz Erdo&amp;#x11f;mu&amp;#x15f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Closas_P/0/1/0/all/0/1"&gt;Pau Closas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overparameterized Linear Regression under Adversarial Attacks. (arXiv:2204.06274v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06274</id>
        <link href="http://arxiv.org/abs/2204.06274"/>
        <updated>2022-04-14T00:58:52.039Z</updated>
        <summary type="html"><![CDATA[As machine learning models start to be used in critical applications, their
vulnerabilities and brittleness become a pressing concern. Adversarial attacks
are a popular framework for studying these vulnerabilities. In this work, we
study the error of linear regression in the face of adversarial attacks. We
provide bounds of the error in terms of the traditional risk and the parameter
norm and show how these bounds can be leveraged and make it possible to use
analysis from non-adversarial setups to study the adversarial risk. The
usefulness of these results is illustrated by shedding light on whether or not
overparameterized linear models can be adversarially robust. We show that
adding features to linear models might be either a source of additional
robustness or brittleness. We show that these differences appear due to scaling
and how the $\ell_1$ and $\ell_2$ norms of random projections concentrate. We
also show how the reformulation we propose allows for solving adversarial
training as a convex optimization problem. This is then used as a tool to study
how adversarial training and other regularization methods might affect the
robustness of the estimated models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Ant&amp;#xf4;nio H. Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safer Autonomous Driving in a Stochastic, Partially-Observable Environment by Hierarchical Contingency Planning. (arXiv:2204.06509v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06509</id>
        <link href="http://arxiv.org/abs/2204.06509"/>
        <updated>2022-04-14T00:58:52.037Z</updated>
        <summary type="html"><![CDATA[When learning to act in a stochastic, partially observable environment, an
intelligent agent should be prepared to anticipate a change in its belief of
the environment state, and be capable of adapting its actions on-the-fly to
changing conditions. As humans, we are able to form contingency plans when
learning a task with the explicit aim of being able to correct errors in the
initial control, and hence prove useful if ever there is a sudden change in our
perception of the environment which requires immediate corrective action. This
is especially the case for autonomous vehicles (AVs) navigating real-world
situations where safety is paramount, and a strong ability to react to a
changing belief about the environment is truly needed.

In this paper we explore an end-to-end approach, from training to execution,
for learning robust contingency plans and combining them with a hierarchical
planner to obtain a robust agent policy in an autonomous navigation task where
other vehicles' behaviours are unknown, and the agent's belief about these
behaviours is subject to sudden, last-second change. We show that our approach
results in robust, safe behaviour in a partially observable, stochastic
environment, generalizing well over environment dynamics not seen during
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lecerf_U/0/1/0/all/0/1"&gt;Ugo Lecerf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yemdji_Tchassi_C/0/1/0/all/0/1"&gt;Christelle Yemdji-Tchassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1"&gt;Pietro Michiardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Massive MIMO Beam Management in Sub-6 GHz 5G NR. (arXiv:2204.06064v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2204.06064</id>
        <link href="http://arxiv.org/abs/2204.06064"/>
        <updated>2022-04-14T00:58:52.035Z</updated>
        <summary type="html"><![CDATA[Beam codebooks are a new feature of massive multiple-input multiple-output
(M-MIMO) in 5G new radio (NR). Codebooks comprised of beamforming vectors are
used to transmit reference signals and obtain limited channel state information
(CSI) from receivers via the codeword index. This enables large arrays that
cannot otherwise obtain sufficient CSI. The performance, however, is limited by
the codebook design. In this paper, we show that machine learning can be used
to train site-specific codebooks for initial access. We design a neural network
based on an autoencoder architecture that uses a beamspace observation in
combination with RF environment characteristics to improve the synchronization
signal (SS) burst codebook. We test our algorithm using a flexible dataset of
channels generated from QuaDRiGa. The results show that our model outperforms
the industry standard (DFT beams) and approaches the optimal performance
(perfect CSI and singular value decomposition (SVD)-based beamforming), using
only a few bits of feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dreifuerst_R/0/1/0/all/0/1"&gt;Ryan M. Dreifuerst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+jr%2E_R/0/1/0/all/0/1"&gt;Robert W. Heath jr.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yazdan_A/0/1/0/all/0/1"&gt;Ali Yazdan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-heterogeneity-aware Mixing for Decentralized Learning. (arXiv:2204.06477v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06477</id>
        <link href="http://arxiv.org/abs/2204.06477"/>
        <updated>2022-04-14T00:58:52.035Z</updated>
        <summary type="html"><![CDATA[Decentralized learning provides an effective framework to train machine
learning models with data distributed over arbitrary communication graphs.
However, most existing approaches toward decentralized learning disregard the
interaction between data heterogeneity and graph topology. In this paper, we
characterize the dependence of convergence on the relationship between the
mixing weights of the graph and the data heterogeneity across nodes. We propose
a metric that quantifies the ability of a graph to mix the current gradients.
We further prove that the metric controls the convergence rate, particularly in
settings where the heterogeneity across nodes dominates the stochasticity
between updates for a given node. Motivated by our analysis, we propose an
approach that periodically and efficiently optimizes the metric using standard
convex constrained optimization and sketching techniques. Through comprehensive
experiments on standard computer vision and NLP benchmarks, we show that our
approach leads to improvement in test performance for a wide range of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1"&gt;Yatin Dandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koloskova_A/0/1/0/all/0/1"&gt;Anastasia Koloskova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation of Lipschitz Functions using Deep Spline Neural Networks. (arXiv:2204.06233v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06233</id>
        <link href="http://arxiv.org/abs/2204.06233"/>
        <updated>2022-04-14T00:58:52.034Z</updated>
        <summary type="html"><![CDATA[Lipschitz-constrained neural networks have many applications in machine
learning. Since designing and training expressive Lipschitz-constrained
networks is very challenging, there is a need for improved methods and a better
theoretical understanding. Unfortunately, it turns out that ReLU networks have
provable disadvantages in this setting. Hence, we propose to use learnable
spline activation functions with at least 3 linear regions instead. We prove
that this choice is optimal among all component-wise $1$-Lipschitz activation
functions in the sense that no other weight constrained architecture can
approximate a larger class of functions. Additionally, this choice is at least
as expressive as the recently introduced non component-wise Groupsort
activation function for spectral-norm-constrained weights. Previously published
numerical results support our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neumayer_S/0/1/0/all/0/1"&gt;Sebastian Neumayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goujon_A/0/1/0/all/0/1"&gt;Alexis Goujon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohra_P/0/1/0/all/0/1"&gt;Pakshal Bohra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unser_M/0/1/0/all/0/1"&gt;Michael Unser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-distribution Detection with Deep Nearest Neighbors. (arXiv:2204.06507v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06507</id>
        <link href="http://arxiv.org/abs/2204.06507"/>
        <updated>2022-04-14T00:58:52.034Z</updated>
        <summary type="html"><![CDATA[Out-of-distribution (OOD) detection is a critical task for deploying machine
learning models in the open world. Distance-based methods have demonstrated
promise, where testing samples are detected as OOD if they are relatively far
away from in-distribution (ID) data. However, prior methods impose a strong
distributional assumption of the underlying feature space, which may not always
hold. In this paper, we explore the efficacy of non-parametric nearest-neighbor
distance for OOD detection, which has been largely overlooked in the
literature. Unlike prior works, our method does not impose any distributional
assumption, hence providing stronger flexibility and generality. We demonstrate
the effectiveness of nearest-neighbor-based OOD detection on several benchmarks
and establish superior performance. Under the same model trained on
ImageNet-1k, our method substantially reduces the false positive rate
(FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a
parametric approach Mahalanobis distance in detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yiyou Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yifei Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaojin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sigma-Delta and Distributed Noise-Shaping Quantization Methods for Random Fourier Features. (arXiv:2106.02614v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02614</id>
        <link href="http://arxiv.org/abs/2106.02614"/>
        <updated>2022-04-14T00:58:52.034Z</updated>
        <summary type="html"><![CDATA[We propose the use of low bit-depth Sigma-Delta and distributed noise-shaping
methods for quantizing the Random Fourier features (RFFs) associated with
shift-invariant kernels. We prove that our quantized RFFs -- even in the case
of $1$-bit quantization -- allow a high accuracy approximation of the
underlying kernels, and the approximation error decays at least polynomially
fast as the dimension of the RFFs increases. We also show that the quantized
RFFs can be further compressed, yielding an excellent trade-off between memory
use and accuracy. Namely, the approximation error now decays exponentially as a
function of the bits used. Moreover, we empirically show by testing the
performance of our methods on several machine learning tasks that our method
compares favorably to other state of the art quantization methods in this
context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannan_H/0/1/0/all/0/1"&gt;Harish Kannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;Alexander Cloninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1"&gt;Rayan Saab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data. (arXiv:2201.07284v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.07284</id>
        <link href="http://arxiv.org/abs/2201.07284"/>
        <updated>2022-04-14T00:58:52.034Z</updated>
        <summary type="html"><![CDATA[Efficient anomaly detection and diagnosis in multivariate time-series data is
of great importance for modern industrial applications. However, building a
system that is able to quickly and accurately pinpoint anomalous observations
is a challenging problem. This is due to the lack of anomaly labels, high data
volatility and the demands of ultra-low inference times in modern applications.
Despite the recent developments of deep learning approaches for anomaly
detection, only a few of them can address all of these challenges. In this
paper, we propose TranAD, a deep transformer network based anomaly detection
and diagnosis model which uses attention-based sequence encoders to swiftly
perform inference with the knowledge of the broader temporal trends in the
data. TranAD uses focus score-based self-conditioning to enable robust
multi-modal feature extraction and adversarial training to gain stability.
Additionally, model-agnostic meta learning (MAML) allows us to train the model
using limited data. Extensive empirical studies on six publicly available
datasets demonstrate that TranAD can outperform state-of-the-art baseline
methods in detection and diagnosis performance with data and time-efficient
training. Specifically, TranAD increases F1 scores by up to 17%, reducing
training times by up to 99% compared to the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuli_S/0/1/0/all/0/1"&gt;Shreshth Tuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casale_G/0/1/0/all/0/1"&gt;Giuliano Casale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jennings_N/0/1/0/all/0/1"&gt;Nicholas R. Jennings&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADASYN-Random Forest Based Intrusion Detection Model. (arXiv:2105.04301v5 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04301</id>
        <link href="http://arxiv.org/abs/2105.04301"/>
        <updated>2022-04-14T00:58:52.033Z</updated>
        <summary type="html"><![CDATA[Intrusion detection has been a key topic in the field of cyber security, and
the common network threats nowadays have the characteristics of varieties and
variation. Considering the serious imbalance of intrusion detection datasets
will result in low classification performance on attack behaviors of small
sample size and difficulty to detect network attacks accurately and
efficiently, using Adaptive Synthetic Sampling (ADASYN) method to balance
datasets was proposed in this paper. In addition, Random Forest algorithm was
used to train intrusion detection classifiers. Through the comparative
experiment of Intrusion detection on CICIDS 2017 dataset, it is found that
ADASYN with Random Forest performs better. Based on the experimental results,
the improvement of precision, recall, F1 scores and AUC values after ADASYN is
then analyzed. Experiments show that the proposed method can be applied to
intrusion detection with large data, and can effectively improve the
classification accuracy of network attack behaviors. Compared with traditional
machine learning models, it has better performance, generalization ability and
robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhewei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenwen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linyue Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic and Discriminative Imitation (D2-Imitation): Revisiting Adversarial Imitation for Sample Efficiency. (arXiv:2112.06054v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.06054</id>
        <link href="http://arxiv.org/abs/2112.06054"/>
        <updated>2022-04-14T00:58:52.033Z</updated>
        <summary type="html"><![CDATA[Sample efficiency is crucial for imitation learning methods to be applicable
in real-world applications. Many studies improve sample efficiency by extending
adversarial imitation to be off-policy regardless of the fact that these
off-policy extensions could either change the original objective or involve
complicated optimization. We revisit the foundation of adversarial imitation
and propose an off-policy sample efficient approach that requires no
adversarial training or min-max optimization. Our formulation capitalizes on
two key insights: (1) the similarity between the Bellman equation and the
stationary state-action distribution equation allows us to derive a novel
temporal difference (TD) learning approach; and (2) the use of a deterministic
policy simplifies the TD learning. Combined, these insights yield a practical
algorithm, Deterministic and Discriminative Imitation (D2-Imitation), which
operates by first partitioning samples into two replay buffers and then
learning a deterministic policy via off-policy reinforcement learning. Our
empirical results show that D2-Imitation is effective in achieving good sample
efficiency, outperforming several off-policy extension approaches of
adversarial imitation on many control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingfei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1"&gt;Sam Devlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1"&gt;Katja Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Synthetic Data adoption in regulated domains. (arXiv:2204.06297v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06297</id>
        <link href="http://arxiv.org/abs/2204.06297"/>
        <updated>2022-04-14T00:58:52.029Z</updated>
        <summary type="html"><![CDATA[The switch from a Model-Centric to a Data-Centric mindset is putting emphasis
on data and its quality rather than algorithms, bringing forward new
challenges. In particular, the sensitive nature of the information in highly
regulated scenarios needs to be accounted for. Specific approaches to address
the privacy issue have been developed, as Privacy Enhancing Technologies.
However, they frequently cause loss of information, putting forward a crucial
trade-off among data quality and privacy. A clever way to bypass such a
conundrum relies on Synthetic Data: data obtained from a generative process,
learning the real data properties. Both Academia and Industry realized the
importance of evaluating synthetic data quality: without all-round reliable
metrics, the innovative data generation task has no proper objective function
to maximize. Despite that, the topic remains under-explored. For this reason,
we systematically catalog the important traits of synthetic data quality and
privacy, and devise a specific methodology to test them. The result is DAISYnt
(aDoption of Artificial Intelligence SYnthesis): a comprehensive suite of
advanced tests, which sets a de facto standard for synthetic data evaluation.
As a practical use-case, a variety of generative algorithms have been trained
on real-world Credit Bureau Data. The best model has been assessed, using
DAISYnt on the different synthetic replicas. Further potential uses, among
others, entail auditing and fine-tuning of generative models or ensuring high
quality of a given synthetic dataset. From a prescriptive viewpoint,
eventually, DAISYnt may pave the way to synthetic data adoption in highly
regulated domains, ranging from Finance to Healthcare, through Insurance and
Education.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Visani_G/0/1/0/all/0/1"&gt;Giorgio Visani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graffi_G/0/1/0/all/0/1"&gt;Giacomo Graffi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfero_M/0/1/0/all/0/1"&gt;Mattia Alfero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagli_E/0/1/0/all/0/1"&gt;Enrico Bagli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capuzzo_D/0/1/0/all/0/1"&gt;Davide Capuzzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chesani_F/0/1/0/all/0/1"&gt;Federico Chesani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Self-Modulating Attention in Continuous Time Space with Applications to Sequential Recommendation. (arXiv:2204.06517v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.06517</id>
        <link href="http://arxiv.org/abs/2204.06517"/>
        <updated>2022-04-14T00:58:52.023Z</updated>
        <summary type="html"><![CDATA[User interests are usually dynamic in the real world, which poses both
theoretical and practical challenges for learning accurate preferences from
rich behavior data. Among existing user behavior modeling solutions, attention
networks are widely adopted for its effectiveness and relative simplicity.
Despite being extensively studied, existing attentions still suffer from two
limitations: i) conventional attentions mainly take into account the spatial
correlation between user behaviors, regardless the distance between those
behaviors in the continuous time space; and ii) these attentions mostly provide
a dense and undistinguished distribution over all past behaviors then
attentively encode them into the output latent representations. This is however
not suitable in practical scenarios where a user's future actions are relevant
to a small subset of her/his historical behaviors. In this paper, we propose a
novel attention network, named self-modulating attention, that models the
complex and non-linearly evolving dynamic user preferences. We empirically
demonstrate the effectiveness of our method on top-N sequential recommendation
tasks, and the results on three large-scale real-world datasets show that our
model can achieve state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1"&gt;Haoyu Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1"&gt;Nianzu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_D/0/1/0/all/0/1"&gt;Daiyue Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jianping Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic. (arXiv:2204.06362v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06362</id>
        <link href="http://arxiv.org/abs/2204.06362"/>
        <updated>2022-04-14T00:58:52.021Z</updated>
        <summary type="html"><![CDATA[The use of Machine Learning (ML) has rapidly spread across several fields,
having encountered many applications in Structural Dynamics and Vibroacoustic
(SD\&V). The increasing capabilities of ML to unveil insights from data, driven
by unprecedented data availability, algorithms advances and computational
power, enhance decision making, uncertainty handling, patterns recognition and
real-time assessments. Three main applications in SD\&V have taken advantage of
these benefits. In Structural Health Monitoring, ML detection and prognosis
lead to safe operation and optimized maintenance schedules. System
identification and control design are leveraged by ML techniques in Active
Noise Control and Active Vibration Control. Finally, the so-called ML-based
surrogate models provide fast alternatives to costly simulations, enabling
robust and optimized product design. Despite the many works in the area, they
have not been reviewed and analyzed. Therefore, to keep track and understand
this ongoing integration of fields, this paper presents a survey of ML
applications in SD\&V analyses, shedding light on the current state of
implementation and emerging opportunities. The main methodologies, advantages,
limitations, and recommendations based on scientific knowledge were identified
for each of the three applications. Moreover, the paper considers the role of
Digital Twins and Physics Guided ML to overcome current challenges and power
future research progress. As a result, the survey provides a broad overview of
the present landscape of ML applied in SD\&V and guides the reader to an
advanced understanding of progress and prospects in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunha_B/0/1/0/all/0/1"&gt;Barbara Cunha&lt;/a&gt; (LTDS), &lt;a href="http://arxiv.org/find/cs/1/au:+Droz_C/0/1/0/all/0/1"&gt;Christophe Droz&lt;/a&gt; (I4S), &lt;a href="http://arxiv.org/find/cs/1/au:+Zine_A/0/1/0/all/0/1"&gt;Abdelmalek Zine&lt;/a&gt; (ICJ), &lt;a href="http://arxiv.org/find/cs/1/au:+Foulard_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Foulard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichchou_M/0/1/0/all/0/1"&gt;Mohamed Ichchou&lt;/a&gt; (LTDS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local and global topological complexity measures OF ReLU neural network functions. (arXiv:2204.06062v1 [math.AT])]]></title>
        <id>http://arxiv.org/abs/2204.06062</id>
        <link href="http://arxiv.org/abs/2204.06062"/>
        <updated>2022-04-14T00:58:52.020Z</updated>
        <summary type="html"><![CDATA[We apply a generalized piecewise-linear (PL) version of Morse theory due to
Grunert-Kuhnel-Rote to define and study new local and global notions of
topological complexity for fully-connected feedforward ReLU neural network
functions, F: R^n -> R. Along the way, we show how to construct, for each such
F, a canonical polytopal complex K(F) and a deformation retract of the domain
onto K(F), yielding a convenient compact model for performing calculations. We
also give a combinatorial description of local complexity for depth 2 networks,
and a construction showing that local complexity can be arbitrarily high.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Grigsby_J/0/1/0/all/0/1"&gt;J. Elisenda Grigsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lindsey_K/0/1/0/all/0/1"&gt;Kathryn Lindsey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Masden_M/0/1/0/all/0/1"&gt;Marissa Masden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Synthetic Data adoption in regulated domains. (arXiv:2204.06297v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06297</id>
        <link href="http://arxiv.org/abs/2204.06297"/>
        <updated>2022-04-14T00:58:51.989Z</updated>
        <summary type="html"><![CDATA[The switch from a Model-Centric to a Data-Centric mindset is putting emphasis
on data and its quality rather than algorithms, bringing forward new
challenges. In particular, the sensitive nature of the information in highly
regulated scenarios needs to be accounted for. Specific approaches to address
the privacy issue have been developed, as Privacy Enhancing Technologies.
However, they frequently cause loss of information, putting forward a crucial
trade-off among data quality and privacy. A clever way to bypass such a
conundrum relies on Synthetic Data: data obtained from a generative process,
learning the real data properties. Both Academia and Industry realized the
importance of evaluating synthetic data quality: without all-round reliable
metrics, the innovative data generation task has no proper objective function
to maximize. Despite that, the topic remains under-explored. For this reason,
we systematically catalog the important traits of synthetic data quality and
privacy, and devise a specific methodology to test them. The result is DAISYnt
(aDoption of Artificial Intelligence SYnthesis): a comprehensive suite of
advanced tests, which sets a de facto standard for synthetic data evaluation.
As a practical use-case, a variety of generative algorithms have been trained
on real-world Credit Bureau Data. The best model has been assessed, using
DAISYnt on the different synthetic replicas. Further potential uses, among
others, entail auditing and fine-tuning of generative models or ensuring high
quality of a given synthetic dataset. From a prescriptive viewpoint,
eventually, DAISYnt may pave the way to synthetic data adoption in highly
regulated domains, ranging from Finance to Healthcare, through Insurance and
Education.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Visani_G/0/1/0/all/0/1"&gt;Giorgio Visani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graffi_G/0/1/0/all/0/1"&gt;Giacomo Graffi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfero_M/0/1/0/all/0/1"&gt;Mattia Alfero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagli_E/0/1/0/all/0/1"&gt;Enrico Bagli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capuzzo_D/0/1/0/all/0/1"&gt;Davide Capuzzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chesani_F/0/1/0/all/0/1"&gt;Federico Chesani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-heterogeneity-aware Mixing for Decentralized Learning. (arXiv:2204.06477v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06477</id>
        <link href="http://arxiv.org/abs/2204.06477"/>
        <updated>2022-04-14T00:58:51.989Z</updated>
        <summary type="html"><![CDATA[Decentralized learning provides an effective framework to train machine
learning models with data distributed over arbitrary communication graphs.
However, most existing approaches toward decentralized learning disregard the
interaction between data heterogeneity and graph topology. In this paper, we
characterize the dependence of convergence on the relationship between the
mixing weights of the graph and the data heterogeneity across nodes. We propose
a metric that quantifies the ability of a graph to mix the current gradients.
We further prove that the metric controls the convergence rate, particularly in
settings where the heterogeneity across nodes dominates the stochasticity
between updates for a given node. Motivated by our analysis, we propose an
approach that periodically and efficiently optimizes the metric using standard
convex constrained optimization and sketching techniques. Through comprehensive
experiments on standard computer vision and NLP benchmarks, we show that our
approach leads to improvement in test performance for a wide range of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1"&gt;Yatin Dandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koloskova_A/0/1/0/all/0/1"&gt;Anastasia Koloskova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Features of the Earth's seasonal hydroclimate: Characterizations and comparisons across the Koppen-Geiger climates and across continents. (arXiv:2204.06544v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2204.06544</id>
        <link href="http://arxiv.org/abs/2204.06544"/>
        <updated>2022-04-14T00:58:51.989Z</updated>
        <summary type="html"><![CDATA[Detailed feature investigations and comparisons across climates, continents
and time series types can progress our understanding and modelling ability of
the Earth's hydroclimate and its dynamics. As a step towards these important
directions, we here propose and extensively apply a multifaceted and
engineering-friendly methodological framework for the thorough characterization
of seasonal hydroclimatic dependence, variability and change at the global
scale. We apply this framework using over 13 000 quarterly temperature,
precipitation and river flow time series. In these time series, the seasonal
hydroclimatic behaviour is represented by 3-month means of earth-observed
variables. In our analyses, we also adopt the well-established Koppen-Geiger
climate classification system and define continental-scale regions with large
or medium density of observational stations. In this context, we provide in
parallel seasonal hydroclimatic feature summaries and comparisons in terms of
autocorrelation, seasonality, temporal variation, entropy, long-range
dependence and trends. We find notable differences to characterize the
magnitudes of most of these features across the various Koppen-Geiger climate
classes, as well as between several continental-scale geographical regions. We,
therefore, deem that the consideration of the comparative summaries could be
more beneficial in water resources engineering contexts than the also provided
global summaries. Lastly, we apply explainable machine learning to compare the
investigated features with respect to how informative they are in explaining
and predicting either the main Koppen-Geiger climate or the continental-scale
region, with the entropy, long-range dependence and trend features being
(roughly) found to be less informative than the remaining ones at the seasonal
time scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Papacharalampous_G/0/1/0/all/0/1"&gt;Georgia Papacharalampous&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tyralis_H/0/1/0/all/0/1"&gt;Hristos Tyralis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Markonis_Y/0/1/0/all/0/1"&gt;Yannis Markonis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maca_P/0/1/0/all/0/1"&gt;Petr Maca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hanel_M/0/1/0/all/0/1"&gt;Martin Hanel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Statistical Learning View of Simple Kriging. (arXiv:2202.07365v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07365</id>
        <link href="http://arxiv.org/abs/2202.07365"/>
        <updated>2022-04-14T00:58:51.989Z</updated>
        <summary type="html"><![CDATA[In the Big Data era, with the ubiquity of geolocation sensors in particular,
massive datasets exhibiting a possibly complex spatial dependence structure are
becoming increasingly available. In this context, the standard probabilistic
theory of statistical learning does not apply directly and guarantees of the
generalization capacity of predictive rules learned from such data are left to
establish. We analyze here the simple Kriging task, the flagship problem in
Geostatistics: the values of a square integrable random field $X=\{X_s\}_{s\in
S}$, $S\subset \mathbb{R}^2$, with unknown covariance structure are to be
predicted with minimum quadratic risk, based upon observing a single
realization of the spatial process at a finite number of locations $s_1,\;
\ldots,\; s_n$ in $S$. Despite the connection of this minimization problem with
kernel ridge regression, establishing the generalization capacity of empirical
risk minimizers is far from straightforward, due to the non i.i.d. nature of
the spatial data $X_{s_1},\; \ldots,\; X_{s_n}$ involved. In this article,
nonasymptotic bounds of order $O_{\mathbb{P}}(1/n)$ are proved for the excess
risk of a plug-in predictive rule mimicking the true minimizer in the case of
isotropic stationary Gaussian processes observed at locations forming a regular
grid. These theoretical results, as well as the role played by the technical
conditions required to establish them, are illustrated by various numerical
experiments and hopefully pave the way for further developments in statistical
learning based on spatial data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Siviero_E/0/1/0/all/0/1"&gt;Emilia Siviero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chautru_E/0/1/0/all/0/1"&gt;Emilie Chautru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Clemencon_S/0/1/0/all/0/1"&gt;Stephan Cl&amp;#xe9;men&amp;#xe7;on&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Need Anisotropic Graph Neural Networks?. (arXiv:2104.01481v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01481</id>
        <link href="http://arxiv.org/abs/2104.01481"/>
        <updated>2022-04-14T00:58:51.988Z</updated>
        <summary type="html"><![CDATA[Common wisdom in the graph neural network (GNN) community dictates that
anisotropic models -- in which messages sent between nodes are a function of
both the source and target node -- are required to achieve state-of-the-art
performance. Benchmarks to date have demonstrated that these models perform
better than comparable isotropic models -- where messages are a function of the
source node only. In this work we provide empirical evidence challenging this
narrative: we propose an isotropic GNN, which we call Efficient Graph
Convolution (EGC), that consistently outperforms comparable anisotropic models,
including the popular GAT or PNA architectures by using spatially-varying
adaptive filters. In addition to raising important questions for the GNN
community, our work has significant real-world implications for efficiency. EGC
achieves higher model accuracy, with lower memory consumption and latency,
along with characteristics suited to accelerator implementation, while being a
drop-in replacement for existing architectures. As an isotropic model, it
requires memory proportional to the number of vertices in the graph
($\mathcal{O}(V)$); in contrast, anisotropic models require memory proportional
to the number of edges ($\mathcal{O}(E)$). We demonstrate that EGC outperforms
existing approaches across 6 large and diverse benchmark datasets, and conclude
by discussing questions that our work raise for the community going forward.
Code and pretrained models for our experiments are provided at
https://github.com/shyam196/egc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tailor_S/0/1/0/all/0/1"&gt;Shyam A. Tailor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opolka_F/0/1/0/all/0/1"&gt;Felix L. Opolka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Practical Robustness Analysis for DNNs based on PAC-Model Learning. (arXiv:2101.10102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10102</id>
        <link href="http://arxiv.org/abs/2101.10102"/>
        <updated>2022-04-14T00:58:51.988Z</updated>
        <summary type="html"><![CDATA[To analyse local robustness properties of deep neural networks (DNNs), we
present a practical framework from a model learning perspective. Based on
black-box model learning with scenario optimisation, we abstract the local
behaviour of a DNN via an affine model with the probably approximately correct
(PAC) guarantee. From the learned model, we can infer the corresponding
PAC-model robustness property. The innovation of our work is the integration of
model learning into PAC robustness analysis: that is, we construct a PAC
guarantee on the model level instead of sample distribution, which induces a
more faithful and accurate robustness evaluation. This is in contrast to
existing statistical methods without model learning. We implement our method in
a prototypical tool named DeepPAC. As a black-box method, DeepPAC is scalable
and efficient, especially when DNNs have complex structures or high-dimensional
inputs. We extensively evaluate DeepPAC, with 4 baselines (using formal
verification, statistical methods, testing and adversarial attack) and 20 DNN
models across 3 datasets, including MNIST, CIFAR-10, and ImageNet. It is shown
that DeepPAC outperforms the state-of-the-art statistical method PROVERO, and
it achieves more practical robustness analysis than the formal verification
tool ERAN. Also, its results are consistent with existing DNN testing work like
DeepGini.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Renjue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pengfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Cheng-Chao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Youcheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1"&gt;Bai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slope stability predictions on spatially variable random fields using machine learning surrogate models. (arXiv:2204.06097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06097</id>
        <link href="http://arxiv.org/abs/2204.06097"/>
        <updated>2022-04-14T00:58:51.987Z</updated>
        <summary type="html"><![CDATA[Random field Monte Carlo (MC) reliability analysis is a robust stochastic
method to determine the probability of failure. This method, however, requires
a large number of numerical simulations demanding high computational costs.
This paper explores the efficiency of different machine learning (ML)
algorithms used as surrogate models trained on a limited number of random field
slope stability simulations in predicting the results of large datasets. The MC
data in this paper require only the examination of failure or non-failure,
circumventing the time-consuming calculation of factors of safety. An extensive
dataset is generated, consisting of 120,000 finite difference MC slope
stability simulations incorporating different levels of soil heterogeneity and
anisotropy. The Bagging Ensemble, Random Forest and Support Vector classifiers
are found to be the superior models for this problem amongst 9 different models
and ensemble classifiers. Trained only on 0.47% of data (500 samples), the ML
model can classify the entire 120,000 samples with an accuracy of %85 and AUC
score of %91. The performance of ML methods in classifying the random field
slope stability results generally reduces with higher anisotropy and
heterogeneity of soil. The ML assisted MC reliability analysis proves a robust
stochastic method where errors in the predicted probability of failure using %5
of MC data is only %0.46 in average. The approach reduced the computational
time from 306 days to less than 6 hours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aminpour_M/0/1/0/all/0/1"&gt;Mohammad Aminpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaie_R/0/1/0/all/0/1"&gt;Reza Alaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kardani_N/0/1/0/all/0/1"&gt;Navid Kardani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moridpour_S/0/1/0/all/0/1"&gt;Sara Moridpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nazem_M/0/1/0/all/0/1"&gt;Majidreza Nazem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online greedy identification of linear dynamical systems. (arXiv:2204.06375v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06375</id>
        <link href="http://arxiv.org/abs/2204.06375"/>
        <updated>2022-04-14T00:58:51.987Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of exploration in an unknown environment. For
linear dynamical systems, we use an experimental design framework and introduce
an online greedy policy where the control maximizes the information of the next
step. In a setting with a limited number of experimental trials, our algorithm
has low complexity and shows experimentally competitive performances compared
to more elaborate gradient-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Blanke_M/0/1/0/all/0/1"&gt;Matthieu Blanke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lelarge_M/0/1/0/all/0/1"&gt;Marc Lelarge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Gradients for the Approximately Vanishing Ideal. (arXiv:2202.03349v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.03349</id>
        <link href="http://arxiv.org/abs/2202.03349"/>
        <updated>2022-04-14T00:58:51.986Z</updated>
        <summary type="html"><![CDATA[The vanishing ideal of a set of points $X\subseteq \mathbb{R}^n$ is the set
of polynomials that evaluate to $0$ over all points $\mathbf{x} \in X$ and
admits an efficient representation by a finite set of polynomials called
generators. To accommodate the noise in the data set, we introduce the
Conditional Gradients Approximately Vanishing Ideal algorithm (CGAVI) for the
construction of the set of generators of the approximately vanishing ideal. The
constructed set of generators captures polynomial structures in data and gives
rise to a feature map that can, for example, be used in combination with a
linear classifier for supervised learning. In CGAVI, we construct the set of
generators by solving specific instances of (constrained) convex optimization
problems with the Pairwise Frank-Wolfe algorithm (PFW). Among other things, the
constructed generators inherit the LASSO generalization bound and not only
vanish on the training but also on out-sample data. Moreover, CGAVI admits a
compact representation of the approximately vanishing ideal by constructing few
generators with sparse coefficient vectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wirth_E/0/1/0/all/0/1"&gt;E. Wirth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokutta_S/0/1/0/all/0/1"&gt;S. Pokutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Annotation of Therapeutic Working Alliance in Psychotherapy. (arXiv:2204.05522v1 [q-bio.NC] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2204.05522</id>
        <link href="http://arxiv.org/abs/2204.05522"/>
        <updated>2022-04-14T00:58:51.986Z</updated>
        <summary type="html"><![CDATA[The therapeutic working alliance is an important predictor of the outcome of
the psychotherapy treatment. In practice, the working alliance is estimated
from a set of scoring questionnaires in an inventory that both the patient and
the therapists fill out. In this work, we propose an analytical framework of
directly inferring the therapeutic working alliance from the natural language
within the psychotherapy sessions in a turn-level resolution with deep
embeddings such as the Doc2Vec and SentenceBERT models. The transcript of each
psychotherapy session can be transcribed and generated in real-time from the
session speech recordings, and these embedded dialogues are compared with the
distributed representations of the statements in the working alliance
inventory. We demonstrate, in a real-world dataset with over 950 sessions of
psychotherapy treatments in anxiety, depression, schizophrenia and suicidal
patients, the effectiveness of this method in mapping out trajectories of
patient-therapist alignment and the interpretability that can offer insights in
clinical psychiatry. We believe such a framework can be provide timely feedback
to the therapist regarding the quality of the conversation in interview
sessions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Lin_B/0/1/0/all/0/1"&gt;Baihan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Cecchi_G/0/1/0/all/0/1"&gt;Guillermo Cecchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Bouneffouf_D/0/1/0/all/0/1"&gt;Djallel Bouneffouf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overparameterized Linear Regression under Adversarial Attacks. (arXiv:2204.06274v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06274</id>
        <link href="http://arxiv.org/abs/2204.06274"/>
        <updated>2022-04-14T00:58:51.986Z</updated>
        <summary type="html"><![CDATA[As machine learning models start to be used in critical applications, their
vulnerabilities and brittleness become a pressing concern. Adversarial attacks
are a popular framework for studying these vulnerabilities. In this work, we
study the error of linear regression in the face of adversarial attacks. We
provide bounds of the error in terms of the traditional risk and the parameter
norm and show how these bounds can be leveraged and make it possible to use
analysis from non-adversarial setups to study the adversarial risk. The
usefulness of these results is illustrated by shedding light on whether or not
overparameterized linear models can be adversarially robust. We show that
adding features to linear models might be either a source of additional
robustness or brittleness. We show that these differences appear due to scaling
and how the $\ell_1$ and $\ell_2$ norms of random projections concentrate. We
also show how the reformulation we propose allows for solving adversarial
training as a convex optimization problem. This is then used as a tool to study
how adversarial training and other regularization methods might affect the
robustness of the estimated models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Ant&amp;#xf4;nio H. Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Operator with Regularity Structure for Modeling Dynamics Driven by SPDEs. (arXiv:2204.06255v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06255</id>
        <link href="http://arxiv.org/abs/2204.06255"/>
        <updated>2022-04-14T00:58:51.985Z</updated>
        <summary type="html"><![CDATA[Stochastic partial differential equations (SPDEs) are significant tools for
modeling dynamics in many areas including atmospheric sciences and physics.
Neural Operators, generations of neural networks with capability of learning
maps between infinite-dimensional spaces, are strong tools for solving
parametric PDEs. However, they lack the ability to modeling SPDEs which usually
have poor regularity due to the driving noise. As the theory of regularity
structure has achieved great successes in analyzing SPDEs and provides the
concept model feature vectors that well-approximate SPDEs' solutions, we
propose the Neural Operator with Regularity Structure (NORS) which incorporates
the feature vectors for modeling dynamics driven by SPDEs. We conduct
experiments on various of SPDEs including the dynamic Phi41 model and the 2d
stochastic Navier-Stokes equation, and the results demonstrate that the NORS is
resolution-invariant, efficient, and achieves one order of magnitude lower
error with a modest amount of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Peiyan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bingguang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1"&gt;Shiqi Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rongchan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhi-Ming Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Graph Embedding and Joint Sparse Regularization for Multi-label Feature Selection. (arXiv:2204.06445v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06445</id>
        <link href="http://arxiv.org/abs/2204.06445"/>
        <updated>2022-04-14T00:58:51.943Z</updated>
        <summary type="html"><![CDATA[Multi-label learning is often used to mine the correlation between variables
and multiple labels, and its research focuses on fully extracting the
information between variables and labels. The $\ell_{2,1}$ regularization is
often used to get a sparse coefficient matrix, but the problem of
multicollinearity among variables cannot be effectively solved. In this paper,
the proposed model can choose the most relevant variables by solving a joint
constraint optimization problem using the $\ell_{2,1}$ regularization and
Frobenius regularization. In manifold regularization, we carry out a random
walk strategy based on the joint structure to construct a neighborhood graph,
which is highly robust to outliers. In addition, we give an iterative algorithm
of the proposed method and proved the convergence of this algorithm. The
experiments on the real-world data sets also show that the comprehensive
performance of our method is consistently better than the classical method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1"&gt;Haibao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhai_H/0/1/0/all/0/1"&gt;Hongzhi Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Speech Pathology a Biomarker in Automatic Speaker Verification?. (arXiv:2204.06450v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2204.06450</id>
        <link href="http://arxiv.org/abs/2204.06450"/>
        <updated>2022-04-14T00:58:51.943Z</updated>
        <summary type="html"><![CDATA[With the advancements in deep learning (DL) and an increasing interest in
data-driven speech processing methods, a major challenge for speech data
scientists in the healthcare domain is the anonymization of pathological
speech, which is a required step to be able to make them accessible as a public
training resource. In this paper, we investigate pathological speech data and
compare their speaker verifiability with that of healthy individuals. We
utilize a large pathological speech corpus of more than 2,000 test subjects
with various speech and voice disorders from different ages and apply DL-based
automatic speaker verification (ASV) techniques. As a result, we obtained a
mean equal error rate (EER) of 0.86% with a standard deviation of 0.16%, which
is a factor of three lower than comparable healthy speech databases. We further
perform detailed analyses of external influencing factors on ASV such as age,
pathology, recording environment, and utterance length, to explore their
respective effect. Our findings indicate that speech pathology is a potential
biomarker in ASV. This is potentially of high interest for the anonymization of
pathological speech data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arasteh_S/0/1/0/all/0/1"&gt;Soroosh Tayebi Arasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weise_T/0/1/0/all/0/1"&gt;Tobias Weise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_M/0/1/0/all/0/1"&gt;Maria Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noth_E/0/1/0/all/0/1"&gt;Elmar N&amp;#xf6;th&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Seung Hee Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Exponentially Tilted Gaussian Prior for Variational Autoencoders. (arXiv:2111.15646v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.15646</id>
        <link href="http://arxiv.org/abs/2111.15646"/>
        <updated>2022-04-14T00:58:51.943Z</updated>
        <summary type="html"><![CDATA[An important property for deep neural networks is the ability to perform
robust out-of-distribution detection on previously unseen data. This property
is essential for safety purposes when deploying models for real world
applications. Recent studies show that probabilistic generative models can
perform poorly on this task, which is surprising given that they seek to
estimate the likelihood of training data. To alleviate this issue, we propose
the exponentially tilted Gaussian prior distribution for the Variational
Autoencoder (VAE) which pulls points onto the surface of a hyper-sphere in
latent space. This achieves state-of-the art results on the area under the
curve-receiver operator characteristics metric using just the log-likelihood
that the VAE naturally assigns. Because this prior is a simple modification of
the traditional VAE prior, it is faster and easier to implement than
competitive methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Floto_G/0/1/0/all/0/1"&gt;Griffin Floto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kremer_S/0/1/0/all/0/1"&gt;Stefan Kremer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nica_M/0/1/0/all/0/1"&gt;Mihai Nica&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Multiple-Objective Reinforcement Learning for Chip Placement. (arXiv:2204.06407v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06407</id>
        <link href="http://arxiv.org/abs/2204.06407"/>
        <updated>2022-04-14T00:58:51.942Z</updated>
        <summary type="html"><![CDATA[Recently, successful applications of reinforcement learning to chip placement
have emerged. Pretrained models are necessary to improve efficiency and
effectiveness. Currently, the weights of objective metrics (e.g., wirelength,
congestion, and timing) are fixed during pretraining. However, fixed-weighed
models cannot generate the diversity of placements required for engineers to
accommodate changing requirements as they arise. This paper proposes flexible
multiple-objective reinforcement learning (MORL) to support objective functions
with inference-time variable weights using just a single pretrained model. Our
macro placement results show that MORL can generate the Pareto frontier of
multiple objectives effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1"&gt;Fu-Chieh Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1"&gt;Yu-Wei Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Ya-Wen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Ssu-Rui Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cioba_A/0/1/0/all/0/1"&gt;Alexandru Cioba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_I/0/1/0/all/0/1"&gt;I-Lun Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1"&gt;Jhih-Wei Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng-Yuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chien-Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ren-Chu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yao-Wen Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tai-Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tung-Chieh Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Epistemic Neural Networks. (arXiv:2107.08924v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08924</id>
        <link href="http://arxiv.org/abs/2107.08924"/>
        <updated>2022-04-14T00:58:51.921Z</updated>
        <summary type="html"><![CDATA[Effective decision, exploration, and adaptation often require an agent to
know what it knows and, also, what it does not know. This capability relies on
the quality of \textit{joint} predictions of labels assigned to multiple
inputs. Conventional neural networks lack this capability and, since most
research has focused on marginal predictions, this shortcoming has been largely
overlooked. By assessing the quality of joint predictions it is possible to
determine whether a neural network effectively distinguishes between epistemic
uncertainty (that due to lack of knowledge) and aleatoric uncertainty (that due
to chance). We introduce the \textit{epistemic neural network} (ENN) as a
general interface for uncertainty modeling in deep learning. While prior
approaches to uncertainty modeling can be viewed as ENNs, the new interface
facilitates comparison of joint predictions, and the design of novel
architectures and algorithms. In particular, we introduce the \textit{epinet}:
an architecture that can supplement any existing neural network, including
pretrained models, and trained with modest incremental computation to represent
uncertainty. With an epinet, conventional neural networks outperform very large
ensembles, consisting of hundreds or more particles, with orders of magnitude
less computation. We demonstrate this efficacy across synthetic data, ImageNet,
and sequential decision problems. As part of this effort we open-source
experiment code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1"&gt;Ian Osband&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zheng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asghari_S/0/1/0/all/0/1"&gt;Seyed Mohammad Asghari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwaracherla_V/0/1/0/all/0/1"&gt;Vikranth Dwaracherla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahimi_M/0/1/0/all/0/1"&gt;Morteza Ibrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU. (arXiv:2204.06240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06240</id>
        <link href="http://arxiv.org/abs/2204.06240"/>
        <updated>2022-04-14T00:58:51.901Z</updated>
        <summary type="html"><![CDATA[The click-through rate (CTR) prediction task is to predict whether a user
will click on the recommended item. As mind-boggling amounts of data are
produced online daily, accelerating CTR prediction model training is critical
to ensuring an up-to-date model and reducing the training cost. One approach to
increase the training speed is to apply large batch training. However, as shown
in computer vision and natural language processing tasks, training with a large
batch easily suffers from the loss of accuracy. Our experiments show that
previous scaling rules fail in the training of CTR prediction neural networks.
To tackle this problem, we first theoretically show that different frequencies
of ids make it challenging to scale hyperparameters when scaling the batch
size. To stabilize the training process in a large batch size setting, we
develop the adaptive Column-wise Clipping (CowClip). It enables an easy and
effective scaling rule for the embeddings, which keeps the learning rate
unchanged and scales the L2 loss. We conduct extensive experiments with four
CTR prediction networks on two real-world datasets and successfully scaled 128
times the original batch size without accuracy loss. In particular, for CTR
prediction model DeepFM training on the Criteo dataset, our optimization
framework enlarges the batch size from 1K to 128K with over 0.1% AUC
improvement and reduces training time from 12 hours to 10 minutes on a single
V100 GPU. Our code locates at https://github.com/zhengzangw/LargeBatchCTR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zangwei Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pengtai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1"&gt;Xuan Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1"&gt;Da Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1"&gt;Chenguang Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1"&gt;Leqi Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yijie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Ming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xiangzhuo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuzhao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Ziheng Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Youlong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-scale multi-objective influence maximisation with network downscaling. (arXiv:2204.06250v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2204.06250</id>
        <link href="http://arxiv.org/abs/2204.06250"/>
        <updated>2022-04-14T00:58:51.901Z</updated>
        <summary type="html"><![CDATA[Finding the most influential nodes in a network is a computationally hard
problem with several possible applications in various kinds of network-based
problems. While several methods have been proposed for tackling the influence
maximisation (IM) problem, their runtime typically scales poorly when the
network size increases. Here, we propose an original method, based on network
downscaling, that allows a multi-objective evolutionary algorithm (MOEA) to
solve the IM problem on a reduced scale network, while preserving the relevant
properties of the original network. The downscaled solution is then upscaled to
the original network, using a mechanism based on centrality metrics such as
PageRank. Our results on eight large networks (including two with $\sim$50k
nodes) demonstrate the effectiveness of the proposed method with a more than
10-fold runtime gain compared to the time needed on the original network, and
an up to $82\%$ time reduction compared to CELF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cunegatti_E/0/1/0/all/0/1"&gt;Elia Cunegatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iacca_G/0/1/0/all/0/1"&gt;Giovanni Iacca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1"&gt;Doina Bucur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The sparse Polynomial Chaos expansion: a fully Bayesian approach with joint priors on the coefficients and global selection of terms. (arXiv:2204.06043v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2204.06043</id>
        <link href="http://arxiv.org/abs/2204.06043"/>
        <updated>2022-04-14T00:58:51.901Z</updated>
        <summary type="html"><![CDATA[Polynomial chaos expansion (PCE) is a versatile tool widely used in
uncertainty quantification and machine learning, but its successful application
depends strongly on the accuracy and reliability of the resulting PCE-based
response surface. High accuracy typically requires high polynomial degrees,
demanding many training points especially in high-dimensional problems through
the curse of dimensionality. So-called sparse PCE concepts work with a much
smaller selection of basis polynomials compared to conventional PCE approaches
and can overcome the curse of dimensionality very efficiently, but have to pay
specific attention to their strategies of choosing training points.
Furthermore, the approximation error resembles an uncertainty that most
existing PCE-based methods do not estimate. In this study, we develop and
evaluate a fully Bayesian approach to establish the PCE representation via
joint shrinkage priors and Markov chain Monte Carlo. The suggested Bayesian PCE
model directly aims to solve the two challenges named above: achieving a sparse
PCE representation and estimating uncertainty of the PCE itself. The embedded
Bayesian regularizing via the joint shrinkage prior allows using higher
polynomial degrees for given training points due to its ability to handle
underdetermined situations, where the number of considered PCE coefficients
could be much larger than the number of available training points. We also
explore multiple variable selection methods to construct sparse PCE expansions
based on the established Bayesian representations, while globally selecting the
most meaningful orthonormal polynomials given the available training data. We
demonstrate the advantages of our Bayesian PCE and the corresponding
sparsity-inducing methods on several benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Burkner_P/0/1/0/all/0/1"&gt;Paul-Christian B&amp;#xfc;rkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kroker_I/0/1/0/all/0/1"&gt;Ilja Kr&amp;#xf6;ker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oladyshkin_S/0/1/0/all/0/1"&gt;Sergey Oladyshkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_W/0/1/0/all/0/1"&gt;Wolfgang Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A quantum generative model for multi-dimensional time series using Hamiltonian learning. (arXiv:2204.06150v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2204.06150</id>
        <link href="http://arxiv.org/abs/2204.06150"/>
        <updated>2022-04-14T00:58:51.901Z</updated>
        <summary type="html"><![CDATA[Synthetic data generation has proven to be a promising solution for
addressing data availability issues in various domains. Even more challenging
is the generation of synthetic time series data, where one has to preserve
temporal dynamics, i.e., the generated time series must respect the original
relationships between variables across time. Recently proposed techniques such
as generative adversarial networks (GANs) and quantum-GANs lack the ability to
attend to the time series specific temporal correlations adequately. We propose
using the inherent nature of quantum computers to simulate quantum dynamics as
a technique to encode such features. We start by assuming that a given time
series can be generated by a quantum process, after which we proceed to learn
that quantum process using quantum machine learning. We then use the learned
model to generate out-of-sample time series and show that it captures unique
and complex features of the learned time series. We also study the class of
time series that can be modeled using this technique. Finally, we
experimentally demonstrate the proposed algorithm on an 11-qubit trapped-ion
quantum machine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Horowitz_H/0/1/0/all/0/1"&gt;Haim Horowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rao_P/0/1/0/all/0/1"&gt;Pooja Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Radha_S/0/1/0/all/0/1"&gt;Santosh Kumar Radha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DT2CAM: A Decision Tree to Content Addressable Memory Framework. (arXiv:2204.06114v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2204.06114</id>
        <link href="http://arxiv.org/abs/2204.06114"/>
        <updated>2022-04-14T00:58:51.900Z</updated>
        <summary type="html"><![CDATA[Decision trees are considered one of the most powerful tools for data
classification. Accelerating the decision tree search is crucial for
on-the-edge applications that have limited power and latency budget. In this
paper, we propose a Content Addressable Memory (CAM) Compiler for Decision Tree
(DT) inference acceleration. We propose a novel "adaptive-precision" scheme
that results in a compact implementation and enables an efficient bijective
mapping to Ternary Content Addressable Memories while maintaining high
inference accuracies. In addition, a Resistive-CAM (ReCAM) functional
synthesizer is developed for mapping the decision tree to the ReCAM and
performing functional simulations for energy, latency, and accuracy
evaluations. We study the decision tree accuracy under hardware non-idealities
including device defects, manufacturing variability, and input encoding noise.
We test our framework on various DT datasets including \textit{Give Me Some
Credit}, \textit{Titanic}, and \textit{COVID-19}. Our results reveal up to
{42.4\%} energy savings and up to 17.8x better energy-delay-area product
compared to the state-of-art hardware accelerators, and up to 333 million
decisions per sec for the pipelined implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rakka_M/0/1/0/all/0/1"&gt;Mariam Rakka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fouda_M/0/1/0/all/0/1"&gt;Mohammed E. Fouda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanj_R/0/1/0/all/0/1"&gt;Rouwaida Kanj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurdahi_F/0/1/0/all/0/1"&gt;Fadi Kurdahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Hypergraph Laplacian for Hypergraph Learning. (arXiv:2106.06666v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06666</id>
        <link href="http://arxiv.org/abs/2106.06666"/>
        <updated>2022-04-14T00:58:51.900Z</updated>
        <summary type="html"><![CDATA[Hypergraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph-structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-range relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-and-play module for improving the representational power of
HGCNNs.Specifically, HERALD adaptively optimizes the adjacency relationship
between vertices and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuzhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xi Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Runiu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shu-Tao Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Error Bounds for Multiclass Sparse Linear Classifiers. (arXiv:2204.06264v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2204.06264</id>
        <link href="http://arxiv.org/abs/2204.06264"/>
        <updated>2022-04-14T00:58:51.900Z</updated>
        <summary type="html"><![CDATA[We consider high-dimensional multiclass classification by sparse multinomial
logistic regression. Unlike binary classification, in the multiclass setup one
can think about an entire spectrum of possible notions of sparsity associated
with different structural assumptions on the regression coefficients matrix. We
propose a computationally feasible feature selection procedure based on
penalized maximum likelihood with convex penalties capturing a specific type of
sparsity at hand. In particular, we consider global sparsity, double row-wise
sparsity, and low-rank sparsity, and show that with the properly chosen tuning
parameters the derived plug-in classifiers attain the minimax generalization
error bounds (in terms of misclassification excess risk) within the
corresponding classes of multiclass sparse linear classifiers. The developed
approach is general and can be adapted to other types of sparsity as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Levy_T/0/1/0/all/0/1"&gt;Tomer Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Abramovich_F/0/1/0/all/0/1"&gt;Felix Abramovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Epistemic Neural Networks. (arXiv:2107.08924v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08924</id>
        <link href="http://arxiv.org/abs/2107.08924"/>
        <updated>2022-04-14T00:58:51.900Z</updated>
        <summary type="html"><![CDATA[Effective decision, exploration, and adaptation often require an agent to
know what it knows and, also, what it does not know. This capability relies on
the quality of \textit{joint} predictions of labels assigned to multiple
inputs. Conventional neural networks lack this capability and, since most
research has focused on marginal predictions, this shortcoming has been largely
overlooked. By assessing the quality of joint predictions it is possible to
determine whether a neural network effectively distinguishes between epistemic
uncertainty (that due to lack of knowledge) and aleatoric uncertainty (that due
to chance). We introduce the \textit{epistemic neural network} (ENN) as a
general interface for uncertainty modeling in deep learning. While prior
approaches to uncertainty modeling can be viewed as ENNs, the new interface
facilitates comparison of joint predictions, and the design of novel
architectures and algorithms. In particular, we introduce the \textit{epinet}:
an architecture that can supplement any existing neural network, including
pretrained models, and trained with modest incremental computation to represent
uncertainty. With an epinet, conventional neural networks outperform very large
ensembles, consisting of hundreds or more particles, with orders of magnitude
less computation. We demonstrate this efficacy across synthetic data, ImageNet,
and sequential decision problems. As part of this effort we open-source
experiment code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1"&gt;Ian Osband&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zheng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asghari_S/0/1/0/all/0/1"&gt;Seyed Mohammad Asghari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwaracherla_V/0/1/0/all/0/1"&gt;Vikranth Dwaracherla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahimi_M/0/1/0/all/0/1"&gt;Morteza Ibrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiyuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Bayesian Computation via Classification. (arXiv:2111.11507v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.11507</id>
        <link href="http://arxiv.org/abs/2111.11507"/>
        <updated>2022-04-14T00:58:51.900Z</updated>
        <summary type="html"><![CDATA[Approximate Bayesian Computation (ABC) enables statistical inference in
simulator-based models whose likelihoods are difficult to calculate but easy to
simulate from. ABC constructs a kernel-type approximation to the posterior
distribution through an accept/reject mechanism which compares summary
statistics of real and simulated data. To obviate the need for summary
statistics, we directly compare empirical distributions with a Kullback-Leibler
(KL) divergence estimator obtained via contrastive learning. In particular, we
blend flexible machine learning classifiers within ABC to automate fake/real
data comparisons. We consider the traditional accept/reject kernel as well as
an exponential weighting scheme which does not require the ABC acceptance
threshold. Our theoretical results show that the rate at which our ABC
posterior distributions concentrate around the true parameter depends on the
estimation error of the classifier. We derive limiting posterior shape results
and find that, with a properly scaled exponential kernel, asymptotic normality
holds. We demonstrate the usefulness of our approach on simulated examples as
well as real data in the context of stock volatility estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuexi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kaji_T/0/1/0/all/0/1"&gt;Tetsuya Kaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rockova_V/0/1/0/all/0/1"&gt;Veronika Ro&amp;#x10d;kov&amp;#xe1;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2204.05999</id>
        <link href="http://arxiv.org/abs/2204.05999"/>
        <updated>2022-04-14T00:58:51.899Z</updated>
        <summary type="html"><![CDATA[Code is seldom written in a single left-to-right pass and is instead
repeatedly edited and refined. We introduce InCoder, a unified generative model
that can perform program synthesis (via left-to-right generation) as well as
editing (via infilling). InCoder is trained to generate code files from a large
corpus of permissively licensed code, where regions of code have been randomly
masked and moved to the end of each file, allowing code infilling with
bidirectional context. Our model is the first generative model that is able to
directly perform zero-shot code infilling, which we evaluate on challenging
tasks such as type inference, comment generation, and variable re-naming. We
find that the ability to condition on bidirectional context substantially
improves performance on these tasks, while still performing comparably on
standard program synthesis benchmarks in comparison to left-to-right only
models pretrained at similar scale. The InCoder models and code are publicly
released. https://sites.google.com/view/incoder-code-models]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1"&gt;Daniel Fried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1"&gt;Armen Aghajanyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jessy Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sida Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1"&gt;Eric Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Freda Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1"&gt;Ruiqi Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Graph Embedding and Joint Sparse Regularization for Multi-label Feature Selection. (arXiv:2204.06445v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06445</id>
        <link href="http://arxiv.org/abs/2204.06445"/>
        <updated>2022-04-14T00:58:51.899Z</updated>
        <summary type="html"><![CDATA[Multi-label learning is often used to mine the correlation between variables
and multiple labels, and its research focuses on fully extracting the
information between variables and labels. The $\ell_{2,1}$ regularization is
often used to get a sparse coefficient matrix, but the problem of
multicollinearity among variables cannot be effectively solved. In this paper,
the proposed model can choose the most relevant variables by solving a joint
constraint optimization problem using the $\ell_{2,1}$ regularization and
Frobenius regularization. In manifold regularization, we carry out a random
walk strategy based on the joint structure to construct a neighborhood graph,
which is highly robust to outliers. In addition, we give an iterative algorithm
of the proposed method and proved the convergence of this algorithm. The
experiments on the real-world data sets also show that the comprehensive
performance of our method is consistently better than the classical method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1"&gt;Haibao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhai_H/0/1/0/all/0/1"&gt;Hongzhi Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Diverse Solutions in Deep Reinforcement Learning by Maximizing State-Action-Based Mutual Information. (arXiv:2103.07084v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07084</id>
        <link href="http://arxiv.org/abs/2103.07084"/>
        <updated>2022-04-14T00:58:51.899Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning algorithms are typically limited to learning a single
solution for a specified task, even though diverse solutions often exist.
Recent studies showed that learning a set of diverse solutions is beneficial
because diversity enables robust few-shot adaptation. Although existing methods
learn diverse solutions by using the mutual information as unsupervised
rewards, such an approach often suffers from the bias of the gradient estimator
induced by value function approximation. In this study, we propose a novel
method that can learn diverse solutions without suffering the bias problem. In
our method, a policy conditioned on a continuous or discrete latent variable is
trained by directly maximizing the variational lower bound of the mutual
information, instead of using the mutual information as unsupervised rewards as
in previous studies. Through extensive experiments on robot locomotion tasks,
we demonstrate that the proposed method successfully learns an infinite set of
diverse solutions by learning continuous latent variables, which is more
challenging than learning a finite number of solutions. Subsequently, we show
that our method enables more effective few-shot adaptation compared with
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Osa_T/0/1/0/all/0/1"&gt;Takayuki Osa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tangkaratt_V/0/1/0/all/0/1"&gt;Voot Tangkaratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Utilizing variational autoencoders in the Bayesian inverse problem of photoacoustic tomography. (arXiv:2204.06270v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2204.06270</id>
        <link href="http://arxiv.org/abs/2204.06270"/>
        <updated>2022-04-14T00:58:51.898Z</updated>
        <summary type="html"><![CDATA[There has been an increasing interest in utilizing machine learning methods
in inverse problems and imaging. Most of the work has, however, concentrated on
image reconstruction problems, and the number of studies regarding the full
solution of the inverse problem is limited. In this work, we study a machine
learning based approach for the Bayesian inverse problem of photoacoustic
tomography. We develop an approach for estimating the posterior distribution in
photoacoustic tomography using an approach based on the variational
autoencoder. The approach is evaluated with numerical simulations and compared
to the solution of the inverse problem using a Bayesian approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sahlstrom_T/0/1/0/all/0/1"&gt;Teemu Sahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tarvainen_T/0/1/0/all/0/1"&gt;Tanja Tarvainen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-uniform central limit theory with applications to anytime-valid causal inference. (arXiv:2103.06476v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06476</id>
        <link href="http://arxiv.org/abs/2103.06476"/>
        <updated>2022-04-14T00:58:51.898Z</updated>
        <summary type="html"><![CDATA[This work introduces time-uniform analogues of confidence intervals based on
the central limit theorem (CLT). Our methods take the form of confidence
sequences (CS) -- sequences of confidence intervals that are uniformly valid
over time. CSs provide valid inference at arbitrary stopping times, incurring
no penalties for "peeking" at the data, unlike classical confidence intervals
which require the sample size to be fixed in advance. Existing CSs in the
literature are nonasymptotic, requiring strong assumptions on the data, while
the classical (fixed-time) CLT is ubiquitous due to the weak assumptions it
imposes. Our work bridges the gap by introducing time-uniform CSs that only
require CLT-like assumptions. While the CLT approximates the distribution of a
sample average by that of a Gaussian at a fixed sample size, we use strong
invariance principles like the seminal work of Koml\'os, Major, and Tusn\'ady
to uniformly approximate the entire sample average process by an implicit
Brownian motion. Applying Robbins' normal mixture martingale method to this
Brownian motion then yields closed-form time-uniform boundaries. We combine
these boundaries with doubly robust estimators to derive nonparametric CSs for
the average treatment effect (and other causal estimands). These allow
randomized experiments and observational studies to be continuously monitored
and adaptively stopped, all while controlling the type-I error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Waudby_Smith_I/0/1/0/all/0/1"&gt;Ian Waudby-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Arbour_D/0/1/0/all/0/1"&gt;David Arbour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sinha_R/0/1/0/all/0/1"&gt;Ritwik Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kennedy_E/0/1/0/all/0/1"&gt;Edward H. Kennedy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sigma-Delta and Distributed Noise-Shaping Quantization Methods for Random Fourier Features. (arXiv:2106.02614v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02614</id>
        <link href="http://arxiv.org/abs/2106.02614"/>
        <updated>2022-04-14T00:58:51.898Z</updated>
        <summary type="html"><![CDATA[We propose the use of low bit-depth Sigma-Delta and distributed noise-shaping
methods for quantizing the Random Fourier features (RFFs) associated with
shift-invariant kernels. We prove that our quantized RFFs -- even in the case
of $1$-bit quantization -- allow a high accuracy approximation of the
underlying kernels, and the approximation error decays at least polynomially
fast as the dimension of the RFFs increases. We also show that the quantized
RFFs can be further compressed, yielding an excellent trade-off between memory
use and accuracy. Namely, the approximation error now decays exponentially as a
function of the bits used. Moreover, we empirically show by testing the
performance of our methods on several machine learning tasks that our method
compares favorably to other state of the art quantization methods in this
context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannan_H/0/1/0/all/0/1"&gt;Harish Kannan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;Alexander Cloninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1"&gt;Rayan Saab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks. (arXiv:2201.11440v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.11440</id>
        <link href="http://arxiv.org/abs/2201.11440"/>
        <updated>2022-04-14T00:58:51.897Z</updated>
        <summary type="html"><![CDATA[Novel and high-performance medical image classification pipelines are heavily
utilizing ensemble learning strategies. The idea of ensemble learning is to
assemble diverse models or multiple predictions and, thus, boost prediction
performance. However, it is still an open question to what extent as well as
which ensemble learning strategies are beneficial in deep learning based
medical image classification pipelines. In this work, we proposed a
reproducible medical image classification pipeline for analyzing the
performance impact of the following ensemble learning techniques: Augmenting,
Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing
and image augmentation methods as well as 9 deep convolution neural network
architectures. It was applied on four popular medical imaging datasets with
varying complexity. Furthermore, 12 pooling functions for combining multiple
predictions were analyzed, ranging from simple statistical functions like
unweighted averaging up to more complex learning-based functions like support
vector machines. Our results revealed that Stacking achieved the largest
performance gain of up to 13% F1-score increase. Augmenting showed consistent
improvement capabilities by up to 4% and is also applicable to single model
based pipelines. Cross-validation based Bagging demonstrated significant
performance gain close to Stacking, which resulted in an F1-score increase up
to +11%. Furthermore, we demonstrated that simple statistical pooling functions
are equal or often even better than more complex pooling functions. We
concluded that the integration of ensemble learning techniques is a powerful
method for any medical image classification pipeline to improve robustness and
boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1"&gt;Dominik M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soto_Rey_I/0/1/0/all/0/1"&gt;I&amp;#xf1;aki Soto-Rey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1"&gt;Frank Kramer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximating Continuous Functions on Persistence Diagrams Using Template Functions. (arXiv:1902.07190v3 [cs.CG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.07190</id>
        <link href="http://arxiv.org/abs/1902.07190"/>
        <updated>2022-04-14T00:58:51.897Z</updated>
        <summary type="html"><![CDATA[The persistence diagram is an increasingly useful tool from Topological Data
Analysis, but its use alongside typical machine learning techniques requires
mathematical finesse. The most success to date has come from methods that map
persistence diagrams into vector spaces, in a way which maximizes the structure
preserved. This process is commonly referred to as featurization. In this
paper, we describe a mathematical framework for featurization called
\emph{template functions}, and we show that it addresses the problem of
approximating continuous functions on compact subsets of the space of
persistence diagrams. Specifically, we begin by characterizing relative
compactness with respect to the bottleneck distance, and then provide explicit
theoretical methods for constructing compact-open dense subsets of continuous
functions on persistence diagrams. These dense subsets -- obtained via template
functions -- are leveraged for supervised learning tasks with persistence
diagrams. Specifically, we test the method for classification and regression
algorithms on several examples including shape data and dynamical systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perea_J/0/1/0/all/0/1"&gt;Jose A. Perea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Munch_E/0/1/0/all/0/1"&gt;Elizabeth Munch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khasawneh_F/0/1/0/all/0/1"&gt;Firas A. Khasawneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Probabilistic Time Series Forecasting using Augmented Recurrent Input for Dynamic Systems. (arXiv:2106.05848v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05848</id>
        <link href="http://arxiv.org/abs/2106.05848"/>
        <updated>2022-04-14T00:58:51.897Z</updated>
        <summary type="html"><![CDATA[The demand of probabilistic time series forecasting has been recently raised
in various dynamic system scenarios, for example, system identification and
prognostic and health management of machines. To this end, we combine the
advances in both deep generative models and state space model (SSM) to come up
with a novel, data-driven deep probabilistic sequence model. Specifically, we
follow the popular encoder-decoder generative structure to build the recurrent
neural networks (RNN) assisted variational sequence model on an augmented
recurrent input space, which could induce rich stochastic sequence dependency.
Besides, in order to alleviate the inconsistency issue of the posterior between
training and predicting as well as improving the mining of dynamic patterns, we
(i) propose using a lagged hybrid output as input for the posterior at next
time step, which brings training and predicting into alignment; and (ii)
further devise a generalized auto-regressive strategy that encodes all the
historical dependencies for the posterior. Thereafter, we first investigate the
methodological characteristics of the proposed deep probabilistic sequence
model on toy cases, and then comprehensively demonstrate the superiority of our
model against existing deep probabilistic SSM models through extensive
numerical experiments on eight system identification benchmarks from various
dynamic systems. Finally, we apply our sequence model to a real-world
centrifugal compressor forecasting problem, and again verify its outstanding
performance by quantifying the time series predictive distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haitao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Changjun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaomo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xudong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuhua Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaofang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Collaborative Learning Framework for Next POI Recommendation. (arXiv:2204.06516v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.06516</id>
        <link href="http://arxiv.org/abs/2204.06516"/>
        <updated>2022-04-14T00:58:51.893Z</updated>
        <summary type="html"><![CDATA[Next Point-of-Interest (POI) recommendation has become an indispensable
functionality in Location-based Social Networks (LBSNs) due to its
effectiveness in helping people decide the next POI to visit. However, accurate
recommendation requires a vast amount of historical check-in data, thus
threatening user privacy as the location-sensitive data needs to be handled by
cloud servers. Although there have been several on-device frameworks for
privacy-preserving POI recommendations, they are still resource-intensive when
it comes to storage and computation, and show limited robustness to the high
sparsity of user-POI interactions. On this basis, we propose a novel
decentralized collaborative learning framework for POI recommendation (DCLR),
which allows users to train their personalized models locally in a
collaborative manner. DCLR significantly reduces the local models' dependence
on the cloud for training, and can be used to expand arbitrary centralized
recommendation models. To counteract the sparsity of on-device user data when
learning each local model, we design two self-supervision signals to pretrain
the POI representations on the server with geographical and categorical
correlations of POIs. To facilitate collaborative learning, we innovatively
propose to incorporate knowledge from either geographically or semantically
similar users into each local model with attentive aggregation and mutual
information maximization. The collaborative learning process makes use of
communications between devices while requiring only minor engagement from the
central server for identifying user groups, and is compatible with common
privacy preservation mechanisms like differential privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1"&gt;Jing Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_N/0/1/0/all/0/1"&gt;Nguyen Quoc Viet Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negative Sampling for Recommendation. (arXiv:2204.06520v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.06520</id>
        <link href="http://arxiv.org/abs/2204.06520"/>
        <updated>2022-04-14T00:58:51.867Z</updated>
        <summary type="html"><![CDATA[How to effectively sample high-quality negative instances is important for
well training a recommendation model. We argue that a high-quality negative
should be both \textit{informativeness} and \textit{unbiasedness}. Although
previous studies have proposed some approaches to address the informativeness
in negative sampling, few has been done to discriminating false negative from
true negative for unbiased negative sampling, not to mention taking both into
consideration. This paper first adopts a parameter learning perspective to
analyze negative informativeness and unbiasedness in loss gradient-based model
training. We argue that both negative sampling and collaborative filtering
include an implicit task of negative classification, from which we report an
insightful yet beneficial finding about the order relation in predicted
negatives' scores. Based on our finding and by regarding negatives as random
variables, we next derive the class condition density of true negatives and
that of false negatives. We also design a Bayesian classifier for negative
classification, from which we define a quantitative unbiasedness measure for
negatives. Finally, we propose to use a harmonic mean of informativeness and
unbiasedness to sample high-quality negatives. Experimental studies validate
the superiority of our negative sampling algorithm over the peers in terms of
better sampling quality and better recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisCUIT: Visual Auditor for Bias in CNN Image Classifier. (arXiv:2204.05899v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.05899</id>
        <link href="http://arxiv.org/abs/2204.05899"/>
        <updated>2022-04-14T00:58:51.860Z</updated>
        <summary type="html"><![CDATA[CNN image classifiers are widely used, thanks to their efficiency and
accuracy. However, they can suffer from biases that impede their practical
applications. Most existing bias investigation techniques are either
inapplicable to general image classification tasks or require significant user
efforts in perusing all data subgroups to manually specify which data
attributes to inspect. We present VisCUIT, an interactive visualization system
that reveals how and why a CNN classifier is biased. VisCUIT visually
summarizes the subgroups on which the classifier underperforms and helps users
discover and characterize the cause of the underperformances by revealing image
concepts responsible for activating neurons that contribute to
misclassifications. VisCUIT runs in modern browsers and is open-source,
allowing people to easily access and extend the tool to other model
architectures and datasets. VisCUIT is available at the following public demo
link: https://poloclub.github.io/VisCUIT. A video demo is available at
https://youtu.be/eNDbSyM4R_4.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seongmin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1"&gt;Judy Hoffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coverage and Capacity Optimization in STAR-RISs Assisted Networks: A Machine Learning Approach. (arXiv:2204.06390v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2204.06390</id>
        <link href="http://arxiv.org/abs/2204.06390"/>
        <updated>2022-04-14T00:58:51.850Z</updated>
        <summary type="html"><![CDATA[Coverage and capacity are the important metrics for performance evaluation in
wireless networks, while the coverage and capacity have several conflicting
relationships, e.g. high transmit power contributes to large coverage but high
inter-cell interference reduces the capacity performance. Therefore, in order
to strike a balance between the coverage and capacity, a novel model is
proposed for the coverage and capacity optimization of simultaneously
transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs)
assisted networks. To solve the coverage and capacity optimization (CCO)
problem, a machine learning-based multi-objective optimization algorithm, i.e.,
the multi-objective proximal policy optimization (MO-PPO) algorithm, is
proposed. In this algorithm, a loss function-based update strategy is the core
point, which is able to calculate weights for both loss functions of coverage
and capacity by a min-norm solver at each update. The numerical results
demonstrate that the investigated update strategy outperforms the fixed
weight-based MO algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xinyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_W/0/1/0/all/0/1"&gt;Wenqiang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agapitos_A/0/1/0/all/0/1"&gt;Alexandros Agapitos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GenIE: Generative Information Extraction. (arXiv:2112.08340v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.08340</id>
        <link href="http://arxiv.org/abs/2112.08340"/>
        <updated>2022-04-14T00:58:51.840Z</updated>
        <summary type="html"><![CDATA[Structured and grounded representation of text is typically formalized by
closed information extraction, the problem of extracting an exhaustive set of
(subject, relation, object) triplets that are consistent with a predefined set
of entities and relations from a knowledge base schema. Most existing works are
pipelines prone to error accumulation, and all approaches are only applicable
to unrealistically small numbers of entities and relations. We introduce GenIE
(generative information extraction), the first end-to-end autoregressive
formulation of closed information extraction. GenIE naturally exploits the
language knowledge from the pre-trained transformer by autoregressively
generating relations and entities in textual form. Thanks to a new bi-level
constrained generation strategy, only triplets consistent with the predefined
knowledge base schema are produced. Our experiments show that GenIE is
state-of-the-art on closed information extraction, generalizes from fewer
training data points than baselines, and scales to a previously unmanageable
number of entities and relations. With this work, closed information extraction
becomes practical in realistic scenarios, providing new opportunities for
downstream tasks. Finally, this work paves the way towards a unified end-to-end
approach to the core tasks of information extraction. Code, data and models
available at https://github.com/epfl-dlab/GenIE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1"&gt;Martin Josifoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1"&gt;Nicola De Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1"&gt;Maxime Peyrard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1"&gt;Fabio Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1"&gt;Robert West&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Non-parametric Bayesian Hawkes Processes. (arXiv:1810.03730v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.03730</id>
        <link href="http://arxiv.org/abs/1810.03730"/>
        <updated>2022-04-14T00:58:51.831Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop an efficient nonparametric Bayesian estimation of
the kernel function of Hawkes processes. The non-parametric Bayesian approach
is important because it provides flexible Hawkes kernels and quantifies their
uncertainty. Our method is based on the cluster representation of Hawkes
processes. Utilizing the finite support assumption of the Hawkes process, we
efficiently sample random branching structures and thus, we split the Hawkes
process into clusters of Poisson processes. We derive two algorithms -- a block
Gibbs sampler and a maximum a posteriori estimator based on expectation
maximization -- and we show that our methods have a linear time complexity,
both theoretically and empirically. On synthetic data, we show our methods to
be able to infer flexible Hawkes triggering kernels. On two large-scale Twitter
diffusion datasets, we show that our methods outperform the current
state-of-the-art in goodness-of-fit and that the time complexity is linear in
the size of the dataset. We also observe that on diffusions related to online
videos, the learned kernels reflect the perceived longevity for different
content types such as music or pets videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1"&gt;Christian Walder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lexing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Non-parametric Bayesian Hawkes Processes. (arXiv:1810.03730v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.03730</id>
        <link href="http://arxiv.org/abs/1810.03730"/>
        <updated>2022-04-14T00:58:51.823Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop an efficient nonparametric Bayesian estimation of
the kernel function of Hawkes processes. The non-parametric Bayesian approach
is important because it provides flexible Hawkes kernels and quantifies their
uncertainty. Our method is based on the cluster representation of Hawkes
processes. Utilizing the finite support assumption of the Hawkes process, we
efficiently sample random branching structures and thus, we split the Hawkes
process into clusters of Poisson processes. We derive two algorithms -- a block
Gibbs sampler and a maximum a posteriori estimator based on expectation
maximization -- and we show that our methods have a linear time complexity,
both theoretically and empirically. On synthetic data, we show our methods to
be able to infer flexible Hawkes triggering kernels. On two large-scale Twitter
diffusion datasets, we show that our methods outperform the current
state-of-the-art in goodness-of-fit and that the time complexity is linear in
the size of the dataset. We also observe that on diffusions related to online
videos, the learned kernels reflect the perceived longevity for different
content types such as music or pets videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1"&gt;Christian Walder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1"&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lexing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification. (arXiv:2204.06305v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.06305</id>
        <link href="http://arxiv.org/abs/2204.06305"/>
        <updated>2022-04-14T00:58:51.800Z</updated>
        <summary type="html"><![CDATA[Prompt-based learning (i.e., prompting) is an emerging paradigm for
exploiting knowledge learned by a pretrained language model. In this paper, we
propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method
to automatically select label mappings for few-shot text classification with
prompting. Our method exploits one-to-many label mappings and a
statistics-based algorithm to select label mappings given a prompt template.
Our experiments demonstrate that AMuLaP achieves competitive performance on the
GLUE benchmark without human effort or external resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Han Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Diffusion and VCA-Assisted Image Segmentation of Hyperspectral Images. (arXiv:2204.06298v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.06298</id>
        <link href="http://arxiv.org/abs/2204.06298"/>
        <updated>2022-04-14T00:58:51.793Z</updated>
        <summary type="html"><![CDATA[Hyperspectral images encode rich structure that can be exploited for material
discrimination by machine learning algorithms. This article introduces the
Active Diffusion and VCA-Assisted Image Segmentation (ADVIS) for active
material discrimination. ADVIS selects high-purity, high-density pixels that
are far in diffusion distance (a data-dependent metric) from other high-purity,
high-density pixels in the hyperspectral image. The ground truth labels of
these pixels are queried and propagated to the rest of the image. The ADVIS
active learning algorithm is shown to strongly outperform its fully
unsupervised clustering algorithm counterpart, suggesting that the
incorporation of a very small number of carefully-selected ground truth labels
can result in substantially superior material discrimination in hyperspectral
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1"&gt;Sam L. Polk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1"&gt;Kangning Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1"&gt;Robert J. Plemmons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1"&gt;James M. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Receding Neuron Importances for Structured Pruning. (arXiv:2204.06404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06404</id>
        <link href="http://arxiv.org/abs/2204.06404"/>
        <updated>2022-04-14T00:58:51.785Z</updated>
        <summary type="html"><![CDATA[Structured pruning efficiently compresses networks by identifying and
removing unimportant neurons. While this can be elegantly achieved by applying
sparsity-inducing regularisation on BatchNorm parameters, an L1 penalty would
shrink all scaling factors rather than just those of superfluous neurons. To
tackle this issue, we introduce a simple BatchNorm variation with bounded
scaling parameters, based on which we design a novel regularisation term that
suppresses only neurons with low importance. Under our method, the weights of
unnecessary neurons effectively recede, producing a polarised bimodal
distribution of importances. We show that neural networks trained this way can
be pruned to a larger extent and with less deterioration. We one-shot prune VGG
and ResNet architectures at different ratios on CIFAR and ImagenNet datasets.
In the case of VGG-style networks, our method significantly outperforms
existing approaches particularly under a severe pruning regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suteu_M/0/1/0/all/0/1"&gt;Mihai Suteu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yike Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity. (arXiv:2204.06242v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.06242</id>
        <link href="http://arxiv.org/abs/2204.06242"/>
        <updated>2022-04-14T00:58:51.777Z</updated>
        <summary type="html"><![CDATA[Many real-world systems are described not only by data from a single source
but via multiple data views. For example, in genomic medicine, a patient can be
described by data from different molecular layers. This raises the need for
multi-view models that are able to disentangle variation within and across data
views in an interpretable manner. Latent variable models with structured
sparsity are a commonly used tool to address this modeling task but
interpretability is cumbersome since it requires a direct inspection and
interpretation of each factor via a specialized domain expert. Here, we propose
MuVI, a novel approach for domain-informed multi-view latent variable models,
facilitating the analysis of multi-view data in an inherently explainable
manner. We demonstrate that our model (i) is able to integrate noisy domain
expertise in form of feature sets, (ii) is robust to noise in the encoded
domain knowledge, (iii) results in identifiable factors and (iv) is able to
infer interpretable and biologically meaningful axes of variation in a
real-world multi-view dataset of cancer patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qoku_A/0/1/0/all/0/1"&gt;Arber Qoku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buettner_F/0/1/0/all/0/1"&gt;Florian Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Exponentially Tilted Gaussian Prior for Variational Autoencoders. (arXiv:2111.15646v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.15646</id>
        <link href="http://arxiv.org/abs/2111.15646"/>
        <updated>2022-04-14T00:58:51.775Z</updated>
        <summary type="html"><![CDATA[An important property for deep neural networks is the ability to perform
robust out-of-distribution detection on previously unseen data. This property
is essential for safety purposes when deploying models for real world
applications. Recent studies show that probabilistic generative models can
perform poorly on this task, which is surprising given that they seek to
estimate the likelihood of training data. To alleviate this issue, we propose
the exponentially tilted Gaussian prior distribution for the Variational
Autoencoder (VAE) which pulls points onto the surface of a hyper-sphere in
latent space. This achieves state-of-the art results on the area under the
curve-receiver operator characteristics metric using just the log-likelihood
that the VAE naturally assigns. Because this prior is a simple modification of
the traditional VAE prior, it is faster and easier to implement than
competitive methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Floto_G/0/1/0/all/0/1"&gt;Griffin Floto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kremer_S/0/1/0/all/0/1"&gt;Stefan Kremer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nica_M/0/1/0/all/0/1"&gt;Mihai Nica&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models. (arXiv:2204.06029v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.06029</id>
        <link href="http://arxiv.org/abs/2204.06029"/>
        <updated>2022-04-14T00:58:51.659Z</updated>
        <summary type="html"><![CDATA[Named Entity Recognition (NER) is a basic NLP task and finds major
applications in conversational and search systems. It helps us identify key
entities in a sentence used for the downstream application. NER or similar slot
filling systems for popular languages have been heavily used in commercial
applications. In this work, we focus on Marathi, an Indian language, spoken
prominently by the people of Maharashtra state. Marathi is a low resource
language and still lacks useful NER resources. We present L3Cube-MahaNER, the
first major gold standard named entity recognition dataset in Marathi. We also
describe the manual annotation guidelines followed during the process. In the
end, we benchmark the dataset on different CNN, LSTM, and Transformer based
models like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides
the best performance among all the models. The data and models are available at
https://github.com/l3cube-pune/MarathiNLP .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_P/0/1/0/all/0/1"&gt;Parth Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranade_A/0/1/0/all/0/1"&gt;Aparna Ranade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabane_M/0/1/0/all/0/1"&gt;Maithili Sabane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1"&gt;Onkar Litake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Raviraj Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of motor insurance claims occurrence as an imbalanced machine learning problem. (arXiv:2204.06109v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2204.06109</id>
        <link href="http://arxiv.org/abs/2204.06109"/>
        <updated>2022-04-14T00:58:51.659Z</updated>
        <summary type="html"><![CDATA[The insurance industry, with its large datasets, is a natural place to use
big data solutions. However it must be stressed, that significant number of
applications for machine learning in insurance industry, like fraud detection
or claim prediction, deals with the problem of machine learning on an
imbalanced data set. This is due to the fact that frauds or claims are rare
events when compared with the entire population of drivers. The problem of
imbalanced learning is often hard to overcome. Therefore, the main goal of this
work is to present and apply various methods of dealing with an imbalanced
dataset in the context of claim occurrence prediction in car insurance. In
addition, the above techniques are used to compare the results of machine
learning algorithms in the context of claim occurrence prediction in car
insurance. Our study covers the following techniques: logistic-regression,
decision tree, random forest, xgBoost, feed-forward network. The problem is the
classification one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Baran_S/0/1/0/all/0/1"&gt;Sebastian Baran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Rola_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Rola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-based Deep Learning Architecture with Optimal Integration Layer for Image Parsing. (arXiv:2204.06214v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.06214</id>
        <link href="http://arxiv.org/abs/2204.06214"/>
        <updated>2022-04-14T00:58:51.629Z</updated>
        <summary type="html"><![CDATA[Deep learning models have been efficient lately on image parsing tasks.
However, deep learning models are not fully capable of exploiting visual and
contextual information simultaneously. The proposed three-layer context-based
deep architecture is capable of integrating context explicitly with visual
information. The novel idea here is to have a visual layer to learn visual
characteristics from binary class-based learners, a contextual layer to learn
context, and then an integration layer to learn from both via genetic
algorithm-based optimal fusion to produce a final decision. The experimental
outcomes when evaluated on benchmark datasets are promising. Further analysis
shows that optimized network weights can improve performance and make stable
predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_R/0/1/0/all/0/1"&gt;Ranju Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azam_B/0/1/0/all/0/1"&gt;Basim Azam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_B/0/1/0/all/0/1"&gt;Brijesh Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Effective and Efficient Reduction of Large Adaptation Spaces in Self-Adaptive Systems. (arXiv:2204.06254v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2204.06254</id>
        <link href="http://arxiv.org/abs/2204.06254"/>
        <updated>2022-04-14T00:58:51.629Z</updated>
        <summary type="html"><![CDATA[Many software systems today face uncertain operating conditions, such as
sudden changes in the availability of resources or unexpected user behavior.
Without proper mitigation these uncertainties can jeopardize the system goals.
Self-adaptation is a common approach to tackle such uncertainties. When the
system goals may be compromised, the self-adaptive system has to select the
best adaptation option to reconfigure by analyzing the possible adaptation
options, i.e., the adaptation space. Yet, analyzing large adaptation spaces
using rigorous methods can be resource- and time-consuming, or even be
infeasible. One approach to tackle this problem is by using online machine
learning to reduce adaptation spaces. However, existing approaches require
domain expertise to perform feature engineering to define the learner, and
support online adaptation space reduction only for specific goals. To tackle
these limitations, we present 'Deep Learning for Adaptation Space Reduction
Plus' -- DLASeR+ in short. DLASeR+ offers an extendable learning framework for
online adaptation space reduction that does not require feature engineering,
while supporting three common types of adaptation goals: threshold,
optimization, and set-point goals. We evaluate DLASeR+ on two instances of an
Internet-of-Things application with increasing sizes of adaptation spaces for
different combinations of adaptation goals. We compare DLASeR+ with a baseline
that applies exhaustive analysis and two state-of-the-art approaches for
adaptation space reduction that rely on learning. Results show that DLASeR+ is
effective with a negligible effect on the realization of the adaptation goals
compared to an exhaustive analysis approach, and supports three common types of
adaptation goals beyond the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1"&gt;Danny Weyns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gheibi_O/0/1/0/all/0/1"&gt;Omid Gheibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quin_F/0/1/0/all/0/1"&gt;Federico Quin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donckt_J/0/1/0/all/0/1"&gt;Jeroen Van Der Donckt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experimental Standards for Deep Learning Research: A Natural Language Processing Perspective. (arXiv:2204.06251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.06251</id>
        <link href="http://arxiv.org/abs/2204.06251"/>
        <updated>2022-04-14T00:58:51.628Z</updated>
        <summary type="html"><![CDATA[The field of Deep Learning (DL) has undergone explosive growth during the
last decade, with a substantial impact on Natural Language Processing (NLP) as
well. Yet, as with other fields employing DL techniques, there has been a lack
of common experimental standards compared to more established disciplines.
Starting from fundamental scientific principles, we distill ongoing discussions
on experimental standards in DL into a single, widely-applicable methodology.
Following these best practices is crucial to strengthening experimental
evidence, improve reproducibility and enable scientific progress. These
standards are further collected in a public repository to help them
transparently adapt to future needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1"&gt;Dennis Ulmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1"&gt;Elisa Bassignana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1"&gt;Max M&amp;#xfc;ller-Eberstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varab_D/0/1/0/all/0/1"&gt;Daniel Varab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mike Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hardmeier_C/0/1/0/all/0/1"&gt;Christian Hardmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1"&gt;Barbara Plank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time series features for supporting hydrometeorological explorations and predictions in ungauged locations using large datasets. (arXiv:2204.06540v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2204.06540</id>
        <link href="http://arxiv.org/abs/2204.06540"/>
        <updated>2022-04-14T00:58:51.589Z</updated>
        <summary type="html"><![CDATA[Regression-based frameworks for streamflow regionalization are built around
catchment attributes that traditionally originate from catchment hydrology,
flood frequency analysis and their interplay. In this work, we deviated from
this traditional path by formulating and extensively investigating the first
regression-based streamflow regionalization frameworks that largely emerge from
general-purpose time series features for data science and, more precisely, from
a large variety of such features. We focused on 28 features that included
(partial) autocorrelation, entropy, temporal variation, seasonality, trend,
lumpiness, stability, nonlinearity, linearity, spikiness, curvature and others.
We estimated these features for daily temperature, precipitation and streamflow
time series from 511 catchments, and then merged them within regionalization
contexts with traditional topographic, land cover, soil and geologic
attributes. Precipitation and temperature features (e.g., the spectral entropy,
seasonality strength and lag-1 autocorrelation of the precipitation time
series, and the stability and trend strength of the temperature time series)
were found to be useful predictors of many streamflow features. The same
applies to traditional attributes, such as the catchment mean elevation.
Relationships between predictor and dependent variables were also revealed,
while the spectral entropy, the seasonality strength and several
autocorrelation features of the streamflow time series were found to be more
regionalizable than others.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Papacharalampous_G/0/1/0/all/0/1"&gt;Georgia Papacharalampous&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tyralis_H/0/1/0/all/0/1"&gt;Hristos Tyralis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SRMD: Sparse Random Mode Decomposition. (arXiv:2204.06108v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2204.06108</id>
        <link href="http://arxiv.org/abs/2204.06108"/>
        <updated>2022-04-14T00:58:51.579Z</updated>
        <summary type="html"><![CDATA[Signal decomposition and multiscale signal analysis provide many useful tools
for time-frequency analysis. We proposed a random feature method for analyzing
time-series data by constructing a sparse approximation to the spectrogram. The
randomization is both in the time window locations and the frequency sampling,
which lowers the overall sampling and computational cost. The sparsification of
the spectrogram leads to a sharp separation between time-frequency clusters
which makes it easier to identify intrinsic modes, and thus leads to a new
data-driven mode decomposition. The applications include signal representation,
outlier removal, and mode decomposition. On the benchmark tests, we show that
our approach outperforms other state-of-the-art decomposition methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Richardson_N/0/1/0/all/0/1"&gt;Nicholas Richardson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schaeffer_H/0/1/0/all/0/1"&gt;Hayden Schaeffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tran_G/0/1/0/all/0/1"&gt;Giang Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GenIE: Generative Information Extraction. (arXiv:2112.08340v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.08340</id>
        <link href="http://arxiv.org/abs/2112.08340"/>
        <updated>2022-04-14T00:58:51.579Z</updated>
        <summary type="html"><![CDATA[Structured and grounded representation of text is typically formalized by
closed information extraction, the problem of extracting an exhaustive set of
(subject, relation, object) triplets that are consistent with a predefined set
of entities and relations from a knowledge base schema. Most existing works are
pipelines prone to error accumulation, and all approaches are only applicable
to unrealistically small numbers of entities and relations. We introduce GenIE
(generative information extraction), the first end-to-end autoregressive
formulation of closed information extraction. GenIE naturally exploits the
language knowledge from the pre-trained transformer by autoregressively
generating relations and entities in textual form. Thanks to a new bi-level
constrained generation strategy, only triplets consistent with the predefined
knowledge base schema are produced. Our experiments show that GenIE is
state-of-the-art on closed information extraction, generalizes from fewer
training data points than baselines, and scales to a previously unmanageable
number of entities and relations. With this work, closed information extraction
becomes practical in realistic scenarios, providing new opportunities for
downstream tasks. Finally, this work paves the way towards a unified end-to-end
approach to the core tasks of information extraction. Code, data and models
available at https://github.com/epfl-dlab/GenIE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1"&gt;Martin Josifoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1"&gt;Nicola De Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1"&gt;Maxime Peyrard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1"&gt;Fabio Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1"&gt;Robert West&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimators of Entropy and Information via Inference in Probabilistic Models. (arXiv:2202.12363v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.12363</id>
        <link href="http://arxiv.org/abs/2202.12363"/>
        <updated>2022-04-14T00:58:51.503Z</updated>
        <summary type="html"><![CDATA[Estimating information-theoretic quantities such as entropy and mutual
information is central to many problems in statistics and machine learning, but
challenging in high dimensions. This paper presents estimators of entropy via
inference (EEVI), which deliver upper and lower bounds on many information
quantities for arbitrary variables in a probabilistic generative model. These
estimators use importance sampling with proposal distribution families that
include amortized variational inference and sequential Monte Carlo, which can
be tailored to the target model and used to squeeze true information values
with high accuracy. We present several theoretical properties of EEVI and
demonstrate scalability and efficacy on two problems from the medical domain:
(i) in an expert system for diagnosing liver disorders, we rank medical tests
according to how informative they are about latent diseases, given a pattern of
observed symptoms and patient attributes; and (ii) in a differential equation
model of carbohydrate metabolism, we find optimal times to take blood glucose
measurements that maximize information about a diabetic patient's insulin
sensitivity, given their meal and medication schedule.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Saad_F/0/1/0/all/0/1"&gt;Feras A. Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cusumano_Towner_M/0/1/0/all/0/1"&gt;Marco Cusumano-Towner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mansinghka_V/0/1/0/all/0/1"&gt;Vikash K. Mansinghka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How flat is a normal mixture on top?]]></title>
        <id>https://www.johndcook.com/blog/?p=103049</id>
        <link href="https://www.johndcook.com/blog/2022/04/13/how-flat-is-a-normal-mixture-on-top/"/>
        <updated>2022-04-14T00:38:00.000Z</updated>
        <summary type="html"><![CDATA[Male and female heights both have a standard deviation of about 3 inches, with means of 70 inches and 64 inches. That’s a good first-pass model using round numbers. If you ask what the height of an average adult is, not specifying male or female, you get a mixture of two normal distributions. If we […]
How flat is a normal mixture on top? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["My A.I. writes music better than humans. World-class education in A.I. + music -> decades of work -> censored from Facebook, Twitter, soon to be downvoted or unfairly-banned from Reddit. It's making the most beautiful music I've ever heard, and society despises it."]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u34quf/my_ai_writes_music_better_than_humans_worldclass/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u34quf/my_ai_writes_music_better_than_humans_worldclass/"/>
        <updated>2022-04-14T00:33:06.000Z</updated>
        <summary type="html"><![CDATA[Thirty years it's taken me, A.I. that is not just as good as humans but better than humans at composing music:
 https://i.imgur.com/hReXJq1.png
 It passes the Turing Test, and it is also a revolution in the field of music in and of itself.
 In the meantime, no one has said anything nice to me in thirty years; just insults. I would feel dumb rewarding humanity with my creation; it would send the wrong message; it would affirm their bad behavior. Garbage species. Low IQ.
    submitted by    /u/PussyFiller2022  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What JAX NN library to use?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u34oh2/d_what_jax_nn_library_to_use/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u34oh2/d_what_jax_nn_library_to_use/"/>
        <updated>2022-04-14T00:29:42.000Z</updated>
        <summary type="html"><![CDATA[I've been exploring the jax ecosystem and its many neural network libraries but I can't seem to settle on one. The main 5 which i am considering are Trax, Objax, Equinox, Flax, and Elegy, however I would like to hear which jax NN lib you use and why.
    submitted by    /u/Southern-Trip-1102  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best sample text for voice synthesis? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u33z5d/best_sample_text_for_voice_synthesis_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u33z5d/best_sample_text_for_voice_synthesis_d/"/>
        <updated>2022-04-13T23:53:34.000Z</updated>
        <summary type="html"><![CDATA[I'm planning to create a clone of my own voice. Is there some kind of ideal sample text to record? I need 300 sentences.
    submitted by    /u/headwar  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fallacy of Becoming Data-driven – Part 1: Becoming Value-obsessed]]></title>
        <id>https://www.datasciencecentral.com/?p=57182</id>
        <link href="https://www.datasciencecentral.com/datastrategist-datascience-datamanagement-valueengineering/"/>
        <updated>2022-04-13T21:27:31.000Z</updated>
        <summary type="html"><![CDATA[I’m sure we all remember the story of “The Little Engine That Could.” A little railroad engine was built for pulling a few cars on and off the switches. When more powerful engines are asked to pull a load over a steep hill, they respond “I can’t; that is too much a pull for me”.… Read More »Fallacy of Becoming Data-driven – Part 1: Becoming Value-obsessed
The post Fallacy of Becoming Data-driven – Part 1: Becoming Value-obsessed appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] ML model to generate paths (lines) for a given image]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2y2mc/r_ml_model_to_generate_paths_lines_for_a_given/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2y2mc/r_ml_model_to_generate_paths_lines_for_a_given/"/>
        <updated>2022-04-13T19:20:36.000Z</updated>
        <summary type="html"><![CDATA[I am a researcher working on creating paths to indicate the primary and secondary neuronal connections in Corneal Confocal Microscopy images. The ground truth I have is images and sets of two-dimensional lines that indicate the primary and secondary paths as indicated in the image below (Ignore the green dots). The secondary and primary paths are always connected to each other.
 ​
 https://preview.redd.it/r025qcpjddt81.jpg?width=834&format=pjpg&auto=webp&s=11a641988e0c6f8c1e17290fe9588f89ef530635
 I am looking to find the most appropriate models to use for this task. The first thing that came to my mind was semantic segmentation. However, I am looking for other approaches that can be more suitable for drawing 1-pixel lines especially since the ground truth paths are indicated as 1-pixel-wide lines (1-pixel thickness) but the connections in the images have wider thicknesses.
 Any ideas for architecture or methods?
    submitted by    /u/madr3z  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Control access to Amazon SageMaker Feature Store offline using AWS Lake Formation]]></title>
        <id>681eb7cd028c64101dbdc23c7af7af61a3cc8aa9</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/control-access-to-amazon-sagemaker-feature-store-offline-using-aws-lake-formation/"/>
        <updated>2022-04-13T18:54:26.000Z</updated>
        <summary type="html"><![CDATA[You can establish feature stores to provide a central repository for machine learning (ML) features that can be shared with data science teams across your organization for training, batch scoring, and real-time inference. Data science teams can reuse features stored in the central repository, avoiding the need to reengineer feature pipelines for different projects and […]]]></summary>
        <author>
            <name>Arnaud Lauer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Trippy Dream 35 - Psychedelic Special Request]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2xaaj/ai_trippy_dream_35_psychedelic_special_request/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2xaaj/ai_trippy_dream_35_psychedelic_special_request/"/>
        <updated>2022-04-13T18:43:55.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LordPewPew777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Followup response from BAAI on "A Roadmap for Big Model"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2x0jd/n_followup_response_from_baai_on_a_roadmap_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2x0jd/n_followup_response_from_baai_on_a_roadmap_for/"/>
        <updated>2022-04-13T18:31:30.000Z</updated>
        <summary type="html"><![CDATA[Source: https://www.baai.ac.cn/portal/article/index/cid/4/id/404.html
  
Statement on the Alleged Plagiarism by “A Roadmap for Big Model”
 It has come to our attention that the survey report “A Roadmap for Big Model” uploaded on arXiv by a BAAI team is suspected of plagiarism. Immediately upon learning of the allegations, an internal investigation was organized to confirm the issue. BAAI is also initiating an independent review by third-party experts to further asses the issue and accountabilities. As a research institution that attaches great importance to academic standards, BAAI holds a zero-tolerance policy towards academic misconduct. We express our sincerest apologies to the authors of the original papers and to all of those affected.
 The report in question constitutes a collection …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Goodhart’s Law]]></title>
        <id>6256f702f6721b003db4b685</id>
        <link href="https://openai.com/blog/measuring-goodharts-law/"/>
        <updated>2022-04-13T18:00:00.000Z</updated>
        <summary type="html"><![CDATA[Goodhart’s law famously says: “When a measure becomes a target, it ceases to be a good measure.” Although originally from economics, it’s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure.]]></summary>
        <author>
            <name>Jacob Hilton</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIT Schwarzman College of Computing unveils Break Through Tech AI]]></title>
        <id>https://news.mit.edu/2022/mit-schwarzman-college-computing-unveils-break-through-tech-ai-0413</id>
        <link href="https://news.mit.edu/2022/mit-schwarzman-college-computing-unveils-break-through-tech-ai-0413"/>
        <updated>2022-04-13T17:45:00.000Z</updated>
        <summary type="html"><![CDATA[New program strives to bridge the talent gap for underrepresented groups in the tech industry.]]></summary>
        <author>
            <name>MIT Schwarzman College of Computing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Image Restoration Using Swin Transformer in JavaScript]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2vphr/p_image_restoration_using_swin_transformer_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2vphr/p_image_restoration_using_swin_transformer_in/"/>
        <updated>2022-04-13T17:33:23.000Z</updated>
        <summary type="html"><![CDATA[Important note: Right now, the model only supports up sampling from any dimension to at most 256 pixels. I'll likely fix this restriction in the next few days.
 A few days back, I was searching for AI-based image up sampling models in for use within an offline JavaScript app. The latest approaches, such as SwinIR were unavailable for Javascript, so I just created a notebook that converts the SwinIR model from torch to TFJS in a relatively short kaggle kernel. I believe other transformer architectures can also be ported to JS like this. This is the link to the original paper of SwinIR.
 It requires around 1 GB of RAM to run. The size of model folder is 44 MB. It is quantized to float16.
 Anyway, hope someone will find this useful for their website or some other app.
    submitted by    /u/Deep-Station-1746  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Replacing 3x3 convolutions with two 2x2 convolutions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2vim0/d_replacing_3x3_convolutions_with_two_2x2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2vim0/d_replacing_3x3_convolutions_with_two_2x2/"/>
        <updated>2022-04-13T17:24:49.000Z</updated>
        <summary type="html"><![CDATA[Something that's always puzzled me is the ubiquitousness of 3x3 convolutions in computer vision. If I recall past discussion accurately, the main benefits of odd-sized kernels are that
  
With the proper padding they maintain the width and height of their inputs, which makes it easier to think about/design neural network architectures. This is not possible with even kernels, unless you swallow the bullet and use asymmetric padding (which is rejected due to aesthetic reasons)
 Output pixels have a 1-to-1 mapping with input pixels (since odd-sized kernels have a proper "center"). This is considered a nice property -- perhaps (for instance) avoiding aliasing issues during segmentation tasks.
  
Given these two points, we use 3x3 convolutions since they're the smallest odd-sized filter (exclud…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ohio State University Researchers Develop SAT2LoD2: An Open-Source Python Tool For 3D Landscape Modelling Using Satelite Imagery]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2v7il/ohio_state_university_researchers_develop/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2v7il/ohio_state_university_researchers_develop/"/>
        <updated>2022-04-13T17:10:48.000Z</updated>
        <summary type="html"><![CDATA[3D landscape modeling has seen a rise in its popularity and applications in recent years. It has countless applications in the fields of civil engineering, earth sciences, military applications, and many others. Geometric 3D models are typically developed using the city geography markup language (CityGML), and the Level-of-Detail (LoD) building model is the preferred model for building 3D models using CityGML. 
 The use of Satellite imagery for landscape modeling provides the advantage of covering a wide area and is low cost. However, developing LoD2 models using satellite imagery remains a big challenge. Building models in such a way involves complex steps demanding heuristics-based approaches and ML-based detection paradigms.
 In a recent paper, researchers at the Ohio State University propose a SAT2LoD2 to facilitate the development of 3D landscape models. SAT2LoD2 is an open-source, python-based GUI-enabled software that takes the satellite images as inputs and returns LoD2 building models as outputs. The software also has the feature of taking road networks and custom maps as additional inputs for better results.
 Continue Reading
 Paper: https://arxiv.org/pdf/2204.04139v1.pdf
 Github: https://github.com/gdaosu/lod2buildingmodel
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple and Effective Zero-Shot Task-Oriented Dialogue]]></title>
        <id>http://ai.googleblog.com/2022/04/simple-and-effective-zero-shot-task.html</id>
        <link href="http://ai.googleblog.com/2022/04/simple-and-effective-zero-shot-task.html"/>
        <updated>2022-04-13T17:06:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Jeffrey Zhao and Raghav Gupta, Software Engineers, Google Research 
Modern conversational agents need to integrate with an ever-increasing number of services to perform a wide variety of tasks, from booking flights and finding restaurants, to playing music and telling jokes. Adding this functionality can be difficult — for each new task, one needs to collect new data and retrain the models that power the conversational agent. This is because most task-oriented dialogue (TOD) models are trained on a single task-specific ontology. An ontology is generally represented as a list of possible user intents (e.g., if the user wants to book a flight, if the user wants to play some music, etc.) and possible parameter slots to extract from the conversation (e.g., the date of the flight, the…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hilbert transform and Fourier series]]></title>
        <id>https://www.johndcook.com/blog/?p=102974</id>
        <link href="https://www.johndcook.com/blog/2022/04/13/hilbert-fourier/"/>
        <updated>2022-04-13T16:05:15.000Z</updated>
        <summary type="html"><![CDATA[A few days ago I wrote about the Hilbert transform and gave as an example that the Hilbert transform of sine is cosine. We’ll bootstrap that example to find the Hilbert transform of any periodic function from its Fourier series. The Hilbert transform of a function f(t) is a function fH(x) defined by where the […]
Hilbert transform and Fourier series first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Do Deep Neural Networks Contribute to Multivariate Time Series Anomaly Detection ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2szhq/r_do_deep_neural_networks_contribute_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2szhq/r_do_deep_neural_networks_contribute_to/"/>
        <updated>2022-04-13T15:31:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/MVTS_Ano  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Open problem in modern RL that doesn't need a massive computational resources]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2so8x/d_open_problem_in_modern_rl_that_doesnt_need_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2so8x/d_open_problem_in_modern_rl_that_doesnt_need_a/"/>
        <updated>2022-04-13T15:17:52.000Z</updated>
        <summary type="html"><![CDATA[What are open and/or interesting problems in modern reinforcement learning that can be tackled by the average PhD/PostDoc who doesn't have access to a massive compute cluster? The problem shouldn't need us to train our model for 10 months like OpenAI's Dota2 model. Please share your thoughts.
    submitted by    /u/ginger_beer_m  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manage dialog to elicit Amazon Lex slots in Amazon Connect contact flows]]></title>
        <id>06e1c05fec26be8f89c634c5de937fdd773f426a</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/manage-dialog-to-elicit-amazon-lex-slots-in-amazon-connect-contact-flows/"/>
        <updated>2022-04-13T15:09:58.000Z</updated>
        <summary type="html"><![CDATA[Amazon Lex can add powerful automation to contact center solutions, so you can enable self-service via interactive voice response (IVR) interactions or route calls to the appropriate agent based on caller input. These capabilities can increase customer satisfaction by streamlining the user experience, and improve containment rates in the contact center. In both the self-service […]]]></summary>
        <author>
            <name>Brian Yost</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are the biggest developments in CV in last 5 years?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2sdkt/d_what_are_the_biggest_developments_in_cv_in_last/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2sdkt/d_what_are_the_biggest_developments_in_cv_in_last/"/>
        <updated>2022-04-13T15:04:39.000Z</updated>
        <summary type="html"><![CDATA[I've been helping a friend of mine learn about CV, but my knowledge starts getting spotty around 2017-2018. In this spirit I'm hoping to discuss the biggest developments in CV in the last 5 years. I know that ViTs have been developed in that time but I'm hoping to fill in my knowledge gaps!
 A list of topics, important papers, big ideas, or anything else is much appreciated!
 ​
 Edit: Thank you for the discussion everyone 🙌! I've been in and out of meetings but am reading through all responses now 
    submitted by    /u/SleekEagle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Engineers enlist AI to help scale up advanced solar cell manufacturing]]></title>
        <id>https://news.mit.edu/2022/ai-perovskite-solar-manufacturing-0413</id>
        <link href="https://news.mit.edu/2022/ai-perovskite-solar-manufacturing-0413"/>
        <updated>2022-04-13T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[Perovskite materials would be superior to silicon in PV cells, but manufacturing such cells at scale is a huge hurdle. Machine learning can help.]]></summary>
        <author>
            <name>David L. Chandler | MIT News Office</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is the number of dimensions in the latent space equal to the number of the neurons of the layer? Or perhaps number of neurons in the whole neural network?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u2rrwh/is_the_number_of_dimensions_in_the_latent_space/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u2rrwh/is_the_number_of_dimensions_in_the_latent_space/"/>
        <updated>2022-04-13T14:37:16.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/qtgp7dzx2bt81.png?width=850&format=png&auto=webp&s=ccf391e8a1613d5405c137296bdf853010fc3f19
 Not speaking specifically about autoencoders here, but about general neural networks. As I understand correctly, "latent space" refers to one of the fully connected layers of the network and the dimensionality of the space is equal to the number of the neurons in this layer. This would mean, that each of the layers has a different "latent space" representation of the learned data distribution. Do I understand it correctly? 
 I got really confused because people seem to sometimes refer to latent space as to all of the possible activations of all of the neurons in the network (each neuron of the network is one dimension of a latent space) OR EVEN to all of the PARAMETERS of the network (each parameter is one dimension of the latent space (??)). Do we have a separate name for these? How do we call the parameter space of a neural network? Is my original intuition even correct?
    submitted by    /u/bzqp2  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there AIs which are able to simulate a human body when you shoot/hit it, that you can use for video games?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2rp6f/are_there_ais_which_are_able_to_simulate_a_human/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2rp6f/are_there_ais_which_are_able_to_simulate_a_human/"/>
        <updated>2022-04-13T14:33:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TheblackRook3  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does the reward in reinforcement learning have to be immediate reward?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u2rkve/does_the_reward_in_reinforcement_learning_have_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u2rkve/does_the_reward_in_reinforcement_learning_have_to/"/>
        <updated>2022-04-13T14:28:00.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to train a seq2seq model that generates a sentence with T words using reinforcement learning. The input and all the previously generated words form the state of the environment, and generating a word in the sentence is considered an action. In the previous methods [1, 2], the immediate reward r(s_t, a_t, s_{t+1}) for the t-th action a_t is 0 when t < T, and the reward is the CIDEr score (a scalar that measures the quality of the sentence) of the entire sentence when t = T. The policy is updated after the entire sentence is generated.
 I designed a new reward for each action, and the new reward for the t-th action is not zero when t < T. However, the reward of each action can only be calculated when the entire sentence is generated since it relies on the CIDEr score of the entire sentence, i.e. the reward for all the actions relies on the final state s_T. Can I still define the reward in the form of r(s_t, a_t, s_{t+1}) ?
 [1] Rennie et al. Self-critical sequence training for image captioning, CVPR 2017: 7008-7024.
 [2] Ranzato et al. Sequence level training with recurrent neural networks, ICLR 2016.
    submitted by    /u/entalent  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Voice Synthesis Business: 2022 Update]]></title>
        <id>https://medium.com/p/68401b4b0f57</id>
        <link href="https://becominghuman.ai/the-voice-synthesis-business-2022-update-68401b4b0f57?source=rss----5e5bef33608a---4"/>
        <updated>2022-04-13T14:27:46.000Z</updated>
        <summary type="html"><![CDATA[In the past few years, high-quality automated text-to-speech synthesis has effectively become a commodity, with easy access to cloud-based…
Continue reading on Becoming Human: Artificial Intelligence Magazine »]]></summary>
        <author>
            <name>Robert Dale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about math]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u2q8ih/question_about_math/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u2q8ih/question_about_math/"/>
        <updated>2022-04-13T13:23:24.000Z</updated>
        <summary type="html"><![CDATA[I am reading that paper A Distributional Perspective on Reinforcement Learning, and it is related to measure theory. Is it worth to spend time to study whole real analysis and measure theory?
    submitted by    /u/Professional_Card176  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLCommons’ David Kanter, NVIDIA’s Daniel Galvez on Improving AI with Publicly Accessible Datasets]]></title>
        <id>https://blogs.nvidia.com/?p=56527</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/13/mlcommons/"/>
        <updated>2022-04-13T13:00:56.000Z</updated>
        <summary type="html"><![CDATA[In deep learning and machine learning, having a large enough dataset is key to training a system and getting it to produce results. So what does a ML researcher do when there just isn’t enough publicly accessible data? Enter the MLCommons Association, a global engineering consortium with the aim of making ML better for everyone. Read article >
The post MLCommons’ David Kanter, NVIDIA’s Daniel Galvez on Improving AI with Publicly Accessible Datasets appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Clarissa Eyu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just Tech: Centering Community-Driven Innovation at the Margins Episode 3 with Dr. Sasha Costanza-Chock]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=834241</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/just-tech-centering-community-driven-innovation-at-the-margins-episode-3-with-dr-sasha-costanza-chock/"/>
        <updated>2022-04-13T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 135 | April 13, 2022 In “Just Tech: Centering Community-Driven Innovation at the Margins,” Senior Principal Researcher Mary L. Gray explores how technology and community intertwine and the role technology can play in supporting community-driven innovation and community-based organizations. Dr. Gray and her team are working to bring computer science, engineering, social science, and […]
The post Just Tech: Centering Community-Driven Innovation at the Margins Episode 3 with Dr. Sasha Costanza-Chock appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How and where do you serve your model? Using kubernetes, docker, metal? Self developed or existing tools?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2pi8b/p_how_and_where_do_you_serve_your_model_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2pi8b/p_how_and_where_do_you_serve_your_model_using/"/>
        <updated>2022-04-13T12:45:45.000Z</updated>
        <summary type="html"><![CDATA[Hi, I’m a machine learning platform engineer. I’ve been using, exploring and developing model deployment tools and platform for several years.
 Very often, I found that many of the tools or managed service of AI platform, are not very welcome by many users. Some think these tools are unnecessarily complicated.
 I'm currently developing a library in my free time trying to fill the gap. And I also want the library to get well integrated with most users' deployment environments.
 Would you like to share how and where do you serve your model? Using kubernetes? Self developed or existing tools? Thanks～
 P.S. If you are interested, you can visit my project to submit an issue/PR or join the discussions, welcome to help: Pinferencia
 View Poll
    submitted by    /u/Remote_Cancel_7977  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logarithms yearning to be free]]></title>
        <id>https://www.johndcook.com/blog/?p=102966</id>
        <link href="https://www.johndcook.com/blog/2022/04/13/logarithms-yearning-to-be-free/"/>
        <updated>2022-04-13T12:00:22.000Z</updated>
        <summary type="html"><![CDATA[I got an evaluation copy of The Best Writing on Mathematics 2021 yesterday. One article jumped out as I was skimming the table of contents: A Zeroth Power Is Often a Logarithm Yearning to Be Free by Sanjoy Mahajan. Great title. There are quite a few theorems involving powers that have an exceptional case that […]
Logarithms yearning to be free first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digital Folktales, a collection of short stories about internet folklore, written and illustrated by Artificial Intelligence]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2o9jd/digital_folktales_a_collection_of_short_stories/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2o9jd/digital_folktales_a_collection_of_short_stories/"/>
        <updated>2022-04-13T11:35:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/fabianmosele  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does an oscillating explained_variance signify during training? (PPO)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u2o03v/what_does_an_oscillating_explained_variance/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u2o03v/what_does_an_oscillating_explained_variance/"/>
        <updated>2022-04-13T11:19:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/C_BearHill  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I created a YouTube Thumbnail Dataset, and need some insight]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2np4x/p_i_created_a_youtube_thumbnail_dataset_and_need/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2np4x/p_i_created_a_youtube_thumbnail_dataset_and_need/"/>
        <updated>2022-04-13T11:00:24.000Z</updated>
        <summary type="html"><![CDATA[Hi guys! I recently created & published a dataset of YouTube video thumbnails on Kaggle (YouTube Thumbnail Dataset), I've tried to make the dataset as diverse as possible, It contains thumbnails from all varieties of YouTube channels. This dataset goes hand in hand with another dataset (containing YouTube video annotations) that I created, namely YouTubers Saying Things.
 The dataset contains 91 unique YouTube channels, and 10 categories, these categories are assigned by me manually to these channels. (Comedy, Science, Automobile, VideoGames, Food, Entertainment, Informative, Blog, News, Tech)
 All kinds of feedback and criticism are welcome, and also if you guys want some particular channel to be included in both these datasets, feel free to comment on this post, or raise an issue on the Github repositories for both these datasets, I will surely add them in the next version.
 Links to the datasets:
  
YouTubers Saying Things Kaggle, Github
 YouTube Thumbnail Dataset Kaggle, Github
  
   submitted by    /u/alcatraz2217  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[https://youtu.be/0x0to1wNh6s]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2mqwe/httpsyoutube0x0to1wnh6s/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2mqwe/httpsyoutube0x0to1wnh6s/"/>
        <updated>2022-04-13T09:55:13.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/0x0to1wNh6s A new enterprise project model supported by AI. In the near future, the growing introduction of automation and artificial intelligence will require the updating of most of the activities in the production world, along with changes to contracts, tasks, and integration processes between man and machine.
 This is supported by the Accenture "IT's Learning" study, according to which 81% of jobs will suffer the impact of AI and robotization.
    submitted by    /u/neologos52  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve XGboost classification algorithm with small dataset, based on similar bigger dataset ? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2mbus/improve_xgboost_classification_algorithm_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2mbus/improve_xgboost_classification_algorithm_with/"/>
        <updated>2022-04-13T09:23:57.000Z</updated>
        <summary type="html"><![CDATA[Hi, I am doing researches about transfer learning for XGboost. 
 I am currently working with a small dataset from a company in Spain (short history) and the scoring is poor. I have worked before with the same company in France and the scoring was great as I had plenty of data thanks to a big history. How could I improve my score with the data from Spain with the help of data from France ? 
 Could I use transfer learning, or data mutualization, or data augmentation ? If anyone has faced before a similar problem, or has read some papers about it, I would love to hear about it. 
 Thank you!
    submitted by    /u/Cutset  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to improve your video editing software with AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2ljhp/how_to_improve_your_video_editing_software_with_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2ljhp/how_to_improve_your_video_editing_software_with_ai/"/>
        <updated>2022-04-13T08:24:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/tah_zem  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] A Modern Self-Referential Weight Matrix That Learns to Modify Itself]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2k8xh/r_a_modern_selfreferential_weight_matrix_that/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2k8xh/r_a_modern_selfreferential_weight_matrix_that/"/>
        <updated>2022-04-13T06:48:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SB3- HER+DQN for my simple discrete map env but the training result is pretty bad]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u2jq4n/sb3_herdqn_for_my_simple_discrete_map_env_but_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u2jq4n/sb3_herdqn_for_my_simple_discrete_map_env_but_the/"/>
        <updated>2022-04-13T06:12:00.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I am creating a multiple-goal environment. Which is an 8*8 discrete map with a start and terminal state (only one) change after each episode. The reward is 100 for reaching the terminal state and -1 for the rest. In fact, I am not sure if the reward is reasonable. 
 I used PPO from SB3 and I can easily finish it. But when I go offline, using HER+DQN, the training is very bad. 
 Feel free to run it here or take a look at the env and training result. Thank you so much!
 https://colab.research.google.com/drive/1Mt5Yje7GTyjOBHL09zC9C1L05xpTAK9v?usp=sharing
    submitted by    /u/AnimatorRemarkable20  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers Propose a Novel Framework ‘LilNetX’ For Training Deep Neural Network With Extreme Model Compression, and Structured Sparsification]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u2j6cv/researchers_propose_a_novel_framework_lilnetx_for/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u2j6cv/researchers_propose_a_novel_framework_lilnetx_for/"/>
        <updated>2022-04-13T05:36:06.000Z</updated>
        <summary type="html"><![CDATA[In this research, the researchers from the paper ‘ LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification’ talk about the importance of larger parameter-heavy and computationally costly architectures in deep neural networks (DNNs) and how it improves the computer vision tasks. They also mentioned in the paper that it is not as simple as it seems since, as the DNNs become more common in the business, they are frequently required to be trained multiple times, communicated across the network to various devices, and executed under hardware limits with minimum loss of accuracy, all while maintaining accuracy. Then the question arises of how to reduce the models’ size on the devices while still enhancing their run-time. Explorations in this field have tended to take one of two paths: lowering model size via compression approaches or reducing computing demands through model pruning.
 The main achievement of this research from the University of Maryland and Google Research is the introduction of ‘LilNetX’, an end-to-end trainable neural network technique that allows learning models with specified accuracy-rate-computation trade-offs. Prior work has taken a piecemeal approach to these difficulties, which necessitates post-processing or multistage training, which is not efficient and does not scale well for big datasets or architectures. To encourage modest model size, the strategy is to create a joint training goal that penalizes the self-information of network parameters in a reparameterized latent space while simultaneously incorporating priors to increase structured sparsity in the parameter space to decrease computation.
 Continue Reading
 Paper: https://arxiv.org/pdf/2204.02965.pdf
 Github: https://github.com/Sharath-girish/LilNetX
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top Ethical Challenges in AI – The Price of Progress]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2j0rc/top_ethical_challenges_in_ai_the_price_of_progress/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2j0rc/top_ethical_challenges_in_ai_the_price_of_progress/"/>
        <updated>2022-04-13T05:26:01.000Z</updated>
        <summary type="html"><![CDATA[What does 2022 look like for AI? Let's find out.
 https://us.sganalytics.com/blog/top-ethical-challenges-in-ai-the-price-of-progress/
    submitted by    /u/JencyJane  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias in Artificial Intelligence: Is Diversity the Key to the Future Of AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2izci/bias_in_artificial_intelligence_is_diversity_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2izci/bias_in_artificial_intelligence_is_diversity_the/"/>
        <updated>2022-04-13T05:23:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/JencyJane  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What would happen if you connect inputs and outputs randomly to a large hebbian Spiking NN and let it learn shape itself in an environment.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u2iy5i/what_would_happen_if_you_connect_inputs_and/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u2iy5i/what_would_happen_if_you_connect_inputs_and/"/>
        <updated>2022-04-13T05:21:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/The_impact_theory  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What would happen if you connect inputs and outputs randomly to a large hebbian Spiking NN and let it learn shape itself in an environment.]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u2ivzv/what_would_happen_if_you_connect_inputs_and/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u2ivzv/what_would_happen_if_you_connect_inputs_and/"/>
        <updated>2022-04-13T05:17:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/The_impact_theory  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wrote about KNN — Introduction to DataScience Book]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2ityj/wrote_about_knn_introduction_to_datascience_book/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2ityj/wrote_about_knn_introduction_to_datascience_book/"/>
        <updated>2022-04-13T05:13:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mindaslab  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[noob here who doesn’t really understand calculus]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u2ifr5/noob_here_who_doesnt_really_understand_calculus/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u2ifr5/noob_here_who_doesnt_really_understand_calculus/"/>
        <updated>2022-04-13T04:49:51.000Z</updated>
        <summary type="html"><![CDATA[If i want to take the partial derivative of the error with respect to a certain weight, it would be similar to taking the derivative of say y = value * weight + bias
 but if i hold the value and bias still, the derivative just becomes the value of the weight, like how the derivative of y = 3x is just 3… so what do I do? it doesn’t make sense to multiply 3 by a learning variable and make that the new weight, so what am I missing?
    submitted by    /u/-i-hate-this-place-  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Number of Feature VS Action Space in Multi-agent Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u2h8it/number_of_feature_vs_action_space_in_multiagent/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u2h8it/number_of_feature_vs_action_space_in_multiagent/"/>
        <updated>2022-04-13T03:41:14.000Z</updated>
        <summary type="html"><![CDATA[Hi All,
 I am working on a MARL fintech project where I use DDQN and for Q-value, I use LSTM bacause it is time series data. This is a project overview. It has 7 features which is derivatives of ask and bid price and has 12 action spaces for action taking.
 Is it possible to generate a good reliable model using only 7 features for 12 action spaces?
 Number of feature or quality of feature is important for taking good decision in RL. 
 Open for Suggestion
 #Reinforcement_Learning #MARL
    submitted by    /u/laxuu  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What are some interesting hidden stuff about CNNs?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2goyo/d_what_are_some_interesting_hidden_stuff_about/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2goyo/d_what_are_some_interesting_hidden_stuff_about/"/>
        <updated>2022-04-13T03:10:53.000Z</updated>
        <summary type="html"><![CDATA[Hey all,
 Im trying to get up to date with Deep Learning literature, so the last week I was going through CNNs. Here's a general view of what Ive learned so far.
  
Large filters suck, you can get better accuracy with smaller filters and more non linearities
 
Depth is the most important for CNNs over width or filter sizes.
 
ReLU activation is generally better, as Sigmoid/tanh gradients tend to fall off towards the ends
 
Convolution layers are only translation invariant. Stacking multiple features together and passing them through MaxPool helps rotational invariance and scaling although not completely 
 
Residual connection help address vanishing gradient and help improve the overall training procedure
 
Inception models worked well, as they mixed different filter sizes together helping the model learn diverse features
 
Most current work is with Transformers, although Im not sure why. ConvNext shows similar performance can be achieved through large CNNs
 
 Do add to this if I missed anything, or if there's anything you don't know about
    submitted by    /u/Bibbidi_Babbidi_Boo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there anyone interested in re-implementing APT?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u2gcn4/is_there_anyone_interested_in_reimplementing_apt/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u2gcn4/is_there_anyone_interested_in_reimplementing_apt/"/>
        <updated>2022-04-13T02:52:42.000Z</updated>
        <summary type="html"><![CDATA[Hi, these day I really interested in self-supervised RL. Especially only based on state novelty. So I wanted to re-implement APT(Behavior From the Void: Unsupervised Active Pre-Training). but my re-implementation showed not meaningful behaviors compared to official implementation. official implementation uses drq-v2 and intrinsic curiosity module. So, I want to re-implement APT as described in paper(using drq-v1 and contrastive learning).
 Is there someone to check my reimplementation?
 https://github.com/seolhokim/apt
 In that repository, DrQ-v1 works well, but only apt doesn't work! I can't understand why agent stop moving when pre-training.
 ​
 Really thank you for reading.
    submitted by    /u/Spiritual_Fig3632  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to create scenes with text - Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors, a 5-minute paper summary by Casual GAN Papers]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u2e7nz/how_to_create_scenes_with_text_makeascene/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u2e7nz/how_to_create_scenes_with_text_makeascene/"/>
        <updated>2022-04-13T01:03:22.000Z</updated>
        <summary type="html"><![CDATA[The authors of Make-A-Scene propose a novel text-to-image method that leverages the information from an additional input condition called a “scene” in the form of segmentation tokens to improve the quality of generated images and enable scene editing, out-of-distribution prompts, and text-editing of anchor scenes.
 As for the details, let’s dive in, shall we?
 Full summary: https://t.me/casual_gan/284
 Blog post: https://www.casualganpapers.com/text-to-image-vqvae-scene-generation/Make-A-Scene-explained.html
 Make-A-Scene
 arxiv / code (by Casual GAN Papers Community)
 Join the discord community and follow on Twitter for weekly AI paper summaries!
    submitted by    /u/KirillTheMunchKing  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to create scenes with text - Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors, a 5-minute paper summary by Casual GAN Papers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u2e4hy/d_how_to_create_scenes_with_text_makeascene/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u2e4hy/d_how_to_create_scenes_with_text_makeascene/"/>
        <updated>2022-04-13T00:59:02.000Z</updated>
        <summary type="html"><![CDATA[The authors of Make-A-Scene propose a novel text-to-image method that leverages the information from an additional input condition called a “scene” in the form of segmentation tokens to improve the quality of generated images and enable scene editing, out-of-distribution prompts, and text-editing of anchor scenes.
 As for the details, let’s dive in, shall we?
 Full summary: https://t.me/casual_gan/284
 Blog post: https://www.casualganpapers.com/text-to-image-vqvae-scene-generation/Make-A-Scene-explained.html
 Make-A-Scene
 arxiv / code (by Casual GAN Papers Community)
 Join the discord community and follow on Twitter for weekly AI paper summaries!
    submitted by    /u/KirillTheMunchKing  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How IoT Uses Machine Learning To Change The World]]></title>
        <id>https://www.datasciencecentral.com/?p=57202</id>
        <link href="https://www.datasciencecentral.com/how-iot-uses-machine-learning-to-change-the-world/"/>
        <updated>2022-04-12T22:09:51.000Z</updated>
        <summary type="html"><![CDATA[IoT and Machine Learning are the most advanced and evolving technologies that continue to rise in today’s modern world, simplifying human efforts and making lives easier. These technologies have proved to streamline operations and workflows for various industries and provide more robust and scalable applications that allow users to make things done seamlessly.  In recent… Read More »How IoT Uses Machine Learning To Change The World
The post How IoT Uses Machine Learning To Change The World appeared first on Data Science Central.]]></summary>
        <author>
            <name>Avani Trivedi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Win a Kaggle Competition with Bayesian Optimization]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u2a39q/how_to_win_a_kaggle_competition_with_bayesian/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u2a39q/how_to_win_a_kaggle_competition_with_bayesian/"/>
        <updated>2022-04-12T21:34:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aidev2040  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Win a Kaggle Competition with Bayesian Optimization]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u29wzk/how_to_win_a_kaggle_competition_with_bayesian/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u29wzk/how_to_win_a_kaggle_competition_with_bayesian/"/>
        <updated>2022-04-12T21:26:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aidev2040  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should I use A Encoder Decoder CNN]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u29rv7/should_i_use_a_encoder_decoder_cnn/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u29rv7/should_i_use_a_encoder_decoder_cnn/"/>
        <updated>2022-04-12T21:20:03.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to make a model to play a car racing simulator. I have a dataset with the inputs used(human) to get fast lap times. I would like to make a model that reads the game video output and predicts the arrow key inputs to get a fast lap time. It seems, to me, that a CNN with encoder-decoder layers trained on the keyboard inputs would work. Is this a good architecture? I'm also having a hard time finding useful literature.
 please let me know if there is anything I should look into or do differently.
    submitted by    /u/newroadkill  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Substantial plagiarism in BAAI’s “a Road Map for Big Models”]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u27xf2/n_substantial_plagiarism_in_baais_a_road_map_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u27xf2/n_substantial_plagiarism_in_baais_a_road_map_for/"/>
        <updated>2022-04-12T19:58:40.000Z</updated>
        <summary type="html"><![CDATA[BAAI recently released a two hundred page position paper about large transformer models which contains sections that are plagiarized by over a dozen other papers.
 In a massive fit of irony, this was found by Nicholas Carlini, a research who (among other things) is famous for studying how language models copy outputs from their training data. Read the blog post here
    submitted by    /u/StellaAthena  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lidar-Camera Deep Fusion for Multi-Modal 3D Detection]]></title>
        <id>http://ai.googleblog.com/2022/04/lidar-camera-deep-fusion-for-multi.html</id>
        <link href="http://ai.googleblog.com/2022/04/lidar-camera-deep-fusion-for-multi.html"/>
        <updated>2022-04-12T19:58:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Yingwei Li, Student Researcher, Google Cloud and Adams Wei Yu, Research Scientist, Google Research, Brain Team  
LiDAR and visual cameras are two types of complementary sensors used for 3D object detection in autonomous vehicles and robots. LiDAR, which is a remote sensing technique that uses light in the form of a pulsed laser to measure ranges, provides low-resolution shape and depth information, while cameras provide high-resolution shape and texture information. While the features captured by LiDAR and cameras should be merged together to provide optimal 3D object detection, it turns out that most state-of-the-art 3D object detectors use LiDAR as the only input. The main reason is that to develop robust 3D object detection models, most methods need to augment and transform th…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drag-and-drop Data Pipelining: The Next Disruptor in ML]]></title>
        <id>https://www.datasciencecentral.com/?p=57115</id>
        <link href="https://www.datasciencecentral.com/drag-and-drop-data-pipelining-the-next-disruptor-in-ml/"/>
        <updated>2022-04-12T19:05:34.000Z</updated>
        <summary type="html"><![CDATA[Recent advances in machine learning (ML) and artificial intelligence (AI) technologies are helping enterprises across industries quickly move from their use cases from the pilot stage to production and operationalization. According to a report by McKinsey & Company, by 2030, businesses that fully absorb AI could double their cash flow, while companies that don’t could… Read More »Drag-and-drop Data Pipelining: The Next Disruptor in ML
The post Drag-and-drop Data Pipelining: The Next Disruptor in ML appeared first on Data Science Central.]]></summary>
        <author>
            <name>Jessica Gupta</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances Highlight the Future of IoT Security]]></title>
        <id>https://www.datasciencecentral.com/?p=57133</id>
        <link href="https://www.datasciencecentral.com/advances-highlight-the-future-of-iot-security/"/>
        <updated>2022-04-12T18:55:02.000Z</updated>
        <summary type="html"><![CDATA[As the Internet of Things (IoT) is gradually moving from being a centralized structure to a more complex network of innumerable decentralized smart devices, the need for security of data will be acknowledged to a greater degree, thereby promoting the expansion of the global IoT security market. The larger the volume of the data transferred… Read More »Advances Highlight the Future of IoT Security
The post Advances Highlight the Future of IoT Security appeared first on Data Science Central.]]></summary>
        <author>
            <name>Nikita Godse</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top Trends & Predictions That Will Drive Data Science, AI and Machine Learning in 2022]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u25gk5/top_trends_predictions_that_will_drive_data/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u25gk5/top_trends_predictions_that_will_drive_data/"/>
        <updated>2022-04-12T18:10:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/saik2363  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI: OpenAI DALL-E 2 generates amazing images, Google's 540 billion parameters language model, Clearview AI branches out beyond police, and more!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u253tu/last_week_in_ai_openai_dalle_2_generates_amazing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u253tu/last_week_in_ai_openai_dalle_2_generates_amazing/"/>
        <updated>2022-04-12T17:54:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top Trends & Predictions That Will Drive Data Science in 2022]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u250fy/top_trends_predictions_that_will_drive_data/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u250fy/top_trends_predictions_that_will_drive_data/"/>
        <updated>2022-04-12T17:50:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/saik2363  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversation about the future, life and AGI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u24thn/conversation_about_the_future_life_and_agi/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u24thn/conversation_about_the_future_life_and_agi/"/>
        <updated>2022-04-12T17:41:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/HumanSeeing  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI predicts if and when someone will experience cardiac arrest]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u24o8c/ai_predicts_if_and_when_someone_will_experience/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u24o8c/ai_predicts_if_and_when_someone_will_experience/"/>
        <updated>2022-04-12T17:35:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/qptbook  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What's the status of live speech-to-speech conversions? (not TTS)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u24ngz/d_whats_the_status_of_live_speechtospeech/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u24ngz/d_whats_the_status_of_live_speechtospeech/"/>
        <updated>2022-04-12T17:34:20.000Z</updated>
        <summary type="html"><![CDATA[I've been trying to find information about the subject, but almost every result is TTS, and the only example of what I actually want (Respeecher) costs 2 grand a year. Are there any (preferably open-source) other alternatives?
    submitted by    /u/UncertainOutcome  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The last Woolly Mammoth on Earth]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u24d2o/the_last_woolly_mammoth_on_earth/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u24d2o/the_last_woolly_mammoth_on_earth/"/>
        <updated>2022-04-12T17:22:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Ok-Passion-6574  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The last Woolly Mammoth on Earth]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u24c4v/the_last_woolly_mammoth_on_earth/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u24c4v/the_last_woolly_mammoth_on_earth/"/>
        <updated>2022-04-12T17:21:01.000Z</updated>
        <summary type="html"><![CDATA[Is it good or bad?
 Also, I was wondering what art goes big as an NFT?
    submitted by    /u/Ok-Passion-6574  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Moment A Neural Net Became Sentient For The First Time - AI Art Story [4K] #shorts]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u24agy/the_moment_a_neural_net_became_sentient_for_the/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u24agy/the_moment_a_neural_net_became_sentient_for_the/"/>
        <updated>2022-04-12T17:19:02.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/fooo-ooo  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stanford Researchers Introduced a Novel Deep Learning Computer-Assisted System for Real-Time Open Surgery and AVOS (the Annotation Videos of Open Surgery) Dataset￼]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u238lt/stanford_researchers_introduced_a_novel_deep/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u238lt/stanford_researchers_introduced_a_novel_deep/"/>
        <updated>2022-04-12T16:33:55.000Z</updated>
        <summary type="html"><![CDATA[In recent years, the rise of Deep Learning has continuously brought innovations to many fields, and the medical domain is one of them. AI applications in this field are countless: from pre-operative diagnosis to disease classification, from skill assessment to post-operative rehabilitation. Among them, systems to assess surgical skills and provide feedback to improve technique could help in decreasing the number of complications in surgical procedures, which are still the third leading cause of death globally.
 AI can be an additional coach for surgical trainees and an expert colleague for experienced surgeons. But, to train an AI system, reliable data are fundamental. The more utilized type of data in this context is undoubtedly video streams, as a camera is less invasive than other types of sensors, such as ArmBand or EEG, which could weigh on the surgeon’s performance given their physical bulk. This applies particularly to laparoscopic surgery, where an in-body fiber-optic camera is used to visualize the operating area and facilitate rapid data collection. For this reason, the majority of computer-assisted systems focus on laparoscopic surgery. 
 Continue Reading
 Paper: https://arxiv.org/pdf/2112.07219.pdf
 https://preview.redd.it/84mwdw81l4t81.png?width=741&format=png&auto=webp&s=636aff067560876d14f37caccb83bd951e991c68
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of workshops at major conferences. [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u22sv1/comparison_of_workshops_at_major_conferences_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u22sv1/comparison_of_workshops_at_major_conferences_d/"/>
        <updated>2022-04-12T16:14:44.000Z</updated>
        <summary type="html"><![CDATA[I understand that workshop quality is dependent more on the workshop itself rather than the host conference. But, in general, how are workshops from CVPR, NeurIPS, ICLR, ICML, etc. viewed by the community in relation to one another?
    submitted by    /u/avd4292  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What can you tell us about him?]]></title>
        <id>https://medium.com/p/fdb761737457</id>
        <link href="https://becominghuman.ai/what-can-you-tell-us-about-him-fdb761737457?source=rss----5e5bef33608a---4"/>
        <updated>2022-04-12T16:07:15.000Z</updated>
        <summary type="html"><![CDATA[(A Sci-Fi Ultrashort)]]></summary>
        <author>
            <name>Tyger A.C</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Leniax - A Lenia simulation library powered by JAX]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u225iw/project_leniax_a_lenia_simulation_library_powered/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u225iw/project_leniax_a_lenia_simulation_library_powered/"/>
        <updated>2022-04-12T15:46:31.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone!
 I'm really happy to finally publish the work I've been doing on the Cellular Automata called Lenia. It is a JAX library called Leniax and allows one to simulate thousands of simulations in parallel using CPU, GPU, or TPU.
 With it, you can:
  
Simulate Conway's Game of Life
 Simulate multiple Lenia simulations in parallel
 Use gradient descent to search for Continuous CA parameters
 Launch a QD search to discover a ton of diversity in Lenia.
  
Check out the blog post for some visual results
 The main goal of this work was to advance the state of automatic discovery for those systems. 10 months ago, I bet on QD to do so, turns out it indeed works! QD algorithms really rock!
 The code is completely open-source with all the examples, notebooks, and even experiments I ran. (See the doc for more links)
 I would love to have feedback on this and of course, if you find that subject interesting, engage with our community!
 Cheers!
    submitted by    /u/morgangiraud  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Effective Image Pre-Processing Techniques for Enhancing Defects in an Image?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u21tcd/d_effective_image_preprocessing_techniques_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u21tcd/d_effective_image_preprocessing_techniques_for/"/>
        <updated>2022-04-12T15:31:27.000Z</updated>
        <summary type="html"><![CDATA[So I am doing some object detection on Pavement Defects. I've already collected the data with some annotations but the model is performing rather poorly. For example, the `maP` is about `0.12` for the whole data.
 By examining the data, I think one of the reasons is that some of the defects such as cracks, or faded pavement markings are not so clear and either casted by a shadow or too bright from the sun.
 Image Example #1
 Or from motion blur
 Image Example #2
 Is there any image preprocessing technique aside maybe from CLAHE that could be applied? Moreover, I am currently using YOLOv5 for this.
    submitted by    /u/sarmientoj24  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIT’s FutureMakers programs help kids get their minds around — and hands on — AI]]></title>
        <id>https://news.mit.edu/2022/futuremakers-programs-kids-get-their-minds-around-and-hands-ai-0412</id>
        <link href="https://news.mit.edu/2022/futuremakers-programs-kids-get-their-minds-around-and-hands-ai-0412"/>
        <updated>2022-04-12T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[The programs are designed to foster an understanding of how artificial intelligence technologies work, including their social implications.]]></summary>
        <author>
            <name>Kim Patch | MIT Media Lab</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning vs. Cookie Consent Systems]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u1xifd/machine_learning_vs_cookie_consent_systems/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u1xifd/machine_learning_vs_cookie_consent_systems/"/>
        <updated>2022-04-12T12:06:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/DaveBowman1975  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best GridWorld environment?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1xeea/best_gridworld_environment/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1xeea/best_gridworld_environment/"/>
        <updated>2022-04-12T12:00:47.000Z</updated>
        <summary type="html"><![CDATA[In your opinion, what is the best gridworld environment? I want to compare different RL algorithms on it. 
 I’m looking for something super basic: - start and goal state - some obstacles - customisable: move the start and goal state, place obstacles in different points, modify reward map etc. - computationally efficient
 Thank you
    submitted by    /u/wiston_smith  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Custom Callback for Max Episode Reward using Stable Baselines3 with Custom Env]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1wvf6/custom_callback_for_max_episode_reward_using/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1wvf6/custom_callback_for_max_episode_reward_using/"/>
        <updated>2022-04-12T11:30:48.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I've built a custom gym env and am using Stable Baselines3 to train an agent. I would like to visualise in TensorBoard the maximum reward achieved for each episode. I have these values in a list in my env, and I am trying to create a custom Callback to plot this in TensorBoard but it's not working. I've looked over the documentation and other forums but can't figure out how to do this. Can anyone help me out? 🙏🏽 Thank you!
    submitted by    /u/leozinho2r  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] the copent package v0.2.3 available on PyPI now]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1wjhx/p_the_copent_package_v023_available_on_pypi_now/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1wjhx/p_the_copent_package_v023_available_on_pypi_now/"/>
        <updated>2022-04-12T11:11:03.000Z</updated>
        <summary type="html"><![CDATA[The copent package implements the method for estimating copula entropy (mutual information) and transfer entropy (conditional mutual information / conditional independence).
 This version add a new feature (an argument 'mode') for dealing with large data when memory is limited.
 Github: https://github.com/majianthu/pycopent
 PyPI: https://pypi.org/project/copent/
 any comments are welcome.
    submitted by    /u/majianthu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Feedback on a worked Continuous Deployment Example (CI/CD/CT)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1vqj7/d_feedback_on_a_worked_continuous_deployment/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1vqj7/d_feedback_on_a_worked_continuous_deployment/"/>
        <updated>2022-04-12T10:21:20.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone! At ZenML, we released today an integration that allows users to train and deploy models from pipelines in a simple way. I wanted to ask the community here whether the example we showcased makes sense in a real-world setting:
 Context
 ZenML is an extensible, open-source MLOps framework to create production-ready machine learning pipelines. Built for data scientists, it has a simple, flexible syntax, is cloud- and tool-agnostic, and has interfaces/abstractions that are catered towards ML workflows. Seldon Core is a production grade open source model serving platform. It packs a wide range of features built around deploying models to REST/GRPC microservices that include monitoring and logging, model explainers, outlier detectors and various continuous deployment strategies such…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can we decrease the training time of a deep learning model by using a domain specific pretrained backbone instead of the standard imagenet?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1uunt/d_can_we_decrease_the_training_time_of_a_deep/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1uunt/d_can_we_decrease_the_training_time_of_a_deep/"/>
        <updated>2022-04-12T09:20:11.000Z</updated>
        <summary type="html"><![CDATA[I am working in the retail domain atm, and train a lot of image classifiers. I have always used imagenet as pretrained to train my model upon. 
 I thought it would be straightforward to train a backbone on a big retail dataset(1000+ classes), and then use that as pretrained and it'll reduce the time it takes for my models to generalize.
 Turns out, the model took more epochs to train when using the retail backbone, then the imagenet one. 
 Isn't this counter-intuitive? What else can I do to make by backbone better?
    submitted by    /u/lMAObigZEDONG  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-sourced NetHack 2021 NeurIPS Challenge winning agent]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1tog0/opensourced_nethack_2021_neurips_challenge/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1tog0/opensourced_nethack_2021_neurips_challenge/"/>
        <updated>2022-04-12T07:54:45.000Z</updated>
        <summary type="html"><![CDATA[Recently, we have released the source code of our winning solution for the NetHack 2021 NeurIPS Challenge:
 https://github.com/maciej-sypetkowski/autoascend
 We hope that it will help in leveraging this complex environment, that still seems to be beyond capabilities of reinforcement learning. Check out links in the README "Description" section for more context.
    submitted by    /u/procedural_only  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1sqa2/project/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1sqa2/project/"/>
        <updated>2022-04-12T06:46:59.000Z</updated>
        <summary type="html"><![CDATA[Do any of u have any good rl project suggestion or a complete project for college major , I have only done work on some self playing Atari , mario games , if u have any good idea please suggest 🙌
    submitted by    /u/stoned_egineer  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Removing Unpredictable Samples from a Training Set]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1sofz/d_removing_unpredictable_samples_from_a_training/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1sofz/d_removing_unpredictable_samples_from_a_training/"/>
        <updated>2022-04-12T06:43:28.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I have a fairly interesting project that I am working on. I have a model that has some samples which are completely unpredictable, random noise, and some that are reliably predictable.
 How would you go about separating out the samples which can be predicted, identifying them going forward, and retraining on a cleaned dataset with only those samples?
 Interested to see someone else's approach to this.
 Edit: I forgot to mention that my data is from an embedding matrix from ordinal categorical features.
    submitted by    /u/Katapilla_Killa  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares: Stone Golem Ruins || Clip Guided Diffusion AI Art Video [4K 20 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u1qrrz/artificial_nightmares_stone_golem_ruins_clip/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u1qrrz/artificial_nightmares_stone_golem_ruins_clip/"/>
        <updated>2022-04-12T04:52:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What's your experience with Model-Agnostic Meta-Learning in RL?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1qgcm/d_whats_your_experience_with_modelagnostic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1qgcm/d_whats_your_experience_with_modelagnostic/"/>
        <updated>2022-04-12T04:24:10.000Z</updated>
        <summary type="html"><![CDATA[There is the original paper and there was a subsequent paper by other authors titled: On the Convergence Theory of Debiased Model-Agnostic Meta_Reinforcement Learning 
 I've been working on implementing the latter paper on the HalfCheetah environment. However, my attempts have been unsuccessful so far (I know the authors provided the code, but I am trying to write my own code to check my understanding). I'd like to know any tips/tricks that anyone can share and just to know about people's experiences, especially using MAML for RL.
    submitted by    /u/carlml  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to deal with the fact that whatever idea I have has already been published.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1qfxq/how_to_deal_with_the_fact_that_whatever_idea_i/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1qfxq/how_to_deal_with_the_fact_that_whatever_idea_i/"/>
        <updated>2022-04-12T04:23:12.000Z</updated>
        <summary type="html"><![CDATA[I'm always having these ideas for projects and papers, then I look around a bit, and I find someone that has already studied that idea and published it.
 It's genuinely annoying, It's been 6 months now, and all the papers are newly published (2021 mostly) so It's even more annoying.
 How do you deal with that ? and How do you find a niche that no one is touching.
 I just started a PhD, so It's really stressing me out. I feel like I'll never be able to advance on my thesis, and that I should just quit, because better work has already been done.
    submitted by    /u/AlanRoofies  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training a DQN agent for platformer game]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1q15v/training_a_dqn_agent_for_platformer_game/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1q15v/training_a_dqn_agent_for_platformer_game/"/>
        <updated>2022-04-12T03:53:17.000Z</updated>
        <summary type="html"><![CDATA[Does anyone have experience training agents to play platformer games like mario? I am trying to train an agent for the platformer game Jump King to get past atleast a few levels, using DQN but the agent is performing poorly after 8000 episodes of training, (one episode being the agent spawns at the start and has 15 seconds or so to jump around gaining reward) he is barely able to get past the first level most of the time :c
 I am using a very basic sequential network of 2 Linear layers with inputDim 4, outputDim 4, and hiddenDim 32. and because my state is not using any image data, its just (current_level, x_pos, y_pos, jumpCount) as input to the network . As for the reward, I am using the y position to give reward if the agent is getting to a new level (large reward) or making progress in the current level (curr_y > old_y), otherwise the agent gets a negative reward.
 Should I consider using a CNN and image data to train this agent like in the atari games paper, or is using image data and a conv net going to perform worse rather than using my current state? Should I consider combining image data with the current state, or just keeping the current non-image data state but ?
 Also, roughly how long should I be training the agent for? is 8000 episodes not enough? 1 episode takes roughly 7 seconds in time (it is using pygame engine and I turned off the rendering and I think that made it a little bit faster)
 This is my first time training an agent for a hard game like this using DQN so I would appreciate any tips/advice to improve the agent! repo: https://github.com/senweim/JumpKingAtHome
    submitted by    /u/TernaryJimbo  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Faster version of cv2.BFMatcher(cv2.NORM_L2) optimized for keypoints matching]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1oh1e/p_faster_version_of_cv2bfmatchercv2norm_l2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1oh1e/p_faster_version_of_cv2bfmatchercv2norm_l2/"/>
        <updated>2022-04-12T02:30:49.000Z</updated>
        <summary type="html"><![CDATA[Hi there, in the case if any of you use the openCV BFMatcher with NORM_L2, you can try to use my recent pet project: https://github.com/kmkolasinski/fast-bfmatcher 
 Basically the speed-up is achieved by using faster replacement for BLAS, a BLIS library and some custom implementations written in C and cython.
    submitted by    /u/kmkolasinski  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientific Functions in NumPy and SciPy]]></title>
        <id>https://machinelearningmastery.com/?p=13442</id>
        <link href="https://machinelearningmastery.com/scientific-functions-in-numpy-and-scipy/"/>
        <updated>2022-04-12T02:24:11.000Z</updated>
        <summary type="html"><![CDATA[Python is a general-purpose computation language, but it is very welcomed in scientific computing. It can replace R and Matlab […]
The post Scientific Functions in NumPy and SciPy appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Adrian Tam</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are there any comparison studies on learning rate schedules for generative transformers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1nt7m/d_are_there_any_comparison_studies_on_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1nt7m/d_are_there_any_comparison_studies_on_learning/"/>
        <updated>2022-04-12T01:58:05.000Z</updated>
        <summary type="html"><![CDATA[My current research heavily involves generative vision transformers and after some experimentation it seems like the choice of a LR scheduler is a crucial factor for proper convergence. Does anyone know of any comparison studies done recently that explore various types of schedulers for generative tasks?
    submitted by    /u/Megixist  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My epiphany on synthetic media five years later, and what I feel is coming within the next five years]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u1nch6/my_epiphany_on_synthetic_media_five_years_later/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u1nch6/my_epiphany_on_synthetic_media_five_years_later/"/>
        <updated>2022-04-12T01:34:42.000Z</updated>
        <summary type="html"><![CDATA[Roughly five years ago, I created this thread where I outlined my realization about the imminency of synthetic media. 
 This was before transformers blew up, before StyleGAN, before GPT-2, when WaveNet and DeepDream were still among the best we could do, and when predictive text algorithms that were barely better than Markov Chains were still the state of the art. In five short years, the state of artificial intelligence has changed overwhelmingly, to the point it's barely recognizable. Looking back to 2017, I now get this sense of everything feeling so primitive and fake. I've stated many times that AI before roughly 2019 was a bunch of digital magic tricks, and the field as a whole was essentially a giant Potemkin village that utilized clever sleight of hand and advertising to make it se…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Fine-Tuning LayoutLM v2 For Invoice Recognition]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1mgxu/n_finetuning_layoutlm_v2_for_invoice_recognition/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1mgxu/n_finetuning_layoutlm_v2_for_invoice_recognition/"/>
        <updated>2022-04-12T00:51:26.000Z</updated>
        <summary type="html"><![CDATA[With the advent of deep learning models, automated data extraction is becoming more accessible. In this article, we demonstrate step-by-step how to fine-tune layoutLM V2 on invoices starting from data annotation to model training and inference. 
 Enjoy the read and if you have any questions, leave them below.
    submitted by    /u/UBIAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which environment impress you? (related with software architecture, API, ...)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1lqa1/which_environment_impress_you_related_with/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1lqa1/which_environment_impress_you_related_with/"/>
        <updated>2022-04-12T00:14:26.000Z</updated>
        <summary type="html"><![CDATA[I want to hear about your impressive environment! Specifically, I want to make my custom environment well using various library like openAI gym.
 In this contxet, I find out the highway-env https://github.com/eleurent/highway-env/ !
 I think this environment has convinient API for users.
 Thus, I make my custom env with referencing the highway-env
 https://preview.redd.it/ndl1h1dvpzs81.png?width=711&format=png&auto=webp&s=267320a9713d98e7e49c4bb89423e5a9612bad8e
 In this line, could you speak your best environment? It doesn't matter about your best env has any advantage!
    submitted by    /u/Seungeon94  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategies to deal with Large Action Spaces]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1kiox/strategies_to_deal_with_large_action_spaces/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1kiox/strategies_to_deal_with_large_action_spaces/"/>
        <updated>2022-04-11T23:15:04.000Z</updated>
        <summary type="html"><![CDATA[Hey guys,
 I tried building a PPO model for Wordle.
 My initial test was checking the performance of the model with just 100 words. The agent was able to learn within a few thousand epochs and had an average guess length of about 2.8 before it could correctly identify the words.
 However, when i extend the action space to the entire 2.3k words, the model barely learns. Even after a few 100k iterations, the mean length revolves around 5.9 (given wordle has a max of 6 attempts per game)
 Any suggestions on how to help the agent learn faster in large action spaces?
 ​
 I also tried an embedding based approach, but the performance was very similar.
 ​
 Thanks
    submitted by    /u/altair9335  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Transformers replicate Hippocampal representations; notably place and grid cells in the brain]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1jbr2/r_transformers_replicate_hippocampal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1jbr2/r_transformers_replicate_hippocampal/"/>
        <updated>2022-04-11T22:19:32.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2112.04035
 Yes, the paper is cautious about comparing the model one-to-one to the brain
  
“Note, we are not saying the brain is closely related to transformers because it learns the same neural representations, instead we are saying the relationship is close because we have shown a mathematical relationship between transformers and carefully formulated neuroscience models of the hippocampal formation.”
  
While objections like "its just correlation/relation, its not exactly the same!!" are true to an extend, its still a very unexpected observation that, they're even remotely similar. Needless to say, Transformers were not inspired from the brain - and as more evidence collates (https://www.nature.com/articles/s42003-022-03036-1 --> Activations are linearly correlatable) it does feel mysterious; perhaps atleast some of the systems used by the brain converge on an efficient pattern discovered by our backpropogated friends...
 [insert 'coincidence? I think not!' meme]
    submitted by    /u/Competitive-Rub-1958  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI when given the prompt of “Amy Schumer” on wombo.art]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u1hmjr/ai_when_given_the_prompt_of_amy_schumer_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u1hmjr/ai_when_given_the_prompt_of_amy_schumer_on/"/>
        <updated>2022-04-11T21:06:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/9YearOldGeneralOfPew  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are your best results for ProcGen: CoinRun?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1eu6n/what_are_your_best_results_for_procgen_coinrun/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1eu6n/what_are_your_best_results_for_procgen_coinrun/"/>
        <updated>2022-04-11T19:06:45.000Z</updated>
        <summary type="html"><![CDATA[Has anyone managed to get a consistent score of > 9 on CoinRun? I understand that some generated levels require LSTMs in order to be solvable 100% of the time, but even excluding these hard-core levels I can see some occasions where my agents are not operating with 100% effectiveness. For some reason "fully solving" CoinRun seems harder than expected.
 The papers on CoinRun usually just show the results after 100mm steps or so, but I am more interested in what the community has achieved with "normal setups".
    submitted by    /u/tmuxed  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to train the NN model with a custom dataset?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u1drul/how_to_train_the_nn_model_with_a_custom_dataset/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u1drul/how_to_train_the_nn_model_with_a_custom_dataset/"/>
        <updated>2022-04-11T18:02:25.000Z</updated>
        <summary type="html"><![CDATA[Hi all.
 I have been trying to work on an object detection project. Basically trying to play around with the codes in the documentation for a custom dataset.
 I am using Yolov3 and I trained my model using darknet and seems like the model is learning it wrong because of which the weights are not correct either. I don't know how to check that but when doing forward propagation, the array seems to be ok, without nan values, but the confidence is mostly 0's and some 0.25's. Anyone who can guide me, on where I could have gone wrong?
 ​
 code: https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html [yolov3 secion]
 output: 
 outputs = [[0.03846154 0.03846154 0.27884614 0.21634616 0.5 0.25 ] [0.03846154 0.03846154 0. 0.47596154 0. 0. ] [0.03846154 0.03846154 0.89663464 0.78365386 0.5 0.25 ] ... [0.99038464 0.99038464 0.02403846 0.03125 0.5 0.25 ] [0.99038464 0.99038464 0.03846154 0.07211538 0.5 0. ] [0.99038464 0.99038464 0.07932692 0.05528846 0.5 0. ]] 
 confidence = 0.25 0.0 0.25 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.25 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.25 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 0.0 0.0 0.0 0.0 0.0 0.25 ...
    submitted by    /u/ersa17  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to use the same action in trained RL network, when model is retested?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1dnth/how_to_use_the_same_action_in_trained_rl_network/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1dnth/how_to_use_the_same_action_in_trained_rl_network/"/>
        <updated>2022-04-11T17:57:53.000Z</updated>
        <summary type="html"><![CDATA[I trained RL agent using stable baseline library and gym env. When I am trying to test agent, this makes different action when I am re running again. I used the same seed in test env.
 for i in range(length-lags-1): action, _states = model.predict(obs_test) obs_test, rewards, dones, info = env_test
 When I am runnig again the above code, I am getting the different results.
    submitted by    /u/Mariam_Dundua  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is the smallest, most capable, generative language model available now?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1c8ga/d_what_is_the_smallest_most_capable_generative/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1c8ga/d_what_is_the_smallest_most_capable_generative/"/>
        <updated>2022-04-11T16:55:07.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for a generative-LM equivalent of an EfficientNet-Lite, for inference on devices with limited to no VRAM. I know about some popular ones like DistilGPT2. But it's been 2 years after its release. Surely, someone improved their size/performance ratio, right... right?
 Thank you for your time. 🤗
    submitted by    /u/Deep-Station-1746  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How would you rank major tech companies' research labs for prestige? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u1bu8z/how_would_you_rank_major_tech_companies_research/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u1bu8z/how_would_you_rank_major_tech_companies_research/"/>
        <updated>2022-04-11T16:37:33.000Z</updated>
        <summary type="html"><![CDATA[This is just for fun, not to be taken too seriously. But I'm curious what are the reputations among the community for various research divisions (specifically AIML) of major companies, ie: Google, Facebook/Meta, Microsoft, Amazon, NVIDIA, IBM, etc.
 My perceived (albeit naive) view is Google > Facebook > MSR are top tier. Don't know much about the others. But I've read that some people consider MSR most prestigious due to their academic environment. But I've seen that Google and FB dominate in terms of major publications, ie: vision transformers are associated with Google. 
    submitted by    /u/avd4292  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI News: New Robot Fingertips Can Feel | AI Tracking Satellite | SingularityDAO DynaSets | Tesla Optimus Specs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u1bsev/ai_news_new_robot_fingertips_can_feel_ai_tracking/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u1bsev/ai_news_new_robot_fingertips_can_feel_ai_tracking/"/>
        <updated>2022-04-11T16:35:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/getrich_or_diemining  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[a quick high-level overview of diffusion models (like dall-e 2)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u1bbmu/a_quick_highlevel_overview_of_diffusion_models/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u1bbmu/a_quick_highlevel_overview_of_diffusion_models/"/>
        <updated>2022-04-11T16:14:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/individual_kex  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unity RL ml agents module, walker example]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u1bb4t/unity_rl_ml_agents_module_walker_example/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u1bb4t/unity_rl_ml_agents_module_walker_example/"/>
        <updated>2022-04-11T16:13:23.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I'm trying to teach my custom fbx model to walk with the help of ppo, as in the example from ml agents. I have difficulties with the exact import and the assignment of rigidbody here, that is, the neural network is being trained, but for some reason physics does not work. Has anyone seen it, or does anyone have an example of how to train a unity custom fbx model using ml agents?
 Thx all!
    submitted by    /u/IndependenceCivil576  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I train an AI to write articles based on my own work?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u1ae67/how_can_i_train_an_ai_to_write_articles_based_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u1ae67/how_can_i_train_an_ai_to_write_articles_based_on/"/>
        <updated>2022-04-11T15:30:04.000Z</updated>
        <summary type="html"><![CDATA[Hi all!
 As a sort of art experiment, I want to train an AI to write tech news articles based on my own work.
 I worked as a freelance writer for several years and have thousands of articles (each as a Word doc) on tech news.
 I want to use those articles to train the AI, then have it generate new articles to post to a blog. I have a pretty good understanding of machine learning, but have never trained a model myself. I'm hoping you all can provide some direction. Some specific questions:
  
Can you recommend a model?
 For each training article, can I provide a "source" (like another news article) so the AI understands where the content in the training article came from? *
 For each generated article, can I provide a news article source for it to base its content on? **
 Can I use the Word docs as the training set, or do I need to convert them into something else for training?
  
*as an example: If I wrote an article on the release of a new Raspberry Pi board, my source might be the press release on the Raspberry Pi website.
 **as an example: If I want it to generate an article about a new drone delivery service, my input source might be a news article on Reuters or something.
    submitted by    /u/TheSerialHobbyist  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Squirrel: A new OS library for fast & flexible large-scale data loading]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u19io6/p_squirrel_a_new_os_library_for_fast_flexible/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u19io6/p_squirrel_a_new_os_library_for_fast_flexible/"/>
        <updated>2022-04-11T14:50:45.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 Today we open-sourced Squirrel, a data infrastructure library that my colleagues and I have been working on over the past 1.5 years: https://github.com/merantix-momentum/squirrel-core
 We’re a team of ~30 ML engineers developing machine learning solutions for industry and research. Across all our projects, we need to load large-scale data in a fast and cost-efficient way, while keeping the flexibility to work with any possible dataset, loaded from local storage, remote data buckets or via APIs such as HuggingFace. Not finding what we were looking for, we decided to build it ourselves. 
 Squirrel has already proven its value in our deep learning projects at Merantix Momentum and shows competitive benchmark results (check them out here). 
 We’re super excited to share it with the OSS community and hope that you can benefit from it as well! 
 Looking forward to hearing your feedback and questions :)
    submitted by    /u/Nextpenade  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Renting lots of GPUs (100-200) in single environment?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u18ryi/p_renting_lots_of_gpus_100200_in_single/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u18ryi/p_renting_lots_of_gpus_100200_in_single/"/>
        <updated>2022-04-11T14:15:57.000Z</updated>
        <summary type="html"><![CDATA[I want to apply an already trained ML model on a huge textual data set. I have funds to rent Cloud GPUs, but have not much experience using them. Preferably, I do the setting up of the environment only once (downloading of the data, model, software packages, etc) only once and then simply send ~100-200 scripts each to their own GPU for processing. Then at the end everything is in the same location and I can easily send back the final result file (~100-200 output files concatenated together) back to my PC.
 Any advice on how to do that? All GPU renting servers only have 1-8 GPUs per server and do not (seem) to allow for sharing of the environment, which seems very inefficient to me. All comments are appreciated.
    submitted by    /u/Intelligent-End2673  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[are there any open source video ads generation model out there?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u18lrc/are_there_any_open_source_video_ads_generation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u18lrc/are_there_any_open_source_video_ads_generation/"/>
        <updated>2022-04-11T14:07:58.000Z</updated>
        <summary type="html"><![CDATA[Hey is there any models to generate videos for advertisment either as text-to-video images-to-video or video-variation creation, if not would video variation generative models would be a good fit for create ads ??
    submitted by    /u/National-Departure78  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DALL-E 2, the future of AI research, and OpenAI’s business model]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u179b4/dalle_2_the_future_of_ai_research_and_openais/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u179b4/dalle_2_the_future_of_ai_research_and_openais/"/>
        <updated>2022-04-11T13:02:20.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bendee983  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[how do i learn artificial intelligence from the basics?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u163kr/how_do_i_learn_artificial_intelligence_from_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u163kr/how_do_i_learn_artificial_intelligence_from_the/"/>
        <updated>2022-04-11T12:00:30.000Z</updated>
        <summary type="html"><![CDATA[Is there any resources which has example driven explanations, from scratch or basics? I have seen some websites just jumping into "use this module/library" Without explaining what it does or how it works, just some basic examples so that i can build on top or experiment by my own.
    submitted by    /u/-1Mbps  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] Algorithmic stability of minibatch SGD]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u153d3/rp_algorithmic_stability_of_minibatch_sgd/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u153d3/rp_algorithmic_stability_of_minibatch_sgd/"/>
        <updated>2022-04-11T11:01:01.000Z</updated>
        <summary type="html"><![CDATA[Hi, was wondering if anyone else has looked into "A PAC-Bayesian Analysis of Randomized Learning
 with Application to Stochastic Gradient Descent" and in particular eqn. 2, which is the derivation of Section 3.5 of "Train faster, generalize better: Stability of stochastic gradient descent" adapted for the case where the underlying loss we are interested in guaranteeing generalisation for is upper bounded by M (rather than 1 as assumed by Hardt et al).
 In the case of minibatch SGD, the number of datapoints n becomes the number of minibatches, as ideally one would like to reduce the number of steps T by maximizing the learning rate, which requires maximizing the minibatch size for the loss to actually converge to 0 on the training data. 
 However, what I am unsure about is, specifically for the classification task where we typically minimize the cross-entropy objective, whether the cross-entropy objective is an upper bound on any kind of M-bounded loss function. In the ideal world, I would like to show that it upper bounds the 0-1 loss which means the cross-entropy over the dataset is an upper bound on the classification accuracy and any generalization statement automatically becomes a statement about the very practical metric of accuracy. 
 Such a statement about cross-entropy upper-bounding 0-1 is made in Section 3C of "Theoretical Issues in Deep Networks: Approximation, Optimization and Generalization". However, one can provide a counterexample in the limit of the softmax "temperature" parameter where the predicted class distribution becomes uniform, in the case of 2 classes, for the typical case of log being the natural logarithm (it is no longer a counter-example if log base 2 is used).
 I haven't been able to show or find proof that this statement "xent >= 0-1" is true (for some logarithm base and some number of classes) and was hoping that someone might have.
    submitted by    /u/wakeupandshave  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using the NEAT algorithm to teach elves to deliver presents]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u14op0/using_the_neat_algorithm_to_teach_elves_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u14op0/using_the_neat_algorithm_to_teach_elves_to/"/>
        <updated>2022-04-11T10:35:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/zuparnowa  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implementing RL algorithm on apache spark]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u14990/implementing_rl_algorithm_on_apache_spark/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u14990/implementing_rl_algorithm_on_apache_spark/"/>
        <updated>2022-04-11T10:06:25.000Z</updated>
        <summary type="html"><![CDATA[I want to run RL algorithm on Apache Spark. However, RL does not exists in Spark's MLib. Is it possible to implement it? any links may help. Thank you in advance
    submitted by    /u/fatenLouati  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trippy AI Dream 16 - Gothic Style Jungle Fever - VQGAN CliP Rife-Rea...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u13z0g/trippy_ai_dream_16_gothic_style_jungle_fever/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u13z0g/trippy_ai_dream_16_gothic_style_jungle_fever/"/>
        <updated>2022-04-11T09:47:54.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LordPewPew777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trippy AI Dream 23 - Flower Power² VQGAN CliP Rife-RealESRGAN]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u13x95/trippy_ai_dream_23_flower_power²_vqgan_clip/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u13x95/trippy_ai_dream_23_flower_power²_vqgan_clip/"/>
        <updated>2022-04-11T09:44:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LordPewPew777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trippy AI Dream 32 - WE REACHED 100 SUBSCRIBERS !!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u13waw/trippy_ai_dream_32_we_reached_100_subscribers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u13waw/trippy_ai_dream_32_we_reached_100_subscribers/"/>
        <updated>2022-04-11T09:42:36.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LordPewPew777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MindSpore has implemented a visible-infrared recognition algorithm]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u12gyu/mindspore_has_implemented_a_visibleinfrared/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u12gyu/mindspore_has_implemented_a_visibleinfrared/"/>
        <updated>2022-04-11T08:00:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Creative_Habit_6868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Channel Augmented Joint Learning for Visible-Infrared Recognition]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u12eoc/r_channel_augmented_joint_learning_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u12eoc/r_channel_augmented_joint_learning_for/"/>
        <updated>2022-04-11T07:55:32.000Z</updated>
        <summary type="html"><![CDATA[Since going open source in March 2020, MindSpore gone from strength to strength. The deep learning framework has been downloaded by over 1.2 million users; algorithms running on MindSpore have been published in AI journals or presented at conferences; and countless developments have been released in device-edge-cloud scenarios to transform business fields, such as intelligent manufacturing, cloud, wireless, data communication, energy, and consumer business. 
 Built on extensive experience from the scientific, academic, and industrial sectors, MindSpore-based AI papers accounted for 11% of all AI papers in October 2021, ranking No.2 worldwide by month, and No.3 worldwide in Q4 2021. In this blog post, based on a paper published in ICCV 2021 by Professor Mang Ye of Wuhan University, we intro…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I want to learn AI From beginning ? from where can i start?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u10am1/i_want_to_learn_ai_from_beginning_from_where_can/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u10am1/i_want_to_learn_ai_from_beginning_from_where_can/"/>
        <updated>2022-04-11T05:26:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Late_Illustrator_545  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Baidu Researchers Propose PP-YOLOE Object Detector: an Evolved Version of YOLO Achieving SOTA Performance in Object Detection]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u0z8uu/baidu_researchers_propose_ppyoloe_object_detector/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u0z8uu/baidu_researchers_propose_ppyoloe_object_detector/"/>
        <updated>2022-04-11T04:21:10.000Z</updated>
        <summary type="html"><![CDATA[Object detection is a crucial problem in computer vision, and YOLO (You Only Look Once) one-stage object detectors have set the bar for performance since the release of YOLOv1 in 2015. The YOLO series has undergone considerable network and structural improvements over the years. The most recent version, YOLOX, has attained an optimal balance of speed and accuracy on the NVIDIA Tesla V100 Tensor Core GPU.
 Baidu researchers have improved their earlier PP-YOLOv2 model, resulting in PP-YOLOE, a cutting-edge industrial object detector that beats YOLOv5 and YOLOX in speed and accuracy trade-off. The team’s PP-YOLOE-l variant outperforms PP-YOLOv2 by 1.9 percent AP and YOLOX-l by 1.3 percent AP on COCO datasets.
 The PP-YOLOv2 baseline model architecture comprises a ResNet50-vd backbone with deformable convolution, a PAN neck with an SPP layer and DropBlock, and a lightweight IoU aware head. PP-YOLOv2 assigns only one anchor box to each ground truth object, similar to YOLOv3. It is strongly reliant on hand-crafted design, which may not generalize well enough when trained on other datasets. Conversely, this technique necessitates a lot of additional hyperparameters.
 To overcome this problem, Baidu researchers have added an anchor-free technique to PP-YOLOv2 that tiles one anchor point on each pixel and assigns upper and lower bounds for detecting heads to assign ground facts to a matching feature map. The center of a bounding box can then be determined to choose positive samples from the closest pixels. A 4D vector is also predicted for regression, with minor model speedups and precision losses due to the changes.
 Continue Reading
 Paper: https://arxiv.org/pdf/2203.16250.pdf
 Github: https://github.com/PaddlePaddle/PaddleDetection
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is reinforcement learning being used for the development of self-driving cars]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u0y1iv/is_reinforcement_learning_being_used_for_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u0y1iv/is_reinforcement_learning_being_used_for_the/"/>
        <updated>2022-04-11T03:10:19.000Z</updated>
        <summary type="html"><![CDATA[We will introduce the general process of self-driving tasks first and then the development of Reinforcement Learning in self-driving cars. 
 The general process of self-driving tasks includes perceiving, decision-making, planning and controlling. The tasks of perceiving have adopted deep learning and that did a good job. Being different from monitoring learning, decision intelligence AI methods, which are represented by reinforcement learning, model the environment as Markov Decision Process(MDP)to get optimization. In sequential decision problems the utility of agent's actions do not depend on single decisions, expressed with the state, which the agent would have gotten into, as the result of this decision, but rather on the whole sequence of agent's action. 
 Here, one thing that needs t…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Looking for Ideas in Pre-training a RL Agent]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0xgq5/r_looking_for_ideas_in_pretraining_a_rl_agent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0xgq5/r_looking_for_ideas_in_pretraining_a_rl_agent/"/>
        <updated>2022-04-11T02:38:36.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I've been working on reinforcement learning lately, but wanted to come to the general ML subreddit to seek inspiration from other disciplines.
 I've been working on strategies to decrease the training time for my real-world inverted pendulum experiment. Specifically, I am trying to pre-train the Q network in a simulation before deploying.
 The strategy that I have found most successful right now is this:
 start with randomly generated weights
 REPEAT OF AN EPOCH:
 - Load new_weights to Q Network
 - initialize an environment with randomly generated parameters (i.e. random mass, lengths, etc).
 - Train agent on environment for 100 episodes
 - Save new_weights
 I have tried a variety of strategies to add a little bit more control over this process. I've tried a soft update that never showed improvement.
 W = old_weights * (1 - alpha) + new_weights * alpha 
 I have tried an additive update which was slightly successful. Measured the success of each network as the sum of rewards over the epoch.
 A = (old_R)/(old_R+new_R) ; B = (new_R)/(old_R+new_R) W = old_weights * A + new_weights * B 
 But none of these work as well as just using the most recent weights. I've included some results if anyone's interested. The first graph is three test trials with random initial weights, the second graph is with pre-trained weights.
 This is a pretty hand-wavy way of doing this, does anyone have any suggestions to do this better?
 ​
 https://preview.redd.it/9mnewmibbts81.png?width=375&format=png&auto=webp&s=a6ff866a31987375d276d12f69dbe2af40380bf4
 https://preview.redd.it/096n7ctcbts81.png?width=375&format=png&auto=webp&s=b19d800fbb416fca288e86640d9458c8993e0759
 ​
    submitted by    /u/nickthorpie  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Song writing Ai]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u0xayx/song_writing_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u0xayx/song_writing_ai/"/>
        <updated>2022-04-11T02:29:33.000Z</updated>
        <summary type="html"><![CDATA[Hi all, 
 I’m hoping someone could point me in The direction of an AI that I could dump all my previous song writing into that would spit out something "inspired by’ it.
 Mostly a bit of fun but interested in seeing what it throws back out at me.
 thanks in advance for any hot tips.
    submitted by    /u/doccaballero  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Recommendations for high frequency multivariate time series data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0x8lr/p_recommendations_for_high_frequency_multivariate/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0x8lr/p_recommendations_for_high_frequency_multivariate/"/>
        <updated>2022-04-11T02:25:48.000Z</updated>
        <summary type="html"><![CDATA[Hey there! I'm looking for advice on datasets to use for a project. We are looking for the following traits:
 ​
 1) Multivariate (at least 3 or 4, and probably no more than 50 or 100 as an upper bound).
 2) High frequency (Ideally at least once every 5-10 minutes)
 3) We need to have some notion of an underlying 'state' of the data for certain windows. E.g. in an energy setting, period X was the 'family at home using appliances' state. Or in the healthcare setting, period X is 'the patient is in a stable state' while period Y is something like 'the patient experiences a cardiac event'
 ​
 ​
 Nice to have:
 4) It'd be great if some features had some level of seasonality while others didn't.
 ​
 ​
 Do folks have any recommendations for datasets that meet some (or hopefully all) of the criteria? I did some light pursuing on UCI, but it seems like much of it is not high frequency enough, and/or doesn't have some notion of underlying states.
    submitted by    /u/CS_Student95  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What to do when the authors don't release source code?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0vqjt/d_what_to_do_when_the_authors_dont_release_source/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0vqjt/d_what_to_do_when_the_authors_dont_release_source/"/>
        <updated>2022-04-11T01:04:44.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am currently working on a research paper that I aim to publish at a reputable conference shortly. In our work, we borrow a feature engineering technique from one of the papers that have not been previously applied to the domain (time series AD) before that paper. However, the authors haven't released the source code of their implementation of the model (but the feature engineering technique is publicly available). I feel like that is an important baseline and just failing to include it would get my paper rejected. I have contacted all the authors for the source code, but none of them responded. The architecture they use is a fairly complicated one and would be very difficult to implement on my own. How do I go about this situation? My advisor told me I can just include a few points on the footnote on why we don't include this as a baseline. Those being:
  
No open-source implementation
 Contacted the authors, didn't receive a response
 The paper has not been published, only uploaded to arxiv.
  
Any help is appreciated!
    submitted by    /u/mythrowaway0852  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-stage Training of Graph Neural Networks for Graph Classification. (arXiv:2011.05097v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05097</id>
        <link href="http://arxiv.org/abs/2011.05097"/>
        <updated>2022-04-11T00:52:29.155Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have received massive attention in the field of
machine learning on graphs. Inspired by the success of neural networks, a line
of research has been conducted to train GNNs to deal with various tasks, such
as node classification, graph classification, and link prediction. In this
work, our task of interest is graph classification. Several GNN models have
been proposed and shown great accuracy in this task. However, the question is
whether usual training methods fully realize the capacity of the GNN models.

In this work, we propose a two-stage training framework based on triplet
loss. In the first stage, GNN is trained to map each graph to a Euclidean-space
vector so that graphs of the same class are close while those of different
classes are mapped far apart. Once graphs are well-separated based on labels, a
classifier is trained to distinguish between different classes. This method is
generic in the sense that it is compatible with any GNN model. By adapting five
GNN models to our method, we demonstrate the consistent improvement in accuracy
and utilization of each GNN's allocated capacity over the original training
method of each model up to 5.4\% points in 12 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1"&gt;Manh Tuan Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"&gt;Noseong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kijung Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overlapping Spaces for Compact Graph Representations. (arXiv:2007.02445v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02445</id>
        <link href="http://arxiv.org/abs/2007.02445"/>
        <updated>2022-04-11T00:52:29.147Z</updated>
        <summary type="html"><![CDATA[Various non-trivial spaces are becoming popular for embedding structured data
such as graphs, texts, or images. Following spherical and hyperbolic spaces,
more general product spaces have been proposed. However, searching for the best
configuration of product space is a resource-intensive procedure, which reduces
the practical applicability of the idea. We generalize the concept of product
space and introduce an overlapping space that does not have the configuration
search problem. The main idea is to allow subsets of coordinates to be shared
between spaces of different types (Euclidean, hyperbolic, spherical). As a
result, parameter optimization automatically learns the optimal configuration.
Additionally, overlapping spaces allow for more compact representations since
their geometry is more complex. Our experiments confirm that overlapping spaces
outperform the competitors in graph embedding tasks. Here, we consider both
distortion setup, where the aim is to preserve distances, and ranking setup,
where the relative order should be preserved. The proposed method effectively
solves the problem and outperforms the competitors in both settings. We also
perform an empirical analysis in a realistic information retrieval task, where
we compare all spaces by incorporating them into DSSM. In this case, the
proposed overlapping space consistently achieves nearly optimal results without
any configuration tuning. This allows for reducing training time, which can be
significant in large-scale applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shevkunov_K/0/1/0/all/0/1"&gt;Kirill Shevkunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1"&gt;Liudmila Prokhorenkova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CONet: Channel Optimization for Convolutional Neural Networks. (arXiv:2108.06822v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06822</id>
        <link href="http://arxiv.org/abs/2108.06822"/>
        <updated>2022-04-11T00:52:29.140Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) has shifted network design from using human
intuition to leveraging search algorithms guided by evaluation metrics. We
study channel size optimization in convolutional neural networks (CNN) and
identify the role it plays in model accuracy and complexity. Current channel
size selection methods are generally limited by discrete sample spaces while
suffering from manual iteration and simple heuristics. To solve this, we
introduce an efficient dynamic scaling algorithm -- CONet -- that automatically
optimizes channel sizes across network layers for a given CNN. Two metrics --
"\textit{Rank}" and "\textit{Rank Average Slope}" -- are introduced to identify
the information accumulated in training. The algorithm dynamically scales
channel sizes up or down over a fixed searching phase. We conduct experiments
on CIFAR10/100 and ImageNet datasets and show that CONet can find efficient and
accurate architectures searched in ResNet, DARTS, and DARTS+ spaces that
outperform their baseline models.

This document supersedes previously published paper in ICCV2021-NeurArch
workshop. An additional section is included on manual scaling of channel size
in CNNs to numerically validate of the metrics used in searching optimum
channel configurations in CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1"&gt;Mahdi S. Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jia Shu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1"&gt;Andre Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jingxuan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1"&gt;Mathieu Tuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Sepehr Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadakia_A/0/1/0/all/0/1"&gt;Arsh Kadakia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1"&gt;Konstantinos N. Plataniotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-Free Sparse Bayesian Learning. (arXiv:2105.10439v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10439</id>
        <link href="http://arxiv.org/abs/2105.10439"/>
        <updated>2022-04-11T00:52:29.133Z</updated>
        <summary type="html"><![CDATA[Sparse Bayesian learning (SBL) is a powerful framework for tackling the
sparse coding problem while also providing uncertainty quantification. The most
popular inference algorithms for SBL exhibit prohibitively large computational
costs for high-dimensional problems due to the need to maintain a large
covariance matrix. To resolve this issue, we introduce a new method for
accelerating SBL inference -- named covariance-free expectation maximization
(CoFEM) -- that avoids explicit computation of the covariance matrix. CoFEM
solves multiple linear systems to obtain unbiased estimates of the posterior
statistics needed by SBL. This is accomplished by exploiting innovations from
numerical linear algebra such as preconditioned conjugate gradient and a
little-known diagonal estimation rule. For a large class of compressed sensing
matrices, we provide theoretical justifications for why our method scales well
in high-dimensional settings. Through simulations, we show that CoFEM can be up
to thousands of times faster than existing baselines without sacrificing coding
accuracy. Through applications to calcium imaging deconvolution and
multi-contrast MRI reconstruction, we show that CoFEM enables SBL to tractably
tackle high-dimensional sparse coding problems of practical interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1"&gt;Alexander Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1"&gt;Andrew H. Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1"&gt;Berkin Bilgic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ba_D/0/1/0/all/0/1"&gt;Demba Ba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood Inference from Sampled Trajectories. (arXiv:2106.01808v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01808</id>
        <link href="http://arxiv.org/abs/2106.01808"/>
        <updated>2022-04-11T00:52:29.125Z</updated>
        <summary type="html"><![CDATA[Simulation-based inference enables learning the parameters of a model even
when its likelihood cannot be computed in practice. One class of methods uses
data simulated with different parameters to infer models of the
likelihood-to-evidence ratio, or equivalently the posterior function. Here we
frame the inference task as an estimation of an energy function parametrized
with an artificial neural network. We present an intuitive approach where the
optimal model of the likelihood-to-evidence ratio is found by maximizing the
likelihood of simulated data. Within this framework, the connection between the
task of simulation-based inference and mutual information maximization is
clear, and we show how several known methods of posterior estimation relate to
alternative lower bounds to mutual information. These distinct objective
functions aim at the same optimal energy form and therefore can be directly
benchmarked. We compare their accuracy in the inference of model parameters,
focusing on four dynamical systems that encompass common challenges in time
series analysis: dynamics driven by multiplicative noise, nonlinear
interactions, chaotic behavior, and high-dimensional parameter space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isacchini_G/0/1/0/all/0/1"&gt;Giulio Isacchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spisak_N/0/1/0/all/0/1"&gt;Natanael Spisak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nourmohammad_A/0/1/0/all/0/1"&gt;Armita Nourmohammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mora_T/0/1/0/all/0/1"&gt;Thierry Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walczak_A/0/1/0/all/0/1"&gt;Aleksandra M. Walczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Polynomial Transformations. (arXiv:2204.04209v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.04209</id>
        <link href="http://arxiv.org/abs/2204.04209"/>
        <updated>2022-04-11T00:52:29.101Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning high dimensional polynomial
transformations of Gaussians. Given samples of the form $p(x)$, where $x\sim
N(0, \mathrm{Id}_r)$ is hidden and $p: \mathbb{R}^r \to \mathbb{R}^d$ is a
function where every output coordinate is a low-degree polynomial, the goal is
to learn the distribution over $p(x)$. This problem is natural in its own
right, but is also an important special case of learning deep generative
models, namely pushforwards of Gaussians under two-layer neural networks with
polynomial activations. Understanding the learnability of such generative
models is crucial to understanding why they perform so well in practice.

Our first main result is a polynomial-time algorithm for learning quadratic
transformations of Gaussians in a smoothed setting. Our second main result is a
polynomial-time algorithm for learning constant-degree polynomial
transformations of Gaussian in a smoothed setting, when the rank of the
associated tensors is small. In fact our results extend to any
rotation-invariant input distribution, not just Gaussian. These are the first
end-to-end guarantees for learning a pushforward under a neural network with
more than one layer.

Along the way, we also give the first polynomial-time algorithms with
provable guarantees for tensor ring decomposition, a popular generalization of
tensor decomposition that is used in practice to implicitly store large
tensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sitan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring AI Systems Beyond Accuracy. (arXiv:2204.04211v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2204.04211</id>
        <link href="http://arxiv.org/abs/2204.04211"/>
        <updated>2022-04-11T00:52:29.094Z</updated>
        <summary type="html"><![CDATA[Current test and evaluation (T&E) methods for assessing machine learning (ML)
system performance often rely on incomplete metrics. Testing is additionally
often siloed from the other phases of the ML system lifecycle. Research
investigating cross-domain approaches to ML T&E is needed to drive the state of
the art forward and to build an Artificial Intelligence (AI) engineering
discipline. This paper advocates for a robust, integrated approach to testing
by outlining six key questions for guiding a holistic T&E strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turri_V/0/1/0/all/0/1"&gt;Violet Turri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dzombak_R/0/1/0/all/0/1"&gt;Rachel Dzombak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heim_E/0/1/0/all/0/1"&gt;Eric Heim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanHoudnos_N/0/1/0/all/0/1"&gt;Nathan VanHoudnos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palat_J/0/1/0/all/0/1"&gt;Jay Palat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Anusha Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-Based Vulnerability Analysis of Cyber-Physical Systems. (arXiv:2103.06271v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06271</id>
        <link href="http://arxiv.org/abs/2103.06271"/>
        <updated>2022-04-11T00:52:29.087Z</updated>
        <summary type="html"><![CDATA[This work focuses on the use of deep learning for vulnerability analysis of
cyber-physical systems (CPS). Specifically, we consider a control architecture
widely used in CPS (e.g., robotics), where the low-level control is based on
e.g., the extended Kalman filter (EKF) and an anomaly detector. To facilitate
analyzing the impact potential sensing attacks could have, our objective is to
develop learning-enabled attack generators capable of designing stealthy
attacks that maximally degrade system operation. We show how such problem can
be cast within a learning-based grey-box framework where parts of the runtime
information are known to the attacker, and introduce two models based on
feed-forward neural networks (FNN); both models are trained offline, using a
cost function that combines the attack effects on the estimation error and the
residual signal used for anomaly detection, so that the trained models are
capable of recursively generating such effective sensor attacks in real-time.
The effectiveness of the proposed methods is illustrated on several case
studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khazraei_A/0/1/0/all/0/1"&gt;Amir Khazraei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hallyburton_S/0/1/0/all/0/1"&gt;Spencer Hallyburton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qitong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1"&gt;Miroslav Pajic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Low-Cost Robot Science Kit for Education with Symbolic Regression for Hypothesis Discovery and Validation. (arXiv:2204.04187v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2204.04187</id>
        <link href="http://arxiv.org/abs/2204.04187"/>
        <updated>2022-04-11T00:52:29.079Z</updated>
        <summary type="html"><![CDATA[The next generation of physical science involves robot scientists -
autonomous physical science systems capable of experimental design, execution,
and analysis in a closed loop. Such systems have shown real-world success for
scientific exploration and discovery, including the first discovery of a
best-in-class material. To build and use these systems, the next generation
workforce requires expertise in diverse areas including ML, control systems,
measurement science, materials synthesis, decision theory, among others.
However, education is lagging. Educators need a low-cost, easy-to-use platform
to teach the required skills. Industry can also use such a platform for
developing and evaluating autonomous physical science methodologies. We present
the next generation in science education, a kit for building a low-cost
autonomous scientist. The kit was used during two courses at the University of
Maryland to teach undergraduate and graduate students autonomous physical
science. We discuss its use in the course and its greater capability to teach
the dual tasks of autonomous model exploration, optimization, and
determination, with an example of autonomous experimental "discovery" of the
Henderson-Hasselbalch equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Saar_L/0/1/0/all/0/1"&gt;Logan Saar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liang_H/0/1/0/all/0/1"&gt;Haotong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wang_A/0/1/0/all/0/1"&gt;Alex Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+McDannald_A/0/1/0/all/0/1"&gt;Austin McDannald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Rodriguez_E/0/1/0/all/0/1"&gt;Efrain Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Takeuchi_I/0/1/0/all/0/1"&gt;Ichiro Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kusne_A/0/1/0/all/0/1"&gt;A. Gilad Kusne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural graph embeddings via matrix factorization for link prediction: smoothing or truncating negatives?. (arXiv:2011.09907v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09907</id>
        <link href="http://arxiv.org/abs/2011.09907"/>
        <updated>2022-04-11T00:52:29.060Z</updated>
        <summary type="html"><![CDATA[Learning good quality neural graph embeddings has long been achieved by
minimzing the pointwise mutual information (PMI) for co-occuring nodes in
simulated random walks. This design choice has been mostly popularized by the
direct application of the highly-successful word embedding algorithm word2vec
to predicting the formation of new links in social, co-citation, and biological
networks. However, such a skeumorphic design of graph embedding methods entails
a truncation of information coming from pairs of nodes with low PMI. To
circumvent this issue, we propose an improved approach to learning low-rank
factorization embeddings that incorporate information from such unlikely pairs
of nodes and show that it can improve the link prediction performance of
baseline methods from 1.2% to 24.2%. Based on our results and observations we
outline further steps that could improve the design of next graph embedding
algorithms that are based on matrix factorizaion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agibetov_A/0/1/0/all/0/1"&gt;Asan Agibetov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Data Augmentation Selection and Parametrization in Contrastive Self-Supervised Speech Representation Learning. (arXiv:2204.04170v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.04170</id>
        <link href="http://arxiv.org/abs/2204.04170"/>
        <updated>2022-04-11T00:52:29.052Z</updated>
        <summary type="html"><![CDATA[Contrastive learning enables learning useful audio and speech representations
without ground-truth labels by maximizing the similarity between latent
representations of similar signal segments. In this framework various data
augmentation techniques are usually exploited to help enforce desired
invariances within the learned representations, improving performance on
various audio tasks thanks to more robust embeddings. Now, selecting the most
relevant augmentations has proven crucial for better downstream performances.
Thus, this work introduces a conditional independance-based method which allows
for automatically selecting a suitable distribution on the choice of
augmentations and their parametrization from a set of predefined ones, for
contrastive self-supervised pre-training. This is performed with respect to a
downstream task of interest, hence saving a costly hyper-parameter search.
Experiments performed on two different downstream tasks validate the proposed
approach showing better results than experimenting without augmentation or with
baseline augmentations. We furthermore conduct a qualitative analysis of the
automatically selected augmentations and their variation according to the
considered final downstream dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1"&gt;Salah Zaiem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1"&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essid_S/0/1/0/all/0/1"&gt;Slim Essid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement. (arXiv:2102.05185v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05185</id>
        <link href="http://arxiv.org/abs/2102.05185"/>
        <updated>2022-04-11T00:52:29.045Z</updated>
        <summary type="html"><![CDATA[In representation learning, there has been recent interest in developing
algorithms to disentangle the ground-truth generative factors behind a dataset,
and metrics to quantify how fully this occurs. However, these algorithms and
metrics often assume that both representations and ground-truth factors are
flat, continuous, and factorized, whereas many real-world generative processes
involve rich hierarchical structure, mixtures of discrete and continuous
variables with dependence between them, and even varying intrinsic
dimensionality. In this work, we develop benchmarks, algorithms, and metrics
for learning such hierarchical representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1"&gt;Andrew Slavin Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive dynamic programming for nonaffine nonlinear optimal control problem with state constraints. (arXiv:1911.11397v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.11397</id>
        <link href="http://arxiv.org/abs/1911.11397"/>
        <updated>2022-04-11T00:52:29.037Z</updated>
        <summary type="html"><![CDATA[This paper presents a constrained adaptive dynamic programming (CADP)
algorithm to solve general nonlinear nonaffine optimal control problems with
known dynamics. Unlike previous ADP algorithms, it can directly deal with
problems with state constraints. Firstly, a constrained generalized policy
iteration (CGPI) framework is developed to handle state constraints by
transforming the traditional policy improvement process into a constrained
policy optimization problem. Next, we propose an actor-critic variant of CGPI,
called CADP, in which both policy and value functions are approximated by
multi-layer neural networks to directly map the system states to control inputs
and value function, respectively. CADP linearizes the constrained optimization
problem locally into a quadratically constrained linear programming problem,
and then obtains the optimal update of the policy network by solving its dual
problem. A trust region constraint is added to prevent excessive policy update,
thus ensuring linearization accuracy. We determine the feasibility of the
policy optimization problem by calculating the minimum trust region boundary
and update the policy using two recovery rules when infeasible. The vehicle
control problem in the path-tracking task is used to demonstrate the
effectiveness of this proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jingliang Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengbo Eben Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zhenzhong Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Bo Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-Coder: Program Synthesis for Tensor Manipulations. (arXiv:2003.09040v4 [cs.PL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.09040</id>
        <link href="http://arxiv.org/abs/2003.09040"/>
        <updated>2022-04-11T00:52:29.028Z</updated>
        <summary type="html"><![CDATA[The success and popularity of deep learning is on the rise, partially due to
powerful deep learning frameworks such as TensorFlow and PyTorch that make it
easier to develop deep learning models. However, these libraries also come with
steep learning curves, since programming in these frameworks is quite different
from traditional imperative programming with explicit loops and conditionals.
In this work, we present a tool called TF-Coder for programming by example in
TensorFlow. TF-Coder uses a bottom-up weighted enumerative search, with
value-based pruning of equivalent expressions and flexible type- and
value-based filtering to ensure that expressions adhere to various requirements
imposed by the TensorFlow library. We train models to predict TensorFlow
operations from features of the input and output tensors and natural language
descriptions of tasks, to prioritize relevant operations during search.
TF-Coder solves 63 of 70 real-world tasks within 5 minutes, sometimes finding
simpler solutions in less time compared to experienced human programmers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kensen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1"&gt;David Bieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Karaoker: Alignment-free singing voice synthesis with speech training data. (arXiv:2204.04127v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.04127</id>
        <link href="http://arxiv.org/abs/2204.04127"/>
        <updated>2022-04-11T00:52:29.008Z</updated>
        <summary type="html"><![CDATA[Existing singing voice synthesis models (SVS) are usually trained on singing
data and depend on either error-prone time-alignment and duration features or
explicit music score information. In this paper, we propose Karaoker, a
multispeaker Tacotron-based model conditioned on voice characteristic features
that is trained exclusively on spoken data without requiring time-alignments.
Karaoker synthesizes singing voice following a multi-dimensional template
extracted from a source waveform of an unseen speaker/singer. The model is
jointly conditioned with a single deep convolutional encoder on continuous data
including pitch, intensity, harmonicity, formants, cepstral peak prominence and
octaves. We extend the text-to-speech training objective with feature
reconstruction, classification and speaker identification tasks that guide the
model to an accurate result. Except for multi-tasking, we also employ a
Wasserstein GAN training scheme as well as new losses on the acoustic model's
output to further refine the quality of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kakoulidis_P/0/1/0/all/0/1"&gt;Panos Kakoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ellinas_N/0/1/0/all/0/1"&gt;Nikolaos Ellinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vamvoukakis_G/0/1/0/all/0/1"&gt;Georgios Vamvoukakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Markopoulos_K/0/1/0/all/0/1"&gt;Konstantinos Markopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sung_J/0/1/0/all/0/1"&gt;June Sig Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jho_G/0/1/0/all/0/1"&gt;Gunu Jho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsiakoulis_P/0/1/0/all/0/1"&gt;Pirros Tsiakoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chalamandaris_A/0/1/0/all/0/1"&gt;Aimilios Chalamandaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Stochastic Extragradient for Bilinear Games using Restarted Iteration Averaging. (arXiv:2107.00464v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00464</id>
        <link href="http://arxiv.org/abs/2107.00464"/>
        <updated>2022-04-11T00:52:28.995Z</updated>
        <summary type="html"><![CDATA[We study the stochastic bilinear minimax optimization problem, presenting an
analysis of the same-sample Stochastic ExtraGradient (SEG) method with constant
step size, and presenting variations of the method that yield favorable
convergence. In sharp contrasts with the basic SEG method whose last iterate
only contracts to a fixed neighborhood of the Nash equilibrium, SEG augmented
with iteration averaging provably converges to the Nash equilibrium under the
same standard settings, and such a rate is further improved by incorporating a
scheduled restarting procedure. In the interpolation setting where noise
vanishes at the Nash equilibrium, we achieve an optimal convergence rate up to
tight constants. We present numerical experiments that validate our theoretical
findings and demonstrate the effectiveness of the SEG method when equipped with
iteration averaging and restarting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_C/0/1/0/all/0/1"&gt;Chris Junchi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Loizou_N/0/1/0/all/0/1"&gt;Nicolas Loizou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPSAF: A Generalized Probabilistic Surrogate-Assisted Framework for Constrained Single- and Multi-objective Optimization. (arXiv:2204.04054v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2204.04054</id>
        <link href="http://arxiv.org/abs/2204.04054"/>
        <updated>2022-04-11T00:52:28.988Z</updated>
        <summary type="html"><![CDATA[Significant effort has been made to solve computationally expensive
optimization problems in the past two decades, and various optimization methods
incorporating surrogates into optimization have been proposed. Most research
focuses on either exploiting the surrogate by defining a utility optimization
problem or customizing an existing optimization method to use one or multiple
approximation models. However, only a little attention has been paid to generic
concepts applicable to different types of algorithms and optimization problems
simultaneously. Thus this paper proposes a generalized probabilistic
surrogate-assisted framework (GPSAF), applicable to a broad category of
unconstrained and constrained, single- and multi-objective optimization
algorithms. The idea is based on a surrogate assisting an existing optimization
method. The assistance is based on two distinct phases, one facilitating
exploration and another exploiting the surrogates. The exploration and
exploitation of surrogates are automatically balanced by performing a
probabilistic knockout tournament among different clusters of solutions. A
study of multiple well-known population-based optimization algorithms is
conducted with and without the proposed surrogate assistance on single- and
multi-objective optimization problems with a maximum solution evaluation budget
of 300 or less. The results indicate the effectiveness of applying GPSAF to an
optimization algorithm and the competitiveness with other surrogate-assisted
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Blank_J/0/1/0/all/0/1"&gt;Julian Blank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Deb_K/0/1/0/all/0/1"&gt;Kalyanmoy Deb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Checking HateCheck: a cross-functional analysis of behaviour-aware learning for hate speech detection. (arXiv:2204.04042v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.04042</id>
        <link href="http://arxiv.org/abs/2204.04042"/>
        <updated>2022-04-11T00:52:28.970Z</updated>
        <summary type="html"><![CDATA[Behavioural testing -- verifying system capabilities by validating
human-designed input-output pairs -- is an alternative evaluation method of
natural language processing systems proposed to address the shortcomings of the
standard approach: computing metrics on held-out data. While behavioural tests
capture human prior knowledge and insights, there has been little exploration
on how to leverage them for model training and development. With this in mind,
we explore behaviour-aware learning by examining several fine-tuning schemes
using HateCheck, a suite of functional tests for hate speech detection systems.
To address potential pitfalls of training on data originally intended for
evaluation, we train and evaluate models on different configurations of
HateCheck by holding out categories of test cases, which enables us to estimate
performance on potentially overlooked system properties. The fine-tuning
procedure led to improvements in the classification accuracy of held-out
functionalities and identity groups, suggesting that models can potentially
generalise to overlooked functionalities. However, performance on held-out
functionality classes and i.i.d. hate speech detection data decreased, which
indicates that generalisation occurs mostly across functionalities from the
same class and that the procedure led to overfitting to the HateCheck data
distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_P/0/1/0/all/0/1"&gt;Pedro Henrique Luz de Araujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1"&gt;Benjamin Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural network training under semidefinite constraints. (arXiv:2201.00632v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.00632</id>
        <link href="http://arxiv.org/abs/2201.00632"/>
        <updated>2022-04-11T00:52:28.958Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with the training of neural networks (NNs) under
semidefinite constraints, which allows for NN training with robustness and
stability guarantees. In particular, we set up an efficient and scalable
training scheme for NN training problems of this kind based on interior point
methods, while we also exploit the structure of the underlying matrix
constraint. We apply our training scheme to several relevant examples that have
been studied in the literature and newly present the application of the method
to the training of Wasserstein generative adversarial networks (WGANs). In
numerical examples, we show the superiority of our method and its applicability
to WGAN training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pauli_P/0/1/0/all/0/1"&gt;Patricia Pauli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funcke_N/0/1/0/all/0/1"&gt;Niklas Funcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramlich_D/0/1/0/all/0/1"&gt;Dennis Gramlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Msalmi_M/0/1/0/all/0/1"&gt;Mohamed Amine Msalmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allgower_F/0/1/0/all/0/1"&gt;Frank Allg&amp;#xf6;wer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-Aware Predictive Monitoring of Business Processes. (arXiv:2104.09962v2 [cs.AI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2104.09962</id>
        <link href="http://arxiv.org/abs/2104.09962"/>
        <updated>2022-04-11T00:52:28.957Z</updated>
        <summary type="html"><![CDATA[The real-time prediction of business processes using historical event data is
an important capability of modern business process monitoring systems. Existing
process prediction methods are able to also exploit the data perspective of
recorded events, in addition to the control-flow perspective. However, while
well-structured numerical or categorical attributes are considered in many
prediction techniques, almost no technique is able to utilize text documents
written in natural language, which can hold information critical to the
prediction task. In this paper, we illustrate the design, implementation, and
evaluation of a novel text-aware process prediction model based on Long
Short-Term Memory (LSTM) neural networks and natural language models. The
proposed model can take categorical, numerical and textual attributes in event
data into account to predict the activity and timestamp of the next event, the
outcome, and the cycle time of a running process instance. Experiments show
that the text-aware model is able to outperform state-of-the-art process
prediction methods on simulated and real-world event logs containing textual
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1"&gt;Marco Pegoraro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uysal_M/0/1/0/all/0/1"&gt;Merih Seran Uysal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgi_D/0/1/0/all/0/1"&gt;David Benedikt Georgi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1"&gt;Wil M.P. van der Aalst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Optimization for Reinforcement Learning Tasks Using Sparse Computations. (arXiv:2201.02571v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.02571</id>
        <link href="http://arxiv.org/abs/2201.02571"/>
        <updated>2022-04-11T00:52:28.956Z</updated>
        <summary type="html"><![CDATA[This article proposes a sparse computation-based method for optimizing neural
networks for reinforcement learning (RL) tasks. This method combines two ideas:
neural network pruning and taking into account input data correlations; it
makes it possible to update neuron states only when changes in them exceed a
certain threshold. It significantly reduces the number of multiplications when
running neural networks. We tested different RL tasks and achieved 20-150x
reduction in the number of multiplications. There were no substantial
performance losses; sometimes the performance even improved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivanov_D/0/1/0/all/0/1"&gt;Dmitry Ivanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiselev_M/0/1/0/all/0/1"&gt;Mikhail Kiselev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larionov_D/0/1/0/all/0/1"&gt;Denis Larionov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Manifold View of Adversarial Risk. (arXiv:2203.13277v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.13277</id>
        <link href="http://arxiv.org/abs/2203.13277"/>
        <updated>2022-04-11T00:52:28.956Z</updated>
        <summary type="html"><![CDATA[The adversarial risk of a machine learning model has been widely studied.
Most previous works assume that the data lies in the whole ambient space. We
propose to take a new angle and take the manifold assumption into
consideration. Assuming data lies in a manifold, we investigate two new types
of adversarial risk, the normal adversarial risk due to perturbation along
normal direction, and the in-manifold adversarial risk due to perturbation
within the manifold. We prove that the classic adversarial risk can be bounded
from both sides using the normal and in-manifold adversarial risks. We also
show with a surprisingly pessimistic case that the standard adversarial risk
can be nonzero even when both normal and in-manifold risks are zero. We
finalize the paper with empirical studies supporting our theoretical results.
Our results suggest the possibility of improving the robustness of a classifier
by only focusing on the normal adversarial risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenjia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yikai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaoling Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1"&gt;Mayank Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1"&gt;Dimitris Metaxas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An analysis of over-sampling labeled data in semi-supervised learning with FixMatch. (arXiv:2201.00604v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.00604</id>
        <link href="http://arxiv.org/abs/2201.00604"/>
        <updated>2022-04-11T00:52:28.955Z</updated>
        <summary type="html"><![CDATA[Most semi-supervised learning methods over-sample labeled data when
constructing training mini-batches. This paper studies whether this common
practice improves learning and how. We compare it to an alternative setting
where each mini-batch is uniformly sampled from all the training data, labeled
or not, which greatly reduces direct supervision from true labels in typical
low-label regimes. However, this simpler setting can also be seen as more
general and even necessary in multi-task problems where over-sampling labeled
data would become intractable. Our experiments on semi-supervised CIFAR-10
image classification using FixMatch show a performance drop when using the
uniform sampling approach which diminishes when the amount of labeled data or
the training time increases. Further, we analyse the training dynamics to
understand how over-sampling of labeled data compares to uniform sampling. Our
main finding is that over-sampling is especially beneficial early in training
but gets less important in the later stages when more pseudo-labels become
correct. Nevertheless, we also find that keeping some true labels remains
important to avoid the accumulation of confirmation errors from incorrect
pseudo-labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabadan_M/0/1/0/all/0/1"&gt;Miquel Mart&amp;#xed; i Rabad&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bujwid_S/0/1/0/all/0/1"&gt;Sebastian Bujwid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pieropan_A/0/1/0/all/0/1"&gt;Alessandro Pieropan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1"&gt;Hossein Azizpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maki_A/0/1/0/all/0/1"&gt;Atsuto Maki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey. (arXiv:2203.14009v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.14009</id>
        <link href="http://arxiv.org/abs/2203.14009"/>
        <updated>2022-04-11T00:52:28.954Z</updated>
        <summary type="html"><![CDATA[Deep neuroevolution and deep Reinforcement Learning have received a lot of
attention in the last years. Some works have compared them, highlighting theirs
pros and cons, but an emerging trend consists in combining them so as to
benefit from the best of both worlds. In this paper, we provide a survey of
this emerging trend by organizing the literature into related groups of works
and casting all the existing combinations in each group into a generic
framework. We systematically cover all easily available papers irrespective of
their publication status, focusing on the combination mechanisms rather than on
the experimental results. In total, we cover 45 algorithms more recent than
2017. We hope this effort will favor the growth of the domain by facilitating
the understanding of the relationships between the methods, leading to deeper
analyses, outlining missing useful comparisons and suggesting new combinations
of mechanisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1"&gt;Olivier Sigaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Method Based On Neural Tangent Kernels. (arXiv:2204.04090v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.04090</id>
        <link href="http://arxiv.org/abs/2204.04090"/>
        <updated>2022-04-11T00:52:28.943Z</updated>
        <summary type="html"><![CDATA[The recent development of Generative adversarial networks (GANs) has driven
many computer vision applications. Despite the great synthesis quality,
training GANs often confronts several issues, including non-convergence, mode
collapse, and gradient vanishing. There exist several workarounds, for example,
regularizing Lipschitz continuity and adopting Wasserstein distance. Although
these methods can partially solve the problems, we argue that the problems are
result from modeling the discriminator with deep neural networks. In this
paper, we base on newly derived deep neural network theories called Neural
Tangent Kernel (NTK) and propose a new generative algorithm called generative
adversarial NTK (GA-NTK). The GA-NTK models the discriminator as a Gaussian
Process (GP). With the help of the NTK theories, the training dynamics of
GA-NTK can be described with a closed-form formula. To synthesize data with the
closed-form formula, the objectives can be simplified into a single-level
adversarial optimization problem. We conduct extensive experiments on
real-world datasets, and the results show that GA-NTK can generate images
comparable to those by GANs but is much easier to train under various
conditions. We also study the current limitations of GA-NTK and propose some
workarounds to make GA-NTK more practical.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu-Rong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1"&gt;Sheng Yen Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shan-Hung Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranking with submodular functions on a budget. (arXiv:2204.04168v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2204.04168</id>
        <link href="http://arxiv.org/abs/2204.04168"/>
        <updated>2022-04-11T00:52:28.935Z</updated>
        <summary type="html"><![CDATA[Submodular maximization has been the backbone of many important
machine-learning problems, and has applications to viral marketing,
diversification, sensor placement, and more. However, the study of maximizing
submodular functions has mainly been restricted in the context of selecting a
set of items. On the other hand, many real-world applications require a
solution that is a ranking over a set of items. The problem of ranking in the
context of submodular function maximization has been considered before, but to
a much lesser extent than item-selection formulations. In this paper, we
explore a novel formulation for ranking items with submodular valuations and
budget constraints. We refer to this problem as max-submodular ranking (MSR).
In more detail, given a set of items and a set of non-decreasing submodular
functions, where each function is associated with a budget, we aim to find a
ranking of the set of items that maximizes the sum of values achieved by all
functions under the budget constraints. For the MSR problem with cardinality-
and knapsack-type budget constraints we propose practical algorithms with
approximation guarantees. In addition, we perform an empirical evaluation,
which demonstrates the superior performance of the proposed algorithms against
strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guangyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tatti_N/0/1/0/all/0/1"&gt;Nikolaj Tatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gionis_A/0/1/0/all/0/1"&gt;Aristides Gionis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity versus Depth: An Information Theoretic Analysis. (arXiv:2203.00246v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.00246</id>
        <link href="http://arxiv.org/abs/2203.00246"/>
        <updated>2022-04-11T00:52:28.927Z</updated>
        <summary type="html"><![CDATA[Deep learning has proven effective across a range of data sets. In light of
this, a natural inquiry is: "for what data generating processes can deep
learning succeed?" In this work, we study the sample complexity of learning
multilayer data generating processes of a sort for which deep neural networks
seem to be suited. We develop general and elegant information-theoretic tools
that accommodate analysis of any data generating process -- shallow or deep,
parametric or nonparametric, noiseless or noisy. We then use these tools to
characterize the dependence of sample complexity on the depth of multilayer
processes. Our results indicate roughly linear dependence on depth. This is in
contrast to previous results that suggest exponential or high-order polynomial
dependence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hong Jun Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures. (arXiv:2112.05224v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.05224</id>
        <link href="http://arxiv.org/abs/2112.05224"/>
        <updated>2022-04-11T00:52:28.924Z</updated>
        <summary type="html"><![CDATA[We investigate a new threat to neural sequence-to-sequence (seq2seq) models:
training-time attacks that cause models to "spin" their outputs so as to
support an adversary-chosen sentiment or point of view -- but only when the
input contains adversary-chosen trigger words. For example, a spinned
summarization model outputs positive summaries of any text that mentions the
name of some individual or organization.

Model spinning introduces a "meta-backdoor" into a model. Whereas
conventional backdoors cause models to produce incorrect outputs on inputs with
the trigger, outputs of spinned models preserve context and maintain standard
accuracy metrics, yet also satisfy a meta-task chosen by the adversary.

Model spinning enables propaganda-as-a-service, where propaganda is defined
as biased speech. An adversary can create customized language models that
produce desired spins for chosen triggers, then deploy these models to generate
disinformation (a platform attack), or else inject them into ML training
pipelines (a supply-chain attack), transferring malicious functionality to
downstream models trained by victims.

To demonstrate the feasibility of model spinning, we develop a new
backdooring technique. It stacks an adversarial meta-task onto a seq2seq model,
backpropagates the desired meta-task output to points in the word-embedding
space we call "pseudo-words," and uses pseudo-words to shift the entire output
distribution of the seq2seq model. We evaluate this attack on language
generation, summarization, and translation models with different triggers and
meta-tasks such as sentiment, toxicity, and entailment. Spinned models largely
maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs
to satisfy the adversary's meta-task. We also show that, in the case of a
supply-chain attack, the spin functionality transfers to downstream models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1"&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1"&gt;Vitaly Shmatikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Adaptive Batchnorm for Personalized Healthcare. (arXiv:2112.00734v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.00734</id>
        <link href="http://arxiv.org/abs/2112.00734"/>
        <updated>2022-04-11T00:52:28.917Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in applying machine learning techniques for
healthcare. Recently, federated machine learning (FL) is gaining popularity
since it allows researchers to train powerful models without compromising data
privacy and security. However, the performance of existing FL approaches often
deteriorates when encountering non-iid situations where there exist
distribution gaps among clients, and few previous efforts focus on
personalization in healthcare. In this article, we propose AdaFed to tackle
domain shifts and obtain personalized models for local clients. AdaFed learns
the similarity between clients via the statistics of the batch normalization
layers while preserving the specificity of each client with different local
batch normalization. Comprehensive experiments on five healthcare benchmarks
demonstrate that AdaFed achieves better accuracy compared to state-of-the-art
methods (e.g., \textbf{10}\%+ accuracy improvement for PAMAP2) with faster
convergence speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Hands as Probes for Interactive Object Understanding. (arXiv:2112.09120v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.09120</id>
        <link href="http://arxiv.org/abs/2112.09120"/>
        <updated>2022-04-11T00:52:28.917Z</updated>
        <summary type="html"><![CDATA[Interactive object understanding, or what we can do to objects and how is a
long-standing goal of computer vision. In this paper, we tackle this problem
through observation of human hands in in-the-wild egocentric videos. We
demonstrate that observation of what human hands interact with and how can
provide both the relevant data and the necessary supervision. Attending to
hands, readily localizes and stabilizes active objects for learning and reveals
places where interactions with objects occur. Analyzing the hands shows what we
can do to objects and how. We apply these basic principles on the EPIC-KITCHENS
dataset, and successfully learn state-sensitive features, and object
affordances (regions of interaction and afforded grasps), purely by observing
hands in egocentric videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_M/0/1/0/all/0/1"&gt;Mohit Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1"&gt;Sahil Modi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1"&gt;Rishabh Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Saurabh Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image prediction of disease progression by style-based manifold extrapolation. (arXiv:2111.11439v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.11439</id>
        <link href="http://arxiv.org/abs/2111.11439"/>
        <updated>2022-04-11T00:52:28.915Z</updated>
        <summary type="html"><![CDATA[Disease-modifying management aims to prevent deterioration and progression of
the disease, not just relieve symptoms. Unfortunately, the development of
necessary therapies is often hampered by the failure to recognize the
presymptomatic disease and limited understanding of disease development. We
present a generic solution for this problem by a methodology that allows the
prediction of progression risk and morphology in individuals using a latent
extrapolation optimization approach. To this end, we combined a regularized
generative adversarial network (GAN) and a latent nearest neighbor algorithm
for joint optimization to generate plausible images of future time points. We
evaluated our method on osteoarthritis (OA) data from a multi-center
longitudinal study (the Osteoarthritis Initiative, OAI). With presymptomatic
baseline data, our model is generative and significantly outperforms the
end-to-end learning model in discriminating the progressive cohort. Two
experiments were performed with seven experienced radiologists. When no
synthetic follow-up radiographs were provided, our model performed better than
all seven radiologists. In cases where the synthetic follow-ups generated by
our model were available, the specificity and sensitivity of all readers in
discriminating progressors increased from $72.3\%$ to $88.6\%$ and from
$42.1\%$ to $51.6\%$, respectively. Our results open up a new possibility of
using model-based morphology and risk prediction to make predictions about
future disease occurrence, as demonstrated in the example of OA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_T/0/1/0/all/0/1"&gt;Tianyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kather_J/0/1/0/all/0/1"&gt;Jakob Nikolas Kather&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pedersoli_F/0/1/0/all/0/1"&gt;Federico Pedersoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zimmermann_M/0/1/0/all/0/1"&gt;Markus Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keil_S/0/1/0/all/0/1"&gt;Sebastian Keil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schulze_Hagen_M/0/1/0/all/0/1"&gt;Maximilian Schulze-Hagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Terwoelbeck_M/0/1/0/all/0/1"&gt;Marc Terwoelbeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Isfort_P/0/1/0/all/0/1"&gt;Peter Isfort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Haarburger_C/0/1/0/all/0/1"&gt;Christoph Haarburger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiessling_F/0/1/0/all/0/1"&gt;Fabian Kiessling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schulz_V/0/1/0/all/0/1"&gt;Volkmar Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuhl_C/0/1/0/all/0/1"&gt;Christiane Kuhl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nebelung_S/0/1/0/all/0/1"&gt;Sven Nebelung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Truhn_D/0/1/0/all/0/1"&gt;Daniel Truhn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Feature Fusion for End-to-End Noise-Robust Speech Recognition. (arXiv:2110.05267v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.05267</id>
        <link href="http://arxiv.org/abs/2110.05267"/>
        <updated>2022-04-11T00:52:28.912Z</updated>
        <summary type="html"><![CDATA[Speech enhancement (SE) aims to suppress the additive noise from a noisy
speech signal to improve the speech's perceptual quality and intelligibility.
However, the over-suppression phenomenon in the enhanced speech might degrade
the performance of downstream automatic speech recognition (ASR) task due to
the missing latent information. To alleviate such problem, we propose an
interactive feature fusion network (IFF-Net) for noise-robust speech
recognition to learn complementary information from the enhanced feature and
original noisy feature. Experimental results show that the proposed method
achieves absolute word error rate (WER) reduction of 4.1% over the best
baseline on RATS Channel-A corpus. Our further analysis indicates that the
proposed IFF-Net can complement some missing information in the over-suppressed
enhanced feature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuchen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_N/0/1/0/all/0/1"&gt;Nana Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chng_E/0/1/0/all/0/1"&gt;Eng Siong Chng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Linear Regression for $\ell_p$ Norms and Beyond. (arXiv:2111.04888v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.04888</id>
        <link href="http://arxiv.org/abs/2111.04888"/>
        <updated>2022-04-11T00:52:28.908Z</updated>
        <summary type="html"><![CDATA[We study active sampling algorithms for linear regression, which aim to query
only a few entries of a target vector $b\in\mathbb R^n$ and output a near
minimizer to $\min_{x\in\mathbb R^d} \|Ax-b\|$, for a design matrix
$A\in\mathbb R^{n \times d}$ and loss $\|\cdot\|$.

For $p$ norm regression for any $0<p<\infty$, we give an algorithm based on
Lewis weight sampling outputting a $(1+\epsilon)$-approximate solution using
just $\tilde O(d/\epsilon^2)$ queries to $b$ for $p\in(0,1)$,
$\tilde{O}(d/\epsilon)$ queries for $1<p<2$, and
$\tilde{O}(d^{p/2}/\epsilon^p)$ queries for $2<p<\infty$. For $0<p<2$, our
bounds are optimal up to log factors, settling the query complexity for this
range. For $2<p<\infty$, our dependence on $d$ is optimal, while our dependence
on $\epsilon$ is off by at most $\epsilon$, up to log factors. Our result
resolves an open question of [CD21], who gave near optimal bounds for the $1$
norm, but required $d^2/\epsilon^2$ samples for $\ell_p$ regression with
$1<p<2$, and gave no bounds for $2<p<\infty$ or $0<p<1$.

We also give the first total sensitivity bound of
$O(d^{\max\{1,p/2\}}\log^2n)$ for loss functions of degree $p$ polynomial
growth, improving a result of [TMF20]. By combining this with our techniques
for $\ell_p$ regression, we obtain an active regression algorithm making
$\tilde O(d^{1+\max\{1,p/2\}}/\mathrm{poly}(\epsilon))$ queries for such loss
functions, including the Tukey and Huber losses, answering another question of
[CD21]. For the Huber loss, we further improve our bound to $\tilde
O(d^{4-2\sqrt2}/\mathrm{poly}(\epsilon))$ samples. Our sensitivity bounds also
have many applications, including Orlicz norm subspace embeddings, robust
subspace approximation, and dimension reduction for smoothed $p$-norms.

Finally, our active sampling results give the first sublinear time algorithms
for Kronecker product regression under every $p$ norm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Christopher Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasuda_T/0/1/0/all/0/1"&gt;Taisuke Yasuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAD: Data-free Adversarial Defense at Test Time. (arXiv:2204.01568v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.01568</id>
        <link href="http://arxiv.org/abs/2204.01568"/>
        <updated>2022-04-11T00:52:28.908Z</updated>
        <summary type="html"><![CDATA[Deep models are highly susceptible to adversarial attacks. Such attacks are
carefully crafted imperceptible noises that can fool the network and can cause
severe consequences when deployed. To encounter them, the model requires
training data for adversarial training or explicit regularization-based
techniques. However, privacy has become an important concern, restricting
access to only trained models but not the training data (e.g. biometric data).
Also, data curation is expensive and companies may have proprietary rights over
it. To handle such situations, we propose a completely novel problem of
'test-time adversarial defense in absence of training data and even their
statistics'. We solve it in two stages: a) detection and b) correction of
adversarial samples. Our adversarial sample detection framework is initially
trained on arbitrary data and is subsequently adapted to the unlabelled test
data through unsupervised domain adaptation. We further correct the predictions
on detected adversarial samples by transforming them in Fourier domain and
obtaining their low frequency component at our proposed suitable radius for
model prediction. We demonstrate the efficacy of our proposed technique via
extensive experiments against several adversarial attacks and for different
model architectures and datasets. For a non-robust Resnet-18 model pre-trained
on CIFAR-10, our detection method correctly identifies 91.42% adversaries.
Also, we significantly improve the adversarial accuracy from 0% to 37.37% with
a minimal drop of 0.02% in clean accuracy on state-of-the-art 'Auto Attack'
without having to retrain the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1"&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawal_R/0/1/0/all/0/1"&gt;Ruchit Rawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1"&gt;Anirban Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring disentangled generative spatio-temporal representation. (arXiv:2202.04821v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.04821</id>
        <link href="http://arxiv.org/abs/2202.04821"/>
        <updated>2022-04-11T00:52:28.906Z</updated>
        <summary type="html"><![CDATA[Disentangled representation learning offers useful properties such as
dimension reduction and interpretability, which are essential to modern deep
learning approaches. Although deep learning techniques have been widely applied
to spatio-temporal data mining, there has been little attention to further
disentangle the latent features and understanding their contribution to the
model performance, particularly their mutual information and correlation across
features. In this study, we adopt two state-of-the-art disentangled
representation learning methods and apply them to three large-scale public
spatio-temporal datasets. To evaluate their performance, we propose an internal
evaluation metric focusing on the degree of correlations among latent variables
of the learned representations and the prediction performance of the downstream
tasks. Empirical results show that our modified method can learn disentangled
representations that achieve the same level of performance as existing
state-of-the-art ST deep learning methods in a spatio-temporal sequence
forecasting problem. Additionally, we find that our methods can be used to
discover real-world spatial-temporal semantics to describe the variables in the
learned representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sichen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1"&gt;Jeffrey Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiability of Label Noise Transition Matrix. (arXiv:2202.02016v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.02016</id>
        <link href="http://arxiv.org/abs/2202.02016"/>
        <updated>2022-04-11T00:52:28.905Z</updated>
        <summary type="html"><![CDATA[The noise transition matrix plays a central role in the problem of learning
from noisy labels. Among many other reasons, a significant number of existing
solutions rely on access to it. Estimating the transition matrix without using
ground truth labels is a critical and challenging task. When label noise
transition depends on each instance, the problem of identifying the
instance-dependent noise transition matrix becomes substantially more
challenging. Despite recent works proposing solutions for learning from
instance-dependent noisy labels, we lack a unified understanding of when such a
problem remains identifiable, and therefore learnable. This paper seeks to
provide answers to a sequence of related questions: What are the primary
factors that contribute to the identifiability of a noise transition matrix?
Can we explain the observed empirical successes? When a problem is not
identifiable, what can we do to make it so? We will relate our theoretical
findings to the literature and hope to provide guidelines for developing
effective solutions for battling instance-dependent label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spatial-Temporal Attention Multi-Graph Convolution Network for Ride-Hailing Demand Prediction Based on Periodicity with Offset. (arXiv:2203.12505v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.12505</id>
        <link href="http://arxiv.org/abs/2203.12505"/>
        <updated>2022-04-11T00:52:28.905Z</updated>
        <summary type="html"><![CDATA[Ride-hailing service is becoming a leading part in urban transportation. To
improve the efficiency of ride-hailing service, accurate prediction of
transportation demand is a fundamental challenge. In this paper, we tackle this
problem from both aspects of network structure and data-set formulation. For
network design, we propose a spatial-temporal attention multi-graph convolution
network (STA-MGCN). A spatial-temporal layer in STA-MGCN is developed to
capture the temporal correlations by temporal attention mechanism and temporal
gate convolution, and the spatial correlations by multigraph convolution. A
feature cluster layer is introduced to learn latent regional functions and to
reduce the computation burden. For the data-set formulation, we develop a novel
approach which considers the transportation feature of periodicity with offset.
Instead of only using history data during the same time period, the history
order demand in forward and backward neighboring time periods from yesterday
and last week are also included. Extensive experiments on the three real-world
datasets of New-York, Chicago and Chengdu show that the proposed algorithm
achieves the state-of-the-art performance for ride-hailing demand prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_D/0/1/0/all/0/1"&gt;Dong Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chenguang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.13005</id>
        <link href="http://arxiv.org/abs/2110.13005"/>
        <updated>2022-04-11T00:52:28.892Z</updated>
        <summary type="html"><![CDATA[In the last few years, the memory requirements to train state-of-the-art
neural networks have far exceeded the DRAM capacities of modern hardware
accelerators. This has necessitated the development of efficient algorithms to
train these neural networks in parallel on large-scale GPU-based clusters.
Since computation is relatively inexpensive on modern GPUs, designing and
implementing extremely efficient communication in these parallel training
algorithms is critical for extracting the maximum performance. This paper
presents AxoNN, a parallel deep learning framework that exploits asynchrony and
message-driven execution to schedule neural network operations on each GPU,
thereby reducing GPU idle time and maximizing hardware efficiency. By using the
CPU memory as a scratch space for offloading data periodically during training,
AxoNN is able to reduce GPU memory consumption by four times. This allows us to
increase the number of parameters per GPU by four times, thus reducing the
amount of communication and increasing performance by over 13%. When tested
against large transformer models with 12-100 billion parameters on 48-384
NVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of
theoretical peak and reduces the training time by 22-37 days (15-25% speedup)
as compared to the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Siddharth Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatele_A/0/1/0/all/0/1"&gt;Abhinav Bhatele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11732</id>
        <link href="http://arxiv.org/abs/2107.11732"/>
        <updated>2022-04-11T00:52:28.882Z</updated>
        <summary type="html"><![CDATA[Analyzing observational data from multiple sources can be useful for
increasing statistical power to detect a treatment effect; however, practical
constraints such as privacy considerations may restrict individual-level
information sharing across data sets. This paper develops federated methods
that only utilize summary-level information from heterogeneous data sets. Our
federated methods provide doubly-robust point estimates of treatment effects as
well as variance estimates. We derive the asymptotic distributions of our
federated estimators, which are shown to be asymptotically equivalent to the
corresponding estimators from the combined, individual-level data. We show that
to achieve these properties, federated methods should be adjusted based on
conditions such as whether models are correctly specified and stable across
heterogeneous data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Ruoxuan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1"&gt;Allison Koenecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1"&gt;Michael Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1"&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.06329</id>
        <link href="http://arxiv.org/abs/2108.06329"/>
        <updated>2022-04-11T00:52:28.881Z</updated>
        <summary type="html"><![CDATA[Recent work building open-domain chatbots has demonstrated that increasing
model size improves performance. On the other hand, latency and connectivity
considerations dictate the move of digital assistants on the device. Giving a
digital assistant like Siri, Alexa, or Google Assistant the ability to discuss
just about anything leads to the need for reducing the chatbot model size such
that it fits on the user's device. We demonstrate that low parameter models can
simultaneously retain their general knowledge conversational abilities while
improving in a specific domain. Additionally, we propose a generic framework
that accounts for variety in question types, tracks reference throughout
multi-turn conversations, and removes inconsistent and potentially toxic
responses. Our framework seamlessly transitions between chatting and performing
transactional tasks, which will ultimately make interactions with digital
assistants more human-like. We evaluate our framework on 1 internal and 4
public benchmark datasets using both automatic (Perplexity) and human (SSA -
Sensibleness and Specificity Average) evaluation metrics and establish
comparable performance while reducing model parameters by 90%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gerhard_Young_G/0/1/0/all/0/1"&gt;Greyson Gerhard-Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1"&gt;Raviteja Anantha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chappidi_S/0/1/0/all/0/1"&gt;Srinivas Chappidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmeister_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Hoffmeister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCA-Net : Utilizing Gated Context Attention for Improving Image Forgery Localization and Detection. (arXiv:2112.04298v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.04298</id>
        <link href="http://arxiv.org/abs/2112.04298"/>
        <updated>2022-04-11T00:52:28.881Z</updated>
        <summary type="html"><![CDATA[Forensic analysis of manipulated pixels requires the identification of
various hidden and subtle features from images. Conventional image recognition
models generally fail at this task because they are biased and more attentive
toward the dominant local and spatial features. In this paper, we propose a
novel Gated Context Attention Network (GCA-Net) that utilizes non-local
attention in conjunction with a gating mechanism in order to capture the finer
image discrepancies and better identify forged regions. The proposed framework
uses high dimensional embeddings to filter and aggregate the relevant context
from coarse feature maps at various stages of the decoding process. This
improves the network's understanding of global differences and reduces
false-positive localizations. Our evaluation on standard image forensic
benchmarks shows that GCA-Net can both compete against and improve over
state-of-the-art networks by an average of 4.7% AUC. Additional ablation
studies also demonstrate the method's robustness against attributions and
resilience to false-positive predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sowmen Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md. Saiful Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1"&gt;Md. Ruhul Amin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03097</id>
        <link href="http://arxiv.org/abs/2103.03097"/>
        <updated>2022-04-11T00:52:28.873Z</updated>
        <summary type="html"><![CDATA[Machine learning systems generally assume that the training and testing
distributions are the same. To this end, a key requirement is to develop models
that can generalize to unseen distributions. Domain generalization (DG), i.e.,
out-of-distribution generalization, has attracted increasing interests in
recent years. Domain generalization deals with a challenging setting where one
or several different but related domain(s) are given, and the goal is to learn
a model that can generalize to an unseen test domain. Great progress has been
made in the area of domain generalization for years. This paper presents the
first review of recent advances in this area. First, we provide a formal
definition of domain generalization and discuss several related fields. We then
thoroughly review the theories related to domain generalization and carefully
analyze the theory behind generalization. We categorize recent algorithms into
three classes: data manipulation, representation learning, and learning
strategy, and present several popular algorithms in detail for each category.
Third, we introduce the commonly used datasets, applications, and our
open-sourced codebase for fair evaluation. Finally, we summarize existing
literature and present some potential research topics for the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1"&gt;Yidong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.09151</id>
        <link href="http://arxiv.org/abs/2108.09151"/>
        <updated>2022-04-11T00:52:28.873Z</updated>
        <summary type="html"><![CDATA[Describing images using natural language is widely known as image captioning,
which has made consistent progress due to the development of computer vision
and natural language generation techniques. Though conventional captioning
models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and
SPICE, the ability of captions to distinguish the target image from other
similar images is under-explored. To generate distinctive captions, a few
pioneers employ contrastive learning or re-weighted the ground-truth captions,
which focuses on one single input image. However, the relationships between
objects in a similar image group (e.g., items or properties within the same
album or fine-grained events) are neglected. In this paper, we improve the
distinctiveness of image captions using a Group-based Distinctive Captioning
Model (GdisCap), which compares each image with other images in one similar
group and highlights the uniqueness of each image. In particular, we propose a
group-based memory attention (GMA) module, which stores object features that
are unique among the image group (i.e., with low similarity to objects in other
images). These unique object features are highlighted when generating captions,
resulting in more distinctive captions. Furthermore, the distinctive words in
the ground-truth captions are selected to supervise the language decoder and
GMA. Finally, we propose a new evaluation metric, distinctive word rate
(DisWordRate) to measure the distinctiveness of captions. Quantitative results
indicate that the proposed method significantly improves the distinctiveness of
several baseline models, and achieves the state-of-the-art performance on both
accuracy and distinctiveness. Results of a user study agree with the
quantitative evaluation and demonstrate the rationality of the new metric
DisWordRate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiuniu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qingzhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Antoni B. Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretext Tasks selection for multitask self-supervised speech representation learning. (arXiv:2107.00594v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00594</id>
        <link href="http://arxiv.org/abs/2107.00594"/>
        <updated>2022-04-11T00:52:28.872Z</updated>
        <summary type="html"><![CDATA[Through solving pretext tasks, self-supervised learning leverages unlabeled
data to extract useful latent representations replacing traditional input
features in the downstream task. In audio/speech signal processing, a wide
range of features where engineered through decades of research efforts. As it
turns out, learning to predict such features (a.k.a pseudo-labels) has proven
to be a particularly relevant pretext task, leading to useful self-supervised
representations which prove to be effective for downstream tasks. However,
methods and common practices for combining such pretext tasks for better
performance on the downstream task have not been explored and understood
properly. In fact, the process relies almost exclusively on a computationally
heavy experimental procedure, which becomes intractable with the increase of
the number of pretext tasks. This paper introduces a method to select a group
of pretext tasks among a set of candidates. The method we propose estimates
calibrated weights for the partial losses corresponding to the considered
pretext tasks during the self-supervised training process. The experiments
conducted on automatic speech recognition, speaker and emotion recognition
validate our approach, as the groups selected and weighted with our method
perform better than classic baselines, thus facilitating the selection and
combination of relevant pseudo-labels for self-supervised representation
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1"&gt;Salah Zaiem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1"&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essid_S/0/1/0/all/0/1"&gt;Slim Essid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heba_A/0/1/0/all/0/1"&gt;Abdel Heba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omni-Training for Data-Efficient Deep Learning. (arXiv:2110.07510v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.07510</id>
        <link href="http://arxiv.org/abs/2110.07510"/>
        <updated>2022-04-11T00:52:28.872Z</updated>
        <summary type="html"><![CDATA[Learning a generalizable deep model from a few examples in a short time
remains a major challenge of machine learning, which has impeded its wide
deployment to many scenarios. Recent advances reveal that a properly
pre-trained model endows an important property: transferability. A higher
transferability of the learned representations indicates a better
generalizability across domains of different distributions (domain
transferability), or across tasks of different semantics (task
transferability). Transferability has become the key to enable data-efficient
deep learning, however, existing pre-training methods focus only on domain
transferability while meta-training methods only on task transferability. This
restricts their data-efficiency in downstream scenarios of diverging domains
and tasks. A finding of this paper is that even a tight combination of
pre-training and meta-training cannot achieve both kinds of transferability.
This motivates the proposed Omni-Training framework towards data-efficient deep
learning. Our first contribution is Omni-Net, a tri-flow architecture. Besides
the joint representation flow, Omni-Net introduces two new parallel flows for
pre-training and meta-training, respectively responsible for learning
representations of domain transferability and task transferability. Omni-Net
coordinates the parallel flows by routing them via the joint-flow, making each
gain the other kind of transferability. Our second contribution is Omni-Loss,
in which a self-distillation regularization is imposed to enable knowledge
transfer across the training process. Omni-Training is a general framework that
accommodates many existing pre-training and meta-training algorithms. A
thorough evaluation on cross-task and cross-domain datasets in classification,
regression and reinforcement learning problems shows that Omni-Training
consistently outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1"&gt;Yang Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhangjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jinghan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constraints Penalized Q-learning for Safe Offline Reinforcement Learning. (arXiv:2107.09003v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09003</id>
        <link href="http://arxiv.org/abs/2107.09003"/>
        <updated>2022-04-11T00:52:28.871Z</updated>
        <summary type="html"><![CDATA[We study the problem of safe offline reinforcement learning (RL), the goal is
to learn a policy that maximizes long-term reward while satisfying safety
constraints given only offline data, without further interaction with the
environment. This problem is more appealing for real world RL applications, in
which data collection is costly or dangerous. Enforcing constraint satisfaction
is non-trivial, especially in offline settings, as there is a potential large
discrepancy between the policy distribution and the data distribution, causing
errors in estimating the value of safety constraints. We show that na\"ive
approaches that combine techniques from safe RL and offline RL can only learn
sub-optimal solutions. We thus develop a simple yet effective algorithm,
Constraints Penalized Q-Learning (CPQ), to solve the problem. Our method admits
the use of data generated by mixed behavior policies. We present a theoretical
analysis and demonstrate empirically that our approach can learn robustly
across a variety of benchmark control tasks, outperforming several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haoran Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xianyuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiangyu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to distribute data across tasks for meta-learning?. (arXiv:2103.08463v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08463</id>
        <link href="http://arxiv.org/abs/2103.08463"/>
        <updated>2022-04-11T00:52:28.781Z</updated>
        <summary type="html"><![CDATA[Meta-learning models transfer the knowledge acquired from previous tasks to
quickly learn new ones. They are trained on benchmarks with a fixed number of
data points per task. This number is usually arbitrary and it is unknown how it
affects performance at testing. Since labelling of data is expensive, finding
the optimal allocation of labels across training tasks may reduce costs. Given
a fixed budget of labels, should we use a small number of highly labelled
tasks, or many tasks with few labels each? Should we allocate more labels to
some tasks and less to others? We show that: 1) If tasks are homogeneous, there
is a uniform optimal allocation, whereby all tasks get the same amount of data;
2) At fixed budget, there is a trade-off between number of tasks and number of
data points per task, with a unique solution for the optimum; 3) When trained
separately, harder task should get more data, at the cost of a smaller number
of tasks; 4) When training on a mixture of easy and hard tasks, more data
should be allocated to easy tasks. Interestingly, Neuroscience experiments have
shown that human visual skills also transfer better from easy tasks. We prove
these results mathematically on mixed linear regression, and we show
empirically that the same results hold for few-shot image classification on
CIFAR-FS and mini-ImageNet. Our results provide guidance for allocating labels
across tasks when collecting data for meta-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cioba_A/0/1/0/all/0/1"&gt;Alexandru Cioba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bromberg_M/0/1/0/all/0/1"&gt;Michael Bromberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niyogi_R/0/1/0/all/0/1"&gt;Ritwik Niyogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1"&gt;Jezabel Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Machine Learning Framework for Virtual Screening in Drug Discovery: a Prospective Quantum Advantage. (arXiv:2204.04017v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2204.04017</id>
        <link href="http://arxiv.org/abs/2204.04017"/>
        <updated>2022-04-11T00:52:28.779Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) for Ligand Based Virtual Screening (LB-VS) is an
important in-silico tool for discovering new drugs in a faster and
cost-effective manner, especially for emerging diseases such as COVID-19. In
this paper, we propose a general-purpose framework combining a classical
Support Vector Classifier (SVC) algorithm with quantum kernel estimation for
LB-VS on real-world databases, and we argue in favor of its prospective quantum
advantage. Indeed, we heuristically prove that our quantum integrated workflow
can, at least in some relevant instances, provide a tangible advantage compared
to state-of-art classical algorithms operating on the same datasets, showing
strong dependence on target and features selection method. Finally, we test our
algorithm on IBM Quantum processors using ADRB2 and COVID-19 datasets, showing
that hardware simulations provide results in line with the predicted
performances and can surpass classical equivalents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Mensa_S/0/1/0/all/0/1"&gt;Stefano Mensa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sahin_E/0/1/0/all/0/1"&gt;Emre Sahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tacchino_F/0/1/0/all/0/1"&gt;Francesco Tacchino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Barkoutsos_P/0/1/0/all/0/1"&gt;Panagiotis Kl. Barkoutsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tavernelli_I/0/1/0/all/0/1"&gt;Ivano Tavernelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KCD: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media. (arXiv:2204.04046v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.04046</id>
        <link href="http://arxiv.org/abs/2204.04046"/>
        <updated>2022-04-11T00:52:28.778Z</updated>
        <summary type="html"><![CDATA[Political perspective detection has become an increasingly important task
that can help combat echo chambers and political polarization. Previous
approaches generally focus on leveraging textual content to identify stances,
while they fail to reason with background knowledge or leverage the rich
semantic and syntactic textual labels in news articles. In light of these
limitations, we propose KCD, a political perspective detection approach to
enable multi-hop knowledge reasoning and incorporate textual cues as
paragraph-level labels. Specifically, we firstly generate random walks on
external knowledge graphs and infuse them with news text representations. We
then construct a heterogeneous information network to jointly model news
content as well as semantic, syntactic and entity cues in news articles.
Finally, we adopt relational graph neural networks for graph-level
representation learning and conduct political perspective detection. Extensive
experiments demonstrate that our approach outperforms state-of-the-art methods
on two benchmark datasets. We further examine the effect of knowledge walks and
textual cues and how they contribute to our approach's data efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shangbin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zilong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1"&gt;Zhenyu Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Minnan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Berth Stay for Tanker Terminals: A Systematic and Dynamic Approach. (arXiv:2204.04085v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2204.04085</id>
        <link href="http://arxiv.org/abs/2204.04085"/>
        <updated>2022-04-11T00:52:28.778Z</updated>
        <summary type="html"><![CDATA[Given the trend of digitization and increasing number of maritime transport,
prediction of vessel berth stay has been triggered for requirements of
operation research and scheduling optimization problem in the era of maritime
big data, which takes a significant part in port efficiency and maritime
logistics enhancement. This study proposes a systematic and dynamic approach of
predicting berth stay for tanker terminals. The approach covers three
innovative aspects: 1) Data source employed is multi-faceted, including cargo
operation data from tanker terminals, time-series data from automatic
identification system (AIS), etc. 2) The process of berth stay is decomposed
into multiple blocks according to data analysis and information extraction
innovatively, and practical operation scenarios are also developed accordingly.
3) The predictive models of berth stay are developed on the basis of prior data
analysis and information extraction under two methods, including regression and
decomposed distribution. The models are evaluated under four dynamic scenarios
with certain designated cargoes among two different terminals. The evaluation
results show that the proposed approach can predict berth stay with the
accuracy up to 98.81% validated by historical baselines, and also demonstrate
the proposed approach has dynamic capability of predicting berth stay among the
scenarios. The model may be potentially applied for short-term pilot-booking or
scheduling optimizations within a reasonable time frame for advancement of port
intelligence and logistics efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1"&gt;Deqing Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiuju Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xiao Feng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wanbing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Update Guided Federated Learning. (arXiv:2204.03920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03920</id>
        <link href="http://arxiv.org/abs/2204.03920"/>
        <updated>2022-04-11T00:52:28.774Z</updated>
        <summary type="html"><![CDATA[Federated learning protects data privacy and security by exchanging models
instead of data. However, unbalanced data distributions among participating
clients compromise the accuracy and convergence speed of federated learning
algorithms. To alleviate this problem, unlike previous studies that limit the
distance of updates for local models, we propose global-update-guided federated
learning (FedGG), which introduces a model-cosine loss into local objective
functions, so that local models can fit local data distributions under the
guidance of update directions of global models. Furthermore, considering that
the update direction of a global model is informative in the early stage of
training, we propose adaptive loss weights based on the update distances of
local models. Numerical simulations show that, compared with other advanced
algorithms, FedGG has a significant improvement on model convergence accuracies
and speeds. Additionally, compared with traditional fixed loss weights,
adaptive loss weights enable our algorithm to be more stable and easier to
implement in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qilong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1"&gt;Shibei Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ontology Matching Through Absolute Orientation of Embedding Spaces. (arXiv:2204.04040v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2204.04040</id>
        <link href="http://arxiv.org/abs/2204.04040"/>
        <updated>2022-04-11T00:52:28.770Z</updated>
        <summary type="html"><![CDATA[Ontology matching is a core task when creating interoperable and linked open
datasets. In this paper, we explore a novel structure-based mapping approach
which is based on knowledge graph embeddings: The ontologies to be matched are
embedded, and an approach known as absolute orientation is used to align the
two embedding spaces. Next to the approach, the paper presents a first,
preliminary evaluation using synthetic and real-world datasets. We find in
experiments with synthetic data, that the approach works very well on similarly
structured graphs; it handles alignment noise better than size and structural
differences in the ontologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1"&gt;Jan Portisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costa_G/0/1/0/all/0/1"&gt;Guilherme Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefani_K/0/1/0/all/0/1"&gt;Karolin Stefani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreplin_K/0/1/0/all/0/1"&gt;Katharina Kreplin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hladik_M/0/1/0/all/0/1"&gt;Michael Hladik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1"&gt;Heiko Paulheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Speaker Diarization. (arXiv:2204.04166v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2204.04166</id>
        <link href="http://arxiv.org/abs/2204.04166"/>
        <updated>2022-04-11T00:52:28.770Z</updated>
        <summary type="html"><![CDATA[Over the last few years, deep learning has grown in popularity for speaker
verification, identification, and diarization. Inarguably, a significant part
of this success is due to the demonstrated effectiveness of their speaker
representations. These, however, are heavily dependent on large amounts of
annotated data and can be sensitive to new domains. This study proposes an
entirely unsupervised deep-learning model for speaker diarization.
Specifically, the study focuses on generating high-quality neural speaker
representations without any annotated data, as well as on estimating secondary
hyperparameters of the model without annotations.

The speaker embeddings are represented by an encoder trained in a
self-supervised fashion using pairs of adjacent segments assumed to be of the
same speaker. The trained encoder model is then used to self-generate
pseudo-labels to subsequently train a similarity score between different
segments of the same call using probabilistic linear discriminant analysis
(PLDA) and further to learn a clustering stopping threshold. We compared our
model to state-of-the-art unsupervised as well as supervised baselines on the
CallHome benchmarks. According to empirical results, our approach outperforms
unsupervised methods when only two speakers are present in the call, and is
only slightly worse than recent supervised models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dissen_Y/0/1/0/all/0/1"&gt;Yehoshua Dissen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1"&gt;Felix Kreuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1"&gt;Joseph Keshet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[C-NMT: A Collaborative Inference Framework for Neural Machine Translation. (arXiv:2204.04043v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.04043</id>
        <link href="http://arxiv.org/abs/2204.04043"/>
        <updated>2022-04-11T00:52:28.767Z</updated>
        <summary type="html"><![CDATA[Collaborative Inference (CI) optimizes the latency and energy consumption of
deep learning inference through the inter-operation of edge and cloud devices.
Albeit beneficial for other tasks, CI has never been applied to the sequence-
to-sequence mapping problem at the heart of Neural Machine Translation (NMT).
In this work, we address the specific issues of collaborative NMT, such as
estimating the latency required to generate the (unknown) output sequence, and
show how existing CI methods can be adapted to these applications. Our
experiments show that CI can reduce the latency of NMT by up to 44% compared to
a non-collaborative approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yukai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiaro_R/0/1/0/all/0/1"&gt;Roberta Chiaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1"&gt;Enrico Macii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poncino_M/0/1/0/all/0/1"&gt;Massimo Poncino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1"&gt;Daniele Jahier Pagliari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings. (arXiv:2204.04063v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.04063</id>
        <link href="http://arxiv.org/abs/2204.04063"/>
        <updated>2022-04-11T00:52:28.767Z</updated>
        <summary type="html"><![CDATA[One intriguing property of adversarial attacks is their "transferability" --
an adversarial example crafted with respect to one deep neural network (DNN)
model is often found effective against other DNNs as well. Intensive research
has been conducted on this phenomenon under simplistic controlled conditions.
Yet, thus far, there is still a lack of comprehensive understanding about
transferability-based attacks ("transfer attacks") in real-world environments.

To bridge this critical gap, we conduct the first large-scale systematic
empirical study of transfer attacks against major cloud-based MLaaS platforms,
taking the components of a real transfer attack into account. The study leads
to a number of interesting findings which are inconsistent to the existing
ones, including: (1) Simple surrogates do not necessarily improve real transfer
attacks. (2) No dominant surrogate architecture is found in real transfer
attacks. (3) It is the gap between posterior (output of the softmax layer)
rather than the gap between logit (so-called $\kappa$ value) that increases
transferability. Moreover, by comparing with prior works, we demonstrate that
transfer attacks possess many previously unknown properties in real-world
environments, such as (1) Model similarity is not a well-defined concept. (2)
$L_2$ norm of perturbation can generate high transferability without usage of
gradient and is a more powerful source than $L_\infty$ norm. We believe this
work sheds light on the vulnerabilities of popular MLaaS platforms and points
to a few promising research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuhao Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Saizhuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenguang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Alex X. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyah_R/0/1/0/all/0/1"&gt;Raheem Beyah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Ting Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPASAD: Ellipsoid decision boundary based Process-Aware Stealthy Attack Detector. (arXiv:2204.04154v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2204.04154</id>
        <link href="http://arxiv.org/abs/2204.04154"/>
        <updated>2022-04-11T00:52:28.734Z</updated>
        <summary type="html"><![CDATA[Due to the importance of Critical Infrastructure (CI) in a nation's economy,
they have been lucrative targets for cyber attackers. These critical
infrastructures are usually Cyber-Physical Systems (CPS) such as power grids,
water, and sewage treatment facilities, oil and gas pipelines, etc. In recent
times, these systems have suffered from cyber attacks numerous times.
Researchers have been developing cyber security solutions for CIs to avoid
lasting damages. According to standard frameworks, cyber security based on
identification, protection, detection, response, and recovery are at the core
of these research. Detection of an ongoing attack that escapes standard
protection such as firewall, anti-virus, and host/network intrusion detection
has gained importance as such attacks eventually affect the physical dynamics
of the system. Therefore, anomaly detection in physical dynamics proves an
effective means to implement defense-in-depth. PASAD is one example of anomaly
detection in the sensor/actuator data, representing such systems' physical
dynamics. We present EPASAD, which improves the detection technique used in
PASAD to detect these micro-stealthy attacks, as our experiments show that
PASAD's spherical boundary-based detection fails to detect. Our method EPASAD
overcomes this by using Ellipsoid boundaries, thereby tightening the boundaries
in various dimensions, whereas a spherical boundary treats all dimensions
equally. We validate EPASAD using the dataset produced by the TE-process
simulator and the C-town datasets. The results show that EPASAD improves
PASAD's average recall by 5.8% and 9.5% for the two datasets, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maurya_V/0/1/0/all/0/1"&gt;Vikas Maurya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1"&gt;Rachit Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Saurabh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Sandeep Kumar Shukla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Coordinative Schedules for Tanker Terminals: An Intelligent Large Spatial-Temporal Data-Driven Approach -- Part 2. (arXiv:2204.03955v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2204.03955</id>
        <link href="http://arxiv.org/abs/2204.03955"/>
        <updated>2022-04-11T00:52:28.710Z</updated>
        <summary type="html"><![CDATA[In this study, a novel coordinative scheduling optimization approach is
proposed to enhance port efficiency by reducing weighted average turnaround
time. The proposed approach is developed as a heuristic algorithm applied and
investigated through different observation windows with weekly rolling horizon
paradigm method. The experimental results show that the proposed approach is
effective and promising on mitigating the turnaround time of vessels. The
results demonstrate that largest potential savings of turnaround time (weighted
average) are around 17 hours (28%) reduction on baseline of 1-week observation,
45 hours (37%) reduction on baseline of 2-week observation and 70 hours (40%)
reduction on baseline of 3-week observation. Even though the experimental
results are based on historical datasets, the results potentially present
significant benefits if real-time applications were applied under a quadratic
computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1"&gt;Deqing Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiuju Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xiao Feng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wanbing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1"&gt;Ning Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Labeling-Free Comparison Testing of Deep Learning Models. (arXiv:2204.03994v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03994</id>
        <link href="http://arxiv.org/abs/2204.03994"/>
        <updated>2022-04-11T00:52:28.681Z</updated>
        <summary type="html"><![CDATA[Various deep neural networks (DNNs) are developed and reported for their
tremendous success in multiple domains. Given a specific task, developers can
collect massive DNNs from public sources for efficient reusing and avoid
redundant work from scratch. However, testing the performance (e.g., accuracy
and robustness) of multiple DNNs and giving a reasonable recommendation that
which model should be used is challenging regarding the scarcity of labeled
data and demand of domain expertise. Existing testing approaches are mainly
selection-based where after sampling, a few of the test data are labeled to
discriminate DNNs. Therefore, due to the randomness of sampling, the
performance ranking is not deterministic. In this paper, we propose a
labeling-free comparison testing approach to overcome the limitations of
labeling effort and sampling randomness. The main idea is to learn a Bayesian
model to infer the models' specialty only based on predicted labels. To
evaluate the effectiveness of our approach, we undertook exhaustive experiments
on 9 benchmark datasets spanning in the domains of image, text, and source
code, and 165 DNNs. In addition to accuracy, we consider the robustness against
synthetic and natural distribution shifts. The experimental results demonstrate
that the performance of existing approaches degrades under distribution shifts.
Our approach outperforms the baseline methods by up to 0.74 and 0.53 on
Spearman's correlation and Kendall's $\tau$, respectively, regardless of the
dataset and distribution shift. Additionally, we investigated the impact of
model quality (accuracy and robustness) and diversity (standard deviation of
the quality) on the testing effectiveness and observe that there is a higher
chance of a good result when the quality is over 50\% and the diversity is
larger than 18\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuejun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordy_M/0/1/0/all/0/1"&gt;Maxime Cordy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadakis_M/0/1/0/all/0/1"&gt;Mike Papadakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Traon_Y/0/1/0/all/0/1"&gt;Yves Le Traon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECG Biometric Recognition: Review, System Proposal, and Benchmark Evaluation. (arXiv:2204.03992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03992</id>
        <link href="http://arxiv.org/abs/2204.03992"/>
        <updated>2022-04-11T00:52:28.658Z</updated>
        <summary type="html"><![CDATA[Electrocardiograms (ECGs) have shown unique patterns to distinguish between
different subjects and present important advantages compared to other biometric
traits, such as difficulty to counterfeit, liveness detection, and ubiquity.
Also, with the success of Deep Learning technologies, ECG biometric recognition
has received increasing interest in recent years. However, it is not easy to
evaluate the improvements of novel ECG proposed methods, mainly due to the lack
of public data and standard experimental protocols. In this study, we perform
extensive analysis and comparison of different scenarios in ECG biometric
recognition. Both verification and identification tasks are investigated, as
well as single- and multi-session scenarios. Finally, we also perform single-
and multi-lead ECG experiments, considering traditional scenarios using
electrodes in the chest and limbs and current user-friendly wearable devices.

In addition, we present ECGXtractor, a robust Deep Learning technology
trained with an in-house large-scale database and able to operate successfully
across various scenarios and multiple databases. We introduce our proposed
feature extractor, trained with multiple sinus-rhythm heartbeats belonging to
55,967 subjects, and provide a general public benchmark evaluation with
detailed experimental protocol. We evaluate the system performance over four
different databases: i) our in-house database, ii) PTB, iii) ECG-ID, and iv)
CYBHi. With the widely used PTB database, we achieve Equal Error Rates of 0.14%
and 2.06% in verification, and accuracies of 100% and 96.46% in identification,
respectively in single- and multi-session analysis. We release the source code,
experimental protocol details, and pre-trained models in GitHub to advance in
the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melzi_P/0/1/0/all/0/1"&gt;Pietro Melzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1"&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1"&gt;Ruben Vera-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Robustness on ImageNet Transfer to Downstream Tasks?. (arXiv:2204.03934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03934</id>
        <link href="http://arxiv.org/abs/2204.03934"/>
        <updated>2022-04-11T00:52:28.647Z</updated>
        <summary type="html"><![CDATA[As clean ImageNet accuracy nears its ceiling, the research community is
increasingly more concerned about robust accuracy under distributional shifts.
While a variety of methods have been proposed to robustify neural networks,
these techniques often target models trained on ImageNet classification. At the
same time, it is a common practice to use ImageNet pretrained backbones for
downstream tasks such as object detection, semantic segmentation, and image
classification from different domains. This raises a question: Can these robust
image classifiers transfer robustness to downstream tasks? For object detection
and semantic segmentation, we find that a vanilla Swin Transformer, a variant
of Vision Transformer tailored for dense prediction tasks, transfers robustness
better than Convolutional Neural Networks that are trained to be robust to the
corrupted version of ImageNet. For CIFAR10 classification, we find that models
that are robustified for ImageNet do not retain robustness when fully
fine-tuned. These findings suggest that current robustification techniques tend
to emphasize ImageNet evaluations. Moreover, network architecture is a strong
source of robustness when we consider transfer learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1"&gt;Yutaro Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1"&gt;Mayu Otani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SnapMode: An Intelligent and Distributed Large-Scale Fashion Image Retrieval Platform Based On Big Data and Deep Generative Adversarial Network Technologies. (arXiv:2204.03998v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.03998</id>
        <link href="http://arxiv.org/abs/2204.03998"/>
        <updated>2022-04-11T00:52:28.640Z</updated>
        <summary type="html"><![CDATA[Fashion is now among the largest industries worldwide, for it represents
human history and helps tell the worlds story. As a result of the Fourth
Industrial Revolution, the Internet has become an increasingly important source
of fashion information. However, with a growing number of web pages and social
data, it is nearly impossible for humans to manually catch up with the ongoing
evolution and the continuously variable content in this domain. The proper
management and exploitation of big data can pave the way for the substantial
growth of the global economy as well as citizen satisfaction. Therefore,
computer scientists have found it challenging to handle e-commerce fashion
websites by using big data and machine learning technologies. This paper first
proposes a scalable focused Web Crawler engine based on the distributed
computing platforms to extract and process fashion data on e-commerce websites.
The role of the proposed platform is then described in developing a
disentangled feature extraction method by employing deep convolutional
generative adversarial networks (DCGANs) for content-based image indexing and
retrieval. Finally, the state-of-the-art solutions are compared, and the
results of the proposed approach are analyzed on a standard dataset. For the
real-life implementation of the proposed solution, a Web-based application is
developed on Apache Storm, Kafka, Solr, and Milvus platforms to create a
fashion search engine called SnapMode.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_N/0/1/0/all/0/1"&gt;Narges Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azmi_R/0/1/0/all/0/1"&gt;Reza Azmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moghadam_S/0/1/0/all/0/1"&gt;Sara Saberi Tehrani Moghadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarvani_M/0/1/0/all/0/1"&gt;Maral Zarvani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel model for end-to-end learning of communications systems: A survey. (arXiv:2204.03944v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03944</id>
        <link href="http://arxiv.org/abs/2204.03944"/>
        <updated>2022-04-11T00:52:28.632Z</updated>
        <summary type="html"><![CDATA[The traditional communication model based on chain of multiple independent
processing blocks is constraint to efficiency and introduces artificial
barriers. Thus, each individually optimized block does not guarantee end-to-end
performance of the system. Recently, end-to-end learning of communications
systems through machine learning (ML) have been proposed to optimize the system
metrics jointly over all components. These methods show performance
improvements but has a limitation that it requires a differentiable channel
model. In this study, we have summarized the existing approaches that
alleviates this problem. We believe that this study will provide better
understanding of the topic and an insight into future research in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1"&gt;Ijaz Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1"&gt;Seokjoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mel-spectrogram features for acoustic vehicle detection and speed estimation. (arXiv:2204.04013v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.04013</id>
        <link href="http://arxiv.org/abs/2204.04013"/>
        <updated>2022-04-11T00:52:28.625Z</updated>
        <summary type="html"><![CDATA[The paper addresses acoustic vehicle detection and speed estimation from
single sensor measurements. We predict the vehicle's pass-by instant by
minimizing clipped vehicle-to-microphone distance, which is predicted from the
mel-spectrogram of input audio, in a supervised learning approach. In addition,
mel-spectrogram-based features are used directly for vehicle speed estimation,
without introducing any intermediate features. The results show that the
proposed features can be used for accurate vehicle detection and speed
estimation, with an average error of 7.87 km/h. If we formulate speed
estimation as a classification problem, with a 10 km/h discretization interval,
the proposed method attains the average accuracy of 48.7% for correct class
prediction and 91.0% when an offset of one class is allowed. The proposed
method is evaluated on a dataset of 304 urban-environment on-field recordings
of ten different vehicles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bulatovic_N/0/1/0/all/0/1"&gt;Nikola Bulatovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djukanovic_S/0/1/0/all/0/1"&gt;Slobodan Djukanovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Complexity of Markov Equilibrium in Stochastic Games. (arXiv:2204.03991v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03991</id>
        <link href="http://arxiv.org/abs/2204.03991"/>
        <updated>2022-04-11T00:52:28.606Z</updated>
        <summary type="html"><![CDATA[We show that computing approximate stationary Markov coarse correlated
equilibria (CCE) in general-sum stochastic games is computationally
intractable, even when there are two players, the game is turn-based, the
discount factor is an absolute constant, and the approximation is an absolute
constant. Our intractability results stand in sharp contrast to normal-form
games where exact CCEs are efficiently computable. A fortiori, our results
imply that there are no efficient algorithms for learning stationary Markov CCE
policies in multi-agent reinforcement learning (MARL), even when the
interaction is two-player and turn-based, and both the discount factor and the
desired approximation of the learned policies is an absolute constant. In turn,
these results stand in sharp contrast to single-agent reinforcement learning
(RL) where near-optimal stationary Markov policies can be efficiently learned.
Complementing our intractability results for stationary Markov CCEs, we provide
a decentralized algorithm (assuming shared randomness among players) for
learning a nonstationary Markov CCE policy with polynomial time and sample
complexity in all problem parameters. Previous work for learning Markov CCE
policies all required exponential time and sample complexity in the number of
players.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1"&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1"&gt;Noah Golowich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiversiTree: Computing Diverse Sets of Near-Optimal Solutions to Mixed-Integer Optimization Problems. (arXiv:2204.03822v1 [cs.DM])]]></title>
        <id>http://arxiv.org/abs/2204.03822</id>
        <link href="http://arxiv.org/abs/2204.03822"/>
        <updated>2022-04-11T00:52:28.599Z</updated>
        <summary type="html"><![CDATA[While most methods for solving mixed-integer optimization problems seek a
single optimal solution, finding a diverse set of near-optimal solutions can
often be more useful. State of the art methods for generating diverse
near-optimal solutions usually take a two-phase approach, first finding a set
of near-optimal solutions and then finding a diverse subset. In contrast, we
present a method of finding a set of diverse solutions by emphasizing diversity
within the search for near-optimal solutions. Specifically, within a
branch-and-bound framework, we investigate parameterized node selection rules
that explicitly consider diversity. Our results indicate that our approach
significantly increases diversity of the final solution set. When compared with
existing methods for finding diverse near-optimal sets, our method runs with
similar run-time as regular node selection methods and gives a diversity
improvement of up to 140%. In contrast, popular node selection rules such as
best-first search gives an improvement of no more than 40%. Further, we find
that our method is most effective when diversity is emphasized more in node
selection when deeper in the tree and when the solution set has grown large
enough.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahanor_I/0/1/0/all/0/1"&gt;Izuwa Ahanor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medal_H/0/1/0/all/0/1"&gt;Hugh Medal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_A/0/1/0/all/0/1"&gt;Andrew C. Trapp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Coordinative Schedules for Tanker Terminals: An Intelligent Large Spatial-Temporal Data-Driven Approach -- Part 1. (arXiv:2204.03899v1 [cs.CE])]]></title>
        <id>http://arxiv.org/abs/2204.03899</id>
        <link href="http://arxiv.org/abs/2204.03899"/>
        <updated>2022-04-11T00:52:28.591Z</updated>
        <summary type="html"><![CDATA[In this study, a novel coordinative scheduling optimization approach is
proposed to enhance port efficiency by reducing average wait time and
turnaround time. The proposed approach consists of enhanced particle swarm
optimization (ePSO) as kernel and augmented firefly algorithm (AFA) as global
optimal search. Two paradigm methods of the proposed approach are investigated,
which are batch method and rolling horizon method. The experimental results
show that both paradigm methods of proposed approach can effectively enhance
port efficiency. The average wait time could be significantly reduced by 86.0%
- 95.5%, and the average turnaround time could eventually save 38.2% - 42.4%
with respect to historical benchmarks. Moreover, the paradigm method of rolling
horizon could reduce to 20 mins on running time over 3-month datasets, rather
than 4 hrs on batch method at corresponding maximum performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1"&gt;Deqing Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiuju Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xiao Feng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wanbing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1"&gt;Ning Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Shuffling: Privacy Amplification via Random Walks. (arXiv:2204.03919v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2204.03919</id>
        <link href="http://arxiv.org/abs/2204.03919"/>
        <updated>2022-04-11T00:52:28.590Z</updated>
        <summary type="html"><![CDATA[Recently, it is shown that shuffling can amplify the central differential
privacy guarantees of data randomized with local differential privacy. Within
this setup, a centralized, trusted shuffler is responsible for shuffling by
keeping the identities of data anonymous, which subsequently leads to stronger
privacy guarantees for systems. However, introducing a centralized entity to
the originally local privacy model loses some appeals of not having any
centralized entity as in local differential privacy. Moreover, implementing a
shuffler in a reliable way is not trivial due to known security issues and/or
requirements of advanced hardware or secure computation technology.

Motivated by these practical considerations, we rethink the shuffle model to
relax the assumption of requiring a centralized, trusted shuffler. We introduce
network shuffling, a decentralized mechanism where users exchange data in a
random-walk fashion on a network/graph, as an alternative of achieving privacy
amplification via anonymity. We analyze the threat model under such a setting,
and propose distributed protocols of network shuffling that is straightforward
to implement in practice. Furthermore, we show that the privacy amplification
rate is similar to other privacy amplification techniques such as uniform
shuffling. To our best knowledge, among the recently studied intermediate trust
models that leverage privacy amplification techniques, our work is the first
that is not relying on any centralized entity to achieve privacy amplification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1"&gt;Seng Pei Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_T/0/1/0/all/0/1"&gt;Tsubasa Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takagi_S/0/1/0/all/0/1"&gt;Shun Takagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kato_F/0/1/0/all/0/1"&gt;Fumiyuki Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1"&gt;Masatoshi Yoshikawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Study of a committee of neural networks for biometric hand-geometry recognition. (arXiv:2204.03935v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03935</id>
        <link href="http://arxiv.org/abs/2204.03935"/>
        <updated>2022-04-11T00:52:28.583Z</updated>
        <summary type="html"><![CDATA[This Paper studies different committees of neural networks for biometric
pattern recognition. We use the neural nets as classifiers for identification
and verification purposes. We show that a committee of nets can improve the
recognition rates when compared with a multi-start initialization algo-rithm
that just picks up the neural net which offers the best performance. On the
other hand, we found that there is no strong correlation between
identifi-cation and verification applications using the same classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1"&gt;Marcos Faundez-Zanuy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disability prediction in multiple sclerosis using performance outcome measures and demographic data. (arXiv:2204.03969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03969</id>
        <link href="http://arxiv.org/abs/2204.03969"/>
        <updated>2022-04-11T00:52:28.574Z</updated>
        <summary type="html"><![CDATA[Literature on machine learning for multiple sclerosis has primarily focused
on the use of neuroimaging data such as magnetic resonance imaging and clinical
laboratory tests for disease identification. However, studies have shown that
these modalities are not consistent with disease activity such as symptoms or
disease progression. Furthermore, the cost of collecting data from these
modalities is high, leading to scarce evaluations. In this work, we used
multi-dimensional, affordable, physical and smartphone-based performance
outcome measures (POM) in conjunction with demographic data to predict multiple
sclerosis disease progression. We performed a rigorous benchmarking exercise on
two datasets and present results across 13 clinically actionable prediction
endpoints and 6 machine learning models. To the best of our knowledge, our
results are the first to show that it is possible to predict disease
progression using POMs and demographic data in the context of both clinical
trials and smartphone-base studies by using two datasets. Moreover, we
investigate our models to understand the impact of different POMs and
demographics on model performance through feature ablation studies. We also
show that model performance is similar across different demographic subgroups
(based on age and sex). To enable this work, we developed an end-to-end
reusable pre-processing and machine learning framework which allows quicker
experimentation over disparate MS datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Subhrajit Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mincu_D/0/1/0/all/0/1"&gt;Diana Mincu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proleev_L/0/1/0/all/0/1"&gt;Lev Proleev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamzadeh_N/0/1/0/all/0/1"&gt;Negar Rostamzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghate_C/0/1/0/all/0/1"&gt;Chintan Ghate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harris_N/0/1/0/all/0/1"&gt;Natalie Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Christina Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schrouff_J/0/1/0/all/0/1"&gt;Jessica Schrouff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1"&gt;Nenad Tomasev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartsell_F/0/1/0/all/0/1"&gt;Fletcher Lee Hartsell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heller_K/0/1/0/all/0/1"&gt;Katherine Heller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KGI: An Integrated Framework for Knowledge Intensive Language Tasks. (arXiv:2204.03985v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03985</id>
        <link href="http://arxiv.org/abs/2204.03985"/>
        <updated>2022-04-11T00:52:28.556Z</updated>
        <summary type="html"><![CDATA[In a recent work, we presented a novel state-of-the-art approach to zero-shot
slot filling that extends dense passage retrieval with hard negatives and
robust training procedures for retrieval augmented generation models. In this
paper, we propose a system based on an enhanced version of this approach where
we train task specific models for other knowledge intensive language tasks,
such as open domain question answering (QA), dialogue and fact checking. Our
system achieves results comparable to the best models in the KILT leaderboards.
Moreover, given a user query, we show how the output from these different
models can be combined to cross-examine each other. Particularly, we show how
accuracy in dialogue can be improved using the QA model. A short video
demonstrating the system is available here -
\url{https://ibm.box.com/v/kgi-interactive-demo} .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Md Faisal Mahbub Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1"&gt;Michael Glass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1"&gt;Gaetano Rossiello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1"&gt;Alfio Gliozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1"&gt;Nandana Mihindukulasooriya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Missingness from Uncontrollable Missingness: Joint Learning Measurement Policy and Imputation. (arXiv:2204.03872v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03872</id>
        <link href="http://arxiv.org/abs/2204.03872"/>
        <updated>2022-04-11T00:52:28.548Z</updated>
        <summary type="html"><![CDATA[Due to the cost or interference of measurement, we need to control
measurement system. Assuming that each variable can be measured sequentially,
there exists optimal policy choosing next measurement for the former
observations. Though optimal measurement policy is actually dependent on the
goal of measurement, we mainly focus on retrieving complete data, so called as
imputation. Also, we adapt the imputation method to missingness varying with
measurement policy. However, learning measurement policy and imputation
requires complete data which is impossible to be observed, unfortunately. To
tackle this problem, we propose a data generation method and joint learning
algorithm. The main idea is that 1) the data generation method is inherited by
imputation method, and 2) the adaptation of imputation encourages measurement
policy to learn more than individual learning. We implemented some variations
of proposed algorithm for two different datasets and various missing rates.
From the experimental results, we demonstrate that our algorithm is generally
applicable and outperforms baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Seongwook Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jaehyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1"&gt;Heejeong Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sull_S/0/1/0/all/0/1"&gt;Sanghoon Sull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CD$^2$-pFed: Cyclic Distillation-guided Channel Decoupling for Model Personalization in Federated Learning. (arXiv:2204.03880v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03880</id>
        <link href="http://arxiv.org/abs/2204.03880"/>
        <updated>2022-04-11T00:52:28.541Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a distributed learning paradigm that enables
multiple clients to collaboratively learn a shared global model. Despite the
recent progress, it remains challenging to deal with heterogeneous data
clients, as the discrepant data distributions usually prevent the global model
from delivering good generalization ability on each participating client. In
this paper, we propose CD^2-pFed, a novel Cyclic Distillation-guided Channel
Decoupling framework, to personalize the global model in FL, under various
settings of data heterogeneity. Different from previous works which establish
layer-wise personalization to overcome the non-IID data across different
clients, we make the first attempt at channel-wise assignment for model
personalization, referred to as channel decoupling. To further facilitate the
collaboration between private and shared weights, we propose a novel cyclic
distillation scheme to impose a consistent regularization between the local and
global model representations during the federation. Guided by the cyclical
distillation, our channel decoupling framework can deliver more accurate and
generalized results for different kinds of heterogeneity, such as feature skew,
label distribution skew, and concept shift. Comprehensive experiments on four
benchmarks, including natural image and medical image analysis tasks,
demonstrate the consistent effectiveness of our method on both local and
external validations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yiqing Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lequan Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment. (arXiv:2204.04016v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.04016</id>
        <link href="http://arxiv.org/abs/2204.04016"/>
        <updated>2022-04-11T00:52:28.534Z</updated>
        <summary type="html"><![CDATA[Speech intelligibility assessment plays an important role in the therapy of
patients suffering from pathological speech disorders. Automatic and objective
measures are desirable to assist therapists in their traditionally subjective
and labor-intensive assessments. In this work, we investigate a novel approach
for obtaining such a measure using the divergence in disentangled latent speech
representations of a parallel utterance pair, obtained from a healthy reference
and a pathological speaker. Experiments on an English database of Cerebral
Palsy patients, using all available utterances per speaker, show high and
significant correlation values (R = -0.9) with subjective intelligibility
measures, while having only minimal deviation (+-0.01) across four different
reference speaker pairs. We also demonstrate the robustness of the proposed
method (R = -0.89 deviating +-0.02 over 1000 iterations) by considering a
significantly smaller amount of utterances per speaker. Our results are among
the first to show that disentangled speech representations can be used for
automatic pathological speech intelligibility assessment, resulting in a
reference speaker pair invariant method, applicable in scenarios with only few
utterances available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Weise_T/0/1/0/all/0/1"&gt;Tobias Weise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klumpp_P/0/1/0/all/0/1"&gt;Philipp Klumpp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Noeth_E/0/1/0/all/0/1"&gt;Elmar Noeth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heismann_B/0/1/0/all/0/1"&gt;Bjoern Heismann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schuster_M/0/1/0/all/0/1"&gt;Maria Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1"&gt;Seung Hee Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Universality of Hadronic Jet Classification. (arXiv:2204.03812v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2204.03812</id>
        <link href="http://arxiv.org/abs/2204.03812"/>
        <updated>2022-04-11T00:52:28.526Z</updated>
        <summary type="html"><![CDATA[The modeling of jet substructure significantly differs between Parton Shower
Monte Carlo (PSMC) programs. Despite this, we observe that machine learning
classifiers trained on different PSMCs learn nearly the same function. This
means that when these classifiers are applied to the same PSMC for testing,
they result in nearly the same performance. This classifier universality
indicates that a machine learning model trained on one simulation and tested on
another simulation (or data) will likely be optimal. Our observations are based
on detailed studies of shallow and deep neural networks applied to simulated
Lorentz boosted Higgs jet tagging at the LHC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Cheung_K/0/1/0/all/0/1"&gt;Kingman Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Chung_Y/0/1/0/all/0/1"&gt;Yi-Lun Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Hsu_S/0/1/0/all/0/1"&gt;Shih-Chieh Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Nachman_B/0/1/0/all/0/1"&gt;Benjamin Nachman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SuperNet in Neural Architecture Search: A Taxonomic Survey. (arXiv:2204.03916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03916</id>
        <link href="http://arxiv.org/abs/2204.03916"/>
        <updated>2022-04-11T00:52:28.509Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNN) have made significant progress in a wide range of
visual recognition tasks such as image classification, object detection, and
semantic segmentation. The evolution of convolutional architectures has led to
better performance by incurring expensive computational costs. In addition,
network design has become a difficult task, which is labor-intensive and
requires a high level of domain knowledge. To mitigate such issues, there have
been studies for a variety of neural architecture search methods that
automatically search for optimal architectures, achieving models with
impressive performance that outperform human-designed counterparts. This survey
aims to provide an overview of existing works in this field of research and
specifically focus on the supernet optimization that builds a neural network
that assembles all the architectures as its sub models by using weight sharing.
We aim to accomplish that by categorizing supernet optimization by proposing
them as solutions to the common challenges found in the literature: data-side
optimization, poor rank correlation alleviation, and transferable NAS for a
number of deployment scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Stephen Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hayeon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Se-Young Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Evaluation of Training Action Space for Reinforcement Learning. (arXiv:2204.03840v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03840</id>
        <link href="http://arxiv.org/abs/2204.03840"/>
        <updated>2022-04-11T00:52:28.500Z</updated>
        <summary type="html"><![CDATA[Training action space selection for reinforcement learning (RL) is
conflict-prone due to complex state-action relationships. To address this
challenge, this paper proposes a Shapley-inspired methodology for training
action space categorization and ranking. To reduce exponential-time shapley
computations, the methodology includes a Monte Carlo simulation to avoid
unnecessary explorations. The effectiveness of the methodology is illustrated
using a cloud infrastructure resource tuning case study. It reduces the search
space by 80\% and categorizes the training action sets into dispensable and
indispensable groups. Additionally, it ranks different training actions to
facilitate high-performance yet cost-efficient RL model design. The proposed
data-driven methodology is extensible to different domains, use cases, and
reinforcement learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1"&gt;Rajat Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_D/0/1/0/all/0/1"&gt;Debojyoti Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Engagement Detection with Multi-Task Training in E-Learning Environments. (arXiv:2204.04020v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.04020</id>
        <link href="http://arxiv.org/abs/2204.04020"/>
        <updated>2022-04-11T00:52:28.493Z</updated>
        <summary type="html"><![CDATA[Recognition of user interaction, in particular engagement detection, became
highly crucial for online working and learning environments, especially during
the COVID-19 outbreak. Such recognition and detection systems significantly
improve the user experience and efficiency by providing valuable feedback. In
this paper, we propose a novel Engagement Detection with Multi-Task Training
(ED-MTT) system which minimizes mean squared error and triplet loss together to
determine the engagement level of students in an e-learning environment. The
performance of this system is evaluated and compared against the
state-of-the-art on a publicly available dataset as well as videos collected
from real-life scenarios. The results show that ED-MTT achieves 6% lower MSE
than the best state-of-the-art performance with highly acceptable training time
and lightweight feature extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Copur_O/0/1/0/all/0/1"&gt;Onur Copur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakip_M/0/1/0/all/0/1"&gt;Mert Nak&amp;#x131;p&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slowack_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Slowack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blockchain as an Enabler for Transfer Learning in Smart Environments. (arXiv:2204.03959v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2204.03959</id>
        <link href="http://arxiv.org/abs/2204.03959"/>
        <updated>2022-04-11T00:52:28.489Z</updated>
        <summary type="html"><![CDATA[The knowledge, embodied in machine learning models for intelligent systems,
is commonly associated with time-consuming and costly processes such as
large-scale data collection, data labelling, network training, and fine-tuning
of models. Sharing and reuse of these elaborated models between intelligent
systems deployed in a different environment, which is known as transfer
learning, would facilitate the adoption of services for the users and
accelerates the uptake of intelligent systems in environments such as smart
building and smart city applications. In this context, the communication and
knowledge exchange between AI-enabled environments depend on a complicated
networks of systems, system of systems, digital assets, and their chain of
dependencies that hardly follows the centralized schema of traditional
information systems. Rather, it requires an adaptive decentralized system
architecture that is empowered by features such as data provenance, workflow
transparency, and validation of process participants. In this research, we
propose a decentralized and adaptive software framework based on blockchain and
knowledge graph technologies that supports the knowledge exchange and
interoperability between IoT-enabled environments, in a transparent and
trustworthy way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anjomshoaa_A/0/1/0/all/0/1"&gt;Amin Anjomshoaa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curry_E/0/1/0/all/0/1"&gt;Edward Curry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does the Market of Citations Reward Reproducible Work?. (arXiv:2204.03829v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2204.03829</id>
        <link href="http://arxiv.org/abs/2204.03829"/>
        <updated>2022-04-11T00:52:28.478Z</updated>
        <summary type="html"><![CDATA[The field of bibliometrics, studying citations and behavior, is critical to
the discussion of reproducibility. Citations are one of the primary incentive
and reward systems for academic work, and so we desire to know if this
incentive rewards reproducible work. Yet to the best of our knowledge, only one
work has attempted to look at this combined space, concluding that
non-reproducible work is more highly cited. We show that answering this
question is more challenging than first proposed, and subtle issues can inhibit
a robust conclusion. To make inferences with more robust behavior, we propose a
hierarchical Bayesian model that incorporates the citation rate over time,
rather than the total number of citations after a fixed amount of time. In
doing so we show that, under current evidence the answer is more likely that
certain fields of study such as Medicine and Machine Learning (ML) do correlate
reproducible works with more citations, but other fields appear to have no
relationship. Further, we find that making code available and thoroughly
referencing prior works appear to also positively correlate with increased
citations. Our code and data can be found at
https://github.com/EdwardRaff/ReproducibleCitations .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"&gt;Edward Raff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A posteriori learning for quasi-geostrophic turbulence parametrization. (arXiv:2204.03911v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2204.03911</id>
        <link href="http://arxiv.org/abs/2204.03911"/>
        <updated>2022-04-11T00:52:27.840Z</updated>
        <summary type="html"><![CDATA[The use of machine learning to build subgrid parametrizations for climate
models is receiving growing attention. State-of-the-art strategies address the
problem as a supervised learning task and optimize algorithms that predict
subgrid fluxes based on information from coarse resolution models. In practice,
training data are generated from higher resolution numerical simulations
transformed in order to mimic coarse resolution simulations. By essence, these
strategies optimize subgrid parametrizations to meet so-called $\textit{a
priori}$ criteria. But the actual purpose of a subgrid parametrization is to
obtain good performance in terms of $\textit{a posteriori}$ metrics which imply
computing entire model trajectories. In this paper, we focus on the
representation of energy backscatter in two dimensional quasi-geostrophic
turbulence and compare parametrizations obtained with different learning
strategies at fixed computational complexity. We show that strategies based on
$\textit{a priori}$ criteria yield parametrizations that tend to be unstable in
direct simulations and describe how subgrid parametrizations can alternatively
be trained end-to-end in order to meet $\textit{a posteriori}$ criteria. We
illustrate that end-to-end learning strategies yield parametrizations that
outperform known empirical and data-driven schemes in terms of performance,
stability and ability to apply to different flow configurations. These results
support the relevance of differentiable programming paradigms for climate
models in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Frezat_H/0/1/0/all/0/1"&gt;Hugo Frezat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Sommer_J/0/1/0/all/0/1"&gt;Julien Le Sommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Fablet_R/0/1/0/all/0/1"&gt;Ronan Fablet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Balarac_G/0/1/0/all/0/1"&gt;Guillaume Balarac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lguensat_R/0/1/0/all/0/1"&gt;Redouane Lguensat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Partial Model Personalization. (arXiv:2204.03809v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03809</id>
        <link href="http://arxiv.org/abs/2204.03809"/>
        <updated>2022-04-11T00:52:27.816Z</updated>
        <summary type="html"><![CDATA[We consider two federated learning algorithms for training partially
personalized models, where the shared and personal parameters are updated
either simultaneously or alternately on the devices. Both algorithms have been
proposed in the literature, but their convergence properties are not fully
understood, especially for the alternating variant. We provide convergence
analyses of both algorithms in the general nonconvex setting with partial
participation and delineate the regime where one dominates the other. Our
experiments on real-world image, text, and speech datasets demonstrate that (a)
partial personalization can obtain most of the benefits of full model
personalization with a small fraction of personal parameters, and, (b) the
alternating update algorithm often outperforms the simultaneous update
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kshitiz Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1"&gt;Maziar Sanjabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1"&gt;Lin Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decomposition-based Generation Process for Instance-Dependent Partial Label Learning. (arXiv:2204.03845v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03845</id>
        <link href="http://arxiv.org/abs/2204.03845"/>
        <updated>2022-04-11T00:52:27.805Z</updated>
        <summary type="html"><![CDATA[Partial label learning (PLL) is a typical weakly supervised learning problem,
where each training example is associated with a set of candidate labels among
which only one is true. Most existing PLL approaches assume that the incorrect
labels in each training example are randomly picked as the candidate labels and
model the generation process of the candidate labels in a simple way. However,
these approaches usually do not perform as well as expected due to the fact
that the generation process of the candidate labels is always
instance-dependent. Therefore, it deserves to be modeled in a refined way. In
this paper, we consider instance-dependent PLL and assume that the generation
process of the candidate labels could decompose into two sequential parts,
where the correct label emerges first in the mind of the annotator but then the
incorrect labels related to the feature are also selected with the correct
label as candidate labels due to uncertainty of labeling. Motivated by this
consideration, we propose a novel PLL method that performs Maximum A
Posterior(MAP) based on an explicitly modeled generation process of candidate
labels via decomposed probability distribution models. Experiments on benchmark
and real-world datasets validate the effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1"&gt;Congyu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ning Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xin Geng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-learning with online random forests. (arXiv:2204.03771v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03771</id>
        <link href="http://arxiv.org/abs/2204.03771"/>
        <updated>2022-04-11T00:52:27.790Z</updated>
        <summary type="html"><![CDATA[$Q$-learning is the most fundamental model-free reinforcement learning
algorithm. Deployment of $Q$-learning requires approximation of the
state-action value function (also known as the $Q$-function). In this work, we
provide online random forests as $Q$-function approximators and propose a novel
method wherein the random forest is grown as learning proceeds (through
expanding forests). We demonstrate improved performance of our methods over
state-of-the-art Deep $Q$-Networks in two OpenAI gyms (`blackjack' and
`inverted pendulum') but not in the `lunar lander' gym. We suspect that the
resilience to overfitting enjoyed by random forests recommends our method for
common tasks that do not require a strong representation of the problem domain.
We show that expanding forests (in which the number of trees increases as data
comes in) improve performance, suggesting that expanding forests are viable for
other applications of online random forests beyond the reinforcement learning
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Min_J/0/1/0/all/0/1"&gt;Joosung Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elliott_L/0/1/0/all/0/1"&gt;Lloyd T. Elliott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum version of the k-NN classifier based on a quantum sorting algorithm. (arXiv:2204.03761v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2204.03761</id>
        <link href="http://arxiv.org/abs/2204.03761"/>
        <updated>2022-04-11T00:52:27.782Z</updated>
        <summary type="html"><![CDATA[In this work we introduce a quantum sorting algorithm with adaptable
requirements of memory and circuit depth, and then use it to develop a new
quantum version of the classical machine learning algorithm known as k-nearest
neighbors (k-NN). Both the efficiency and performance of this new quantum
version of the k-NN algorithm are compared to those of the classical k-NN and
another quantum version proposed by Schuld et al. \cite{Int13}. Results show
that the efficiency of both quantum algorithms is similar to each other and
superior to that of the classical algorithm. On the other hand, the performance
of our proposed quantum k-NN algorithm is superior to the one proposed by
Schuld et al. and similar to that of the classical k-NN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Quezada_L/0/1/0/all/0/1"&gt;L.F. Quezada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guo-Hua Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Dong_S/0/1/0/all/0/1"&gt;Shi-Hai Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition. (arXiv:2204.03793v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.03793</id>
        <link href="http://arxiv.org/abs/2204.03793"/>
        <updated>2022-04-11T00:52:27.702Z</updated>
        <summary type="html"><![CDATA[Personalization of on-device speech recognition (ASR) has seen explosive
growth in recent years, largely due to the increasing popularity of personal
assistant features on mobile devices and smart home speakers. In this work, we
present Personal VAD 2.0, a personalized voice activity detector that detects
the voice activity of a target speaker, as part of a streaming on-device ASR
system. Although previous proof-of-concept studies have validated the
effectiveness of Personal VAD, there are still several critical challenges to
address before this model can be used in production: first, the quality must be
satisfactory in both enrollment and enrollment-less scenarios; second, it
should operate in a streaming fashion; and finally, the model size should be
small enough to fit a limited latency and CPU/Memory budget. To meet the
multi-faceted requirements, we propose a series of novel designs: 1) advanced
speaker embedding modulation methods; 2) a new training paradigm to generalize
to enrollment-less conditions; 3) architecture and runtime optimizations for
latency and resource restrictions. Extensive experiments on a realistic speech
recognition system demonstrated the state-of-the-art performance of our
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shaojin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rikhye_R/0/1/0/all/0/1"&gt;Rajeev Rikhye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1"&gt;Yanzhang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Quan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Arun Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OMalley_T/0/1/0/all/0/1"&gt;Tom O&amp;#x27;Malley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1"&gt;Ian McGraw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Energy Evaluation Using Marginalized Annealed Importance Sampling. (arXiv:2204.03784v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03784</id>
        <link href="http://arxiv.org/abs/2204.03784"/>
        <updated>2022-04-11T00:52:27.666Z</updated>
        <summary type="html"><![CDATA[The evaluation of the free energy of a stochastic model is considered to be a
significant issue in various fields of physics and machine learning. However,
the exact free energy evaluation is computationally infeasible because it
includes an intractable partition function. Annealed importance sampling (AIS)
is a type of importance sampling based on the Markov chain Monte Carlo method,
which is similar to a simulated annealing, and can effectively approximate the
free energy. This study proposes a new AIS-based approach, referred to as
marginalized AIS (mAIS). The statistical efficiency of mAIS is investigated in
detail based on a theoretical and numerical perspectives. Based on the
investigation, it has been proved that mAIS is more effective than AIS under a
certain condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yasuda_M/0/1/0/all/0/1"&gt;Muneki Yasuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takahashi_C/0/1/0/all/0/1"&gt;Chako Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis. (arXiv:2204.03804v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2204.03804</id>
        <link href="http://arxiv.org/abs/2204.03804"/>
        <updated>2022-04-11T00:52:27.658Z</updated>
        <summary type="html"><![CDATA[Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic
information but is limited in practice due to excessive data acquisition time.
In this paper, we propose a novel deep-learning model for joint reconstruction
and synthesis of multi-modal MRI using incomplete k-space data of several
source modalities as inputs. The output of our model includes reconstructed
images of the source modalities and high-quality image synthesized in the
target modality. Our proposed model is formulated as a variational problem that
leverages several learnable modality-specific feature extractors and a
multimodal synthesis module. We propose a learnable optimization algorithm to
solve this model, which induces a multi-phase network whose parameters can be
trained using multi-modal MRI data. Moreover, a bilevel-optimization framework
is employed for robust parameter training. We demonstrate the effectiveness of
our approach using extensive numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bian_W/0/1/0/all/0/1"&gt;Wanyu Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojing Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunmei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework. (arXiv:2204.03719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03719</id>
        <link href="http://arxiv.org/abs/2204.03719"/>
        <updated>2022-04-11T00:52:27.650Z</updated>
        <summary type="html"><![CDATA[Class imbalance poses new challenges when it comes to classifying data
streams. Many algorithms recently proposed in the literature tackle this
problem using a variety of data-level, algorithm-level, and ensemble
approaches. However, there is a lack of standardized and agreed-upon procedures
on how to evaluate these algorithms. This work presents a taxonomy of
algorithms for imbalanced data streams and proposes a standardized, exhaustive,
and informative experimental testbed to evaluate algorithms in a collection of
diverse and challenging imbalanced data stream scenarios. The experimental
study evaluates 24 state-of-the-art data streams algorithms on 515 imbalanced
data streams that combine static and dynamic class imbalance ratios,
instance-level difficulties, concept drift, real-world and semi-synthetic
datasets in binary and multi-class scenarios. This leads to the largest
experimental study conducted so far in the data stream mining domain. We
discuss the advantages and disadvantages of state-of-the-art classifiers in
each of these scenarios and we provide general recommendations to end-users for
selecting the best algorithms for imbalanced data streams. Additionally, we
formulate open challenges and future directions for this domain. Our
experimental testbed is fully reproducible and easy to extend with new methods.
This way we propose the first standardized approach to conducting experiments
in imbalanced data streams that can be used by other researchers to create
trustworthy and fair evaluation of newly proposed methods. Our experimental
framework can be downloaded from
https://github.com/canoalberto/imbalanced-streams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aguiar_G/0/1/0/all/0/1"&gt;Gabriel Aguiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1"&gt;Bartosz Krawczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cano_A/0/1/0/all/0/1"&gt;Alberto Cano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Event-Triggered Federated Learning with Heterogeneous Communication Thresholds. (arXiv:2204.03726v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03726</id>
        <link href="http://arxiv.org/abs/2204.03726"/>
        <updated>2022-04-11T00:52:27.631Z</updated>
        <summary type="html"><![CDATA[A recent emphasis of distributed learning research has been on federated
learning (FL), in which model training is conducted by the data-collecting
devices. Existing research on FL has mostly focused on a star topology learning
architecture with synchronized (time-triggered) model training rounds, where
the local models of the devices are periodically aggregated by a centralized
coordinating node. However, in many settings, such a coordinating node may not
exist, motivating efforts to fully decentralize FL. In this work, we propose a
novel methodology for distributed model aggregations via asynchronous,
event-triggered consensus iterations over the network graph topology. We
consider heterogeneous communication event thresholds at each device that weigh
the change in local model parameters against the available local resources in
deciding the benefit of aggregations at each iteration. Through theoretical
analysis, we demonstrate that our methodology achieves asymptotic convergence
to the globally optimal learning model under standard assumptions in
distributed learning and graph consensus literature, and without restrictive
connectivity requirements on the underlying topology. Subsequent numerical
results demonstrate that our methodology obtains substantial improvements in
communication requirements compared with FL baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zehtabi_S/0/1/0/all/0/1"&gt;Shahryar Zehtabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1"&gt;Seyyedali Hosseinalipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1"&gt;Christopher G. Brinton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global ECG Classification by Self-Operational Neural Networks with Feature Injection. (arXiv:2204.03768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03768</id>
        <link href="http://arxiv.org/abs/2204.03768"/>
        <updated>2022-04-11T00:52:27.624Z</updated>
        <summary type="html"><![CDATA[Objective: Global (inter-patient) ECG classification for arrhythmia detection
over Electrocardiogram (ECG) signal is a challenging task for both humans and
machines. The main reason is the significant variations of both normal and
arrhythmic ECG patterns among patients. Automating this process with utmost
accuracy is, therefore, highly desirable due to the advent of wearable ECG
sensors. However, even with numerous deep learning approaches proposed
recently, there is still a notable gap in the performance of global and
patient-specific ECG classification performances. This study proposes a novel
approach to narrow this gap and propose a real-time solution with shallow and
compact 1D Self-Organized Operational Neural Networks (Self-ONNs). Methods: In
this study, we propose a novel approach for inter-patient ECG classification
using a compact 1D Self-ONN by exploiting morphological and timing information
in heart cycles. We used 1D Self-ONN layers to automatically learn
morphological representations from ECG data, enabling us to capture the shape
of the ECG waveform around the R peaks. We further inject temporal features
based on RR interval for timing characterization. The classification layers can
thus benefit from both temporal and learned features for the final arrhythmia
classification. Results: Using the MIT-BIH arrhythmia benchmark database, the
proposed method achieves the highest classification performance ever achieved,
i.e., 99.21% precision, 99.10% recall, and 99.15% F1-score for normal (N)
segments; 82.19% precision, 82.50% recall, and 82.34% F1-score for the
supra-ventricular ectopic beat (SVEBs); and finally, 94.41% precision, 96.10%
recall, and 95.2% F1-score for the ventricular-ectopic beats (VEBs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zahid_M/0/1/0/all/0/1"&gt;Muhammad Uzair Zahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Design of Salient Object Detection Algorithms with Brain Programming. (arXiv:2204.03722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03722</id>
        <link href="http://arxiv.org/abs/2204.03722"/>
        <updated>2022-04-11T00:52:27.617Z</updated>
        <summary type="html"><![CDATA[Despite recent improvements in computer vision, artificial visual systems'
design is still daunting since an explanation of visual computing algorithms
remains elusive. Salient object detection is one problem that is still open due
to the difficulty of understanding the brain's inner workings. Progress on this
research area follows the traditional path of hand-made designs using
neuroscience knowledge. In recent years two different approaches based on
genetic programming appear to enhance their technique. One follows the idea of
combining previous hand-made methods through genetic programming and fuzzy
logic. The other approach consists of improving the inner computational
structures of basic hand-made models through artificial evolution. This
research work proposes expanding the artificial dorsal stream using a recent
proposal to solve salient object detection problems. This approach uses the
benefits of the two main aspects of this research area: fixation prediction and
detection of salient objects. We decided to apply the fusion of visual saliency
and image segmentation algorithms as a template. The proposed methodology
discovers several critical structures in the template through artificial
evolution. We present results on a benchmark designed by experts with
outstanding results in comparison with the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olague_G/0/1/0/all/0/1"&gt;Gustavo Olague&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menendez_Clavijo_J/0/1/0/all/0/1"&gt;Jose Armando Menendez-Clavijo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olague_M/0/1/0/all/0/1"&gt;Matthieu Olague&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ocampo_A/0/1/0/all/0/1"&gt;Arturo Ocampo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibarra_Vazquez_G/0/1/0/all/0/1"&gt;Gerardo Ibarra-Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_R/0/1/0/all/0/1"&gt;Rocio Ochoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_R/0/1/0/all/0/1"&gt;Roberto Pineda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Generalization and Decomposition in Neural Program Synthesis. (arXiv:2204.03758v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03758</id>
        <link href="http://arxiv.org/abs/2204.03758"/>
        <updated>2022-04-11T00:52:27.601Z</updated>
        <summary type="html"><![CDATA[When writing programs, people have the ability to tackle a new complex task
by decomposing it into smaller and more familiar subtasks. While it is
difficult to measure whether neural program synthesis methods have similar
capabilities, what we can measure is whether they compositionally generalize,
that is, whether a model that has been trained on the simpler subtasks is
subsequently able to solve more complex tasks. In this paper, we focus on
measuring the ability of learned program synthesizers to compositionally
generalize. We first characterize several different axes along which program
synthesis methods would be desired to generalize, e.g., length generalization,
or the ability to combine known subroutines in new ways that do not occur in
the training data. Based on this characterization, we introduce a benchmark
suite of tasks to assess these abilities based on two popular existing
datasets, SCAN and RobustFill. Finally, we make first attempts to improve the
compositional generalization ability of Transformer models along these axes
through novel attention mechanisms that draw inspiration from a human-like
decomposition strategy. Empirically, we find our modified Transformer models
generally perform better than natural baselines, but the tasks remain
challenging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kensen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Joey Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Pengcheng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1"&gt;Charles Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-assisted Generative Adversarial Network for X-Ray Tomography. (arXiv:2204.03703v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2204.03703</id>
        <link href="http://arxiv.org/abs/2204.03703"/>
        <updated>2022-04-11T00:52:27.584Z</updated>
        <summary type="html"><![CDATA[X-ray tomography is capable of imaging the interior of objects in three
dimensions non-invasively, with applications in biomedical imaging, materials
science, electronic inspection, and other fields. The reconstruction process
can be an ill-conditioned inverse problem, requiring regularization to obtain
satisfactory reconstructions. Recently, deep learning has been adopted for
tomographic reconstruction. Unlike iterative algorithms which require a
distribution that is known a priori, deep reconstruction networks can learn a
prior distribution through sampling the training distributions. In this work,
we develop a Physics-assisted Generative Adversarial Network (PGAN), a two-step
algorithm for tomographic reconstruction. In contrast to previous efforts, our
PGAN utilizes maximum-likelihood estimates derived from the measurements to
regularize the reconstruction with both known physics and the learned prior.
Synthetic objects with spatial correlations are integrated circuits (IC) from a
proposed model CircuitFaker. Compared with maximum-likelihood estimation, PGAN
can reduce the photon requirement with limited projection angles to achieve a
given error rate. We further attribute the improvement to the learned prior by
reconstructing objects created without spatial correlations. The advantages of
using a prior from deep learning in X-ray tomography may further enable
low-photon nanoscale imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1"&gt;Jung Ki Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barbastathis_G/0/1/0/all/0/1"&gt;George Barbastathis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glinsky_M/0/1/0/all/0/1"&gt;Michael E. Glinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vaughan_C/0/1/0/all/0/1"&gt;Courtenay T. Vaughan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Larson_K/0/1/0/all/0/1"&gt;Kurt W. Larson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alpert_B/0/1/0/all/0/1"&gt;Bradley K. Alpert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Levine_Z/0/1/0/all/0/1"&gt;Zachary H. Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GreaseVision: Rewriting the Rules of the Interface. (arXiv:2204.03731v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2204.03731</id>
        <link href="http://arxiv.org/abs/2204.03731"/>
        <updated>2022-04-11T00:52:27.576Z</updated>
        <summary type="html"><![CDATA[Digital harms can manifest across any interface. Key problems in addressing
these harms include the high individuality of harms and the fast-changing
nature of digital systems. As a result, we still lack a systematic approach to
study harms and produce interventions for end-users. We put forward
GreaseVision, a new framework that enables end-users to collaboratively develop
interventions against harms in software using a no-code approach and recent
advances in few-shot machine learning. The contribution of the framework and
tool allow individual end-users to study their usage history and create
personalized interventions. Our contribution also enables researchers to study
the distribution of harms and interventions at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1"&gt;Siddhartha Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollnig_K/0/1/0/all/0/1"&gt;Konrad Kollnig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1"&gt;Nigel Shadbolt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixing Signals: Data Augmentation Approach for Deep Learning Based Modulation Recognition. (arXiv:2204.03737v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2204.03737</id>
        <link href="http://arxiv.org/abs/2204.03737"/>
        <updated>2022-04-11T00:52:27.569Z</updated>
        <summary type="html"><![CDATA[With the rapid development of deep learning, automatic modulation recognition
(AMR), as an important task in cognitive radio, has gradually transformed from
traditional feature extraction and classification to automatic classification
by deep learning technology. However, deep learning models are data-driven
methods, which often require a large amount of data as the training support.
Data augmentation, as the strategy of expanding dataset, can improve the
generalization of the deep learning models and thus improve the accuracy of the
models to a certain extent. In this paper, for AMR of radio signals, we propose
a data augmentation strategy based on mixing signals and consider four specific
methods (Random Mixing, Maximum-Similarity-Mixing, $\theta-$Similarity Mixing
and n-times Random Mixing) to achieve data augmentation. Experiments show that
our proposed method can improve the classification accuracy of deep learning
based AMR models in the full public dataset RML2016.10a. In particular, for the
case of a single signal-to-noise ratio signal set, the classification accuracy
can be significantly improved, which verifies the effectiveness of the methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinjie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuangzhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dongwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huaji Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shanqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shilian Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xuan_Q/0/1/0/all/0/1"&gt;Qi Xuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoniu Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Kernel Method to Nonlinear Location Estimation with RSS-based Fingerprint. (arXiv:2204.03724v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2204.03724</id>
        <link href="http://arxiv.org/abs/2204.03724"/>
        <updated>2022-04-11T00:52:27.561Z</updated>
        <summary type="html"><![CDATA[This paper presents a nonlinear location estimation to infer the position of
a user holding a smartphone. We consider a large location with $M$ number of
grid points, each grid point is labeled with a unique fingerprint consisting of
the received signal strength (RSS) values measured from $N$ number of Bluetooth
Low Energy (BLE) beacons. Given the fingerprint observed by the smartphone, the
user's current location can be estimated by finding the top-k similar
fingerprints from the list of fingerprints registered in the database. Besides
the environmental factors, the dynamicity in holding the smartphone is another
source to the variation in fingerprint measurements, yet there are not many
studies addressing the fingerprint variability due to dynamic smartphone
positions held by human hands during online detection. To this end, we propose
a nonlinear location estimation using the kernel method. Specifically, our
proposed method comprises of two steps: 1) a beacon selection strategy to
select a subset of beacons that is insensitive to the subtle change of holding
positions, and 2) a kernel method to compute the similarity between this subset
of observed signals and all the fingerprints registered in the database. The
experimental results based on large-scale data collected in a complex building
indicate a substantial performance gain of our proposed approach in comparison
to state-of-the-art methods. The dataset consisting of the signal information
collected from the beacons is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1"&gt;Pai Chet Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spachos_P/0/1/0/all/0/1"&gt;Petros Spachos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1"&gt;James She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1"&gt;Konstantinos N. Plataniotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing a Framework and a Decision Protocol to Calibrate Recommender Systems. (arXiv:2204.03706v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.03706</id>
        <link href="http://arxiv.org/abs/2204.03706"/>
        <updated>2022-04-11T00:52:27.553Z</updated>
        <summary type="html"><![CDATA[Recommender Systems use the user's profile to generate a recommendation list
with unknown items to a target user. Although the primary goal of traditional
recommendation systems is to deliver the most relevant items, such an effort
unintentionally can cause collateral effects including low diversity and
unbalanced genres or categories, benefiting particular groups of categories.
This paper proposes an approach to create recommendation lists with a
calibrated balance of genres, avoiding disproportion between the user's profile
interests and the recommendation list. The calibrated recommendations consider
concomitantly the relevance and the divergence between the genres distributions
extracted from the user's preference and the recommendation list. The main
claim is that calibration can contribute positively to generate fairer
recommendations. In particular, we propose a new trade-off equation, which
considers the users' bias to provide a recommendation list that seeks for the
users' tendencies. Moreover, we propose a conceptual framework and a decision
protocol to generate more than one thousand combinations of calibrated systems
in order to find the best combination. We compare our approach against
state-of-the-art approaches using multiple domain datasets, which are analyzed
by rank and calibration metrics. The results indicate that the trade-off, which
considers the users' bias, produces positive effects on the precision and to
the fairness, thus generating recommendation lists that respect the genre
distribution and, through the decision protocol, we also found the best system
for each dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1"&gt;Diego Corr&amp;#xea;a da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durao_F/0/1/0/all/0/1"&gt;Frederico Ara&amp;#xfa;jo Dur&amp;#xe3;o&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TemporalUV: Capturing Loose Clothing with Temporally Coherent UV Coordinates. (arXiv:2204.03671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03671</id>
        <link href="http://arxiv.org/abs/2204.03671"/>
        <updated>2022-04-11T00:52:27.536Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to generate temporally coherent UV coordinates
for loose clothing. Our method is not constrained by human body outlines and
can capture loose garments and hair. We implemented a differentiable pipeline
to learn UV mapping between a sequence of RGB inputs and textures via UV
coordinates. Instead of treating the UV coordinates of each frame separately,
our data generation approach connects all UV coordinates via feature matching
for temporal stability. Subsequently, a generative model is trained to balance
the spatial quality and temporal stability. It is driven by supervised and
unsupervised losses in both UV and image spaces. Our experiments show that the
trained models output high-quality UV coordinates and generalize to new poses.
Once a sequence of UV coordinates has been inferred by our model, it can be
used to flexibly synthesize new looks and modified visual styles. Compared to
existing methods, our approach reduces the computational workload to animate
new outfits by several orders of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;You Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Huiqi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1"&gt;Angela Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thuerey_N/0/1/0/all/0/1"&gt;Nils Thuerey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T4PdM: a Deep Neural Network based on the Transformer Architecture for Fault Diagnosis of Rotating Machinery. (arXiv:2204.03725v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2204.03725</id>
        <link href="http://arxiv.org/abs/2204.03725"/>
        <updated>2022-04-11T00:52:27.528Z</updated>
        <summary type="html"><![CDATA[Deep learning and big data algorithms have become widely used in industrial
applications to optimize several tasks in many complex systems. Particularly,
deep learning model for diagnosing and prognosing machinery health has
leveraged predictive maintenance (PdM) to be more accurate and reliable in
decision making, in this way avoiding unnecessary interventions, machinery
accidents, and environment catastrophes. Recently, Transformer Neural Networks
have gained notoriety and have been increasingly the favorite choice for
Natural Language Processing (NLP) tasks. Thus, given their recent major
achievements in NLP, this paper proposes the development of an automatic fault
classifier model for predictive maintenance based on a modified version of the
Transformer architecture, namely T4PdM, to identify multiple types of faults in
rotating machinery. Experimental results are developed and presented for the
MaFaulDa and CWRU databases. T4PdM was able to achieve an overall accuracy of
99.98% and 98% for both datasets, respectively. In addition, the performance of
the proposed model is compared to other previously published works. It has
demonstrated the superiority of the model in detecting and classifying faults
in rotating industrial machinery. Therefore, the proposed Transformer-based
model can improve the performance of machinery fault analysis and diagnostic
processes and leverage companies to a new era of the Industry 4.0. In addition,
this methodology can be adapted to any other task of time series
classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1"&gt;Erick Giovani Sperandio Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Julian Santana Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_I/0/1/0/all/0/1"&gt;Ilan Sousa Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guarieiro_L/0/1/0/all/0/1"&gt;Lilian Lefol Nani Guarieiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain-Inspired Hyperdimensional Computing: How Thermal-Friendly for Edge Computing?. (arXiv:2204.03739v1 [cs.ET])]]></title>
        <id>http://arxiv.org/abs/2204.03739</id>
        <link href="http://arxiv.org/abs/2204.03739"/>
        <updated>2022-04-11T00:52:27.521Z</updated>
        <summary type="html"><![CDATA[Brain-inspired hyperdimensional computing (HDC) is an emerging machine
learning (ML) methods. It is based on large vectors of binary or bipolar
symbols and a few simple mathematical operations. The promise of HDC is a
highly efficient implementation for embedded systems like wearables. While fast
implementations have been presented, other constraints have not been considered
for edge computing. In this work, we aim at answering how thermal-friendly HDC
for edge computing is. Devices like smartwatches, smart glasses, or even mobile
systems have a restrictive cooling budget due to their limited volume. Although
HDC operations are simple, the vectors are large, resulting in a high number of
CPU operations and thus a heavy load on the entire system potentially causing
temperature violations. In this work, the impact of HDC on the chip's
temperature is investigated for the first time. We measure the temperature and
power consumption of a commercial embedded system and compare HDC with
conventional CNN. We reveal that HDC causes up to 6.8{\deg}C higher
temperatures and leads to up to 47% more CPU throttling. Even when both HDC and
CNN aim for the same throughput (i.e., perform a similar number of
classifications per second), HDC still causes higher on-chip temperatures due
to the larger power consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Genssler_P/0/1/0/all/0/1"&gt;Paul R. Genssler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vas_A/0/1/0/all/0/1"&gt;Austin Vas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amrouch_H/0/1/0/all/0/1"&gt;Hussam Amrouch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Multiple Self-Supervised Tasks Improves Model Robustness. (arXiv:2204.03714v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03714</id>
        <link href="http://arxiv.org/abs/2204.03714"/>
        <updated>2022-04-11T00:52:27.513Z</updated>
        <summary type="html"><![CDATA[Deep networks achieve state-of-the-art performance on computer vision tasks,
yet they fail under adversarial attacks that are imperceptible to humans. In
this paper, we propose a novel defense that can dynamically adapt the input
using the intrinsic structure from multiple self-supervised tasks. By
simultaneously using many self-supervised tasks, our defense avoids
over-fitting the adapted image to one specific self-supervised task and
restores more intrinsic structure in the image compared to a single
self-supervised task approach. Our approach further improves robustness and
clean accuracy significantly compared to the state-of-the-art single task
self-supervised defense. Our work is the first to connect multiple
self-supervised tasks to robustness, and suggests that we can achieve better
robustness with more intrinsic signal from visual data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lawhon_M/0/1/0/all/0/1"&gt;Matthew Lawhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1"&gt;Chengzhi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junfeng Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BankNote-Net: Open dataset for assistive universal currency recognition. (arXiv:2204.03738v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03738</id>
        <link href="http://arxiv.org/abs/2204.03738"/>
        <updated>2022-04-11T00:52:27.506Z</updated>
        <summary type="html"><![CDATA[Millions of people around the world have low or no vision. Assistive software
applications have been developed for a variety of day-to-day tasks, including
optical character recognition, scene identification, person recognition, and
currency recognition. This last task, the recognition of banknotes from
different denominations, has been addressed by the use of computer vision
models for image recognition. However, the datasets and models available for
this task are limited, both in terms of dataset size and in variety of
currencies covered. In this work, we collect a total of 24,826 images of
banknotes in variety of assistive settings, spanning 17 currencies and 112
denominations. Using supervised contrastive learning, we develop a machine
learning model for universal currency recognition. This model learns compliant
embeddings of banknote images in a variety of contexts, which can be shared
publicly (as a compressed vector representation), and can be used to train and
test specialized downstream models for any currency, including those not
covered by our dataset or for which only a few real images per denomination are
available (few-shot learning). We deploy a variation of this model for public
use in the last version of the Seeing AI app developed by Microsoft. We share
our encoder model and the embeddings as an open dataset in our BankNote-Net
repository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oviedo_F/0/1/0/all/0/1"&gt;Felipe Oviedo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinnakota_S/0/1/0/all/0/1"&gt;Srinivas Vinnakota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seleznev_E/0/1/0/all/0/1"&gt;Eugene Seleznev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malhotra_H/0/1/0/all/0/1"&gt;Hemant Malhotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1"&gt;Saqib Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Walk Autonomously via Reset-Free Quality-Diversity. (arXiv:2204.03655v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03655</id>
        <link href="http://arxiv.org/abs/2204.03655"/>
        <updated>2022-04-11T00:52:27.486Z</updated>
        <summary type="html"><![CDATA[Quality-Diversity (QD) algorithms can discover large and complex behavioural
repertoires consisting of both diverse and high-performing skills. However, the
generation of behavioural repertoires has mainly been limited to simulation
environments instead of real-world learning. This is because existing QD
algorithms need large numbers of evaluations as well as episodic resets, which
require manual human supervision and interventions. This paper proposes
Reset-Free Quality-Diversity optimization (RF-QD) as a step towards autonomous
learning for robotics in open-ended environments. We build on Dynamics-Aware
Quality-Diversity (DA-QD) and introduce a behaviour selection policy that
leverages the diversity of the imagined repertoire and environmental
information to intelligently select of behaviours that can act as automatic
resets. We demonstrate this through a task of learning to walk within defined
training zones with obstacles. Our experiments show that we can learn full
repertoires of legged locomotion controllers autonomously without manual resets
with high sample efficiency in spite of harsh safety constraints. Finally,
using an ablation of different target objectives, we show that it is important
for RF-QD to have diverse types solutions available for the behaviour selection
policy over solutions optimised with a specific objective. Videos and code
available at https://sites.google.com/view/rf-qd.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1"&gt;Bryan Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichenbach_A/0/1/0/all/0/1"&gt;Alexander Reichenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1"&gt;Antoine Cully&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identification of Autism spectrum disorder based on a novel feature selection method and Variational Autoencoder. (arXiv:2204.03654v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2204.03654</id>
        <link href="http://arxiv.org/abs/2204.03654"/>
        <updated>2022-04-11T00:52:27.479Z</updated>
        <summary type="html"><![CDATA[The development of noninvasive brain imaging such as resting-state functional
magnetic resonance imaging (rs-fMRI) and its combination with AI algorithm
provides a promising solution for the early diagnosis of Autism spectrum
disorder (ASD). However, the performance of the current ASD classification
based on rs-fMRI still needs to be improved. This paper introduces a
classification framework to aid ASD diagnosis based on rs-fMRI. In the
framework, we proposed a novel filter feature selection method based on the
difference between step distribution curves (DSDC) to select remarkable
functional connectivities (FCs) and utilized a multilayer perceptron (MLP)
which was pretrained by a simplified Variational Autoencoder (VAE) for
classification. We also designed a pipeline consisting of a normalization
procedure and a modified hyperbolic tangent (tanh) activation function to
replace the original tanh function, further improving the model accuracy. Our
model was evaluated by 10 times 10-fold cross-validation and achieved an
average accuracy of 78.12%, outperforming the state-of-the-art methods reported
on the same dataset. Given the importance of sensitivity and specificity in
disease diagnosis, two constraints were designed in our model which can improve
the model's sensitivity and specificity by up to 9.32% and 10.21%,
respectively. The added constraints allow our model to handle different
application scenarios and can be used broadly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yanjie Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xi_W/0/1/0/all/0/1"&gt;Wenhui Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yi Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Qade: Solving Differential Equations on Quantum Annealers. (arXiv:2204.03657v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2204.03657</id>
        <link href="http://arxiv.org/abs/2204.03657"/>
        <updated>2022-04-11T00:52:27.472Z</updated>
        <summary type="html"><![CDATA[We present a general method, called Qade, for solving differential equations
using a quantum annealer. The solution is obtained as a linear combination of a
set of basis functions. On current devices, Qade can solve systems of coupled
partial differential equations that depend linearly on the solution and its
derivatives, with non-linear variable coefficients and arbitrary inhomogeneous
terms. We test the method with several examples and find that state-of-the-art
quantum annealers can find the solution accurately for problems requiring a
small enough function basis. We provide a Python package implementing the
method at gitlab.com/jccriado/qade.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Criado_J/0/1/0/all/0/1"&gt;Juan Carlos Criado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Spannowsky_M/0/1/0/all/0/1"&gt;Michael Spannowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive-Gravity: A Defense Against Adversarial Samples. (arXiv:2204.03694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03694</id>
        <link href="http://arxiv.org/abs/2204.03694"/>
        <updated>2022-04-11T00:52:27.460Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel model training solution, denoted as
Adaptive-Gravity, for enhancing the robustness of deep neural network
classifiers against adversarial examples. We conceptualize the model
parameters/features associated with each class as a mass characterized by its
centroid location and the spread (standard deviation of the distance) of
features around the centroid. We use the centroid associated with each cluster
to derive an anti-gravity force that pushes the centroids of different classes
away from one another during network training. Then we customized an objective
function that aims to concentrate each class's features toward their
corresponding new centroid, which has been obtained by anti-gravity force. This
methodology results in a larger separation between different masses and reduces
the spread of features around each centroid. As a result, the samples are
pushed away from the space that adversarial examples could be mapped to,
effectively increasing the degree of perturbation needed for making an
adversarial example. We have implemented this training solution as an iterative
method consisting of four steps at each iteration: 1) centroid extraction, 2)
anti-gravity force calculation, 3) centroid relocation, and 4) gravity
training. Gravity's efficiency is evaluated by measuring the corresponding
fooling rates against various attack models, including FGSM, MIM, BIM, and PGD
using LeNet and ResNet110 networks, benchmarked against MNIST and CIFAR10
classification problems. Test results show that Gravity not only functions as a
powerful instrument to robustify a model against state-of-the-art adversarial
attacks but also effectively improves the model training accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirzaeian_A/0/1/0/all/0/1"&gt;Ali Mirzaeian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+D_S/0/1/0/all/0/1"&gt;Sai Manoj P D&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latibari_B/0/1/0/all/0/1"&gt;Banafsheh S. Latibari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savidis_I/0/1/0/all/0/1"&gt;Ioannis Savidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Homayoun_H/0/1/0/all/0/1"&gt;Houman Homayoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasan_A/0/1/0/all/0/1"&gt;Avesta Sasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood Inference from Sampled Trajectories. (arXiv:2106.01808v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01808</id>
        <link href="http://arxiv.org/abs/2106.01808"/>
        <updated>2022-04-11T00:52:26.948Z</updated>
        <summary type="html"><![CDATA[Simulation-based inference enables learning the parameters of a model even
when its likelihood cannot be computed in practice. One class of methods uses
data simulated with different parameters to infer models of the
likelihood-to-evidence ratio, or equivalently the posterior function. Here we
frame the inference task as an estimation of an energy function parametrized
with an artificial neural network. We present an intuitive approach where the
optimal model of the likelihood-to-evidence ratio is found by maximizing the
likelihood of simulated data. Within this framework, the connection between the
task of simulation-based inference and mutual information maximization is
clear, and we show how several known methods of posterior estimation relate to
alternative lower bounds to mutual information. These distinct objective
functions aim at the same optimal energy form and therefore can be directly
benchmarked. We compare their accuracy in the inference of model parameters,
focusing on four dynamical systems that encompass common challenges in time
series analysis: dynamics driven by multiplicative noise, nonlinear
interactions, chaotic behavior, and high-dimensional parameter space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isacchini_G/0/1/0/all/0/1"&gt;Giulio Isacchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spisak_N/0/1/0/all/0/1"&gt;Natanael Spisak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nourmohammad_A/0/1/0/all/0/1"&gt;Armita Nourmohammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mora_T/0/1/0/all/0/1"&gt;Thierry Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walczak_A/0/1/0/all/0/1"&gt;Aleksandra M. Walczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overlapping Spaces for Compact Graph Representations. (arXiv:2007.02445v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02445</id>
        <link href="http://arxiv.org/abs/2007.02445"/>
        <updated>2022-04-11T00:52:26.939Z</updated>
        <summary type="html"><![CDATA[Various non-trivial spaces are becoming popular for embedding structured data
such as graphs, texts, or images. Following spherical and hyperbolic spaces,
more general product spaces have been proposed. However, searching for the best
configuration of product space is a resource-intensive procedure, which reduces
the practical applicability of the idea. We generalize the concept of product
space and introduce an overlapping space that does not have the configuration
search problem. The main idea is to allow subsets of coordinates to be shared
between spaces of different types (Euclidean, hyperbolic, spherical). As a
result, parameter optimization automatically learns the optimal configuration.
Additionally, overlapping spaces allow for more compact representations since
their geometry is more complex. Our experiments confirm that overlapping spaces
outperform the competitors in graph embedding tasks. Here, we consider both
distortion setup, where the aim is to preserve distances, and ranking setup,
where the relative order should be preserved. The proposed method effectively
solves the problem and outperforms the competitors in both settings. We also
perform an empirical analysis in a realistic information retrieval task, where
we compare all spaces by incorporating them into DSSM. In this case, the
proposed overlapping space consistently achieves nearly optimal results without
any configuration tuning. This allows for reducing training time, which can be
significant in large-scale applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shevkunov_K/0/1/0/all/0/1"&gt;Kirill Shevkunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1"&gt;Liudmila Prokhorenkova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiability of Label Noise Transition Matrix. (arXiv:2202.02016v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.02016</id>
        <link href="http://arxiv.org/abs/2202.02016"/>
        <updated>2022-04-11T00:52:26.931Z</updated>
        <summary type="html"><![CDATA[The noise transition matrix plays a central role in the problem of learning
from noisy labels. Among many other reasons, a significant number of existing
solutions rely on access to it. Estimating the transition matrix without using
ground truth labels is a critical and challenging task. When label noise
transition depends on each instance, the problem of identifying the
instance-dependent noise transition matrix becomes substantially more
challenging. Despite recent works proposing solutions for learning from
instance-dependent noisy labels, we lack a unified understanding of when such a
problem remains identifiable, and therefore learnable. This paper seeks to
provide answers to a sequence of related questions: What are the primary
factors that contribute to the identifiability of a noise transition matrix?
Can we explain the observed empirical successes? When a problem is not
identifiable, what can we do to make it so? We will relate our theoretical
findings to the literature and hope to provide guidelines for developing
effective solutions for battling instance-dependent label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Linear Regression for $\ell_p$ Norms and Beyond. (arXiv:2111.04888v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.04888</id>
        <link href="http://arxiv.org/abs/2111.04888"/>
        <updated>2022-04-11T00:52:26.924Z</updated>
        <summary type="html"><![CDATA[We study active sampling algorithms for linear regression, which aim to query
only a few entries of a target vector $b\in\mathbb R^n$ and output a near
minimizer to $\min_{x\in\mathbb R^d} \|Ax-b\|$, for a design matrix
$A\in\mathbb R^{n \times d}$ and loss $\|\cdot\|$.

For $p$ norm regression for any $0<p<\infty$, we give an algorithm based on
Lewis weight sampling outputting a $(1+\epsilon)$-approximate solution using
just $\tilde O(d/\epsilon^2)$ queries to $b$ for $p\in(0,1)$,
$\tilde{O}(d/\epsilon)$ queries for $1<p<2$, and
$\tilde{O}(d^{p/2}/\epsilon^p)$ queries for $2<p<\infty$. For $0<p<2$, our
bounds are optimal up to log factors, settling the query complexity for this
range. For $2<p<\infty$, our dependence on $d$ is optimal, while our dependence
on $\epsilon$ is off by at most $\epsilon$, up to log factors. Our result
resolves an open question of [CD21], who gave near optimal bounds for the $1$
norm, but required $d^2/\epsilon^2$ samples for $\ell_p$ regression with
$1<p<2$, and gave no bounds for $2<p<\infty$ or $0<p<1$.

We also give the first total sensitivity bound of
$O(d^{\max\{1,p/2\}}\log^2n)$ for loss functions of degree $p$ polynomial
growth, improving a result of [TMF20]. By combining this with our techniques
for $\ell_p$ regression, we obtain an active regression algorithm making
$\tilde O(d^{1+\max\{1,p/2\}}/\mathrm{poly}(\epsilon))$ queries for such loss
functions, including the Tukey and Huber losses, answering another question of
[CD21]. For the Huber loss, we further improve our bound to $\tilde
O(d^{4-2\sqrt2}/\mathrm{poly}(\epsilon))$ samples. Our sensitivity bounds also
have many applications, including Orlicz norm subspace embeddings, robust
subspace approximation, and dimension reduction for smoothed $p$-norms.

Finally, our active sampling results give the first sublinear time algorithms
for Kronecker product regression under every $p$ norm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Christopher Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasuda_T/0/1/0/all/0/1"&gt;Taisuke Yasuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Stochastic Extragradient for Bilinear Games using Restarted Iteration Averaging. (arXiv:2107.00464v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00464</id>
        <link href="http://arxiv.org/abs/2107.00464"/>
        <updated>2022-04-11T00:52:26.916Z</updated>
        <summary type="html"><![CDATA[We study the stochastic bilinear minimax optimization problem, presenting an
analysis of the same-sample Stochastic ExtraGradient (SEG) method with constant
step size, and presenting variations of the method that yield favorable
convergence. In sharp contrasts with the basic SEG method whose last iterate
only contracts to a fixed neighborhood of the Nash equilibrium, SEG augmented
with iteration averaging provably converges to the Nash equilibrium under the
same standard settings, and such a rate is further improved by incorporating a
scheduled restarting procedure. In the interpolation setting where noise
vanishes at the Nash equilibrium, we achieve an optimal convergence rate up to
tight constants. We present numerical experiments that validate our theoretical
findings and demonstrate the effectiveness of the SEG method when equipped with
iteration averaging and restarting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_C/0/1/0/all/0/1"&gt;Chris Junchi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Loizou_N/0/1/0/all/0/1"&gt;Nicolas Loizou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance-Free Sparse Bayesian Learning. (arXiv:2105.10439v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10439</id>
        <link href="http://arxiv.org/abs/2105.10439"/>
        <updated>2022-04-11T00:52:26.898Z</updated>
        <summary type="html"><![CDATA[Sparse Bayesian learning (SBL) is a powerful framework for tackling the
sparse coding problem while also providing uncertainty quantification. The most
popular inference algorithms for SBL exhibit prohibitively large computational
costs for high-dimensional problems due to the need to maintain a large
covariance matrix. To resolve this issue, we introduce a new method for
accelerating SBL inference -- named covariance-free expectation maximization
(CoFEM) -- that avoids explicit computation of the covariance matrix. CoFEM
solves multiple linear systems to obtain unbiased estimates of the posterior
statistics needed by SBL. This is accomplished by exploiting innovations from
numerical linear algebra such as preconditioned conjugate gradient and a
little-known diagonal estimation rule. For a large class of compressed sensing
matrices, we provide theoretical justifications for why our method scales well
in high-dimensional settings. Through simulations, we show that CoFEM can be up
to thousands of times faster than existing baselines without sacrificing coding
accuracy. Through applications to calcium imaging deconvolution and
multi-contrast MRI reconstruction, we show that CoFEM enables SBL to tractably
tackle high-dimensional sparse coding problems of practical interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1"&gt;Alexander Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1"&gt;Andrew H. Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1"&gt;Berkin Bilgic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ba_D/0/1/0/all/0/1"&gt;Demba Ba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-learning with online random forests. (arXiv:2204.03771v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03771</id>
        <link href="http://arxiv.org/abs/2204.03771"/>
        <updated>2022-04-11T00:52:26.891Z</updated>
        <summary type="html"><![CDATA[$Q$-learning is the most fundamental model-free reinforcement learning
algorithm. Deployment of $Q$-learning requires approximation of the
state-action value function (also known as the $Q$-function). In this work, we
provide online random forests as $Q$-function approximators and propose a novel
method wherein the random forest is grown as learning proceeds (through
expanding forests). We demonstrate improved performance of our methods over
state-of-the-art Deep $Q$-Networks in two OpenAI gyms (`blackjack' and
`inverted pendulum') but not in the `lunar lander' gym. We suspect that the
resilience to overfitting enjoyed by random forests recommends our method for
common tasks that do not require a strong representation of the problem domain.
We show that expanding forests (in which the number of trees increases as data
comes in) improve performance, suggesting that expanding forests are viable for
other applications of online random forests beyond the reinforcement learning
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Min_J/0/1/0/all/0/1"&gt;Joosung Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elliott_L/0/1/0/all/0/1"&gt;Lloyd T. Elliott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretext Tasks selection for multitask self-supervised speech representation learning. (arXiv:2107.00594v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00594</id>
        <link href="http://arxiv.org/abs/2107.00594"/>
        <updated>2022-04-11T00:52:26.883Z</updated>
        <summary type="html"><![CDATA[Through solving pretext tasks, self-supervised learning leverages unlabeled
data to extract useful latent representations replacing traditional input
features in the downstream task. In audio/speech signal processing, a wide
range of features where engineered through decades of research efforts. As it
turns out, learning to predict such features (a.k.a pseudo-labels) has proven
to be a particularly relevant pretext task, leading to useful self-supervised
representations which prove to be effective for downstream tasks. However,
methods and common practices for combining such pretext tasks for better
performance on the downstream task have not been explored and understood
properly. In fact, the process relies almost exclusively on a computationally
heavy experimental procedure, which becomes intractable with the increase of
the number of pretext tasks. This paper introduces a method to select a group
of pretext tasks among a set of candidates. The method we propose estimates
calibrated weights for the partial losses corresponding to the considered
pretext tasks during the self-supervised training process. The experiments
conducted on automatic speech recognition, speaker and emotion recognition
validate our approach, as the groups selected and weighted with our method
perform better than classic baselines, thus facilitating the selection and
combination of relevant pseudo-labels for self-supervised representation
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1"&gt;Salah Zaiem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1"&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essid_S/0/1/0/all/0/1"&gt;Slim Essid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heba_A/0/1/0/all/0/1"&gt;Abdel Heba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Polynomial Transformations. (arXiv:2204.04209v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.04209</id>
        <link href="http://arxiv.org/abs/2204.04209"/>
        <updated>2022-04-11T00:52:26.875Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning high dimensional polynomial
transformations of Gaussians. Given samples of the form $p(x)$, where $x\sim
N(0, \mathrm{Id}_r)$ is hidden and $p: \mathbb{R}^r \to \mathbb{R}^d$ is a
function where every output coordinate is a low-degree polynomial, the goal is
to learn the distribution over $p(x)$. This problem is natural in its own
right, but is also an important special case of learning deep generative
models, namely pushforwards of Gaussians under two-layer neural networks with
polynomial activations. Understanding the learnability of such generative
models is crucial to understanding why they perform so well in practice.

Our first main result is a polynomial-time algorithm for learning quadratic
transformations of Gaussians in a smoothed setting. Our second main result is a
polynomial-time algorithm for learning constant-degree polynomial
transformations of Gaussian in a smoothed setting, when the rank of the
associated tensors is small. In fact our results extend to any
rotation-invariant input distribution, not just Gaussian. These are the first
end-to-end guarantees for learning a pushforward under a neural network with
more than one layer.

Along the way, we also give the first polynomial-time algorithms with
provable guarantees for tensor ring decomposition, a popular generalization of
tensor decomposition that is used in practice to implicitly store large
tensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sitan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-stage Training of Graph Neural Networks for Graph Classification. (arXiv:2011.05097v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05097</id>
        <link href="http://arxiv.org/abs/2011.05097"/>
        <updated>2022-04-11T00:52:26.867Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have received massive attention in the field of
machine learning on graphs. Inspired by the success of neural networks, a line
of research has been conducted to train GNNs to deal with various tasks, such
as node classification, graph classification, and link prediction. In this
work, our task of interest is graph classification. Several GNN models have
been proposed and shown great accuracy in this task. However, the question is
whether usual training methods fully realize the capacity of the GNN models.

In this work, we propose a two-stage training framework based on triplet
loss. In the first stage, GNN is trained to map each graph to a Euclidean-space
vector so that graphs of the same class are close while those of different
classes are mapped far apart. Once graphs are well-separated based on labels, a
classifier is trained to distinguish between different classes. This method is
generic in the sense that it is compatible with any GNN model. By adapting five
GNN models to our method, we demonstrate the consistent improvement in accuracy
and utilization of each GNN's allocated capacity over the original training
method of each model up to 5.4\% points in 12 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1"&gt;Manh Tuan Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"&gt;Noseong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kijung Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity versus Depth: An Information Theoretic Analysis. (arXiv:2203.00246v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.00246</id>
        <link href="http://arxiv.org/abs/2203.00246"/>
        <updated>2022-04-11T00:52:26.848Z</updated>
        <summary type="html"><![CDATA[Deep learning has proven effective across a range of data sets. In light of
this, a natural inquiry is: "for what data generating processes can deep
learning succeed?" In this work, we study the sample complexity of learning
multilayer data generating processes of a sort for which deep neural networks
seem to be suited. We develop general and elegant information-theoretic tools
that accommodate analysis of any data generating process -- shallow or deep,
parametric or nonparametric, noiseless or noisy. We then use these tools to
characterize the dependence of sample complexity on the depth of multilayer
processes. Our results indicate roughly linear dependence on depth. This is in
contrast to previous results that suggest exponential or high-order polynomial
dependence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hong Jun Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF-Coder: Program Synthesis for Tensor Manipulations. (arXiv:2003.09040v4 [cs.PL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.09040</id>
        <link href="http://arxiv.org/abs/2003.09040"/>
        <updated>2022-04-11T00:52:26.839Z</updated>
        <summary type="html"><![CDATA[The success and popularity of deep learning is on the rise, partially due to
powerful deep learning frameworks such as TensorFlow and PyTorch that make it
easier to develop deep learning models. However, these libraries also come with
steep learning curves, since programming in these frameworks is quite different
from traditional imperative programming with explicit loops and conditionals.
In this work, we present a tool called TF-Coder for programming by example in
TensorFlow. TF-Coder uses a bottom-up weighted enumerative search, with
value-based pruning of equivalent expressions and flexible type- and
value-based filtering to ensure that expressions adhere to various requirements
imposed by the TensorFlow library. We train models to predict TensorFlow
operations from features of the input and output tensors and natural language
descriptions of tasks, to prioritize relevant operations during search.
TF-Coder solves 63 of 70 real-world tasks within 5 minutes, sometimes finding
simpler solutions in less time compared to experienced human programmers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1"&gt;Kensen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1"&gt;David Bieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural network training under semidefinite constraints. (arXiv:2201.00632v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.00632</id>
        <link href="http://arxiv.org/abs/2201.00632"/>
        <updated>2022-04-11T00:52:26.832Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with the training of neural networks (NNs) under
semidefinite constraints, which allows for NN training with robustness and
stability guarantees. In particular, we set up an efficient and scalable
training scheme for NN training problems of this kind based on interior point
methods, while we also exploit the structure of the underlying matrix
constraint. We apply our training scheme to several relevant examples that have
been studied in the literature and newly present the application of the method
to the training of Wasserstein generative adversarial networks (WGANs). In
numerical examples, we show the superiority of our method and its applicability
to WGAN training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pauli_P/0/1/0/all/0/1"&gt;Patricia Pauli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funcke_N/0/1/0/all/0/1"&gt;Niklas Funcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gramlich_D/0/1/0/all/0/1"&gt;Dennis Gramlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Msalmi_M/0/1/0/all/0/1"&gt;Mohamed Amine Msalmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allgower_F/0/1/0/all/0/1"&gt;Frank Allg&amp;#xf6;wer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trading off Accuracy for Speedup: Multiplier Bootstraps for Subgraph Counts. (arXiv:2009.06170v5 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06170</id>
        <link href="http://arxiv.org/abs/2009.06170"/>
        <updated>2022-04-11T00:52:26.823Z</updated>
        <summary type="html"><![CDATA[We propose a new class of multiplier bootstraps for count functionals,
ranging from a fast, approximate linear bootstrap tailored to sparse, massive
graphs to a quadratic bootstrap procedure that offers refined accuracy for
smaller, denser graphs. For the fast, approximate linear bootstrap, we show
that $\sqrt{n}$-consistent inference of the count functional is attainable in
certain computational regimes that depend on the sparsity level of the graph.
Furthermore, even in more challenging regimes, we prove that our bootstrap
procedure offers valid coverage and vanishing confidence intervals. For the
quadratic bootstrap, we establish an Edgeworth expansion and show that this
procedure offers higher-order accuracy under appropriate sparsity conditions.
We complement our theoretical results with a simulation study and real data
analysis and verify that our procedure offers state-of-the-art performance for
several functionals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qiaohui Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lunde_R/0/1/0/all/0/1"&gt;Robert Lunde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sarkar_P/0/1/0/all/0/1"&gt;Purnamrita Sarkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Energy Evaluation Using Marginalized Annealed Importance Sampling. (arXiv:2204.03784v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03784</id>
        <link href="http://arxiv.org/abs/2204.03784"/>
        <updated>2022-04-11T00:52:26.805Z</updated>
        <summary type="html"><![CDATA[The evaluation of the free energy of a stochastic model is considered to be a
significant issue in various fields of physics and machine learning. However,
the exact free energy evaluation is computationally infeasible because it
includes an intractable partition function. Annealed importance sampling (AIS)
is a type of importance sampling based on the Markov chain Monte Carlo method,
which is similar to a simulated annealing, and can effectively approximate the
free energy. This study proposes a new AIS-based approach, referred to as
marginalized AIS (mAIS). The statistical efficiency of mAIS is investigated in
detail based on a theoretical and numerical perspectives. Based on the
investigation, it has been proved that mAIS is more effective than AIS under a
certain condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yasuda_M/0/1/0/all/0/1"&gt;Muneki Yasuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takahashi_C/0/1/0/all/0/1"&gt;Chako Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handling highly correlated genes in prediction analysis of genomic studies. (arXiv:2007.02455v4 [stat.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02455</id>
        <link href="http://arxiv.org/abs/2007.02455"/>
        <updated>2022-04-11T00:52:26.796Z</updated>
        <summary type="html"><![CDATA[Background: Selecting feature genes to predict phenotypes is one of the
typical tasks in analyzing genomics data. Though many general-purpose
algorithms were developed for prediction, dealing with highly correlated genes
in the prediction model is still not well addressed. High correlation among
genes introduces technical problems, such as multi-collinearity issues, leading
to unreliable prediction models. Furthermore, when a causal gene (whose
variants have an actual biological effect on a phenotype) is highly correlated
with other genes, most algorithms select the feature gene from the correlated
group in a purely data-driven manner. Since the correlation structure among
genes could change substantially when condition changes, the prediction model
based on not correctly selected feature genes is unreliable. Therefore, we aim
to keep the causal biological signal in the prediction process and build a more
robust prediction model.

Method: We propose a grouping algorithm, which treats highly correlated genes
as a group and uses their common pattern to represent the group's biological
signal in feature selection. Our novel grouping algorithm can be integrated
into existing prediction algorithms to enhance their prediction performance.
Our proposed grouping method has two advantages. First, using the gene group's
common patterns makes the prediction more robust and reliable under condition
change. Second, it reports whole correlated gene groups as discovered
biomarkers for prediction tasks, allowing researchers to conduct follow-up
studies to identify causal genes within the identified groups.

Result: Using real benchmark scRNA-seq datasets with simulated cell
phenotypes, we demonstrate our novel method significantly outperforms standard
models in both (1) prediction of cell phenotypes and (2) feature gene
selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xing_L/0/1/0/all/0/1"&gt;Li Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Joun_S/0/1/0/all/0/1"&gt;Songwan Joun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mackay_K/0/1/0/all/0/1"&gt;Kurt Mackay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lesperance_M/0/1/0/all/0/1"&gt;Mary Lesperance&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuekui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeded graph matching for the correlated Wigner model via the projected power method. (arXiv:2204.04099v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2204.04099</id>
        <link href="http://arxiv.org/abs/2204.04099"/>
        <updated>2022-04-11T00:52:26.788Z</updated>
        <summary type="html"><![CDATA[In the graph matching problem we observe two graphs $G,H$ and the goal is to
find an assignment (or matching) between their vertices such that some measure
of edge agreement is maximized. We assume in this work that the observed pair
$G,H$ has been drawn from the correlated Wigner model -- a popular model for
correlated weighted graphs -- where the entries of the adjacency matrices of
$G$ and $H$ are independent Gaussians and each edge of $G$ is correlated with
one edge of $H$ (determined by the unknown matching) with the edge correlation
described by a parameter $\sigma\in [0,1)$. In this paper, we analyse the
performance of the projected power method (PPM) as a seeded graph matching
algorithm where we are given an initial partially correct matching (called the
seed) as side information. We prove that if the seed is close enough to the
ground-truth matching, then with high probability, PPM iteratively improves the
seed and recovers the ground-truth matching (either partially or exactly) in
$\mathcal{O}(\log n)$ iterations. Our results prove that PPM works even in
regimes of constant $\sigma$, thus extending the analysis in (Mao et al.,2021)
for the sparse Erd\"os-Renyi model to the (dense) Wigner model. As a byproduct
of our analysis, we see that the PPM framework generalizes some of the
state-of-art algorithms for seeded graph matching. We support and complement
our theoretical findings with numerical experiments on synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Araya_E/0/1/0/all/0/1"&gt;Ernesto Araya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Braun_G/0/1/0/all/0/1"&gt;Guillaume Braun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tyagi_H/0/1/0/all/0/1"&gt;Hemant Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trippy AI Dream 30 - Howl's Moving Castle Post-Apocalyptic War Scenes VQ...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u0sirs/trippy_ai_dream_30_howls_moving_castle/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u0sirs/trippy_ai_dream_30_howls_moving_castle/"/>
        <updated>2022-04-10T22:16:54.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/LordPewPew777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Circular slide rule]]></title>
        <id>https://www.johndcook.com/blog/?p=102336</id>
        <link href="https://www.johndcook.com/blog/2022/04/10/circular-slide-rule/"/>
        <updated>2022-04-10T22:05:38.000Z</updated>
        <summary type="html"><![CDATA[I explained the basics of how a slide rule works in the previous post. But how does a circular slide rule work? Apparently the prop Mr. Spock is holding is an E6B aircraft slide rule. It includes a circular slide rule and more functionality. Start with an ordinary straight slide rule, with each bar labeled […]
Circular slide rule first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why a slide rule works]]></title>
        <id>https://www.johndcook.com/blog/?p=102298</id>
        <link href="https://www.johndcook.com/blog/2022/04/10/why-a-slide-rule-works/"/>
        <updated>2022-04-10T20:49:00.000Z</updated>
        <summary type="html"><![CDATA[Suppose you have two sticks. The length of one is log x, and the length of the other is log y. If you put the two sticks end to end, the combined length is log x + log y = log xy. That’s the basic idea behind a slide rule. The simplest slide rule consists […]
Why a slide rule works first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there a AI which can turn images into simple versions of the original image?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u0ppb8/is_there_a_ai_which_can_turn_images_into_simple/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u0ppb8/is_there_a_ai_which_can_turn_images_into_simple/"/>
        <updated>2022-04-10T20:02:04.000Z</updated>
        <summary type="html"><![CDATA[So that the wrinkles and shadows are removed, etc.
    submitted by    /u/xXNOdrugsForMEXx  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine Learning - WAYR (What Are You Reading) - Week 135]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0pnhf/d_machine_learning_wayr_what_are_you_reading_week/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0pnhf/d_machine_learning_wayr_what_are_you_reading_week/"/>
        <updated>2022-04-10T20:00:05.000Z</updated>
        <summary type="html"><![CDATA[This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.
 Please try to provide some insight from your understanding and please don't post things which are present in wiki.
 Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.
 Previous weeks :
  
 1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80 81-90 91-100 101-110 111-120 121-130 131-140 
  
 Week 1 Week 11 Week 21 Week 31 Week 41 Week 51 Week 61 Week 71 Week 81 Week 91 Week 101 Week 111 Week 121 Week 131 
  Week 2 Week 1…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[6 Business Applications that Badly Need Better AI]]></title>
        <id>https://www.datasciencecentral.com/?p=57156</id>
        <link href="https://www.datasciencecentral.com/6-business-applications-that-badly-need-better-ai/"/>
        <updated>2022-04-10T19:51:00.000Z</updated>
        <summary type="html"><![CDATA[The success and growth of AI is undeniable. Yet there are still basic tasks performing poorly, despite or because of automation. In some cases, you can blame reliance on outdated AI. In other cases, it is a result of corporate policies or multiple AI systems that compete against each other. The AI systems in question… Read More »6 Business Applications that Badly Need Better AI
The post 6 Business Applications that Badly Need Better AI appeared first on Data Science Central.]]></summary>
        <author>
            <name>Vincent Granville</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How exactly do you define Artificial Intelligence(AI)?]]></title>
        <id>https://www.datasciencecentral.com/?p=57148</id>
        <link href="https://www.datasciencecentral.com/how-exactly-do-you-define-artificial-intelligenceai/"/>
        <updated>2022-04-10T19:44:52.000Z</updated>
        <summary type="html"><![CDATA[How exactly do you define Artificial Intelligence(AI)? This looks like a back to basics/ back to school question – but the answer is not that simple Recently I was trying to find a good academic definition of AI for a research paper. Surprisingly, its not easy. In this post, I present a good definition for… Read More »How exactly do you define Artificial Intelligence(AI)?
The post How exactly do you define Artificial Intelligence(AI)? appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Singularity is Now]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u0pbar/the_singularity_is_now/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u0pbar/the_singularity_is_now/"/>
        <updated>2022-04-10T19:44:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ManandMultiverse  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Public wary of Meta’s Metaverse vision]]></title>
        <id>https://www.datasciencecentral.com/?p=57153</id>
        <link href="https://www.datasciencecentral.com/public-wary-of-metas-metaverse-vision/"/>
        <updated>2022-04-10T18:45:35.000Z</updated>
        <summary type="html"><![CDATA[There are signs that Meta’s plans for the metaverse is faltering, including plummeting stock prices and the company’s announcement that it may withdraw from the EU market. The troubles stem from a myriad of issues, the most significant of which are data collection privacy issues and a lack of investor and public confidence in the… Read More »Public wary of Meta’s Metaverse vision
The post Public wary of Meta’s Metaverse vision appeared first on Data Science Central.]]></summary>
        <author>
            <name>Stephanie Glen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anybody ever programmed a 1st order differential equation model in MuJoCo?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u0o1kx/anybody_ever_programmed_a_1st_order_differential/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u0o1kx/anybody_ever_programmed_a_1st_order_differential/"/>
        <updated>2022-04-10T18:44:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/SmarterCloud  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N]: Dall-E 2 Explained]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0o0yy/n_dalle_2_explained/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0o0yy/n_dalle_2_explained/"/>
        <updated>2022-04-10T18:43:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/giugiacaglia  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google AI Researchers Propose a Meta-Algorithm, Jump Start Reinforcement Learning, That Uses Prior Policies to Create a Learning Curriculum That Improves Performance]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u0n5hv/google_ai_researchers_propose_a_metaalgorithm/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u0n5hv/google_ai_researchers_propose_a_metaalgorithm/"/>
        <updated>2022-04-10T18:02:33.000Z</updated>
        <summary type="html"><![CDATA[In the field of artificial intelligence, reinforcement learning is a type of machine-learning strategy that rewards desirable behaviors while penalizing those which aren’t. An agent can perceive its surroundings and act accordingly through trial and error in general with this form or presence – it’s kind of like getting feedback on what works for you. However, learning rules from scratch in contexts with complex exploration problems is a big challenge in RL. Because the agent does not receive any intermediate incentives, it cannot determine how close it is to complete the goal. As a result, exploring the space at random becomes necessary until the door opens. Given the length of the task and the level of precision required, this is highly unlikely.
 Exploring the state space randomly with preliminary information should be avoided while performing this activity. This prior knowledge aids the agent in determining which states of the environment are desirable and should be investigated further. Offline data collected by human demonstrations, programmed policies, or other RL agents could be used to train a policy and then initiate a new RL policy. This would include copying the pre-trained policy’s neural network to the new RL policy in the scenario where we utilize neural networks to describe the procedures. This process transforms the new RL policy into a pre-trained one. However, as seen below, naively initializing a new RL policy like this frequently fails, especially for value-based RL approaches.
 Continue reading the summary
 Paper: https://arxiv.org/pdf/2204.02372.pdf
 Project: https://jumpstart-rl.github.io/
 https://reddit.com/link/u0n5hv/video/fnktgf0wqqs81/player
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NLQ: Why You Might Not Need To Call A Data Analyst Anymore]]></title>
        <id>https://www.datasciencecentral.com/?p=57139</id>
        <link href="https://www.datasciencecentral.com/nlq-decreasing-reasons-to-call-a-data-analyst/"/>
        <updated>2022-04-10T17:56:50.000Z</updated>
        <summary type="html"><![CDATA[NLQ or Natural Language Query or Text-to-SQL or NL2SQL is an arm of computational linguistics, that helps users to fetch required data, visualizations, and insights, from sentences written in human language. As a business user, knowing data schema, table and column names, knowing metadata, having technical know-how of a BI tool or data querying skills… Read More »NLQ: Why You Might Not Need To Call A Data Analyst Anymore
The post NLQ: Why You Might Not Need To Call A Data Analyst Anymore appeared first on Data Science Central.]]></summary>
        <author>
            <name>Saurabh Ajmera</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advance in your finance and accounting careers with top technical skills]]></title>
        <id>https://www.datasciencecentral.com/?p=57110</id>
        <link href="https://www.datasciencecentral.com/advance-in-your-finance-and-accounting-careers-with-top-technical-skills/"/>
        <updated>2022-04-10T17:31:42.000Z</updated>
        <summary type="html"><![CDATA[Profession in finance and accounting is one of the top career choices for finance and accounting professionals. Employment of accountants and auditors is projected to grow 7 percent from the year 2020 to the year 2030, about as fast as the average for all occupations. About 135,000 openings for accountants and auditors are projected each year,… Read More »Advance in your finance and accounting careers with top technical skills
The post Advance in your finance and accounting careers with top technical skills appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Use static classifiers for dynamic point cloud tasks (3D) and use action classifiers for temporal anomaly detection (2D) - Link to a free online lecture by the author in comments]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0l0mr/r_use_static_classifiers_for_dynamic_point_cloud/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0l0mr/r_use_static_classifiers_for_dynamic_point_cloud/"/>
        <updated>2022-04-10T16:22:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pinter69  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", Zeng et al 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u0jtbs/socratic_models_composing_zeroshot_multimodal/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u0jtbs/socratic_models_composing_zeroshot_multimodal/"/>
        <updated>2022-04-10T15:22:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0jcur/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0jcur/d_simple_questions_thread/"/>
        <updated>2022-04-10T15:00:11.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] image similarity metrics or algorithms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0hxt3/p_image_similarity_metrics_or_algorithms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0hxt3/p_image_similarity_metrics_or_algorithms/"/>
        <updated>2022-04-10T13:47:40.000Z</updated>
        <summary type="html"><![CDATA[I want to perform image similarity between images from frames of 2 different movie trailers.
 I am currently using SSIM and VGG 16 individually. But ssim does not capture color differences and VGG 16 isn't capturing structural integrity.
 I can use both together, but I wanted to know if there is any metric or algorithm which can capture both together with less discrepancies and can capture both together.
 Will appreciate any help. Thank you!
    submitted by    /u/terminatorash2199  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Summer school in between neuroimaging and machine learning]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u0h1y1/summer_school_in_between_neuroimaging_and_machine/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u0h1y1/summer_school_in_between_neuroimaging_and_machine/"/>
        <updated>2022-04-10T12:57:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pasticciociccio  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dall-e and Dall-e2]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u0h1cm/dalle_and_dalle2/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u0h1cm/dalle_and_dalle2/"/>
        <updated>2022-04-10T12:56:37.000Z</updated>
        <summary type="html"><![CDATA[I have been into the website of OpenAI, it is unclear what has been added to Dall-e2, why should we subscribe for a github which will be made public? And what is the color code at the bottom?
    submitted by    /u/pasticciociccio  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Interested in a Postdoctoral position bridging machine learning and neuroimaging?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0gzj1/r_interested_in_a_postdoctoral_position_bridging/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0gzj1/r_interested_in_a_postdoctoral_position_bridging/"/>
        <updated>2022-04-10T12:53:33.000Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks for brain time series - Sano Centre for Computational Personalised Medicine
    submitted by    /u/alecrimi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Graphics: Design your dream body with a slider]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u0gfx7/ai_graphics_design_your_dream_body_with_a_slider/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u0gfx7/ai_graphics_design_your_dream_body_with_a_slider/"/>
        <updated>2022-04-10T12:19:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/much_successes  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] ML with Intermediate Mathematics: VAEs with Normalized Flow (live series)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0fy91/r_ml_with_intermediate_mathematics_vaes_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0fy91/r_ml_with_intermediate_mathematics_vaes_with/"/>
        <updated>2022-04-10T11:47:09.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, I'd like to share with you an exciting upcoming live series by Prof. Richard Xu of Hong Kong Baptist University. If you're interested, please click here to register!
 Description:
 "I have been planning to start a machine learning live series on topics that involve some intermediate mathematics, so I can help you to clarify some concepts. In order to fully grasp these concepts, you need to have sound knowledge of linear algebra, calculus, statistics and probability. However, if you just want to come and hear it for fun, please do so as well!
 The first topic is variational autoencoders with normalized flow, which I'll fully explain its beautiful mathematics over a period of a few sessions. You can find my notes on my GitHub site:
 https://github.com/roboticcam/machine-learning-notes/blob/master/files/vb_nf.pdf
 I will post the Zoom link to the registered participants.
 Please join us!"
    submitted by    /u/ML_Live_Series  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[research] Issues visualising a Resnet]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0fb2m/research_issues_visualising_a_resnet/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0fb2m/research_issues_visualising_a_resnet/"/>
        <updated>2022-04-10T11:00:37.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 We have a model used in cardiac MRi imaging, it is used to select the best image in a series of images. It consists of images -> Resnet -> LSTM -> output.
 The heatmap we generate from the Resnet alone shows output like the image attached, instead of actual anatomy, there is only little squares. We think this is likely due to the residual in the Resnet because it is not present in a VGG, but does anyone else have a better explanation, and an idea of how to visualise a Resnet?
 Resnet Saliency Map
    submitted by    /u/Radiology_AI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Critical analysis of deconfounded pretraining to improve visio-linguistic models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0e7n8/r_critical_analysis_of_deconfounded_pretraining/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0e7n8/r_critical_analysis_of_deconfounded_pretraining/"/>
        <updated>2022-04-10T09:33:54.000Z</updated>
        <summary type="html"><![CDATA[Hi reddit, happy to share our new paper "Critical analysis of deconfounded pretraining to improve visio-linguistic models".
 In a nutshell, it's on the problem of out-of-distribution performance for visio-linguistic models, and it takes a closer look / surfaces some issues with an existing technique for improving OOD performance by doing automatic deconfounding (inspired by the causality framework of Structural Causal Models).
 ​
  
Paper: https://www.frontiersin.org/articles/10.3389/frai.2022.736791/full
 Code: https://github.com/Natithan/p1_causality
 Abstract: 
 An important problem with many current visio-linguistic models is that they often depend on spurious correlations. A typical example of a spurious correlation between two variables is one that is due to a third variable causing …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Papers That Solve Novel Problems? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0ckoe/best_papers_that_solve_novel_problems_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0ckoe/best_papers_that_solve_novel_problems_d/"/>
        <updated>2022-04-10T07:26:12.000Z</updated>
        <summary type="html"><![CDATA[We often talk about how the publish-or-perish paradigm leads to constant minor improvements on the same problems (Image classification, text generation, etc). What are some of the best papers that do the opposite? Rather than solving known problems in a marginally better way, they solve a new problem with known (or modified) methods.
    submitted by    /u/SuspiciousWalrus99  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Advice on training document layout analysis models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0cgmz/discussion_advice_on_training_document_layout/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0cgmz/discussion_advice_on_training_document_layout/"/>
        <updated>2022-04-10T07:17:30.000Z</updated>
        <summary type="html"><![CDATA[So, a bit of background, I am doing an RnD project in the area of improving the layout analysis of scientific documents. The proposed method is to use an active learning loop on standard object detection models and target those classes/layouts which are performing poorly and train the model based on them.
 Now, we have some selection strategies based on submodular selection functions to target the pages we want. I also have set up code to extract embeddings which will help me do the selection. But I don't have prior experience in active learning, especially setting it up with detectron2, because it seems to register a dataset to train, and it is really difficult to change dynamically in the middle of training, which is my use case.
 So I need some advice on the following:-
  
The document analysis datasets are huge, DocBank is 50GB of images alone. How can I effectively store the embeddings in memory when I will call my selection algorithms mentioned above?
 How to set up an active learning loop in detectron2 for object detection. Or are there any alternatives? Some resources/code would be better
 There is some literature evidence suggesting that simple CNN backbone embeddings represent an image better than FasterRCNN or MaskRCNN embeddings. Specifically, this paper seems to be working on spliced image retrieval and it claims the following. Any thoughts/prior experience on this?
 Finally, is there any evidence supporting improvement in accuracy/precision in object detection using active learning? Or are there some better training paradigms?
  
Thank you for your patience.
    submitted by    /u/ExoticAd6868  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does AI exist that takes an image of a real person and edit/generate photos of them?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u0bbal/does_ai_exist_that_takes_an_image_of_a_real/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u0bbal/does_ai_exist_that_takes_an_image_of_a_real/"/>
        <updated>2022-04-10T05:55:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/NootropicLove  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classical Dynamic Programming ve Policy Iteration]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u0b8nw/classical_dynamic_programming_ve_policy_iteration/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u0b8nw/classical_dynamic_programming_ve_policy_iteration/"/>
        <updated>2022-04-10T05:50:26.000Z</updated>
        <summary type="html"><![CDATA[Cracking my head trying to figure out the differences between classical dynamic programming and policy iteration. I understand that policy iteration in itself is a form of dynamic programming. But if we were to compare the traditional operations of dynamic programming with policy iteration, what would be the differences.
 Thank you Heaps
    submitted by    /u/BalramVeeragoo  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers, Including Yann Lecun, Propose ‘projUNN’: An Efficient Method For Training Deep Neural Networks With Unitary Matrices]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/u0a6yb/researchers_including_yann_lecun_propose_projunn/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/u0a6yb/researchers_including_yann_lecun_propose_projunn/"/>
        <updated>2022-04-10T04:38:13.000Z</updated>
        <summary type="html"><![CDATA[When deep networks or inputs involve extensive data sequences, learning in neural networks can be unstable. Recurrent states in vanilla recurrent neural networks (RNNs) are generated by repeatedly applying a linear transformation followed by a pointwise nonlinearity. This becomes unstable when the linear transformation’s eigenvalues are not of magnitude one. 
 Unitary matrices have been utilized to solve the problem of disappearing and exploding gradients because they have eigenvalues of size one, naturally, and have been. Unitary convolutional layers have recently been developed in a similar way to aid in the development of more stable deep networks with norm-preserving transformations.
 The loss function’s derivative with respect to the weights is called a gradient. During backpropagation in neural networks, it is utilized to update the weights to minimize the loss function. When traveled backward with each layer, the derivative or slope continuously grows lower, resulting in a vanishing gradient. When the weight update is exponentially small, the training time is excessively long. In the worst-case scenario, the neural network training may be stopped entirely. Exploding gradients, on the other hand, occur when the slope increases with each successive layer during backpropagation. The gradient will never converge due to the high weights, causing it to oscillate around the minima without ever reaching a global minima point.
 Continue Reading
 Paper: https://arxiv.org/pdf/2203.05483.pdf
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Market Basket Analysis real-world examples and insights?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u0a0kd/d_market_basket_analysis_realworld_examples_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u0a0kd/d_market_basket_analysis_realworld_examples_and/"/>
        <updated>2022-04-10T04:25:35.000Z</updated>
        <summary type="html"><![CDATA[I want to know more about Market Basket Analysis's real-world use case and unique insights/business value derived from performing the Association rule mining. 
 I heard about the Beer-Diaper case study but many other sources invalidated it as a spurious correlation. Can someone share any example of business insights from Market Basket analysis and any interesting patterns they were able to observe??
    submitted by    /u/invincible_moron  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is my understanding to why future rewards being considered correct?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u09lgv/is_my_understanding_to_why_future_rewards_being/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u09lgv/is_my_understanding_to_why_future_rewards_being/"/>
        <updated>2022-04-10T03:59:28.000Z</updated>
        <summary type="html"><![CDATA[To my understanding, the Q value is updated like this:
 Q[s,a] = Q[s,a] + lr * (reward + gamma * max(Q[s,a]t+1) — Q[s,a]) 
 Where future state reward is considered since the best current reward doesn't grantee the optimal path. E.g.: 
  
Path A: Q[s,a]t = 1, Q[s,a]t+1 = 10 Total: 11
 Path B: Q[s,a]t = 5, Q[s,a]t+1 = 1 Total: 6
  
Not sure if this is a good analogy but here it gives the result that A is the optimal path even though its immediate reward at (t) is less than B.
 Further Question:
 Is there additional benefits to considering future rewards further than Q[s,a]t+1 ?
 Example: 
 Q[s,a] = Q[s,a] + lr * (reward + gamma* max(Q[s,a]t+2) - gamma * max(Q[s,a]t+1) — Q[s,a]) 
    submitted by    /u/DangerNoodle314  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How are multiple training examples used in DMD, SINDy, etc.?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u084uh/d_how_are_multiple_training_examples_used_in_dmd/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u084uh/d_how_are_multiple_training_examples_used_in_dmd/"/>
        <updated>2022-04-10T02:28:22.000Z</updated>
        <summary type="html"><![CDATA[The examples I have seen so far for DMD and SINDy use only 1 trajectory of the dynamical system for training. The input data is a 2D matrix, with the states/features being one dimension and time being the other dimension. But I want to use multiple trajectories of the same dynamical system for training, so the training data would be 3D (i.e., multiple 2D matrices). Are there examples where this has been done?
 Linear regression techniques (like pseudoinverse or LASSO) seem to be used to get the system matrix (in DMD) or the weights for the features (in SINDy). Can these methods be extended to 3D input data?
    submitted by    /u/baigyaanik  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any reason why to use several optimizers in Pytorch implementation of REDQ?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u07y3v/any_reason_why_to_use_several_optimizers_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u07y3v/any_reason_why_to_use_several_optimizers_in/"/>
        <updated>2022-04-10T02:17:13.000Z</updated>
        <summary type="html"><![CDATA[Hi guys. I am currently implementing REDQ by modifying a working implementation of SAC (basically adapted from Spinup) and so far my implementation doesn't work, I am trying to understand why. By looking at the authors' implementation I notice they use 1 pytorch optimizer per Q network, whereas I only use 1 for all parameters. So I wonder, is there any good reason for using several optimizers here?
 Thanks!
    submitted by    /u/yannbouteiller  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning in Noisy Observation space]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u07740/learning_in_noisy_observation_space/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u07740/learning_in_noisy_observation_space/"/>
        <updated>2022-04-10T01:33:08.000Z</updated>
        <summary type="html"><![CDATA[I am fairly new to RL. I'm trying to train an agent (like in gym's Cartpole env) in an environment with noisy (Gaussian noise) observation. I have added Gaussian noise to angle only (not to cart position, cart velocity or ang_velocity). I was fooling around with PPO in stable_baselines but haven't had much luck. Any suggestions on what needs to be tweaked or any good algorithm for this task? 
 Also, I tried changing the default forcing magnitude of 10 to other values like 30 but it didn't help much.
 Thanks
    submitted by    /u/Black_Beard53  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ran 3D art of my AI character thru ArcaneGAN; AI making art of AI.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u04lwl/ran_3d_art_of_my_ai_character_thru_arcanegan_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u04lwl/ran_3d_art_of_my_ai_character_thru_arcanegan_ai/"/>
        <updated>2022-04-09T23:04:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/alex-redacted  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gizmo is eating a clothes basket]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u049oz/gizmo_is_eating_a_clothes_basket/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u049oz/gizmo_is_eating_a_clothes_basket/"/>
        <updated>2022-04-09T22:46:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mspurplekris  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale", Ramrakhya et al 2022 {FB} (log-scaling of crowdsourced imitation learning in VR robotics)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u03y3i/habitatweb_learning_embodied_objectsearch/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u03y3i/habitatweb_learning_embodied_objectsearch/"/>
        <updated>2022-04-09T22:29:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it possible to implement ACER with A2C?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u02tyk/is_it_possible_to_implement_acer_with_a2c/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u02tyk/is_it_possible_to_implement_acer_with_a2c/"/>
        <updated>2022-04-09T21:32:29.000Z</updated>
        <summary type="html"><![CDATA[I'm looking into implementing a replay buffer in A2C. I came upon the ACER [paper](https://arxiv.org/pdf/1611.01224.pdf). From my understanding, it looks like ACER is an extension of A3C, and it seems like the difference between A2C and A3C is that in A2C parameters are updated synchronously and that helps with big batch sizes. 
 Is it still possible to implement some kind of replay buffer on A2C?
 Are there any papers that involve implementing a paper with A2C that you recommend I read?
 I'm new to the area of reinforcement learning, so I would be very grateful for any kind of help you can offer. Thanks in advance
    submitted by    /u/lebr0n99  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/u02p5x/r_local_learning_matters_rethinking_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/u02p5x/r_local_learning_matters_rethinking_data/"/>
        <updated>2022-04-09T21:25:49.000Z</updated>
        <summary type="html"><![CDATA[#cvpr-2022
 Happy to share our CVPR-2022 paper "Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning"
 Paper: https://arxiv.org/pdf/2111.14213.pdf
 Code: https://github.com/mmendiet/FedAlign
 Federated learning (FL) is a promising strategy for performing privacy-preserving, distributed learning with a network of clients (i.e., edge devices). However, the data distribution among clients is often non-IID in nature, making efficient optimization difficult. To alleviate this issue, many FL algorithms focus on mitigating the effects of data heterogeneity across clients by introducing a variety of proximal terms, some incurring considerable compute and/or memory overheads, to restrain local updates with respect to the global model. Instead, we consider rethinking solutions to data heterogeneity in FL with a focus on local learning generality rather than proximal restriction. To this end, we first present a systematic study informed by second-order indicators to better understand algorithm effectiveness in FL. Interestingly, we find that standard regularization methods are surprisingly strong performers in mitigating data heterogeneity effects. Based on our findings, we further propose a simple and effective method, FedAlign, to overcome data heterogeneity and the pitfalls of previous methods. FedAlign achieves competitive accuracy with state-of-the-art FL methods across a variety of settings while minimizing computation and memory overhead.
    submitted by    /u/Extension-Sun1816  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Technology, Old Problems: The Missing Voices in Natural Language Processing]]></title>
        <id>https://www.reddit.com/r/artificial/comments/u02dby/new_technology_old_problems_the_missing_voices_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/u02dby/new_technology_old_problems_the_missing_voices_in/"/>
        <updated>2022-04-09T21:09:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does anyone have a link to 'The RL Discord Server']]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/u00ann/does_anyone_have_a_link_to_the_rl_discord_server/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/u00ann/does_anyone_have_a_link_to_the_rl_discord_server/"/>
        <updated>2022-04-09T19:23:29.000Z</updated>
        <summary type="html"><![CDATA[Supposedly there is a popular discord server for the RL community, however I am having difficulty finding it.
    submitted by    /u/jclaessens  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Check Out This DeepMind’s New Language Model, Chinchilla (70B Parameters), Which Significantly Outperforms Gopher (280B) and GPT-3 (175B) on a Large Range of Downstream Evaluation Tasks]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzzoky/check_out_this_deepminds_new_language_model/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzzoky/check_out_this_deepminds_new_language_model/"/>
        <updated>2022-04-09T18:53:20.000Z</updated>
        <summary type="html"><![CDATA[https://preview.redd.it/pkrbloq8vjs81.png?width=1422&format=png&auto=webp&s=fef693165a6c948f626de613e4e341c25f8cf5f4
 ​
 Extreme-scale language models have recently exhibited incredible performance on natural language processing challenges. This is due to their ever-increasing size, exceeding 500 billion parameters. However, while these models have grown in popularity in recent years, the amount of data utilized to train them has not increased. The current generation of huge language models is clearly undertrained. Three prediction approaches for optimally choosing both model size and training length have been proposed by a DeepMind research team.
 Three approaches have been mentioned to estimate the optimal parameter:
  
Change the size of the models and the number of training tokens.
 IsoFLOP profiles
 Using a parametric loss function to fit a model
  
The ultimate pretraining loss is calculated as the number of model parameters and training tokens. They minimize the loss function under the restriction of the FLOPs function, which is equal to the computational budget because the computational budget is a probabilistic function of the number of observed training tokens and model parameters.
 Continue Reading This Research Summary
 Paper: https://arxiv.org/pdf/2203.15556.pdf
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ICML2022 Domain conflicts system]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzvxb9/d_icml2022_domain_conflicts_system/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzvxb9/d_icml2022_domain_conflicts_system/"/>
        <updated>2022-04-09T15:47:54.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if the Domain conflicts system working well?
 I got an email from the PCs and seems that the domain conflict system is not working well. It said that we can enter the conflict now but I cannot edit the conflict in the system. Could anyone tell me how to do it? Thanks!
  
Dear ICML Authors, 
 As we are seeing this happen, we just wanted to send you a brief explanation -- this only applies to some few papers. A few papers are losing reviews because of newly arising conflicts. If you did not enter your conflicts in CMT during the submission phase (as requested via CMT), then they could not be used in paper assignments. If you enter them now, any reviews by reviewers with conflicting domains will disappear, and you may see fewer reviews as a result. Unfortunately, we have no control over this, as the conflicts should have been entered when the paper was submitted. 
 Best, 
 Stefanie, Le and Csaba
  
   submitted by    /u/Snoo_97274  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Imitating, Fast and Slow: Robust learning from demonstrations via decision-time planning", Qi et al 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tzvnhc/imitating_fast_and_slow_robust_learning_from/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tzvnhc/imitating_fast_and_slow_robust_learning_from/"/>
        <updated>2022-04-09T15:34:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flaming Rose art made with snowpixelapp using AI.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzutk1/flaming_rose_art_made_with_snowpixelapp_using_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzutk1/flaming_rose_art_made_with_snowpixelapp_using_ai/"/>
        <updated>2022-04-09T14:54:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/AIWORQART  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning - looking for some resources]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tztri9/reinforcement_learning_looking_for_some_resources/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tztri9/reinforcement_learning_looking_for_some_resources/"/>
        <updated>2022-04-09T13:59:47.000Z</updated>
        <summary type="html"><![CDATA[Hello friends,
 I'm looking for some resources that would let me quickly start with Reinforcement Learning (preferably in Python). I have some experience with supervised learning (e.g. deep nets) and would like to complement with some RL. Preferably a walkthrough with some examples of implementation. Can you recommend something?
 Thanks in advance!
    submitted by    /u/andy-codes  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do you start a professional career in the Affective Computing field?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzt8s2/how_do_you_start_a_professional_career_in_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzt8s2/how_do_you_start_a_professional_career_in_the/"/>
        <updated>2022-04-09T13:30:47.000Z</updated>
        <summary type="html"><![CDATA[I'm about to graduate with a master's degree in Computer Science and I'm very passionate about Affective Computing. I would like to start looking for a job in this field, but most companies (not consulting) are looking for people with experience or a PhD. What do you recommend me to do? Continue with the PhD or try to find something, maybe in some startup?
    submitted by    /u/_rikya_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm dumb at maths: what does this mean?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tzt78z/im_dumb_at_maths_what_does_this_mean/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tzt78z/im_dumb_at_maths_what_does_this_mean/"/>
        <updated>2022-04-09T13:28:32.000Z</updated>
        <summary type="html"><![CDATA[So learning about e-mail learning same have it all understood except for the max thingy. If you care enough to click this blog it's not my blog):
 https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56
 I don't know how to turn this into to a real example:
 Update q values
 Q[state, action] = Q[state, action] + lr * (reward + gamma * np.max(Q[new_state, :]) — Q[state, action])
 Specifically the last bit: 
 np.max(Q[new_state, :]) — Q[state, action])
 What does the numpy max actually operate on here?
 Any hard examples? Thanks.
    submitted by    /u/Togfox  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning to enable color vision in the dark]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzpknc/deep_learning_to_enable_color_vision_in_the_dark/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzpknc/deep_learning_to_enable_color_vision_in_the_dark/"/>
        <updated>2022-04-09T09:21:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/qptbook  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Poll: How do you deploy models & endpoints?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzp4kv/d_poll_how_do_you_deploy_models_endpoints/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzp4kv/d_poll_how_do_you_deploy_models_endpoints/"/>
        <updated>2022-04-09T08:47:40.000Z</updated>
        <summary type="html"><![CDATA[View Poll
    submitted by    /u/martolini  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] Generate images from text with Latent Diffusion LAION-400M Model + Gradio Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzowos/rp_generate_images_from_text_with_latent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzowos/rp_generate_images_from_text_with_latent/"/>
        <updated>2022-04-09T08:30:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] tinydl - library to help with hyperparameter search and metric reporting in pytorch]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzouo4/p_tinydl_library_to_help_with_hyperparameter/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzouo4/p_tinydl_library_to_help_with_hyperparameter/"/>
        <updated>2022-04-09T08:26:02.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I built a small library to help with hyperparameter search for deep learning models created with pytorch, because I got kinda tired of having to rewrite large pieces code over and over again.
 You can check it out here: https://github.com/michi-jeremias/tinydl or you can even install it with pip (pip install tinydl). I have included a readme and an example of how the library can be used.
 About the library, it's pretty flexible about reporting different metrics to the console and to tensorboard (add_scalar, add_hparam) at each stage of the process, like after a batch, epoch of after a whole run over multiple epochs. It can also be easily extended to include other metrics or new types of outputs.
 Since this is basically my first attempt at a software project that's not intended only to be used by myself I'd be happy about any feedback you have for me!
 If the project doesn't qualify to be posted here due to being too simple/too much on a beginner level, apologies for that.
    submitted by    /u/abacaxiquaxi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] StyleGAN2 Path Length Regularization Implementation Clarification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzoqg2/d_stylegan2_path_length_regularization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzoqg2/d_stylegan2_path_length_regularization/"/>
        <updated>2022-04-09T08:16:53.000Z</updated>
        <summary type="html"><![CDATA[I am trying to implement stylegan2 and there are so many things here that are not explained either well, or at all in the paper.
 ​
  
How exactly is path length regularization implemented? In this PT code we can see that the $|J^T_w.y|$ is computed as follows:
  
​
 def g_path_regularize(fake_img, latents, mean_path_length, decay=0.01): noise = torch.randn_like(fake_img) / math.sqrt( fake_img.shape[2] * fake_img.shape[3] ) grad, = autograd.grad( outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True ) path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1)) path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length) path_penalty = (path_lengths - path_mean).pow(2).mean() return path_penalty, path_mean.detach(), path_lengths 
 This is based on this official TF implementation.
 The problem I have is that from what I understand, fake_img is 4D, and latents is 2D. The grad output in this case will be 2D and grad.pow(2).sum(2) cannot be computed because the third axis does not exist. Obviously people who are using these repos have not reported any issue regarding mismatch of shapes and axes, so I believe there is something else going on. Since I'm trying to implement this in my own network, I cannot get the desired shape any how. I get a 2D gradient output.
    submitted by    /u/feryet  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laptop for beginner?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzoq8s/laptop_for_beginner/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzoq8s/laptop_for_beginner/"/>
        <updated>2022-04-09T08:16:21.000Z</updated>
        <summary type="html"><![CDATA[I'm joining MSc AI & ML this September. I want to buy a laptop. Is MacBook Air sufficient for this? If not what would you recommend to someone like me?
    submitted by    /u/RauhanSheikh  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Jax/Haiku pretrained models: MobileNet, ResNet, VGG, Xception.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzo5ih/p_jaxhaiku_pretrained_models_mobilenet_resnet_vgg/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzo5ih/p_jaxhaiku_pretrained_models_mobilenet_resnet_vgg/"/>
        <updated>2022-04-09T07:31:55.000Z</updated>
        <summary type="html"><![CDATA[I released a repository of models with optional pretrained weights(Weights are taken from TF/Keras) to be used for tasks like prediction, feature extraction and fine-tuning.
 Github: https://github.com/abarcel/haikumodels
 Currently Available Models
  
MobileNet
 ResNet [50, 101, 152]
 VGG [16, 19]
 Xception
  
Also planning to release more, as soon as I find time for it.
    submitted by    /u/abarcel  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Denoising in the latent space]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tznijk/d_denoising_in_the_latent_space/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tznijk/d_denoising_in_the_latent_space/"/>
        <updated>2022-04-09T06:46:23.000Z</updated>
        <summary type="html"><![CDATA[I spent some time reading about and playing around with speech denoising DNNs ~2019. At the time the popular architecture was U-Net (encoder -> bottleneck -> decoder with skip connections) operating on spectrograms. These U-Nets were trained directly on noisy/clean speech pairs and the loss was the difference between the predicted denoised images and actual denoised image. MSE between the predicted/actual images was a baseline loss but people alse added "feature loss" or sometimes a GAN-based loss function as well.
 Anyway a cursory reading of the DALL-E 2 paper has me thinking about that approach. I'm curious to know if a similar approach used for DALL-E has been tried for audio denoising:
  
pre-train an encoder/decoder in a self-supervised fashion on a large dataset of audio
 train a denoiser to operate only in the latent space (ie the most compressed representation that is passed from the encoder to the decoder)
  
step 1 - self-supervised training of encoder/decoder
 https://preview.redd.it/nprc40ob9gs81.png?width=1668&format=png&auto=webp&s=3a9b181ceff6c4530b5f41abf793dfb6409c0ec2
 step 2 - train denoiser in latent space only
 ​
 https://preview.redd.it/hreajdpe9gs81.png?width=1279&format=png&auto=webp&s=e16490fff00fe82487ca214b11b642ffcb30fb1c
 step 3 - do inference by feeding denoised latent space vector into the decoder
 https://preview.redd.it/7nox4ezh9gs81.png?width=2034&format=png&auto=webp&s=ccc69bbb2096d84a9e4a824000e62bb0f80fbe29
 Is this a common approach already? It seems like once you have a good pretrained encoder/decoder pair then the denoiser training would be much more efficient than training an entire network that does everything at once from scratch (smaller search space, faster training loop)
    submitted by    /u/The_Amp_Walrus  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I help the advancement of AI? I want to contribute and make this my career. What should I do?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzmyzt/how_can_i_help_the_advancement_of_ai_i_want_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzmyzt/how_can_i_help_the_advancement_of_ai_i_want_to/"/>
        <updated>2022-04-09T06:07:36.000Z</updated>
        <summary type="html"><![CDATA[Please give a thorough and in-depth response.
    submitted by    /u/trillswan  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Reinforcement Learning for Robot Teams: A Review. (arXiv:2204.03516v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2204.03516</id>
        <link href="http://arxiv.org/abs/2204.03516"/>
        <updated>2022-04-09T00:48:55.535Z</updated>
        <summary type="html"><![CDATA[Purpose of review: Recent advances in sensing, actuation, and computation
have opened the door to multi-robot systems consisting of hundreds/thousands of
robots, with promising applications to automated manufacturing, disaster
relief, harvesting, last-mile delivery, port/airport operations, or search and
rescue. The community has leveraged model-free multi-agent reinforcement
learning (MARL) to devise efficient, scalable controllers for multi-robot
systems (MRS). This review aims to provide an analysis of the state-of-the-art
in distributed MARL for multi-robot cooperation.

Recent findings: Decentralized MRS face fundamental challenges, such as
non-stationarity and partial observability. Building upon the "centralized
training, decentralized execution" paradigm, recent MARL approaches include
independent learning, centralized critic, value decomposition, and
communication learning approaches. Cooperative behaviors are demonstrated
through AI benchmarks and fundamental real-world robotic capabilities such as
multi-robot motion/path planning.

Summary: This survey reports the challenges surrounding decentralized
model-free MARL for multi-robot cooperation and existing classes of approaches.
We present benchmarks and robotic applications along with a discussion on
current open avenues for research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yutong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damani_M/0/1/0/all/0/1"&gt;Mehul Damani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pamela Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuhong Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sartoretti_G/0/1/0/all/0/1"&gt;Guillaume Sartoretti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02190</id>
        <link href="http://arxiv.org/abs/2106.02190"/>
        <updated>2022-04-09T00:48:55.510Z</updated>
        <summary type="html"><![CDATA[We developed Distilled Graph Attention Policy Network (DGAPN), a
reinforcement learning model to generate novel graph-structured chemical
representations that optimize user-defined objectives by efficiently navigating
a physically constrained domain. The framework is examined on the task of
generating molecules that are designed to bind, noncovalently, to functional
sites of SARS-CoV-2 proteins. We present a spatial Graph Attention (sGAT)
mechanism that leverages self-attention over both node and edge attributes as
well as encoding the spatial structure -- this capability is of considerable
interest in synthetic biology and drug discovery. An attentional policy network
is introduced to learn the decision rules for a dynamic, fragment-based
chemical environment, and state-of-the-art policy gradient techniques are
employed to train the network with stability. Exploration is driven by the
stochasticity of the action space design and the innovation reward bonuses
learned and proposed by random network distillation. In experiments, our
framework achieved outstanding results compared to state-of-the-art algorithms,
while reducing the complexity of paths to chemical synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashman_M/0/1/0/all/0/1"&gt;Mikaela Cashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choma_N/0/1/0/all/0/1"&gt;Nicholas Choma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prates_E/0/1/0/all/0/1"&gt;&amp;#xc9;rica T. Prates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vergara_V/0/1/0/all/0/1"&gt;Ver&amp;#xf3;nica G. Melesse Vergara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Andrew Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Manesh Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1"&gt;Austin Clyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1"&gt;Thomas S. Brettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1"&gt;Wibe A. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Head_M/0/1/0/all/0/1"&gt;Martha S. Head&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1"&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugent_P/0/1/0/all/0/1"&gt;Peter Nugent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_D/0/1/0/all/0/1"&gt;Daniel A. Jacobson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1"&gt;James B. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.05845</id>
        <link href="http://arxiv.org/abs/2203.05845"/>
        <updated>2022-04-09T00:48:55.502Z</updated>
        <summary type="html"><![CDATA[Streamlined qBOLD acquisitions enable experimentally straightforward
observations of brain oxygen metabolism. $R_2^\prime$ maps are easily inferred;
however, the Oxygen extraction fraction (OEF) and deoxygenated blood volume
(DBV) are more ambiguously determined from the data. As such, existing
inference methods tend to yield very noisy and underestimated OEF maps, while
overestimating DBV.

This work describes a novel probabilistic machine learning approach that can
infer plausible distributions of OEF and DBV. Initially, we create a model that
produces informative voxelwise prior distribution based on synthetic training
data. Contrary to prior work, we model the joint distribution of OEF and DBV
through a scaled multivariate logit-Normal distribution, which enables the
values to be constrained within a plausible range. The prior distribution model
is used to train an efficient amortized variational Bayesian inference model.
This model learns to infer OEF and DBV by predicting real image data, with few
training data required, using the signal equations as a forward model.

We demonstrate that our approach enables the inference of smooth OEF and DBV
maps, with a physiologically plausible distribution that can be adapted through
specification of an informative prior distribution. Other benefits include
model comparison (via the evidence lower bound) and uncertainty quantification
for identifying image artefacts. Results are demonstrated on a small study
comparing subjects undergoing hyperventilation and at rest. We illustrate that
the proposed approach allows measurement of gray matter differences in OEF and
DBV and enables voxelwise comparison between conditions, where we observe
significant increases in OEF and $R_2^\prime$ during hyperventilation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Simpson_I/0/1/0/all/0/1"&gt;Ivor J.A. Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McManamon_A/0/1/0/all/0/1"&gt;Ashley McManamon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Orzsik_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs &amp;#xd6;rzsik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stone_A/0/1/0/all/0/1"&gt;Alan J. Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blockley_N/0/1/0/all/0/1"&gt;Nicholas P. Blockley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asllani_I/0/1/0/all/0/1"&gt;Iris Asllani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colasanti_A/0/1/0/all/0/1"&gt;Alessandro Colasanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cercignani_M/0/1/0/all/0/1"&gt;Mara Cercignani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First-Order Algorithms for Nonlinear Generalized Nash Equilibrium Problems. (arXiv:2204.03132v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2204.03132</id>
        <link href="http://arxiv.org/abs/2204.03132"/>
        <updated>2022-04-09T00:48:55.494Z</updated>
        <summary type="html"><![CDATA[We consider the problem of computing an equilibrium in a class of nonlinear
generalized Nash equilibrium problems (NGNEPs) in which the strategy sets for
each player are defined by equality and inequality constraints that may depend
on the choices of rival players. While the asymptotic global convergence and
local convergence rate of solution procedures have been studied in this
setting, the analysis of iteration complexity is still in its infancy. Our
contribution is to provide two simple first-order algorithmic frameworks based
on the quadratic penalty method and the augmented Lagrangian method,
respectively, with an accelerated mirror-prox algorithm as the inner loop. We
provide nonasymptotic theoretical guarantees for these algorithms. More
specifically, we establish the global convergence rate of our algorithms for
solving (strongly) monotone NGNEPs and we provide iteration complexity bounds
expressed in terms of the number of gradient evaluations. Experimental results
demonstrate the efficiency of our algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zampetakis_M/0/1/0/all/0/1"&gt;Manolis Zampetakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores. (arXiv:2204.03219v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.03219</id>
        <link href="http://arxiv.org/abs/2204.03219"/>
        <updated>2022-04-09T00:48:55.487Z</updated>
        <summary type="html"><![CDATA[Mean opinion score (MOS) is a typical subjective evaluation metric for speech
synthesis systems. Since collecting MOS is time-consuming, it would be
desirable if there are accurate MOS prediction models for automatic evaluation.
In this work, we propose DDOS, a novel MOS prediction model. DDOS utilizes
domain adaptive pre-training to further pre-train self-supervised learning
models on synthetic speech. And a proposed module is added to model the opinion
score distribution of each utterance. With the proposed components, DDOS
outperforms previous works on BVCC dataset. And the zero shot transfer result
on BC2019 dataset is significantly improved. DDOS also wins second place in
Interspeech 2022 VoiceMOS challenge in terms of system-level score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tseng_W/0/1/0/all/0/1"&gt;Wei-Cheng Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kao_W/0/1/0/all/0/1"&gt;Wei-Tsung Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Image-to-Image Translation with Generative Prior. (arXiv:2204.03641v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03641</id>
        <link href="http://arxiv.org/abs/2204.03641"/>
        <updated>2022-04-09T00:48:55.479Z</updated>
        <summary type="html"><![CDATA[Unsupervised image-to-image translation aims to learn the translation between
two visual domains without paired data. Despite the recent progress in image
translation models, it remains challenging to build mappings between complex
domains with drastic visual discrepancies. In this work, we present a novel
framework, Generative Prior-guided UNsupervised Image-to-image Translation
(GP-UNIT), to improve the overall quality and applicability of the translation
algorithm. Our key insight is to leverage the generative prior from pre-trained
class-conditional GANs (e.g., BigGAN) to learn rich content correspondences
across various domains. We propose a novel coarse-to-fine scheme: we first
distill the generative prior to capture a robust coarse-level content
representation that can link objects at an abstract semantic level, based on
which fine-level content features are adaptively learned for more accurate
multi-level content correspondences. Extensive experiments demonstrate the
superiority of our versatile framework over state-of-the-art methods in robust,
high-quality and diversified translations, even for challenging and distant
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis. (arXiv:2204.03040v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2204.03040</id>
        <link href="http://arxiv.org/abs/2204.03040"/>
        <updated>2022-04-09T00:48:55.456Z</updated>
        <summary type="html"><![CDATA[In this work, we present the SOMOS dataset, the first large-scale mean
opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS)
samples. It can be employed to train automatic MOS prediction systems focused
on the assessment of modern synthesizers, and can stimulate advancements in
acoustic model evaluation. It consists of 20K synthetic utterances of the LJ
Speech voice, a public domain speech dataset which is a common benchmark for
building neural acoustic models and vocoders. Utterances are generated from 200
TTS systems including vanilla neural acoustic models as well as models which
allow prosodic variations. An LPCNet vocoder is used for all systems, so that
the samples' variation depends only on the acoustic models. The synthesized
utterances provide balanced and adequate domain and length coverage. We collect
MOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and
share practices leading to reliable crowdsourced annotations for this task.
Baseline results of state-of-the-art MOS prediction models on the SOMOS dataset
are presented, while we show the challenges that such models face when assigned
to evaluate synthetic utterances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maniati_G/0/1/0/all/0/1"&gt;Georgia Maniati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vioni_A/0/1/0/all/0/1"&gt;Alexandra Vioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1"&gt;Nikolaos Ellinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikitaras_K/0/1/0/all/0/1"&gt;Karolos Nikitaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klapsas_K/0/1/0/all/0/1"&gt;Konstantinos Klapsas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1"&gt;June Sig Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jho_G/0/1/0/all/0/1"&gt;Gunu Jho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1"&gt;Aimilios Chalamandaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1"&gt;Pirros Tsiakoulis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust and Explainable Autoencoders for Unsupervised Time Series Outlier Detection---Extended Version. (arXiv:2204.03341v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03341</id>
        <link href="http://arxiv.org/abs/2204.03341"/>
        <updated>2022-04-09T00:48:55.449Z</updated>
        <summary type="html"><![CDATA[Time series data occurs widely, and outlier detection is a fundamental
problem in data mining, which has numerous applications. Existing
autoencoder-based approaches deliver state-of-the-art performance on
challenging real-world data but are vulnerable to outliers and exhibit low
explainability. To address these two limitations, we propose robust and
explainable unsupervised autoencoder frameworks that decompose an input time
series into a clean time series and an outlier time series using autoencoders.
Improved explainability is achieved because clean time series are better
explained with easy-to-understand patterns such as trends and periodicities. We
provide insight into this by means of a post-hoc explainability analysis and
empirical studies. In addition, since outliers are separated from clean time
series iteratively, our approach offers improved robustness to outliers, which
in turn improves accuracy. We evaluate our approach on five real-world datasets
and report improvements over the state-of-the-art approaches in terms of
robustness and explainability.

This is an extended version of "Robust and Explainable Autoencoders for
Unsupervised Time Series Outlier Detection", to appear in IEEE ICDE 2022.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kieu_T/0/1/0/all/0/1"&gt;Tung Kieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chenjuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1"&gt;Christian S. Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiteng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kai Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mo\"ET: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning. (arXiv:1906.06717v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.06717</id>
        <link href="http://arxiv.org/abs/1906.06717"/>
        <updated>2022-04-09T00:48:55.442Z</updated>
        <summary type="html"><![CDATA[Rapid advancements in deep learning have led to many recent breakthroughs.
While deep learning models achieve superior performance, often statistically
better than humans, their adoption into safety-critical settings, such as
healthcare or self-driving cars is hindered by their inability to provide
safety guarantees or to expose the inner workings of the model in a human
understandable form. We present Mo\"ET, a novel model based on Mixture of
Experts, consisting of decision tree experts and a generalized linear model
gating function. Thanks to such gating function the model is more expressive
than the standard decision tree. To support non-differentiable decision trees
as experts, we formulate a novel training procedure. In addition, we introduce
a hard thresholding version, Mo\"ETH, in which predictions are made solely by a
single expert chosen via the gating function. Thanks to that property, Mo\"ETH
allows each prediction to be easily decomposed into a set of logical rules in a
form which can be easily verified. While Mo\"ET is a general use model, we
illustrate its power in the reinforcement learning setting. By training Mo\"ET
models using an imitation learning procedure on deep RL agents we outperform
the previous state-of-the-art technique based on decision trees while
preserving the verifiability of the models. Moreover, we show that Mo\"ET can
also be used in real-world supervised problems on which it outperforms other
verifiable machine learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasic_M/0/1/0/all/0/1"&gt;Marko Vasic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrovic_A/0/1/0/all/0/1"&gt;Andrija Petrovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolic_M/0/1/0/all/0/1"&gt;Mladen Nikolic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khurshid_S/0/1/0/all/0/1"&gt;Sarfraz Khurshid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Cooperative Game Theory-based Data Valuation via Data Utility Learning. (arXiv:2107.06336v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06336</id>
        <link href="http://arxiv.org/abs/2107.06336"/>
        <updated>2022-04-09T00:48:55.433Z</updated>
        <summary type="html"><![CDATA[The Shapley value (SV) and Least core (LC) are classic methods in cooperative
game theory for cost/profit sharing problems. Both methods have recently been
proposed as a principled solution for data valuation tasks, i.e., quantifying
the contribution of individual datum in machine learning. However, both SV and
LC suffer computational challenges due to the need for retraining models on
combinatorially many data subsets. In this work, we propose to boost the
efficiency in computing Shapley value or Least core by learning to estimate the
performance of a learning algorithm on unseen data combinations. Theoretically,
we derive bounds relating the error in the predicted learning performance to
the approximation error in SV and LC. Empirically, we show that the proposed
method can significantly improve the accuracy of SV and LC estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning from Only Unlabeled Data with Class-Conditional-Sharing Clients. (arXiv:2204.03304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03304</id>
        <link href="http://arxiv.org/abs/2204.03304"/>
        <updated>2022-04-09T00:48:55.414Z</updated>
        <summary type="html"><![CDATA[Supervised federated learning (FL) enables multiple clients to share the
trained model without sharing their labeled data. However, potential clients
might even be reluctant to label their own data, which could limit the
applicability of FL in practice. In this paper, we show the possibility of
unsupervised FL whose model is still a classifier for predicting class labels,
if the class-prior probabilities are shifted while the class-conditional
distributions are shared among the unlabeled data owned by the clients. We
propose federation of unsupervised learning (FedUL), where the unlabeled data
are transformed into surrogate labeled data for each of the clients, a modified
model is trained by supervised FL, and the wanted model is recovered from the
modified model. FedUL is a very general solution to unsupervised FL: it is
compatible with many supervised FL methods, and the recovery of the wanted
model can be theoretically guaranteed as if the data have been labeled.
Experiments on benchmark and real-world datasets demonstrate the effectiveness
of FedUL. Code is available at https://github.com/lunanbit/FedUL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1"&gt;Nan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraFN: Semi-Supervised Node Classification on Graph with Few Labels via Non-Parametric Distribution Assignment. (arXiv:2204.01303v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.01303</id>
        <link href="http://arxiv.org/abs/2204.01303"/>
        <updated>2022-04-09T00:48:55.406Z</updated>
        <summary type="html"><![CDATA[Despite the success of Graph Neural Networks (GNNs) on various applications,
GNNs encounter significant performance degradation when the amount of
supervision signals, i.e., number of labeled nodes, is limited, which is
expected as GNNs are trained solely based on the supervision obtained from the
labeled nodes. On the other hand,recent self-supervised learning paradigm aims
to train GNNs by solving pretext tasks that do not require any labeled nodes,
and it has shown to even outperform GNNs trained with few labeled nodes.
However, a major drawback of self-supervised methods is that they fall short of
learning class discriminative node representations since no labeled information
is utilized during training. To this end, we propose a novel semi-supervised
method for graphs, GraFN, that leverages few labeled nodes to ensure nodes that
belong to the same class to be grouped together, thereby achieving the best of
both worlds of semi-supervised and self-supervised methods. Specifically, GraFN
randomly samples support nodes from labeled nodes and anchor nodes from the
entire graph. Then, it minimizes the difference between two predicted class
distributions that are non-parametrically assigned by anchor-supports
similarity from two differently augmented graphs. We experimentally show that
GraFN surpasses both the semi-supervised and self-supervised methods in terms
of node classification on real-world graphs. The source code for GraFN is
available at https://github.com/Junseok0207/GraFN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Yunhak Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+In_Y/0/1/0/all/0/1"&gt;Yeonjun In&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1"&gt;Namkyeong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hyun_D/0/1/0/all/0/1"&gt;Dongmin Hyun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chanyoung Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2204.03471</id>
        <link href="http://arxiv.org/abs/2204.03471"/>
        <updated>2022-04-09T00:48:55.399Z</updated>
        <summary type="html"><![CDATA[Adopting reinforcement learning (RL) for traffic signal control is
increasingly popular. Most RL methods use fixed action interval (denoted as
tduration) and actuate or maintain a phase every tduration, which makes the
phase duration less dynamic and flexible. In addition, the actuated phase can
be arbitrary, affecting the real-world deployment, which requires a fixed
cyclical phase structure. To address these challenges, we propose a multi-level
traffic signal control framework, DynLight, which uses an optimization method
Max-QueueLength (M-QL) to determine the phase and uses a deep Q-network to
determine the corresponding duration. Based on DynLight, we further propose
DynLight-C that adopts a well trained deep Q-network of DynLight and replace
M-QL by a fixed cyclical control policy that actuate a set of phases in fixed
order to realize cyclical phase structure. Comprehensive experiments on
multiple real-world datasets demonstrate that DynLight achives a new
state-of-the-art. Furthermore, the deep Q-network of DynLight can learn well on
determining the phase duration and DynLight-C demonstrates high performance for
deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shubin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jianming Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplayer Performative Prediction: Learning in Decision-Dependent Games. (arXiv:2201.03398v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.03398</id>
        <link href="http://arxiv.org/abs/2201.03398"/>
        <updated>2022-04-09T00:48:55.388Z</updated>
        <summary type="html"><![CDATA[Learning problems commonly exhibit an interesting feedback mechanism wherein
the population data reacts to competing decision makers' actions. This paper
formulates a new game theoretic framework for this phenomenon, called
"multi-player performative prediction". We focus on two distinct solution
concepts, namely (i) performatively stable equilibria and (ii) Nash equilibria
of the game. The latter equilibria are arguably more informative, but can be
found efficiently only when the game is monotone. We show that under mild
assumptions, the performatively stable equilibria can be found efficiently by a
variety of algorithms, including repeated retraining and the repeated
(stochastic) gradient method. We then establish transparent sufficient
conditions for strong monotonicity of the game and use them to develop
algorithms for finding Nash equilibria. We investigate derivative free methods
and adaptive gradient algorithms wherein each player alternates between
learning a parametric description of their distribution and gradient steps on
the empirical risk. Synthetic and semi-synthetic numerical experiments
illustrate the results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narang_A/0/1/0/all/0/1"&gt;Adhyyan Narang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faulkner_E/0/1/0/all/0/1"&gt;Evan Faulkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drusvyatskiy_D/0/1/0/all/0/1"&gt;Dmitriy Drusvyatskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazel_M/0/1/0/all/0/1"&gt;Maryam Fazel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1"&gt;Lillian J. Ratliff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composite Spatial Monte Carlo Integration Based on Generalized Least Squares. (arXiv:2204.03248v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2204.03248</id>
        <link href="http://arxiv.org/abs/2204.03248"/>
        <updated>2022-04-09T00:48:55.387Z</updated>
        <summary type="html"><![CDATA[Although evaluation of the expectations on the Ising model is essential in
various applications, this is frequently infeasible because of intractable
multiple summations (or integrations). Spatial Monte Carlo integration (SMCI)
is a sampling-based approximation, and can provide high-accuracy estimations
for such intractable expectations. To evaluate the expectation of a function of
variables in a specific region (called target region), SMCI considers a larger
region containing the target region (called sum region). In SMCI, the multiple
summation for the variables in the sum region is precisely executed, and that
in the outer region is evaluated by the sampling approximation such as the
standard Monte Carlo integration. It is guaranteed that the accuracy of the
SMCI estimator is monotonically improved as the size of the sum region
increases. However, a haphazard expansion of the sum region could cause a
combinatorial explosion. Therefore, we hope to improve the accuracy without
such region expansion. In this study, based on the theory of generalized least
squares, a new effective method is proposed by combining multiple SMCI
estimators. The validity of the proposed method is demonstrated theoretically
and numerically. The results indicate that the proposed method can be effective
in the inverse Ising problem (or Boltzmann machine learning).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sekimoto_K/0/1/0/all/0/1"&gt;Kaiji Sekimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yasuda_M/0/1/0/all/0/1"&gt;Muneki Yasuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated question generation and question answering from Turkish texts. (arXiv:2111.06476v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.06476</id>
        <link href="http://arxiv.org/abs/2111.06476"/>
        <updated>2022-04-09T00:48:55.387Z</updated>
        <summary type="html"><![CDATA[While exam-style questions are a fundamental educational tool serving a
variety of purposes, manual construction of questions is a complex process that
requires training, experience and resources. Automatic question generation (QG)
techniques can be utilized to satisfy the need for a continuous supply of new
questions by streamlining their generation. However, compared to automatic
question answering (QA), QG is a more challenging task. In this work, we
fine-tune a multilingual T5 (mT5) transformer in a multi-task setting for QA,
QG and answer extraction tasks using Turkish QA datasets. To the best of our
knowledge, this is the first academic work that performs automated text-to-text
question generation from Turkish texts. Experimental evaluations show that the
proposed multi-task setting achieves state-of-the-art Turkish question
answering and question generation performance on TQuADv1, TQuADv2 datasets and
XQuAD Turkish split. The source code and the pre-trained models are available
at https://github.com/obss/turkish-question-generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1"&gt;Fatih Cagatay Akyon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavusoglu_D/0/1/0/all/0/1"&gt;Devrim Cavusoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cengiz_C/0/1/0/all/0/1"&gt;Cemil Cengiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1"&gt;Sinan Onur Altinuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1"&gt;Alptekin Temizel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiAuto-DeepONet: A Multi-resolution Autoencoder DeepONet for Nonlinear Dimension Reduction, Uncertainty Quantification and Operator Learning of Forward and Inverse Stochastic Problems. (arXiv:2204.03193v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03193</id>
        <link href="http://arxiv.org/abs/2204.03193"/>
        <updated>2022-04-09T00:48:55.379Z</updated>
        <summary type="html"><![CDATA[A new data-driven method for operator learning of stochastic differential
equations(SDE) is proposed in this paper. The central goal is to solve forward
and inverse stochastic problems more effectively using limited data. Deep
operator network(DeepONet) has been proposed recently for operator learning.
Compared to other neural networks to learn functions, it aims at the problem of
learning nonlinear operators. However, it can be challenging by using the
original model to learn nonlinear operators for high-dimensional stochastic
problems. We propose a new multi-resolution autoencoder DeepONet model referred
to as MultiAuto-DeepONet to deal with this difficulty with the aid of
convolutional autoencoder. The encoder part of the network is designed to
reduce the dimensionality as well as discover the hidden features of
high-dimensional stochastic inputs. The decoder is designed to have a special
structure, i.e. in the form of DeepONet. The first DeepONet in decoder is
designed to reconstruct the input function involving randomness while the
second one is used to approximate the solution of desired equations. Those two
DeepONets has a common branch net and two independent trunk nets. This
architecture enables us to deal with multi-resolution inputs naturally. By
adding $L_1$ regularization to our network, we found the outputs from the
branch net and two trunk nets all have sparse structures. This reduces the
number of trainable parameters in the neural network thus making the model more
efficient. Finally, we conduct several numerical experiments to illustrate the
effectiveness of our proposed MultiAuto-DeepONet model with uncertainty
quantification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiahao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Monte Carlo Tree Search for Weighted Vertex Coloring. (arXiv:2202.01665v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.01665</id>
        <link href="http://arxiv.org/abs/2202.01665"/>
        <updated>2022-04-09T00:48:55.379Z</updated>
        <summary type="html"><![CDATA[This work presents the first study of using the popular Monte Carlo Tree
Search (MCTS) method combined with dedicated heuristics for solving the
Weighted Vertex Coloring Problem. Starting with the basic MCTS algorithm, we
gradually introduce a number of algorithmic variants where MCTS is extended by
various simulation strategies including greedy and local search heuristics. We
conduct experiments on well-known benchmark instances to assess the value of
each studied combination. We also provide empirical evidence to shed light on
the advantages and limits of each strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grelier_C/0/1/0/all/0/1"&gt;Cyril Grelier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goudet_O/0/1/0/all/0/1"&gt;Olivier Goudet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1"&gt;Jin-Kao Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey on recently proposed activation functions for Deep Learning. (arXiv:2204.02921v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.02921</id>
        <link href="http://arxiv.org/abs/2204.02921"/>
        <updated>2022-04-09T00:48:55.379Z</updated>
        <summary type="html"><![CDATA[Artificial neural networks (ANN), typically referred to as neural networks,
are a class of Machine Learning algorithms and have achieved widespread
success, having been inspired by the biological structure of the human brain.
Neural networks are inherently powerful due to their ability to learn complex
function approximations from data. This generalization ability has been able to
impact multidisciplinary areas involving image recognition, speech recognition,
natural language processing, and others. Activation functions are a crucial
sub-component of neural networks. They define the output of a node in the
network given a set of inputs. This survey discusses the main concepts of
activation functions in neural networks, including; a brief introduction to
deep neural networks, a summary of what are activation functions and how they
are used in neural networks, their most common properties, the different types
of activation functions, some of the challenges, limitations, and alternative
solutions faced by activation functions, concluding with the final remarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gustineli_M/0/1/0/all/0/1"&gt;Murilo Gustineli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance matrix preparation for quantum principal component analysis. (arXiv:2204.03495v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2204.03495</id>
        <link href="http://arxiv.org/abs/2204.03495"/>
        <updated>2022-04-09T00:48:55.370Z</updated>
        <summary type="html"><![CDATA[Principal component analysis (PCA) is a dimensionality reduction method in
data analysis that involves diagonalizing the covariance matrix of the dataset.
Recently, quantum algorithms have been formulated for PCA based on
diagonalizing a density matrix. These algorithms assume that the covariance
matrix can be encoded in a density matrix, but a concrete protocol for this
encoding has been lacking. Our work aims to address this gap. Assuming
amplitude encoding of the data, with the data given by the ensemble $\{p_i,|
\psi_i \rangle\}$, then one can easily prepare the ensemble average density
matrix $\overline{\rho} = \sum_i p_i |\psi_i\rangle \langle \psi_i |$. We first
show that $\overline{\rho}$ is precisely the covariance matrix whenever the
dataset is centered. For quantum datasets, we exploit global phase symmetry to
argue that there always exists a centered dataset consistent with
$\overline{\rho}$, and hence $\overline{\rho}$ can always be interpreted as a
covariance matrix. This provides a simple means for preparing the covariance
matrix for arbitrary quantum datasets or centered classical datasets. For
uncentered classical datasets, our method is so-called "PCA without centering",
which we interpret as PCA on a symmetrized dataset. We argue that this closely
corresponds to standard PCA, and we derive equations and inequalities that
bound the deviation of the spectrum obtained with our method from that of
standard PCA. We numerically illustrate our method for the MNIST handwritten
digit dataset. We also argue that PCA on quantum datasets is natural and
meaningful, and we numerically implement our method for molecular ground-state
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gordon_M/0/1/0/all/0/1"&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1"&gt;M. Cerezo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1"&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1"&gt;Patrick J. Coles&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Effectiveness of Pretrained Models for API Learning. (arXiv:2204.03498v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2204.03498</id>
        <link href="http://arxiv.org/abs/2204.03498"/>
        <updated>2022-04-09T00:48:55.369Z</updated>
        <summary type="html"><![CDATA[Developers frequently use APIs to implement certain functionalities, such as
parsing Excel Files, reading and writing text files line by line, etc.
Developers can greatly benefit from automatic API usage sequence generation
based on natural language queries for building applications in a faster and
cleaner manner. Existing approaches utilize information retrieval models to
search for matching API sequences given a query or use RNN-based
encoder-decoder to generate API sequences. As it stands, the first approach
treats queries and API names as bags of words. It lacks deep comprehension of
the semantics of the queries. The latter approach adapts a neural language
model to encode a user query into a fixed-length context vector and generate
API sequences from the context vector.

We want to understand the effectiveness of recent Pre-trained Transformer
based Models (PTMs) for the API learning task. These PTMs are trained on large
natural language corpora in an unsupervised manner to retain contextual
knowledge about the language and have found success in solving similar Natural
Language Processing (NLP) problems. However, the applicability of PTMs has not
yet been explored for the API sequence generation task. We use a dataset that
contains 7 million annotations collected from GitHub to evaluate the PTMs
empirically. This dataset was also used to assess previous approaches. Based on
our results, PTMs generate more accurate API sequences and outperform other
related methods by around 11%. We have also identified two different
tokenization approaches that can contribute to a significant boost in PTMs'
performance for the API sequence generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hadi_M/0/1/0/all/0/1"&gt;Mohammad Abdul Hadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yusuf_I/0/1/0/all/0/1"&gt;Imam Nur Bani Yusuf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thung_F/0/1/0/all/0/1"&gt;Ferdian Thung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luong_K/0/1/0/all/0/1"&gt;Kien Gia Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lingxiao_J/0/1/0/all/0/1"&gt;Jiang Lingxiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1"&gt;Fatemeh H. Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1"&gt;David Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Network-based Android Malware Classification with Jumping Knowledge. (arXiv:2201.07537v5 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.07537</id>
        <link href="http://arxiv.org/abs/2201.07537"/>
        <updated>2022-04-09T00:48:55.369Z</updated>
        <summary type="html"><![CDATA[This paper presents a new Android malware detection method based on Graph
Neural Networks (GNNs) with Jumping-Knowledge (JK). Android function call
graphs (FCGs) consist of a set of program functions and their inter-procedural
calls. Thus, this paper proposes a GNN-based method for Android malware
detection by capturing meaningful intra-procedural call path patterns. In
addition, a Jumping-Knowledge technique is applied to minimize the effect of
the over-smoothing problem, which is common in GNNs. The proposed method has
been extensively evaluated using two benchmark datasets. The results
demonstrate the superiority of our approach compared to state-of-the-art
approaches in terms of key classification metrics, which demonstrates the
potential of GNNs in Android malware detection and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_W/0/1/0/all/0/1"&gt;Wai Weng Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Layeghy_S/0/1/0/all/0/1"&gt;Siamak Layeghy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1"&gt;Mohanad Sarhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1"&gt;Marcus Gallagher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1"&gt;Marius Portmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Erroneous Communication Links. (arXiv:2201.12991v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.12991</id>
        <link href="http://arxiv.org/abs/2201.12991"/>
        <updated>2022-04-09T00:48:55.369Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the federated learning (FL) problem in the
presence of communication errors. We model the link between the devices and the
central node (CN) by a packet erasure channel, where the local parameters from
devices are either erased or received correctly by CN with probability $e$ and
$1-e$, respectively. We provide mathematical proof for the convergence of the
FL algorithm in the presence of communication errors, where the CN uses past
local updates when the fresh updates are not received from some devices. We
show via simulations that by using the past local updates, the FL algorithm can
converge in the presence of communication errors. We also show that when the
dataset is uniformly distributed among devices, the FL algorithm that only uses
fresh updates and discards missing updates might converge faster than the FL
algorithm that uses past local updates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shirvanimoghaddam_M/0/1/0/all/0/1"&gt;Mahyar Shirvanimoghaddam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yifeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guha_A/0/1/0/all/0/1"&gt;Aradhika Guha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salari_A/0/1/0/all/0/1"&gt;Ayoob Salari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exploration of Active Learning for Affective Digital Phenotyping. (arXiv:2204.01915v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.01915</id>
        <link href="http://arxiv.org/abs/2204.01915"/>
        <updated>2022-04-09T00:48:55.369Z</updated>
        <summary type="html"><![CDATA[Some of the most severe bottlenecks preventing widespread development of
machine learning models for human behavior include a dearth of labeled training
data and difficulty of acquiring high quality labels. Active learning is a
paradigm for using algorithms to computationally select a useful subset of data
points to label using metrics for model uncertainty and data similarity. We
explore active learning for naturalistic computer vision emotion data, a
particularly heterogeneous and complex data space due to inherently subjective
labels. Using frames collected from gameplay acquired from a therapeutic
smartphone game for children with autism, we run a simulation of active
learning using gameplay prompts as metadata to aid in the active learning
process. We find that active learning using information generated during
gameplay slightly outperforms random selection of the same number of labeled
frames. We next investigate a method to conduct active learning with subjective
data, such as in affective computing, and where multiple crowdsourced labels
can be acquired for each image. Using the Child Affective Facial Expression
(CAFE) dataset, we simulate an active learning process for crowdsourcing many
labels and find that prioritizing frames using the entropy of the crowdsourced
label distribution results in lower categorical cross-entropy loss compared to
random frame selection. Collectively, these results demonstrate pilot
evaluations of two novel active learning approaches for subjective affective
data collected in noisy settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1"&gt;Peter Washington&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mutlu_C/0/1/0/all/0/1"&gt;Cezmi Mutlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1"&gt;Aaron Kline&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1"&gt;Cathy Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunlap_K/0/1/0/all/0/1"&gt;Kaitlyn Dunlap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kent_J/0/1/0/all/0/1"&gt;Jack Kent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Husic_A/0/1/0/all/0/1"&gt;Arman Husic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1"&gt;Nate Stockham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1"&gt;Brianna Chrisman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1"&gt;Kelley Paskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1"&gt;Jae-Yoon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1"&gt;Dennis P. Wall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECMG: Exemplar-based Commit Message Generation. (arXiv:2203.02700v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.02700</id>
        <link href="http://arxiv.org/abs/2203.02700"/>
        <updated>2022-04-09T00:48:55.368Z</updated>
        <summary type="html"><![CDATA[Commit messages concisely describe the content of code diffs (i.e., code
changes) and the intent behind them. Recently, many approaches have been
proposed to generate commit messages automatically. The information
retrieval-based methods reuse the commit messages of similar code diffs, while
the neural-based methods learn the semantic connection between code diffs and
commit messages. However, the reused commit messages might not accurately
describe the content/intent of code diffs and neural-based methods tend to
generate high-frequent and repetitive tokens in the corpus. In this paper, we
combine the advantages of the two technical routes and propose a novel
exemplar-based neural commit message generation model, which treats the similar
commit message as an exemplar and leverages it to guide the neural network
model to generate an accurate commit message. We perform extensive experiments
and the results confirm the effectiveness of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shia_E/0/1/0/all/0/1"&gt;Ensheng Shia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hongbin Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causality, Causal Discovery, and Causal Inference in Structural Engineering. (arXiv:2204.01543v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.01543</id>
        <link href="http://arxiv.org/abs/2204.01543"/>
        <updated>2022-04-09T00:48:55.368Z</updated>
        <summary type="html"><![CDATA[Much of our experiments are designed to uncover the cause(s) and effect(s)
behind a data generating mechanism (i.e., phenomenon) we happen to be
interested in. Uncovering such relationships allows us to identify the true
working of a phenomenon and, most importantly, articulate a model that may
enable us to further explore the phenomenon on hand and/or allow us to predict
it accurately. Fundamentally, such models are likely to be derived via a causal
approach (as opposed to an observational or empirical mean). In this approach,
causal discovery is required to create a causal model, which can then be
applied to infer the influence of interventions, and answer any hypothetical
questions (i.e., in the form of What ifs? Etc.) that we might have. This paper
builds a case for causal discovery and causal inference and contrasts that
against traditional machine learning approaches; all from a civil and
structural engineering perspective. More specifically, this paper outlines the
key principles of causality and the most commonly used algorithms and packages
for causal discovery and causal inference. Finally, this paper also presents a
series of examples and case studies of how causal concepts can be adopted for
our domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naser_M/0/1/0/all/0/1"&gt;M.Z. Naser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VNIbCReg: VICReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series. (arXiv:2204.02697v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.02697</id>
        <link href="http://arxiv.org/abs/2204.02697"/>
        <updated>2022-04-09T00:48:55.368Z</updated>
        <summary type="html"><![CDATA[One of the latest self-supervised learning (SSL) methods, VICReg, showed a
great performance both in the linear evaluation and the fine-tuning evaluation.
However, VICReg is proposed in computer vision and it learns by pulling
representations of random crops of an image while maintaining the
representation space by the variance and covariance loss. However, VICReg would
be ineffective on non-stationary time series where different parts/crops of
input should be differently encoded to consider the non-stationarity. Another
recent SSL proposal, Temporal Neighborhood Coding (TNC) is effective for
encoding non-stationary time series. This study shows that a combination of a
VICReg-style method and TNC is very effective for SSL on non-stationary time
series, where a non-stationary seismic signal time series is used as an
evaluation dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daesoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aune_E/0/1/0/all/0/1"&gt;Erlend Aune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langet_N/0/1/0/all/0/1"&gt;Nad&amp;#xe8;ge Langet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eidsvik_J/0/1/0/all/0/1"&gt;Jo Eidsvik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Centric Green AI: An Exploratory Empirical Study. (arXiv:2204.02766v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.02766</id>
        <link href="http://arxiv.org/abs/2204.02766"/>
        <updated>2022-04-09T00:48:55.368Z</updated>
        <summary type="html"><![CDATA[With the growing availability of large-scale datasets, and the popularization
of affordable storage and computational capabilities, the energy consumed by AI
is becoming a growing concern. To address this issue, in recent years, studies
have focused on demonstrating how AI energy efficiency can be improved by
tuning the model training strategy. Nevertheless, how modifications applied to
datasets can impact the energy consumption of AI is still an open question. To
fill this gap, in this exploratory study, we evaluate if data-centric
approaches can be utilized to improve AI energy efficiency. To achieve our
goal, we conduct an empirical experiment, executed by considering 6 different
AI algorithms, a dataset comprising 5,574 data points, and two dataset
modifications (number of data points and number of features). Our results show
evidence that, by exclusively conducting modifications on datasets, energy
consumption can be drastically reduced (up to 92.16%), often at the cost of a
negligible or even absent accuracy decline. As additional introductory results,
we demonstrate how, by exclusively changing the algorithm used, energy savings
up to two orders of magnitude can be achieved. In conclusion, this exploratory
investigation empirically demonstrates the importance of applying data-centric
techniques to improve AI energy efficiency. Our results call for a research
agenda that focuses on data-centric techniques, to further enable and
democratize Green AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verdecchia_R/0/1/0/all/0/1"&gt;Roberto Verdecchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1"&gt;Lu&amp;#xed;s Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sallou_J/0/1/0/all/0/1"&gt;June Sallou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Michelle Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wickenden_J/0/1/0/all/0/1"&gt;James Wickenden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hotellier_E/0/1/0/all/0/1"&gt;Estelle Hotellier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning. (arXiv:2108.03706v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03706</id>
        <link href="http://arxiv.org/abs/2108.03706"/>
        <updated>2022-04-09T00:48:55.367Z</updated>
        <summary type="html"><![CDATA[The recent emergence of reinforcement learning has created a demand for
robust statistical inference methods for the parameter estimates computed using
these algorithms. Existing methods for statistical inference in online learning
are restricted to settings involving independently sampled observations, while
existing statistical inference methods in reinforcement learning (RL) are
limited to the batch setting. The online bootstrap is a flexible and efficient
approach for statistical inference in linear stochastic approximation
algorithms, but its efficacy in settings involving Markov noise, such as RL,
has yet to be explored. In this paper, we study the use of the online bootstrap
method for statistical inference in RL. In particular, we focus on the temporal
difference (TD) learning and Gradient TD (GTD) learning algorithms, which are
themselves special instances of linear stochastic approximation under Markov
noise. The method is shown to be distributionally consistent for statistical
inference in policy evaluation, and numerical experiments are included to
demonstrate the effectiveness of this algorithm at statistical inference tasks
across a range of real RL environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ramprasad_P/0/1/0/all/0/1"&gt;Pratik Ramprasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuantong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1"&gt;Will Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings. (arXiv:2111.00185v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.00185</id>
        <link href="http://arxiv.org/abs/2111.00185"/>
        <updated>2022-04-09T00:48:55.367Z</updated>
        <summary type="html"><![CDATA[Policy gradient methods have been frequently applied to problems in control
and reinforcement learning with great success, yet existing convergence
analysis still relies on non-intuitive, impractical and often opaque
conditions. In particular, existing rates are achieved in limited settings,
under strict regularity conditions. In this work, we establish explicit
convergence rates of policy gradient methods, extending the convergence regime
to weakly smooth policy classes with $L_2$ integrable gradient. We provide
intuitive examples to illustrate the insight behind these new conditions.
Notably, our analysis also shows that convergence rates are achievable for both
the standard policy gradient and the natural policy gradient algorithms under
these assumptions. Lastly we provide performance guarantees for the converged
policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Matthew S. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1"&gt;Animesh Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Motion-from-Blur: 3D Shape and Motion Estimation of Motion-blurred Objects in Videos. (arXiv:2111.14465v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.14465</id>
        <link href="http://arxiv.org/abs/2111.14465"/>
        <updated>2022-04-09T00:48:55.367Z</updated>
        <summary type="html"><![CDATA[We propose a method for jointly estimating the 3D motion, 3D shape, and
appearance of highly motion-blurred objects from a video. To this end, we model
the blurred appearance of a fast moving object in a generative fashion by
parametrizing its 3D position, rotation, velocity, acceleration, bounces,
shape, and texture over the duration of a predefined time window spanning
multiple frames. Using differentiable rendering, we are able to estimate all
parameters by minimizing the pixel-wise reprojection error to the input video
via backpropagating through a rendering pipeline that accounts for motion blur
by averaging the graphics output over short time intervals. For that purpose,
we also estimate the camera exposure gap time within the same optimization. To
account for abrupt motion changes like bounces, we model the motion trajectory
as a piece-wise polynomial, and we are able to estimate the specific time of
the bounce at sub-frame accuracy. Experiments on established benchmark datasets
demonstrate that our method outperforms previous methods for fast moving object
deblurring and 3D reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1"&gt;Denys Rozumnyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1"&gt;Martin R. Oswald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1"&gt;Marc Pollefeys&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning of Generative Image Priors for MRI Reconstruction. (arXiv:2202.04175v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.04175</id>
        <link href="http://arxiv.org/abs/2202.04175"/>
        <updated>2022-04-09T00:48:55.367Z</updated>
        <summary type="html"><![CDATA[Multi-institutional efforts can facilitate training of deep MRI
reconstruction models, albeit privacy risks arise during cross-site sharing of
imaging data. Federated learning (FL) has recently been introduced to address
privacy concerns by enabling distributed training without transfer of imaging
data. Existing FL methods for MRI reconstruction employ conditional models to
map from undersampled to fully-sampled acquisitions via explicit knowledge of
the imaging operator. Since conditional models generalize poorly across
different acceleration rates or sampling densities, imaging operators must be
fixed between training and testing, and they are typically matched across
sites. To improve generalization and flexibility in multi-institutional
collaborations, here we introduce a novel method for MRI reconstruction based
on Federated learning of Generative IMage Priors (FedGIMP). FedGIMP leverages a
two-stage approach: cross-site learning of a generative MRI prior, and
subject-specific injection of the imaging operator. The global MRI prior is
learned via an unconditional adversarial model that synthesizes high-quality MR
images based on latent variables. Specificity in the prior is preserved via a
mapper subnetwork that produces site-specific latents. During inference, the
prior is combined with subject-specific imaging operators to enable
reconstruction, and further adapted to individual test samples by minimizing
data-consistency loss. Comprehensive experiments on multi-institutional
datasets clearly demonstrate enhanced generalization performance of FedGIMP
against site-specific and federated methods based on conditional models, as
well as traditional reconstruction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Elmas_G/0/1/0/all/0/1"&gt;Gokberk Elmas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1"&gt;Salman UH Dar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1"&gt;Yilmaz Korkmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ceyani_E/0/1/0/all/0/1"&gt;Emir Ceyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Susam_B/0/1/0/all/0/1"&gt;Burak Susam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1"&gt;Muzaffer &amp;#xd6;zbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1"&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning based Medical Image Deepfake Detection: A Comparative Study. (arXiv:2109.12800v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.12800</id>
        <link href="http://arxiv.org/abs/2109.12800"/>
        <updated>2022-04-09T00:48:55.366Z</updated>
        <summary type="html"><![CDATA[Deep generative networks in recent years have reinforced the need for caution
while consuming various modalities of digital information. One avenue of
deepfake creation is aligned with injection and removal of tumors from medical
scans. Failure to detect medical deepfakes can lead to large setbacks on
hospital resources or even loss of life. This paper attempts to address the
detection of such attacks with a structured case study. Specifically, we
evaluate eight different machine learning algorithms, which including three
conventional machine learning methods, support vector machine, random forest,
decision tree, and five deep learning models, DenseNet121, DenseNet201,
ResNet50, ResNet101, VGG19, on distinguishing between tampered and untampered
images.For deep learning models, the five models are used for feature
extraction, then fine-tune for each pre-trained model is performed. The
findings of this work show near perfect accuracy in detecting instances of
tumor injections and removals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solaiyappan_S/0/1/0/all/0/1"&gt;Siddharth Solaiyappan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yuxin Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning with Almost Sure Constraints. (arXiv:2112.05198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.05198</id>
        <link href="http://arxiv.org/abs/2112.05198"/>
        <updated>2022-04-09T00:48:55.365Z</updated>
        <summary type="html"><![CDATA[In this work we address the problem of finding feasible policies for
Constrained Markov Decision Processes under probability one constraints. We
argue that stationary policies are not sufficient for solving this problem, and
that a rich class of policies can be found by endowing the controller with a
scalar quantity, so called budget, that tracks how close the agent is to
violating the constraint. We show that the minimal budget required to act
safely can be obtained as the smallest fixed point of a Bellman-like operator,
for which we analyze its convergence properties. We also show how to learn this
quantity when the true kernel of the Markov decision process is not known,
while providing sample-complexity bounds. The utility of knowing this minimal
budget relies in that it can aid in the search of optimal or near-optimal
policies by shrinking down the region of the state space the agent must
navigate. Simulations illustrate the different nature of probability one
constraints against the typically used constraints in expectation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellano_A/0/1/0/all/0/1"&gt;Agustin Castellano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_H/0/1/0/all/0/1"&gt;Hancheng Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazerque_J/0/1/0/all/0/1"&gt;Juan Bazerque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallada_E/0/1/0/all/0/1"&gt;Enrique Mallada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Distributed Deep Learning Architectures: Models, Discussions, and Applications. (arXiv:2202.11200v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.11200</id>
        <link href="http://arxiv.org/abs/2202.11200"/>
        <updated>2022-04-09T00:48:55.365Z</updated>
        <summary type="html"><![CDATA[Although deep learning (DL) has already become a state-of-the-art technology
for various data processing tasks, data security and computational overload
problems often arise due to their high data and computational power dependency.
To solve this problem, quantum deep learning (QDL) and distributed deep
learning (DDL) has emerged to complement existing DL methods. Furthermore, a
quantum distributed deep learning (QDDL) technique that combines and maximizes
these advantages is getting attention. This paper compares several model
structures for QDDL and discusses their possibilities and limitations to
leverage QDDL for some representative application scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kwak_Y/0/1/0/all/0/1"&gt;Yunseok Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yun_W/0/1/0/all/0/1"&gt;Won Joon Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jae Pyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cho_H/0/1/0/all/0/1"&gt;Hyunhee Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Choi_M/0/1/0/all/0/1"&gt;Minseok Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jung_S/0/1/0/all/0/1"&gt;Soyi Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1"&gt;Joongheon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial Diffusion Geometry. (arXiv:2103.15783v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15783</id>
        <link href="http://arxiv.org/abs/2103.15783"/>
        <updated>2022-04-09T00:48:55.364Z</updated>
        <summary type="html"><![CDATA[Clustering algorithms partition a dataset into groups of similar points. The
primary contribution of this article is the Multiscale Spatially-Regularized
Diffusion Learning (M-SRDL) clustering algorithm, which uses
spatially-regularized diffusion distances to efficiently and accurately learn
multiple scales of latent structure in hyperspectral images. The M-SRDL
clustering algorithm extracts clusterings at many scales from a hyperspectral
image and outputs these clusterings' variation of information-barycenter as an
exemplar for all underlying cluster structure. We show that incorporating
spatial regularization into a multiscale clustering framework results in
smoother and more coherent clusters when applied to hyperspectral data,
yielding more accurate clustering labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1"&gt;Sam L. Polk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1"&gt;James M. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SplitAVG: A heterogeneity-aware federated deep learning method for medical imaging. (arXiv:2107.02375v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02375</id>
        <link href="http://arxiv.org/abs/2107.02375"/>
        <updated>2022-04-09T00:48:55.364Z</updated>
        <summary type="html"><![CDATA[Federated learning is an emerging research paradigm for enabling
collaboratively training deep learning models without sharing patient data.
However, the data from different institutions are usually heterogeneous across
institutions, which may reduce the performance of models trained using
federated learning. In this study, we propose a novel heterogeneity-aware
federated learning method, SplitAVG, to overcome the performance drops from
data heterogeneity in federated learning. Unlike previous federated methods
that require complex heuristic training or hyper parameter tuning, our SplitAVG
leverages the simple network split and feature map concatenation strategies to
encourage the federated model training an unbiased estimator of the target data
distribution. We compare SplitAVG with seven state-of-the-art federated
learning methods, using centrally hosted training data as the baseline on a
suite of both synthetic and real-world federated datasets. We find that the
performance of models trained using all the comparison federated learning
methods degraded significantly with the increasing degrees of data
heterogeneity. In contrast, SplitAVG method achieves comparable results to the
baseline method under all heterogeneous settings, that it achieves 96.2% of the
accuracy and 110.4% of the mean absolute error obtained by the baseline in a
diabetic retinopathy binary classification dataset and a bone age prediction
dataset, respectively, on highly heterogeneous data partitions. We conclude
that SplitAVG method can effectively overcome the performance drops from
variability in data distributions across institutions. Experimental results
also show that SplitAVG can be adapted to different base networks and
generalized to various types of medical imaging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liangqiong Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Praveer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GFlowNet Foundations. (arXiv:2111.09266v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.09266</id>
        <link href="http://arxiv.org/abs/2111.09266"/>
        <updated>2022-04-09T00:48:55.364Z</updated>
        <summary type="html"><![CDATA[Generative Flow Networks (GFlowNets) have been introduced as a method to
sample a diverse set of candidates in an active learning context, with a
training objective that makes them approximately sample in proportion to a
given reward function. In this paper, we show a number of additional
theoretical properties of GFlowNets. They can be used to estimate joint
probability distributions and the corresponding marginal distributions where
some variables are unspecified and, of particular interest, can represent
distributions over composite objects like sets and graphs. GFlowNets amortize
the work typically done by computationally expensive MCMC methods in a single
but trained generative pass. They could also be used to estimate partition
functions and free energies, conditional probabilities of supersets
(supergraphs) given a subset (subgraph), as well as marginal distributions over
all supersets (supergraphs) of a given set (graph). We introduce variations
enabling the estimation of entropy and mutual information, sampling from a
Pareto frontier, connections to reward-maximizing policies, and extensions to
stochastic environments, continuous actions and modular energy functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deleu_T/0/1/0/all/0/1"&gt;Tristan Deleu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;Edward J. Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahlou_S/0/1/0/all/0/1"&gt;Salem Lahlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1"&gt;Mo Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation. (arXiv:2111.14826v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.14826</id>
        <link href="http://arxiv.org/abs/2111.14826"/>
        <updated>2022-04-09T00:48:55.364Z</updated>
        <summary type="html"><![CDATA[The nonuniform quantization strategy for compressing neural networks usually
achieves better performance than its counterpart, i.e., uniform strategy, due
to its superior representational capacity. However, many nonuniform
quantization methods overlook the complicated projection process in
implementing the nonuniformly quantized weights/activations, which incurs
non-negligible time and space overhead in hardware deployment. In this study,
we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can
maintain the strong representation ability of nonuniform methods while being
hardware-friendly and efficient as the uniform quantization for model
inference. We achieve this through learning the flexible in-equidistant input
thresholds to better fit the underlying distribution while quantizing these
real-valued inputs into equidistant output levels. To train the quantized
network with learnable input thresholds, we introduce a generalized
straight-through estimator (G-STE) for intractable backward derivative
calculation w.r.t. threshold parameters. Additionally, we consider entropy
preserving regularization to further reduce information loss in weight
quantization. Even under this adverse constraint of imposing uniformly
quantized weights and activations, our N2UQ outperforms state-of-the-art
nonuniform quantization methods by 0.5~1.7 on ImageNet, demonstrating the
contribution of N2UQ design. Code and models are available at:
https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zechun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Dong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhiqiang Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Margin Calibration for Long-Tailed Visual Recognition. (arXiv:2112.07225v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.07225</id>
        <link href="http://arxiv.org/abs/2112.07225"/>
        <updated>2022-04-09T00:48:55.364Z</updated>
        <summary type="html"><![CDATA[The long-tailed class distribution in visual recognition tasks poses great
challenges for neural networks on how to handle the biased predictions between
head and tail classes, i.e., the model tends to classify tail classes as head
classes. While existing research focused on data resampling and loss function
engineering, in this paper, we take a different perspective: the classification
margins. We study the relationship between the margins and logits
(classification scores) and empirically observe the biased margins and the
biased logits are positively correlated. We propose MARC, a simple yet
effective MARgin Calibration function to dynamically calibrate the biased
margins for unbiased logits. We validate MARC through extensive experiments on
common long-tailed benchmarks including CIFAR-LT, ImageNet-LT, Places-LT, and
iNaturalist-LT. Experimental results demonstrate that our MARC achieves
favorable results on these benchmarks. In addition, MARC is extremely easy to
implement with just three lines of code. We hope this simple method will
motivate people to rethink the biased margins and biased logits in long-tailed
visual recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1"&gt;Wenxin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1"&gt;Takahiro Shinozaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection. (arXiv:2202.06934v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.06934</id>
        <link href="http://arxiv.org/abs/2202.06934"/>
        <updated>2022-04-09T00:48:55.363Z</updated>
        <summary type="html"><![CDATA[Detection of small objects and objects far away in the scene is a major
challenge in surveillance applications. Such objects are represented by small
number of pixels in the image and lack sufficient details, making them
difficult to detect using conventional detectors. In this work, an open-source
framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides
a generic slicing aided inference and fine-tuning pipeline for small object
detection. The proposed technique is generic in the sense that it can be
applied on top of any available object detector without any fine-tuning.
Experimental evaluations, using object detection baselines on the Visdrone and
xView aerial object detection datasets show that the proposed inference method
can increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and
TOOD detectors, respectively. Moreover, the detection accuracy can be further
increased with a slicing aided fine-tuning, resulting in a cumulative increase
of 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been
integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly
available at https://github.com/obss/sahi.git .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1"&gt;Fatih Cagatay Akyon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1"&gt;Sinan Onur Altinuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1"&gt;Alptekin Temizel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Linear Quadratic Control is Vulnerable Under Cost Manipulation. (arXiv:2203.05774v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.05774</id>
        <link href="http://arxiv.org/abs/2203.05774"/>
        <updated>2022-04-09T00:48:55.363Z</updated>
        <summary type="html"><![CDATA[In this work, we study the deception of a Linear-Quadratic-Gaussian (LQG)
agent by manipulating the cost signals. We show that a small falsification of
the cost parameters will only lead to a bounded change in the optimal policy.
The bound is linear on the amount of falsification the attacker can apply to
the cost parameters. We propose an attack model where the attacker aims to
mislead the agent into learning a `nefarious' policy by intentionally
falsifying the cost parameters. We formulate the attack's problem as a convex
optimization problem and develop necessary and sufficient conditions to check
the achievability of the attacker's goal.

We showcase the adversarial manipulation on two types of LQG learners: the
batch RL learner and the other is the adaptive dynamic programming (ADP)
learner. Our results demonstrate that with only 2.296% of falsification on the
cost data, the attacker misleads the batch RL into learning the 'nefarious'
policy that leads the vehicle to a dangerous position. The attacker can also
gradually trick the ADP learner into learning the same `nefarious' policy by
consistently feeding the learner a falsified cost signal that stays close to
the actual cost signal. The paper aims to raise people's awareness of the
security threats faced by RL-enabled control systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yunhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference Attacks Against Self-supervised Speech Models. (arXiv:2111.05113v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.05113</id>
        <link href="http://arxiv.org/abs/2111.05113"/>
        <updated>2022-04-09T00:48:55.362Z</updated>
        <summary type="html"><![CDATA[Recently, adapting the idea of self-supervised learning (SSL) on continuous
speech has started gaining attention. SSL models pre-trained on a huge amount
of unlabeled audio can generate general-purpose representations that benefit a
wide variety of speech processing tasks. Despite their ubiquitous deployment,
however, the potential privacy risks of these models have not been well
investigated. In this paper, we present the first privacy analysis on several
SSL speech models using Membership Inference Attacks (MIA) under black-box
access. The experiment results show that these pre-trained models are
vulnerable to MIA and prone to membership information leakage with high Area
Under the Curve (AUC) in both utterance-level and speaker-level. Furthermore,
we also conduct several ablation studies to understand the factors that
contribute to the success of MIA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1"&gt;Wei-Cheng Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kao_W/0/1/0/all/0/1"&gt;Wei-Tsung Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientific Discovery and the Cost of Measurement -- Balancing Information and Cost in Reinforcement Learning. (arXiv:2112.07535v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.07535</id>
        <link href="http://arxiv.org/abs/2112.07535"/>
        <updated>2022-04-09T00:48:55.362Z</updated>
        <summary type="html"><![CDATA[The use of reinforcement learning (RL) in scientific applications, such as
materials design and automated chemistry, is increasing. A major challenge,
however, lies in fact that measuring the state of the system is often costly
and time consuming in scientific applications, whereas policy learning with RL
requires a measurement after each time step. In this work, we make the
measurement costs explicit in the form of a costed reward and propose a
framework that enables off-the-shelf deep RL algorithms to learn a policy for
both selecting actions and determining whether or not to measure the current
state of the system at each time step. In this way, the agents learn to balance
the need for information with the cost of information. Our results show that
when trained under this regime, the Dueling DQN and PPO agents can learn
optimal action policies whilst making up to 50\% fewer state measurements, and
recurrent neural networks can produce a greater than 50\% reduction in
measurements. We postulate the these reduction can help to lower the barrier to
applying RL to real-world scientific applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bellinger_C/0/1/0/all/0/1"&gt;Colin Bellinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drozdyuk_A/0/1/0/all/0/1"&gt;Andriy Drozdyuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1"&gt;Isaac Tamblyn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.14417</id>
        <link href="http://arxiv.org/abs/2112.14417"/>
        <updated>2022-04-09T00:48:55.362Z</updated>
        <summary type="html"><![CDATA[The goal of this paper is to investigate a control theoretic analysis of
linear stochastic iterative algorithm and temporal difference (TD) learning.
TD-learning is a linear stochastic iterative algorithm to estimate the value
function of a given policy for a Markov decision process, which is one of the
most popular and fundamental reinforcement learning algorithms. While there has
been a series of successful works in theoretical analysis of TD-learning, it
was not until recently that researchers found some guarantees on its
statistical efficiency. In this paper, we propose a control theoretic
finite-time analysis TD-learning, which exploits standard notions in linear
system control communities. Therefore, the proposed work provides additional
insights on TD-learning and reinforcement learning with simple concepts and
analysis tools in control theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donghwan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitating, Fast and Slow: Robust learning from demonstrations via decision-time planning. (arXiv:2204.03597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03597</id>
        <link href="http://arxiv.org/abs/2204.03597"/>
        <updated>2022-04-09T00:48:55.361Z</updated>
        <summary type="html"><![CDATA[The goal of imitation learning is to mimic expert behavior from
demonstrations, without access to an explicit reward signal. A popular class of
approach infers the (unknown) reward function via inverse reinforcement
learning (IRL) followed by maximizing this reward function via reinforcement
learning (RL). The policies learned via these approaches are however very
brittle in practice and deteriorate quickly even with small test-time
perturbations due to compounding errors. We propose Imitation with Planning at
Test-time (IMPLANT), a new meta-algorithm for imitation learning that utilizes
decision-time planning to correct for compounding errors of any base imitation
policy. In contrast to existing approaches, we retain both the imitation policy
and the rewards model at decision-time, thereby benefiting from the learning
signal of the two components. Empirically, we demonstrate that IMPLANT
significantly outperforms benchmark imitation learning approaches on standard
control environments and excels at zero-shot generalization when subject to
challenging perturbations in test-time dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1"&gt;Carl Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14836</id>
        <link href="http://arxiv.org/abs/2106.14836"/>
        <updated>2022-04-09T00:48:55.361Z</updated>
        <summary type="html"><![CDATA[Representations of the world environment play a crucial role in artificial
intelligence. It is often inefficient to conduct reasoning and inference
directly in the space of raw sensory representations, such as pixel values of
images. Representation learning allows us to automatically discover suitable
representations from raw sensory data. For example, given raw sensory data, a
deep neural network learns nonlinear representations at its hidden layers,
which are subsequently used for classification at its output layer. This
happens implicitly during training through minimizing a supervised or
unsupervised loss. In this paper, we study the dynamics of such implicit
nonlinear representation learning. We identify a pair of a new assumption and a
novel condition, called the common model structure assumption and the
data-architecture alignment condition. Under the common model structure
assumption, the data-architecture alignment condition is shown to be sufficient
for the global convergence and necessary for the global optimality. Moreover,
our theory explains how and when increasing the network size does and does not
improve the training behaviors in the practical regime. Our results provide
practical guidance for designing a model structure: e.g., the common model
structure assumption can be used as a justification for using a particular
model structure instead of others. We also derive a new training framework,
which satisfies the data-architecture alignment condition by automatically
modifying any given training algorithm. Given a standard training algorithm,
the framework running its modified version is empirically shown to maintain
competitive test performances while providing global convergence guarantees for
deep residual neural networks with convolutions, skip connections, and batch
normalization with datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion,
KMNIST and SVHN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhun Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminability-enforcing loss to improve representation learning. (arXiv:2202.07073v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07073</id>
        <link href="http://arxiv.org/abs/2202.07073"/>
        <updated>2022-04-09T00:48:55.361Z</updated>
        <summary type="html"><![CDATA[During the training process, deep neural networks implicitly learn to
represent the input data samples through a hierarchy of features, where the
size of the hierarchy is determined by the number of layers. In this paper, we
focus on enforcing the discriminative power of the high-level representations,
that are typically learned by the deeper layers (closer to the output). To this
end, we introduce a new loss term inspired by the Gini impurity, which is aimed
at minimizing the entropy (increasing the discriminative power) of individual
high-level features with respect to the class labels. Although our Gini loss
induces highly-discriminative features, it does not ensure that the
distribution of the high-level features matches the distribution of the
classes. As such, we introduce another loss term to minimize the
Kullback-Leibler divergence between the two distributions. We conduct
experiments on two image classification data sets (CIFAR-100 and Caltech 101),
considering multiple neural architectures ranging from convolutional networks
(ResNet-17, ResNet-18, ResNet-50) to transformers (CvT). Our empirical results
show that integrating our novel loss terms into the training objective
consistently outperforms the models trained with cross-entropy alone, without
increasing the inference time at all.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1"&gt;Florinel-Alin Croitoru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigore_D/0/1/0/all/0/1"&gt;Diana-Nicoleta Grigore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1"&gt;Radu Tudor Ionescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03528</id>
        <link href="http://arxiv.org/abs/2204.03528"/>
        <updated>2022-04-09T00:48:55.360Z</updated>
        <summary type="html"><![CDATA[Machine Learning with Deep Neural Networks (DNNs) has become a successful
tool in solving tasks across various fields of application. The success of DNNs
is strongly connected to their high complexity in terms of the number of
network layers or of neurons in each layer, which severely complicates to
understand how DNNs solve their learned task. To improve the explainability of
DNNs, we adapt methods from neuroscience because this field has a rich
experience in analyzing complex and opaque systems. In this work, we draw
inspiration from how neuroscience uses topographic maps to visualize the
activity of the brain when it performs certain tasks. Transferring this
approach to DNNs can help to visualize and understand their internal processes
more intuitively, too. However, the inner structures of brains and DNNs differ
substantially. Therefore, to be able to visualize activations of neurons in
DNNs as topographic maps, we research techniques to layout the neurons in a
two-dimensional space in which neurons of similar activity are in the vicinity
of each other. In this work, we introduce and compare different methods to
obtain a topographic layout of the neurons in a network layer. Moreover, we
demonstrate how to use the resulting topographic activation maps to identify
errors or encoded biases in DNNs or data sets. Our novel visualization
technique improves the transparency of DNN-based algorithmic decision-making
systems and is accessible to a broad audience because topographic maps are
intuitive to interpret without expert-knowledge in Machine Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krug_A/0/1/0/all/0/1"&gt;Andreas Krug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratul_R/0/1/0/all/0/1"&gt;Raihan Kabir Ratul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1"&gt;Sebastian Stober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning method for identifying mass composition of ultra-high-energy cosmic rays. (arXiv:2112.02072v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.02072</id>
        <link href="http://arxiv.org/abs/2112.02072"/>
        <updated>2022-04-09T00:48:55.359Z</updated>
        <summary type="html"><![CDATA[We introduce a novel method for identifying the mass composition of
ultra-high-energy cosmic rays using deep learning. The key idea of the method
is to use a chain of two neural networks. The first network predicts the type
of a primary particle for individual events, while the second infers the mass
composition of an ensemble of events. We apply this method to the Monte-Carlo
data for the Telescope Array Surface Detectors readings, on which it yields an
unprecedented low error of 7% for 4-component approximation. We also discuss
the problems of applying the developed method to the experimental data, and the
way they can be resolved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kalashev_O/0/1/0/all/0/1"&gt;O. Kalashev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kharuk_I/0/1/0/all/0/1"&gt;I. Kharuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kuznetsov_M/0/1/0/all/0/1"&gt;M. Kuznetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rubtsov_G/0/1/0/all/0/1"&gt;G. Rubtsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sako_T/0/1/0/all/0/1"&gt;T. Sako&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Tsunesada_Y/0/1/0/all/0/1"&gt;Y. Tsunesada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zhezher_Y/0/1/0/all/0/1"&gt;Ya. Zhezher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Transferring Value Function for Robot Exploration in Subterranean Environments. (arXiv:2204.03140v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2204.03140</id>
        <link href="http://arxiv.org/abs/2204.03140"/>
        <updated>2022-04-09T00:48:55.358Z</updated>
        <summary type="html"><![CDATA[In traditional robot exploration methods, the robot usually does not have
prior biases about the environment it is exploring. Thus the robot assigns
equal importance to the goals which leads to insufficient exploration
efficiency. Alternative, often a hand-tuned policy is used to tweak the value
of goals. In this paper, we present a method to learn how "good" some states
are, measured by the state value function, to provide a hint for the robot to
make exploration decisions. We propose to learn state value functions from
previous offline collected datasets and then transfer and improve the value
function during testing in a new environment. Moreover, the environments
usually have very few and even no extrinsic reward or feedback for the robot.
Therefore in this work, we also tackle the problem of sparse extrinsic rewards
from the environments. We design several intrinsic rewards to encourage the
robot to obtain more information during exploration. These reward functions
then become the building blocks of the state value functions. We test our
method on challenging subterranean and urban environments. To the best of our
knowledge, this work for the first time demonstrates value function prediction
with previous collected datasets to help exploration in challenging
subterranean environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yafei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_J/0/1/0/all/0/1"&gt;John Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security Aspects of Quantum Machine Learning: Opportunities, Threats and Defenses. (arXiv:2204.03625v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2204.03625</id>
        <link href="http://arxiv.org/abs/2204.03625"/>
        <updated>2022-04-09T00:48:55.345Z</updated>
        <summary type="html"><![CDATA[In the last few years, quantum computing has experienced a growth spurt. One
exciting avenue of quantum computing is quantum machine learning (QML) which
can exploit the high dimensional Hilbert space to learn richer representations
from limited data and thus can efficiently solve complex learning tasks.
Despite the increased interest in QML, there have not been many studies that
discuss the security aspects of QML. In this work, we explored the possible
future applications of QML in the hardware security domain. We also expose the
security vulnerabilities of QML and emerging attack models, and corresponding
countermeasures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1"&gt;Satwik Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Swaroop Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Target Speech Separation. (arXiv:2204.03594v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2204.03594</id>
        <link href="http://arxiv.org/abs/2204.03594"/>
        <updated>2022-04-09T00:48:55.344Z</updated>
        <summary type="html"><![CDATA[We introduce a new paradigm for single-channel target source separation where
the sources of interest can be distinguished using non-mutually exclusive
concepts (e.g., loudness, gender, language, spatial location, etc). Our
proposed heterogeneous separation framework can seamlessly leverage datasets
with large distribution shifts and learn cross-domain representations under a
variety of concepts used as conditioning. Our experiments show that training
separation models with heterogeneous conditions facilitates the generalization
to new concepts with unseen out-of-domain data while also performing
substantially higher than single-domain specialist models. Notably, such
training leads to more robust learning of new harder source separation
discriminative concepts and can yield improvements over permutation invariant
training with oracle source selection. We analyze the intrinsic behavior of
source separation training with heterogeneous metadata and propose ways to
alleviate emerging problems with challenging separation conditions. We release
the collection of preparation recipes for all datasets used to further promote
research towards this challenging task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wichern_G/0/1/0/all/0/1"&gt;Gordon Wichern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1"&gt;Aswin Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariate-assisted Sparse Tensor Completion. (arXiv:2103.06428v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06428</id>
        <link href="http://arxiv.org/abs/2103.06428"/>
        <updated>2022-04-09T00:48:55.344Z</updated>
        <summary type="html"><![CDATA[We aim to provably complete a sparse and highly-missing tensor in the
presence of covariate information along tensor modes. Our motivation comes from
online advertising where users click-through-rates (CTR) on ads over various
devices form a CTR tensor that has about 96% missing entries and has many zeros
on non-missing entries, which makes the standalone tensor completion method
unsatisfactory. Beside the CTR tensor, additional ad features or user
characteristics are often available. In this paper, we propose
Covariate-assisted Sparse Tensor Completion (COSTCO) to incorporate covariate
information for the recovery of the sparse tensor. The key idea is to jointly
extract latent components from both the tensor and the covariate matrix to
learn a synthetic representation. Theoretically, we derive the error bound for
the recovered tensor components and explicitly quantify the improvements on
both the reveal probability condition and the tensor recovery accuracy due to
covariates. Finally, we apply COSTCO to an advertisement dataset consisting of
a CTR tensor and ad covariate matrix, leading to 23% accuracy improvement over
the baseline. An important by-product is that ad latent components from COSTCO
reveal interesting ad clusters, which are useful for better ad targeting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ibriga_H/0/1/0/all/0/1"&gt;Hilda S Ibriga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1"&gt;Will Wei Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Auto-Tuning: Cost-Efficient Bayesian Transfer Optimization for Hyperparameter Recommendation. (arXiv:2106.09179v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09179</id>
        <link href="http://arxiv.org/abs/2106.09179"/>
        <updated>2022-04-09T00:48:55.344Z</updated>
        <summary type="html"><![CDATA[With the surge in the number of hyperparameters and training times of modern
machine learning models, hyperparameter tuning is becoming increasingly
expensive. However, after assessing 40 tuning methods systematically, we find
that each faces certain limitations. In particular, methods that speed up
tuning via knowledge transfer typically require the final performance of
hyperparameters and do not focus on low-fidelity information. As we demonstrate
empirically, this common practice is suboptimal and can incur an unnecessary
use of resources. It is more cost-efficient to instead leverage low-fidelity
tuning observations to measure inter-task similarity and transfer knowledge
from existing to new tasks accordingly. However, performing multi-fidelity
tuning comes with its own challenges in the transfer setting: the noise in
additional observations and the need for performance forecasting. Therefore, we
propose and conduct a thorough analysis of a multi-task multi-fidelity Bayesian
optimization framework, which leads to the best instantiation--amortized
auto-tuning (AT2). We further present an offline-computed 27-task
hyperparameter recommendation (HyperRec) database to serve the community.
Extensive experiments on HyperRec and other real-world databases illustrate the
effectiveness of our AT2 method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yuxin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search. (arXiv:2011.02166v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02166</id>
        <link href="http://arxiv.org/abs/2011.02166"/>
        <updated>2022-04-09T00:48:55.343Z</updated>
        <summary type="html"><![CDATA[The convolutional neural network has achieved great success in fulfilling
computer vision tasks despite large computation overhead against efficient
deployment. Structured (channel) pruning is usually applied to reduce the model
redundancy while preserving the network structure, such that the pruned network
can be easily deployed in practice. However, existing structured pruning
methods require hand-crafted rules which may lead to tremendous pruning space.
In this paper, we introduce Differentiable Annealing Indicator Search (DAIS)
that leverages the strength of neural architecture search in the channel
pruning and automatically searches for the effective pruned model with given
constraints on computation overhead. Specifically, DAIS relaxes the binarized
channel indicators to be continuous and then jointly learns both indicators and
model parameters via bi-level optimization. To bridge the non-negligible
discrepancy between the continuous model and the target binarized model, DAIS
proposes an annealing-based procedure to steer the indicator convergence
towards binarized states. Moreover, DAIS designs various regularizations based
on a priori structural knowledge to control the pruning sparsity and to improve
model performance. Experimental results show that DAIS outperforms
state-of-the-art pruning methods on CIFAR-10, CIFAR-100, and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yushuo Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pengyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhengping Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_K/0/1/0/all/0/1"&gt;Kaigui Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Autoencoder based Metamodeling for Multi-Objective Topology Optimization of Electrical Machines. (arXiv:2201.08877v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.08877</id>
        <link href="http://arxiv.org/abs/2201.08877"/>
        <updated>2022-04-09T00:48:55.343Z</updated>
        <summary type="html"><![CDATA[Conventional magneto-static finite element analysis of electrical machine
design is time-consuming and computationally expensive. Since each machine
topology has a distinct set of parameters, design optimization is commonly
performed independently. This paper presents a novel method for predicting Key
Performance Indicators (KPIs) of differently parameterized electrical machine
topologies at the same time by mapping a high dimensional integrated design
parameters in a lower dimensional latent space using a variational autoencoder.
After training, via a latent space, the decoder and multi-layer neural network
will function as meta-models for sampling new designs and predicting associated
KPIs, respectively. This enables parameter-based concurrent multi-topology
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1"&gt;Vivek Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flore_D/0/1/0/all/0/1"&gt;Dominik Flore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schops_S/0/1/0/all/0/1"&gt;Sebastian Sch&amp;#xf6;ps&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-To-End Optimization of Online Neural Network-supported Two-Stage Dereverberation for Hearing Devices. (arXiv:2204.02978v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.02978</id>
        <link href="http://arxiv.org/abs/2204.02978"/>
        <updated>2022-04-09T00:48:55.342Z</updated>
        <summary type="html"><![CDATA[A two-stage online dereverberation algorithm for hearing devices is presented
in this paper. The approach combines a multi-channel multi-frame linear
filtering approach with a single-channel single-frame post-filter. Both
components rely on power spectral density (PSD) estimates provided by deep
neural networks (DNNs). This contribution extends our prior work, which shows
that directly optimizing for a criterion at the output of the multi-channel
linear filtering stage results in a more efficient dereverberation, as compared
to placing the criterion at the output of the DNN to optimize the PSD
estimation. In the present work, we show that the dereverberation performance
of the proposed first stage particularly improves the early-to-mid
reverberation ratio if trained end-to-end. We thus argue that it can be
combined with a post-filtering stage which benefits from the early-to-mid ratio
improvement and is consequently able to efficiently suppress the residual late
reverberation. This proposed two stage procedure is shown to be both very
effective in terms of dereverberation performance and computational demands.
Furthermore, the proposed system can be adapted to the needs of different types
of hearing-device users by controlling the amount of reduction of early
reflections. The proposed system outperforms the previously proposed end-to-end
DNN-supported linear filtering algorithm, as well as other traditional
approaches, based on an evaluation using the noise-free version of the WHAMR!
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lemercier_J/0/1/0/all/0/1"&gt;Jean-Marie Lemercier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thiemann_J/0/1/0/all/0/1"&gt;Joachim Thiemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Koning_R/0/1/0/all/0/1"&gt;Raphael Koning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1"&gt;Timo Gerkmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comparison of mixed-variables Bayesian optimization approaches. (arXiv:2111.01533v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.01533</id>
        <link href="http://arxiv.org/abs/2111.01533"/>
        <updated>2022-04-09T00:48:55.341Z</updated>
        <summary type="html"><![CDATA[Most real optimization problems are defined over a mixed search space where
the variables are both discrete and continuous. In engineering applications,
the objective function is typically calculated with a numerically costly
black-box simulation.General mixed and costly optimization problems are
therefore of a great practical interest, yet their resolution remains in a
large part an open scientific question. In this article, costly mixed problems
are approached through Gaussian processes where the discrete variables are
relaxed into continuous latent variables. The continuous space is more easily
harvested by classical Bayesian optimization techniques than a mixed space
would. Discrete variables are recovered either subsequently to the continuous
optimization, or simultaneously with an additional continuous-discrete
compatibility constraint that is handled with augmented Lagrangians. Several
possible implementations of such Bayesian mixed optimizers are compared. In
particular, the reformulation of the problem with continuous latent variables
is put in competition with searches working directly in the mixed space. Among
the algorithms involving latent variables and an augmented Lagrangian, a
particular attention is devoted to the Lagrange multipliers for which a local
and a global estimation techniques are studied. The comparisons are based on
the repeated optimization of three analytical functions and a beam design
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cuesta_Ramirez_J/0/1/0/all/0/1"&gt;Jhouben Cuesta-Ramirez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Riche_R/0/1/0/all/0/1"&gt;Rodolphe Le Riche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Roustant_O/0/1/0/all/0/1"&gt;Olivier Roustant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Perrin_G/0/1/0/all/0/1"&gt;Guillaume Perrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Durantin_C/0/1/0/all/0/1"&gt;Cedric Durantin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gliere_A/0/1/0/all/0/1"&gt;Alain Gliere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems. (arXiv:2203.06416v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.06416</id>
        <link href="http://arxiv.org/abs/2203.06416"/>
        <updated>2022-04-09T00:48:55.340Z</updated>
        <summary type="html"><![CDATA[When dealing with a series of imminent issues, humans can naturally
concentrate on a subset of these concerning issues by prioritizing them
according to their contributions to motivational indices, e.g., the probability
of winning a game. This idea of concentration offers insights into
reinforcement learning of sophisticated Large-scale Multi-Agent Systems (LMAS)
participated by hundreds of agents. In such an LMAS, each agent receives a long
series of entity observations at each step, which can overwhelm existing
aggregation networks such as graph attention networks and cause inefficiency.
In this paper, we propose a concentration network called ConcNet. First,
ConcNet scores the observed entities considering several motivational indices,
e.g., expected survival time and state value of the agents, and then ranks,
prunes, and aggregates the encodings of observed entities to extract features.
Second, distinct from the well-known attention mechanism, ConcNet has a unique
motivational subnetwork to explicitly consider the motivational indices when
scoring the observed entities. Furthermore, we present a concentration policy
gradient architecture that can learn effective policies in LMAS from scratch.
Extensive experiments demonstrate that the presented architecture has excellent
scalability and flexibility, and significantly outperforms existing methods on
LMAS benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1"&gt;Qingxu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1"&gt;Tenghai Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jianqiang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_Z/0/1/0/all/0/1"&gt;Zhiqiang Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shiguang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Limitations of Multimodal VAEs. (arXiv:2110.04121v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.04121</id>
        <link href="http://arxiv.org/abs/2110.04121"/>
        <updated>2022-04-09T00:48:55.331Z</updated>
        <summary type="html"><![CDATA[Multimodal variational autoencoders (VAEs) have shown promise as efficient
generative models for weakly-supervised data. Yet, despite their advantage of
weak supervision, they exhibit a gap in generative quality compared to unimodal
VAEs, which are completely unsupervised. In an attempt to explain this gap, we
uncover a fundamental limitation that applies to a large family of
mixture-based multimodal VAEs. We prove that the sub-sampling of modalities
enforces an undesirable upper bound on the multimodal ELBO and thereby limits
the generative quality of the respective models. Empirically, we showcase the
generative quality gap on both synthetic and real data and present the
tradeoffs between different variants of multimodal VAEs. We find that none of
the existing approaches fulfills all desired criteria of an effective
multimodal generative model when applied on more complex datasets than those
used in previous benchmarks. In summary, we identify, formalize, and validate
fundamental limitations of VAE-based approaches for modeling weakly-supervised
data and discuss implications for real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daunhawer_I/0/1/0/all/0/1"&gt;Imant Daunhawer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1"&gt;Thomas M. Sutter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1"&gt;Kieran Chin-Cheong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palumbo_E/0/1/0/all/0/1"&gt;Emanuele Palumbo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Improving Selective Prediction Ability of NLP Systems. (arXiv:2008.09371v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09371</id>
        <link href="http://arxiv.org/abs/2008.09371"/>
        <updated>2022-04-09T00:48:55.329Z</updated>
        <summary type="html"><![CDATA[It's better to say "I can't answer" than to answer incorrectly. This
selective prediction ability is crucial for NLP systems to be reliably deployed
in real-world applications. Prior work has shown that existing selective
prediction techniques fail to perform well, especially in the out-of-domain
setting. In this work, we propose a method that improves probability estimates
of models by calibrating them using prediction confidence and difficulty score
of instances. Using these two signals, we first annotate held-out instances and
then train a calibrator to predict the likelihood of correctness of the model's
prediction. We instantiate our method with Natural Language Inference (NLI) and
Duplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and
Out-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the
representations learned by our calibrator result in an improvement of (15.81%,
5.64%) and (6.19%, 13.9%) over 'MaxProb' -- a selective prediction baseline --
on NLI and DD tasks respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1"&gt;Neeraj Varshney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Swaroop Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1"&gt;Chitta Baral&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Sample $\zeta$-mixup: Richer, More Realistic Synthetic Samples from a $p$-Series Interpolant. (arXiv:2204.03323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03323</id>
        <link href="http://arxiv.org/abs/2204.03323"/>
        <updated>2022-04-09T00:48:55.328Z</updated>
        <summary type="html"><![CDATA[Modern deep learning training procedures rely on model regularization
techniques such as data augmentation methods, which generate training samples
that increase the diversity of data and richness of label information. A
popular recent method, mixup, uses convex combinations of pairs of original
samples to generate new samples. However, as we show in our experiments, mixup
can produce undesirable synthetic samples, where the data is sampled off the
manifold and can contain incorrect labels. We propose $\zeta$-mixup, a
generalization of mixup with provably and demonstrably desirable properties
that allows convex combinations of $N \geq 2$ samples, leading to more
realistic and diverse outputs that incorporate information from $N$ original
samples by using a $p$-series interpolant. We show that, compared to mixup,
$\zeta$-mixup better preserves the intrinsic dimensionality of the original
datasets, which is a desirable property for training generalizable models.
Furthermore, we show that our implementation of $\zeta$-mixup is faster than
mixup, and extensive evaluation on controlled synthetic and 24 real-world
natural and medical image classification datasets shows that $\zeta$-mixup
outperforms mixup and traditional data augmentation techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abhishek_K/0/1/0/all/0/1"&gt;Kumar Abhishek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1"&gt;Colin J. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1"&gt;Ghassan Hamarneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference over radiative transfer models using variational and expectation maximization methods. (arXiv:2204.03346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03346</id>
        <link href="http://arxiv.org/abs/2204.03346"/>
        <updated>2022-04-09T00:48:55.327Z</updated>
        <summary type="html"><![CDATA[Earth observation from satellites offers the possibility to monitor our
planet with unprecedented accuracy. Radiative transfer models (RTMs) encode the
energy transfer through the atmosphere, and are used to model and understand
the Earth system, as well as to estimate the parameters that describe the
status of the Earth from satellite observations by inverse modeling. However,
performing inference over such simulators is a challenging problem. RTMs are
nonlinear, non-differentiable and computationally costly codes, which adds a
high level of difficulty in inference. In this paper, we introduce two
computational techniques to infer not only point estimates of biophysical
parameters but also their joint distribution. One of them is based on a
variational autoencoder approach and the second one is based on a Monte Carlo
Expectation Maximization (MCEM) scheme. We compare and discuss benefits and
drawbacks of each approach. We also provide numerical comparisons in synthetic
simulations and the real PROSAIL model, a popular RTM that combines land
vegetation leaf and canopy modeling. We analyze the performance of the two
approaches for modeling and inferring the distribution of three key biophysical
parameters for quantifying the terrestrial biosphere.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Svendsen_D/0/1/0/all/0/1"&gt;Daniel Heestermans Svendsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1"&gt;Daniel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1"&gt;Luca Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1"&gt;Valero Laparra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_A/0/1/0/all/0/1"&gt;Alvaro Moreno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1"&gt;Gustau Camps-Valls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Justice Stories: A Repository of Case Studies. (arXiv:2204.03100v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2204.03100</id>
        <link href="http://arxiv.org/abs/2204.03100"/>
        <updated>2022-04-09T00:48:55.326Z</updated>
        <summary type="html"><![CDATA[The idea of "data justice" is of recent academic vintage. It has arisen over
the past decade in Anglo-European research institutions as an attempt to bring
together a critique of the power dynamics that underlie accelerating trends of
datafication with a normative commitment to the principles of social justice-a
commitment to the achievement of a society that is equitable, fair, and capable
of confronting the root causes of injustice.However, despite the seeming
novelty of such a data justice pedigree, this joining up of the critique of the
power imbalances that have shaped the digital and "big data" revolutions with a
commitment to social equity and constructive societal transformation has a
deeper historical, and more geographically diverse, provenance. As the stories
of the data justice initiatives, activism, and advocacy contained in this
volume well evidence, practices of data justice across the globe have, in fact,
largely preceded the elaboration and crystallisation of the idea of data
justice in contemporary academic discourse. In telling these data justice
stories, we hope to provide the reader with two interdependent tools of data
justice thinking: First, we aim to provide the reader with the critical
leverage needed to discern those distortions and malformations of data justice
that manifest in subtle and explicit forms of power, domination, and coercion.
Second, we aim to provide the reader with access to the historically effective
forms of normativity and ethical insight that have been marshalled by data
justice activists and advocates as tools of societal transformation-so that
these forms of normativity and insight can be drawn on, in turn, as
constructive resources to spur future transformative data justice practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leslie_D/0/1/0/all/0/1"&gt;David Leslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Briggs_M/0/1/0/all/0/1"&gt;Morgan Briggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perini_A/0/1/0/all/0/1"&gt;Antonella Perini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayadeva_S/0/1/0/all/0/1"&gt;Smera Jayadeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rincon_C/0/1/0/all/0/1"&gt;Cami Rinc&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raval_N/0/1/0/all/0/1"&gt;Noopur Raval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birhane_A/0/1/0/all/0/1"&gt;Abeba Birhane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1"&gt;Rosamund Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katell_M/0/1/0/all/0/1"&gt;Michael Katell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aitken_M/0/1/0/all/0/1"&gt;Mhairi Aitken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Misproducted Speech using Spectrogram Inpainting. (arXiv:2204.03379v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.03379</id>
        <link href="http://arxiv.org/abs/2204.03379"/>
        <updated>2022-04-09T00:48:55.326Z</updated>
        <summary type="html"><![CDATA[Learning a new language involves constantly comparing speech productions with
reference productions from the environment. Early in speech acquisition,
children make articulatory adjustments to match their caregivers' speech.
Grownup learners of a language tweak their speech to match the tutor reference.
This paper proposes a method to synthetically generate correct pronunciation
feedback given incorrect production. Furthermore, our aim is to generate the
corrected production while maintaining the speaker's original voice.

The system prompts the user to pronounce a phrase. The speech is recorded,
and the samples associated with the inaccurate phoneme are masked with zeros.
This waveform serves as an input to a speech generator, implemented as a deep
learning inpainting system with a U-net architecture, and trained to output a
reconstructed speech. The training set is composed of unimpaired proper speech
examples, and the generator is trained to reconstruct the original proper
speech. We evaluated the performance of our system on phoneme replacement of
minimal pair words of English as well as on children with pronunciation
disorders. Results suggest that human listeners slightly prefer our generated
speech over a smoothed replacement of the inaccurate phoneme with a production
of a different speaker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ben_Simon_T/0/1/0/all/0/1"&gt;Talia Ben-Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kreuk_F/0/1/0/all/0/1"&gt;Felix Kreuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Awwad_F/0/1/0/all/0/1"&gt;Faten Awwad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Jacob T. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keshet_J/0/1/0/all/0/1"&gt;Joseph Keshet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03227</id>
        <link href="http://arxiv.org/abs/2204.03227"/>
        <updated>2022-04-09T00:48:55.325Z</updated>
        <summary type="html"><![CDATA[Self-attention is a key enabler of state-of-art accuracy for various
transformer-based Natural Language Processing models. This attention mechanism
calculates a correlation score for each word with respect to the other words in
a sentence. Commonly, only a small subset of words highly correlates with the
word under attention, which is only determined at runtime. As such, a
significant amount of computation is inconsequential due to low attention
scores and can potentially be pruned. The main challenge is finding the
threshold for the scores below which subsequent computation will be
inconsequential. Although such a threshold is discrete, this paper formulates
its search through a soft differentiable regularizer integrated into the loss
function of the training. This formulation piggy backs on the back-propagation
training to analytically co-optimize the threshold and the weights
simultaneously, striking a formally optimal balance between accuracy and
computation pruning. To best utilize this mathematical innovation, we devise a
bit-serial architecture, dubbed LeOPArd, for transformer language models with
bit-level early termination microarchitectural mechanism. We evaluate our
design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision
transformer models. Post-layout results show that, on average, LeOPArd yields
1.9x and 3.9x speedup and energy reduction, respectively, while keeping the
average accuracy virtually intact (<0.2% degradation)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenge Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodrati_S/0/1/0/all/0/1"&gt;Soroush Ghodrati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1"&gt;Amir Yazdanbakhsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_H/0/1/0/all/0/1"&gt;Hadi Esmaeilzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Mingu Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pin the Memory: Learning to Generalize Semantic Segmentation. (arXiv:2204.03609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03609</id>
        <link href="http://arxiv.org/abs/2204.03609"/>
        <updated>2022-04-09T00:48:55.321Z</updated>
        <summary type="html"><![CDATA[The rise of deep neural networks has led to several breakthroughs for
semantic segmentation. In spite of this, a model trained on source domain often
fails to work properly in new challenging domains, that is directly concerned
with the generalization capability of the model. In this paper, we present a
novel memory-guided domain generalization method for semantic segmentation
based on meta-learning framework. Especially, our method abstracts the
conceptual knowledge of semantic classes into categorical memory which is
constant beyond the domains. Upon the meta-learning concept, we repeatedly
train memory-guided networks and simulate virtual test to 1) learn how to
memorize a domain-agnostic and distinct information of classes and 2) offer an
externally settled memory as a class-guidance to reduce the ambiguity of
representation in the test data of arbitrary unseen domain. To this end, we
also propose memory divergence and feature cohesion losses, which encourage to
learn memory reading and update processes for category-aware domain
generalization. Extensive experiments for semantic segmentation demonstrate the
superior generalization capability of our method over state-of-the-art works on
various benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jungin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1"&gt;Dongbo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class-Incremental Learning with Strong Pre-trained Models. (arXiv:2204.03634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03634</id>
        <link href="http://arxiv.org/abs/2204.03634"/>
        <updated>2022-04-09T00:48:55.317Z</updated>
        <summary type="html"><![CDATA[Class-incremental learning (CIL) has been widely studied under the setting of
starting from a small number of classes (base classes). Instead, we explore an
understudied real-world setting of CIL that starts with a strong model
pre-trained on a large number of base classes. We hypothesize that a strong
base model can provide a good representation for novel classes and incremental
learning can be done with small adaptations. We propose a 2-stage training
scheme, i) feature augmentation -- cloning part of the backbone and fine-tuning
it on the novel data, and ii) fusion -- combining the base and novel
classifiers into a unified classifier. Experiments show that the proposed
method significantly outperforms state-of-the-art CIL methods on the
large-scale ImageNet dataset (e.g. +10% overall accuracy than the best). We
also propose and analyze understudied practical CIL scenarios, such as
base-novel overlap with distribution shift. Our proposed method is robust and
generalizes to all analyzed CIL settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tz-Ying Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_G/0/1/0/all/0/1"&gt;Gurumurthy Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhizhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1"&gt;Nuno Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1"&gt;Rahul Bhotika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks. (arXiv:2011.11048v6 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11048</id>
        <link href="http://arxiv.org/abs/2011.11048"/>
        <updated>2022-04-09T00:48:55.317Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph
data and have achieved significant progress in graph analysis tasks (e.g., node
classification) in recent years. However, similar to other deep neural networks
like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs),
GNNs behave like a black box with their details hidden from model developers
and users. It is therefore difficult to diagnose possible errors of GNNs.
Despite many visual analytics studies being done on CNNs and RNNs, little
research has addressed the challenges for GNNs. This paper fills the research
gap with an interactive visual analysis tool, GNNLens, to assist model
developers and users in understanding and analyzing GNNs. Specifically,
Parallel Sets View and Projection View enable users to quickly identify and
validate error patterns in the set of wrong predictions; Graph View and Feature
Matrix View offer a detailed analysis of individual nodes to assist users in
forming hypotheses about the error patterns. Since GNNs jointly model the graph
structure and the node features, we reveal the relative influences of the two
types of information by comparing the predictions of three models: GNN,
Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case
studies and interviews with domain experts demonstrate the effectiveness of
GNNLens in facilitating the understanding of GNN models and their errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qianwen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yao Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengfei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Adaptive Graph and Structured Sparsity Regularization for Unsupervised Feature Selection. (arXiv:2010.05454v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05454</id>
        <link href="http://arxiv.org/abs/2010.05454"/>
        <updated>2022-04-09T00:48:55.315Z</updated>
        <summary type="html"><![CDATA[Feature selection is an important data preprocessing in data mining and
machine learning which can be used to reduce the feature dimension without
deteriorating model's performance. Since obtaining annotated data is laborious
or even infeasible in many cases, unsupervised feature selection is more
practical in reality. Though lots of methods for unsupervised feature selection
have been proposed, these methods select features independently, thus it is no
guarantee that the group of selected features is optimal. What's more, the
number of selected features must be tuned carefully to obtain a satisfactory
result. To tackle these problems, we propose a joint adaptive graph and
structured sparsity regularization unsupervised feature selection (JASFS)
method in this paper, in which a $l_{2,0}$-norm regularization term with
respect to transformation matrix is imposed in the manifold learning for
feature selection, and a graph regularization term is incorporated into the
learning model to learn the local geometric structure of data adaptively. An
efficient and simple iterative algorithm is designed to solve the proposed
optimization problem with the analysis of computational complexity. After
optimized, a subset of optimal features will be selected in group, and the
number of selected features will be determined automatically. Experimental
results on eight benchmarks demonstrate the effectiveness and efficiency of the
proposed method compared with several state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenzhen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yuanlong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Design Space Exploration of Nonlinear Systems: Part I. (arXiv:2104.01747v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01747</id>
        <link href="http://arxiv.org/abs/2104.01747"/>
        <updated>2022-04-09T00:48:55.314Z</updated>
        <summary type="html"><![CDATA[System design tools are often only available as input-output blackboxes: for
a given design as input they compute an output representing system behavior.
Blackboxes are intended to be run in the forward direction. This paper presents
a new method of solving the inverse design problem namely, given requirements
or constraints on output, find an input that also optimizes an objective
function. This problem is challenging for several reasons. First, blackboxes
are not designed to be run in reverse. Second, inputs and outputs can be
discrete and continuous. Third, finding designs concurrently satisfying a set
of requirements is hard because designs satisfying individual requirements may
conflict with each other. Fourth, blackbox evaluations can be expensive.
Finally, blackboxes can sometimes fail to produce an output. This paper
presents CNMA, a new method of solving the inverse problem that overcomes these
challenges. CNMA tries to sample only the part of the design space relevant to
solving the problem, leveraging the power of neural networks, Mixed Integer
Linear Programs, and a new learning-from-failure feedback loop. The paper also
presents a parallel version of CNMA that improves the efficiency and quality of
solutions over the sequential version, and tries to steer it away from local
optima. CNMA's performance is evaluated against conventional optimization
methods for seven nonlinear design problems of 8 (two problems), 10, 15, 36 and
60 real-valued dimensions and one with 186 binary dimensions. Conventional
methods evaluated are off-the-shelf implementations of Bayesian Optimization
with Gaussian Processes, Nelder Mead and Random Search. The first two do not
solve problems that are high-dimensional, have discrete and continuous
variables or whose blackboxes can fail to return values. CNMA solves all
problems, and surpasses the performance of conventional methods by up to 87%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narain_S/0/1/0/all/0/1"&gt;Sanjai Narain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mak_E/0/1/0/all/0/1"&gt;Emily Mak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chee_D/0/1/0/all/0/1"&gt;Dana Chee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Englot_B/0/1/0/all/0/1"&gt;Brendan Englot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pochiraju_K/0/1/0/all/0/1"&gt;Kishore Pochiraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1"&gt;Niraj K. Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayan_K/0/1/0/all/0/1"&gt;Karthik Narayan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTensor: Low-Rank Tensor Decomposition with Deep Network Priors. (arXiv:2204.03145v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2204.03145</id>
        <link href="http://arxiv.org/abs/2204.03145"/>
        <updated>2022-04-09T00:48:55.313Z</updated>
        <summary type="html"><![CDATA[DeepTensor is a computationally efficient framework for low-rank
decomposition of matrices and tensors using deep generative networks. We
decompose a tensor as the product of low-rank tensor factors (e.g., a matrix as
the outer product of two vectors), where each low-rank tensor is generated by a
deep network (DN) that is trained in a self-supervised manner to minimize the
mean-squared approximation error. Our key observation is that the implicit
regularization inherent in DNs enables them to capture nonlinear signal
structures (e.g., manifolds) that are out of the reach of classical linear
methods like the singular value decomposition (SVD) and principal component
analysis (PCA). Furthermore, in contrast to the SVD and PCA, whose performance
deteriorates when the tensor's entries deviate from additive white Gaussian
noise, we demonstrate that the performance of DeepTensor is robust to a wide
range of distributions. We validate that DeepTensor is a robust and
computationally efficient drop-in replacement for the SVD, PCA, nonnegative
matrix factorization (NMF), and similar decompositions by exploring a range of
real-world applications, including hyperspectral image denoising, 3D MRI
tomography, and image classification. In particular, DeepTensor offers a 6dB
signal-to-noise ratio improvement over standard denoising methods for signals
corrupted by Poisson noise and learns to decompose 3D tensors 60 times faster
than a single DN equipped with 3D convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Saragadam_V/0/1/0/all/0/1"&gt;Vishwanath Saragadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Veeraraghavan_A/0/1/0/all/0/1"&gt;Ashok Veeraraghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard G. Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids. (arXiv:2204.03305v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.03305</id>
        <link href="http://arxiv.org/abs/2204.03305"/>
        <updated>2022-04-09T00:48:55.313Z</updated>
        <summary type="html"><![CDATA[Improving the user's hearing ability to understand speech in noisy
environments is critical to the development of hearing aid (HA) devices. For
this, it is important to derive a metric that can fairly predict speech
intelligibility for HA users. A straightforward approach is to conduct a
subjective listening test and use the test results as an evaluation metric.
However, conducting large-scale listening tests is time-consuming and
expensive. Therefore, several evaluation metrics were derived as surrogates for
subjective listening test results. In this study, we propose a multi-branched
speech intelligibility prediction model (MBI-Net), for predicting the
subjective intelligibility scores of HA users. MBI-Net consists of two branches
of models, with each branch consisting of a hearing loss model, a cross-domain
feature extraction module, and a speech intelligibility prediction model, to
process speech signals from one channel. The outputs of the two branches are
fused through a linear layer to obtain predicted speech intelligibility scores.
Experimental results confirm the effectiveness of MBI-Net, which produces
higher prediction scores than the baseline system in Track 1 and Track 2 on the
Clarity Prediction Challenge 2022 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zezario_R/0/1/0/all/0/1"&gt;Ryandhimas E. Zezario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuh_C/0/1/0/all/0/1"&gt;Chiou-Shann Fuh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews. (arXiv:2104.05861v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05861</id>
        <link href="http://arxiv.org/abs/2104.05861"/>
        <updated>2022-04-09T00:48:55.313Z</updated>
        <summary type="html"><![CDATA[Context: Mobile app reviews written by users on app stores or social media
are significant resources for app developers.Analyzing app reviews have proved
to be useful for many areas of software engineering (e.g., requirement
engineering, testing). Automatic classification of app reviews requires
extensive efforts to manually curate a labeled dataset. When the classification
purpose changes (e.g. identifying bugs versus usability issues or sentiment),
new datasets should be labeled, which prevents the extensibility of the
developed models for new desired classes/tasks in practice. Recent pre-trained
neural language models (PTM) are trained on large corpora in an unsupervised
manner and have found success in solving similar Natural Language Processing
problems. However, the applicability of PTMs is not explored for app review
classification Objective: We investigate the benefits of PTMs for app review
classification compared to the existing models, as well as the transferability
of PTMs in multiple settings. Method: We empirically study the accuracy and
time efficiency of PTMs compared to prior approaches using six datasets from
literature. In addition, we investigate the performance of the PTMs trained on
app reviews (i.e. domain-specific PTMs) . We set up different studies to
evaluate PTMs in multiple settings: binary vs. multi-class classification,
zero-shot classification (when new labels are introduced to the model),
multi-task setting, and classification of reviews from different resources. The
datasets are manually labeled app review datasets from Google Play Store, Apple
App Store, and Twitter data. In all cases, Micro and Macro Precision, Recall,
and F1-scores will be used and we will report the time required for training
and prediction with the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hadi_M/0/1/0/all/0/1"&gt;Mohammad Abdul Hadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1"&gt;Fatemeh H. Fard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. (arXiv:2004.10888v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.10888</id>
        <link href="http://arxiv.org/abs/2004.10888"/>
        <updated>2022-04-09T00:48:55.312Z</updated>
        <summary type="html"><![CDATA[We present a mean-variance policy iteration (MVPI) framework for risk-averse
control in a discounted infinite horizon MDP optimizing the variance of a
per-step reward random variable. MVPI enjoys great flexibility in that any
policy evaluation method and risk-neutral control method can be dropped in for
risk-averse control off the shelf, in both on- and off-policy settings. This
flexibility reduces the gap between risk-neutral control and risk-averse
control and is achieved by working on a novel augmented MDP directly. We
propose risk-averse TD3 as an example instantiating MVPI, which outperforms
vanilla TD3 and many previous risk-averse control methods in challenging Mujoco
robot simulation tasks under a risk-aware performance metric. This risk-averse
TD3 is the first to introduce deterministic policies and off-policy learning
into risk-averse reinforcement learning, both of which are key to the
performance boost we show in Mujoco domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shangtong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Set Union. (arXiv:2002.09745v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09745</id>
        <link href="http://arxiv.org/abs/2002.09745"/>
        <updated>2022-04-09T00:48:55.311Z</updated>
        <summary type="html"><![CDATA[We study the basic operation of set union in the global model of differential
privacy. In this problem, we are given a universe $U$ of items, possibly of
infinite size, and a database $D$ of users. Each user $i$ contributes a subset
$W_i \subseteq U$ of items. We want an ($\epsilon$,$\delta$)-differentially
private algorithm which outputs a subset $S \subset \cup_i W_i$ such that the
size of $S$ is as large as possible. The problem arises in countless real world
applications; it is particularly ubiquitous in natural language processing
(NLP) applications as vocabulary extraction. For example, discovering words,
sentences, $n$-grams etc., from private text data belonging to users is an
instance of the set union problem.

Known algorithms for this problem proceed by collecting a subset of items
from each user, taking the union of such subsets, and disclosing the items
whose noisy counts fall above a certain threshold. Crucially, in the above
process, the contribution of each individual user is always independent of the
items held by other users, resulting in a wasteful aggregation process, where
some item counts happen to be way above the threshold. We deviate from the
above paradigm by allowing users to contribute their items in a
$\textit{dependent fashion}$, guided by a $\textit{policy}$. In this new
setting ensuring privacy is significantly delicate. We prove that any policy
which has certain $\textit{contractive}$ properties would result in a
differentially private algorithm. We design two new algorithms, one using
Laplace noise and other Gaussian noise, as specific instances of policies
satisfying the contractive properties. Our experiments show that the new
algorithms significantly outperform previously known mechanisms for the
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulhane_P/0/1/0/all/0/1"&gt;Pankaj Gulhane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1"&gt;Janardhan Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Judy Hanwen Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1"&gt;Sergey Yekhanin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delta Keyword Transformer: Bringing Transformers to the Edge through Dynamically Pruned Multi-Head Self-Attention. (arXiv:2204.03479v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03479</id>
        <link href="http://arxiv.org/abs/2204.03479"/>
        <updated>2022-04-09T00:48:55.309Z</updated>
        <summary type="html"><![CDATA[Multi-head self-attention forms the core of Transformer networks. However,
their quadratically growing complexity with respect to the input sequence
length impedes their deployment on resource-constrained edge devices. We
address this challenge by proposing a dynamic pruning method, which exploits
the temporal stability of data across tokens to reduce inference cost. The
threshold-based method only retains significant differences between the
subsequent tokens, effectively reducing the number of multiply-accumulates, as
well as the internal tensor data sizes. The approach is evaluated on the Google
Speech Commands Dataset for keyword spotting, and the performance is compared
against the baseline Keyword Transformer. Our experiments show that we can
reduce ~80% of operations while maintaining the original 98.4% accuracy.
Moreover, a reduction of ~87-94% operations can be achieved when only degrading
the accuracy by 1-4%, speeding up the multi-head self-attention inference by a
factor of ~7.5-16.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jelcicova_Z/0/1/0/all/0/1"&gt;Zuzana Jel&amp;#x10d;icov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhelst_M/0/1/0/all/0/1"&gt;Marian Verhelst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08850</id>
        <link href="http://arxiv.org/abs/2102.08850"/>
        <updated>2022-04-09T00:48:55.309Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has recently seen tremendous success in self-supervised
learning. So far, however, it is largely unclear why the learned
representations generalize so effectively to a large variety of downstream
tasks. We here prove that feedforward models trained with objectives belonging
to the commonly used InfoNCE family learn to implicitly invert the underlying
generative model of the observed data. While the proofs make certain
statistical assumptions about the generative model, we observe empirically that
our findings hold even if these assumptions are severely violated. Our theory
highlights a fundamental connection between contrastive learning, generative
modeling, and nonlinear independent component analysis, thereby furthering our
understanding of the learned representations as well as providing a theoretical
foundation to derive more effective contrastive losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1"&gt;Steffen Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes. (arXiv:2102.00135v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00135</id>
        <link href="http://arxiv.org/abs/2102.00135"/>
        <updated>2022-04-09T00:48:55.308Z</updated>
        <summary type="html"><![CDATA[We present new policy mirror descent (PMD) methods for solving reinforcement
learning (RL) problems with either strongly convex or general convex
regularizers. By exploring the structural properties of these overall highly
nonconvex problems we show that the PMD methods exhibit fast linear rate of
convergence to the global optimality. We develop stochastic counterparts of
these methods, and establish an ${\cal O}(1/\epsilon)$ (resp., ${\cal
O}(1/\epsilon^2)$) sampling complexity for solving these RL problems with
strongly (resp., general) convex regularizers using different sampling schemes,
where $\epsilon$ denote the target accuracy. We further show that the
complexity for computing the gradients of these regularizers, if necessary, can
be bounded by ${\cal O}\{(\log_\gamma \epsilon) [(1-\gamma)L/\mu]^{1/2}\log
(1/\epsilon)\}$ (resp., ${\cal O} \{(\log_\gamma \epsilon )
(L/\epsilon)^{1/2}\}$)for problems with strongly (resp., general) convex
regularizers. Here $\gamma$ denotes the discounting factor. To the best of our
knowledge, these complexity bounds, along with our algorithmic developments,
appear to be new in both optimization and RL literature. The introduction of
these convex regularizers also greatly expands the flexibility and
applicability of RL models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1"&gt;Guanghui Lan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers. (arXiv:2204.03503v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03503</id>
        <link href="http://arxiv.org/abs/2204.03503"/>
        <updated>2022-04-09T00:48:55.306Z</updated>
        <summary type="html"><![CDATA[Automated short answer grading (ASAG) has gained attention in education as a
means to scale educational tasks to the growing number of students. Recent
progress in Natural Language Processing and Machine Learning has largely
influenced the field of ASAG, of which we survey the recent research
advancements. We complement previous surveys by providing a comprehensive
analysis of recently published methods that deploy deep learning approaches. In
particular, we focus our analysis on the transition from hand engineered
features to representation learning approaches, which learn representative
features for the task at hand automatically from large corpora of data. We
structure our analysis of deep learning methods along three categories: word
embeddings, sequential models, and attention-based methods. Deep learning
impacted ASAG differently than other fields of NLP, as we noticed that the
learned representations alone do not contribute to achieve the best results,
but they rather show to work in a complementary way with hand-engineered
features. The best performance are indeed achieved by methods that combine the
carefully hand-engineered features with the power of the semantic descriptions
provided by the latest models, like transformers architectures. We identify
challenges and provide an outlook on research direction that can be addressed
in the future]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haller_S/0/1/0/all/0/1"&gt;Stefan Haller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aldea_A/0/1/0/all/0/1"&gt;Adina Aldea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1"&gt;Christin Seifert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1"&gt;Nicola Strisciuglio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Overview on Artificial Intelligence Techniques for Diagnosis of Schizophrenia Based on Magnetic Resonance Imaging Modalities: Methods, Challenges, and Future Works. (arXiv:2103.03081v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03081</id>
        <link href="http://arxiv.org/abs/2103.03081"/>
        <updated>2022-04-09T00:48:55.306Z</updated>
        <summary type="html"><![CDATA[Schizophrenia (SZ) is a mental disorder that typically emerges in late
adolescence or early adulthood. It reduces the life expectancy of patients by
15 years. Abnormal behavior, perception of emotions, social relationships, and
reality perception are among its most significant symptoms. Past studies have
revealed the temporal and anterior lobes of hippocampus regions of brain get
affected by SZ. Also, increased volume of cerebrospinal fluid (CSF) and
decreased volume of white and gray matter can be observed due to this disease.
The magnetic resonance imaging (MRI) is the popular neuroimaging technique used
to explore structural/functional brain abnormalities in SZ disorder owing to
its high spatial resolution. Various artificial intelligence (AI) techniques
have been employed with advanced image/signal processing methods to obtain
accurate diagnosis of SZ. This paper presents a comprehensive overview of
studies conducted on automated diagnosis of SZ using MRI modalities. Main
findings, various challenges, and future works in developing the automated SZ
detection are described in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghi_D/0/1/0/all/0/1"&gt;Delaram Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1"&gt;Afshin Shoeibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_N/0/1/0/all/0/1"&gt;Navid Ghassemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moridian_P/0/1/0/all/0/1"&gt;Parisa Moridian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadem_A/0/1/0/all/0/1"&gt;Ali Khadem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1"&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teshnehlab_M/0/1/0/all/0/1"&gt;Mohammad Teshnehlab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1"&gt;J. Manuel Gorriz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khozeimeh_F/0/1/0/all/0/1"&gt;Fahime Khozeimeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu-Dong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_U/0/1/0/all/0/1"&gt;U Rajendra Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Contrastive Learning in Image-Text-Label Space. (arXiv:2204.03610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03610</id>
        <link href="http://arxiv.org/abs/2204.03610"/>
        <updated>2022-04-09T00:48:55.305Z</updated>
        <summary type="html"><![CDATA[Visual recognition is recently learned via either supervised learning on
human-annotated image-label data or language-image contrastive learning with
webly-crawled image-text pairs. While supervised learning may result in a more
discriminative representation, language-image pretraining shows unprecedented
zero-shot recognition capability, largely due to the different properties of
data sources and learning objectives. In this work, we introduce a new
formulation by combining the two data sources into a common image-text-label
space. In this space, we propose a new learning paradigm, called Unified
Contrastive Learning (UniCL) with a single learning objective to seamlessly
prompt the synergy of two data types. Extensive experiments show that our UniCL
is an effective way of learning semantically rich yet discriminative
representations, universally for image recognition in zero-shot, linear-probe,
fully finetuning and transfer learning scenarios. Particularly, it attains
gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over
the language-image contrastive learning and supervised learning methods,
respectively. In linear probe setting, it also boosts the performance over the
two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL
stand-alone is a good learner on pure image-label data, rivaling the supervised
learning methods across three image classification datasets and two types of
vision backbones, ResNet and Swin Transformer. Code is available at
https://github.com/microsoft/UniCL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Ce Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis. (arXiv:2204.03572v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2204.03572</id>
        <link href="http://arxiv.org/abs/2204.03572"/>
        <updated>2022-04-09T00:48:55.304Z</updated>
        <summary type="html"><![CDATA[The Epithelial Dysplasia (ED) is a tissue alteration commonly present in
lesions preceding oral cancer, being its presence one of the most important
factors in the progression toward carcinoma. This study proposes a method to
design a low computational cost classification system to support the detection
of dysplastic epithelia, contributing to reduce the variability of pathologist
assessments. We employ a multilayer artificial neural network (MLP-ANN) and
defining the regions of the epithelium to be assessed based on the knowledge of
the pathologist. The performance of the proposed solution was statistically
evaluated. The implemented MLP-ANN presented an average accuracy of 87%, with a
variability much inferior to that obtained from three trained evaluators.
Moreover, the proposed solution led to results which are very close to those
obtained using a convolutional neural network (CNN) implemented by transfer
learning, with 100 times less computational complexity. In conclusion, our
results show that a simple neural network structure can lead to a performance
equivalent to that of much more complex structures, which are routinely used in
the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rocha_K/0/1/0/all/0/1"&gt;Karoline da Rocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bermudez_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; C. M. Bermudez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rivero_E/0/1/0/all/0/1"&gt;Elena R. C. Rivero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Costa_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rcio H. Costa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariance Discovery by Learned Parameter-Sharing. (arXiv:2204.03640v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03640</id>
        <link href="http://arxiv.org/abs/2204.03640"/>
        <updated>2022-04-09T00:48:55.304Z</updated>
        <summary type="html"><![CDATA[Designing equivariance as an inductive bias into deep-nets has been a
prominent approach to build effective models, e.g., a convolutional neural
network incorporates translation equivariance. However, incorporating these
inductive biases requires knowledge about the equivariance properties of the
data, which may not be available, e.g., when encountering a new domain. To
address this, we study how to discover interpretable equivariances from data.
Specifically, we formulate this discovery process as an optimization problem
over a model's parameter-sharing schemes. We propose to use the partition
distance to empirically quantify the accuracy of the recovered equivariance.
Also, we theoretically analyze the method for Gaussian data and provide a bound
on the mean squared gap between the studied discovery scheme and the oracle
scheme. Empirically, we show that the approach recovers known equivariances,
such as permutations and shifts, on sum of numbers and spatially-invariant
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1"&gt;Raymond A. Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuan-Ting Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1"&gt;Mark Hasegawa-Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander G. Schwing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidimensional linked matrix factorization for pan-omics pan-cancer analysis. (arXiv:2002.02601v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02601</id>
        <link href="http://arxiv.org/abs/2002.02601"/>
        <updated>2022-04-09T00:48:55.304Z</updated>
        <summary type="html"><![CDATA[Several modern applications require the integration of multiple large data
matrices that have shared rows and/or columns. For example, cancer studies that
integrate multiple omics platforms across multiple types of cancer, pan-omics
pan-cancer analysis, have extended our knowledge of molecular heterogenity
beyond what was observed in single tumor and single platform studies. However,
these studies have been limited by available statistical methodology. We
propose a flexible approach to the simultaneous factorization and decomposition
of variation across such bidimensionally linked matrices, BIDIFAC+. This
decomposes variation into a series of low-rank components that may be shared
across any number of row sets (e.g., omics platforms) or column sets (e.g.,
cancer types). This builds on a growing literature for the factorization and
decomposition of linked matrices, which has primarily focused on multiple
matrices that are linked in one dimension (rows or columns) only. Our objective
function extends nuclear norm penalization, is motivated by random matrix
theory, gives an identifiable decomposition under relatively mild conditions,
and can be shown to give the mode of a Bayesian posterior distribution. We
apply BIDIFAC+ to pan-omics pan-cancer data from TCGA, identifying shared and
specific modes of variability across 4 different omics platforms and 29
different cancer types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lock_E/0/1/0/all/0/1"&gt;Eric F. Lock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_J/0/1/0/all/0/1"&gt;Jun Young Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hoadley_K/0/1/0/all/0/1"&gt;Katherine A. Hoadley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval Bound Propagation--aided Few-shot Learning. (arXiv:2204.03511v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03511</id>
        <link href="http://arxiv.org/abs/2204.03511"/>
        <updated>2022-04-09T00:48:55.303Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims to transfer the knowledge acquired from training on a
diverse set of tasks, from a given task distribution, to generalize to unseen
tasks, from the same distribution, with a limited amount of labeled data. The
underlying requirement for effective few-shot generalization is to learn a good
representation of the task manifold. One way to encourage this is to preserve
local neighborhoods in the feature space learned by the few-shot learner. To
this end, we introduce the notion of interval bounds from the provably robust
training literature to few-shot learning. The interval bounds are used to
characterize neighborhoods around the training tasks. These neighborhoods can
then be preserved by minimizing the distance between a task and its respective
bounds. We further introduce a novel strategy to artificially form new tasks
for training by interpolating between the available tasks and their respective
interval bounds, to aid in cases with a scarcity of tasks. We apply our
framework to both model-agnostic meta-learning as well as prototype-based
metric-learning paradigms. The efficacy of our proposed approach is evident
from the improved performance on several datasets from diverse domains in
comparison to a sizable number of recent competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1"&gt;Shounak Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mullick_S/0/1/0/all/0/1"&gt;Sankha Subhra Mullick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Swagatam Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An optimized hybrid solution for IoT based lifestyle disease classification using stress data. (arXiv:2204.03573v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2204.03573</id>
        <link href="http://arxiv.org/abs/2204.03573"/>
        <updated>2022-04-09T00:48:55.303Z</updated>
        <summary type="html"><![CDATA[Stress, anxiety, and nervousness are all high-risk health states in everyday
life. Previously, stress levels were determined by speaking with people and
gaining insight into what they had experienced recently or in the past.
Typically, stress is caused by an incidence that occurred a long time ago, but
sometimes it is triggered by unknown factors. This is a challenging and complex
task, but recent research advances have provided numerous opportunities to
automate it. The fundamental features of most of these techniques are electro
dermal activity (EDA) and heart rate values (HRV). We utilized an accelerometer
to measure body motions to solve this challenge. The proposed novel method
employs a test that measures a subject's electrocardiogram (ECG), galvanic skin
values (GSV), HRV values, and body movements in order to provide a low-cost and
time-saving solution for detecting stress lifestyle disease in modern times
using cyber physical systems. This study provides a new hybrid model for
lifestyle disease classification that decreases execution time while picking
the best collection of characteristics and increases classification accuracy.
The developed approach is capable of dealing with the class imbalance problem
by using WESAD (wearable stress and affect dataset) dataset. The new model uses
the Grid search (GS) method to select an optimized set of hyper parameters, and
it uses a combination of the Correlation coefficient based Recursive feature
elimination (CoC-RFE) method for optimal feature selection and gradient
boosting as an estimator to classify the dataset, which achieves high accuracy
and helps to provide smart, accurate, and high-quality healthcare systems. To
demonstrate the validity and utility of the proposed methodology, its
performance is compared to those of other well-established machine learning
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tiwari_S/0/1/0/all/0/1"&gt;Sadhana Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems. (arXiv:2011.07401v2 [cs.PF] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07401</id>
        <link href="http://arxiv.org/abs/2011.07401"/>
        <updated>2022-04-09T00:48:55.303Z</updated>
        <summary type="html"><![CDATA[With the rapid advance of information technology, network systems have become
increasingly complex and hence the underlying system dynamics are often unknown
or difficult to characterize. Finding a good network control policy is of
significant importance to achieve desirable network performance (e.g., high
throughput or low delay). In this work, we consider using model-based
reinforcement learning (RL) to learn the optimal control policy for queueing
networks so that the average job delay (or equivalently the average queue
backlog) is minimized. Traditional approaches in RL, however, cannot handle the
unbounded state spaces of the network control problem. To overcome this
difficulty, we propose a new algorithm, called Reinforcement Learning for
Queueing Networks (RL-QN), which applies model-based RL methods over a finite
subset of the state space, while applying a known stabilizing policy for the
rest of the states. We establish that the average queue backlog under RL-QN
with an appropriately constructed subset can be arbitrarily close to the
optimal result. We evaluate RL-QN in dynamic server allocation, routing and
switching problems. Simulation results show that RL-QN minimizes the average
queue backlog effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1"&gt;Qiaomin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modiano_E/0/1/0/all/0/1"&gt;Eytan Modiano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position-based Prompting for Health Outcome Generation. (arXiv:2204.03489v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03489</id>
        <link href="http://arxiv.org/abs/2204.03489"/>
        <updated>2022-04-09T00:48:55.302Z</updated>
        <summary type="html"><![CDATA[Probing Pre-trained Language Models (PLMs) using prompts has indirectly
implied that language models (LMs) can be treated as knowledge bases. To this
end, this phenomena has been effective especially when these LMs are fine-tuned
towards not just data of a specific domain, but also to the style or linguistic
pattern of the prompts themselves. We observe that, satisfying a particular
linguistic pattern in prompts is an unsustainable constraint that unnecessarily
lengthens the probing task, especially because, they are often manually
designed and the range of possible prompt template patterns can vary depending
on the prompting objective and domain. We therefore explore an idea of using a
position-attention mechanism to capture positional information of each word in
a prompt relative to the mask to be filled, hence avoiding the need to
re-construct prompts when the prompts linguistic pattern changes. Using our
approach, we demonstrate the ability of eliciting answers to rare prompt
templates (in a case study on health outcome generation) such as Postfix and
Mixed patterns whose missing information is respectively at the start and in
multiple random places of the prompt. More so, using various biomedical PLMs,
our approach consistently outperforms a baseline in which the default mask
language model (MLM) representation is used to predict masked tokens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abaho_M/0/1/0/all/0/1"&gt;M. Abaho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1"&gt;D. Bollegala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williamson_P/0/1/0/all/0/1"&gt;P. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodd_S/0/1/0/all/0/1"&gt;S. Dodd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Label Correlations for Second-Order Semantic Dependency Parsing with Mean-Field Inference. (arXiv:2204.03619v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03619</id>
        <link href="http://arxiv.org/abs/2204.03619"/>
        <updated>2022-04-09T00:48:55.302Z</updated>
        <summary type="html"><![CDATA[Second-order semantic parsing with end-to-end mean-field inference has been
shown good performance. In this work we aim to improve this method by modeling
label correlations between adjacent arcs. However, direct modeling leads to
memory explosion because second-order score tensors have sizes of $O(n^3L^2)$
($n$ is the sentence length and $L$ is the number of labels), which is not
affordable. To tackle this computational challenge, we leverage tensor
decomposition techniques, and interestingly, we show that the large
second-order score tensors have no need to be materialized during mean-field
inference, thereby reducing the computational complexity from cubic to
quadratic. We conduct experiments on SemEval 2015 Task 18 English datasets,
showing the effectiveness of modeling label correlations. Our code is publicly
available at https://github.com/sustcsonglin/mean-field-dep-parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Songlin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedCos: A Scene-adaptive Federated Optimization Enhancement for Performance Improvement. (arXiv:2204.03174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03174</id>
        <link href="http://arxiv.org/abs/2204.03174"/>
        <updated>2022-04-09T00:48:55.301Z</updated>
        <summary type="html"><![CDATA[As an emerging technology, federated learning (FL) involves training machine
learning models over distributed edge devices, which attracts sustained
attention and has been extensively studied. However, the heterogeneity of
client data severely degrades the performance of FL compared with that in
centralized training. It causes the locally trained models of clients to move
in different directions. On the one hand, it slows down or even stalls the
global updates, leading to inefficient communication. On the other hand, it
enlarges the distances between local models, resulting in an aggregated global
model with poor performance. Fortunately, these shortcomings can be mitigated
by reducing the angle between the directions that local models move in. Based
on this fact, we propose FedCos, which reduces the directional inconsistency of
local models by introducing a cosine-similarity penalty. It promotes the local
model iterations towards an auxiliary global direction. Moreover, our approach
is auto-adapt to various non-IID settings without an elaborate selection of
hyperparameters. The experimental results show that FedCos outperforms the
well-known baselines and can enhance them under a variety of FL scenes,
including varying degrees of data heterogeneity, different number of
participants, and cross-silo and cross-device settings. Besides, FedCos
improves communication efficiency by 2 to 5 times. With the help of FedCos,
multiple FL methods require significantly fewer communication rounds than
before to obtain a model with comparable performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tingting Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Siyao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jie Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning. (arXiv:2104.08676v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08676</id>
        <link href="http://arxiv.org/abs/2104.08676"/>
        <updated>2022-04-09T00:48:55.301Z</updated>
        <summary type="html"><![CDATA[We introduce distributed NLI, a new NLU task with a goal to predict the
distribution of human judgements for natural language inference. We show that
by applying additional distribution estimation methods, namely, Monte Carlo
(MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation,
models can capture human judgement distribution more effectively than the
softmax baseline. We show that MC Dropout is able to achieve decent performance
without any distribution annotations while Re-Calibration can give further
improvements with extra distribution annotations, suggesting the value of
multiple annotations for one example in modeling the distribution of human
judgements. Despite these improvements, the best results are still far below
the estimated human upper-bound, indicating that predicting the distribution of
human judgements is still an open, challenging problem with a large room for
improvements. We showcase the common errors for MC Dropout and Re-Calibration.
Finally, we give guidelines on the usage of these methods with different levels
of data availability and encourage future work on modeling the human opinion
distribution for language reasoning. Our code and data are publicly available
at https://github.com/easonnie/ChaosNLI]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yixin Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Model Criticism of Variational Auto-Encoders. (arXiv:2204.03030v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03030</id>
        <link href="http://arxiv.org/abs/2204.03030"/>
        <updated>2022-04-09T00:48:55.300Z</updated>
        <summary type="html"><![CDATA[We propose a framework for the statistical evaluation of variational
auto-encoders (VAEs) and test two instances of this framework in the context of
modelling images of handwritten digits and a corpus of English text. Our take
on evaluation is based on the idea of statistical model criticism, popular in
Bayesian data analysis, whereby a statistical model is evaluated in terms of
its ability to reproduce statistics of an unknown data generating process from
which we can obtain samples. A VAE learns not one, but two joint distributions
over a shared sample space, each exploiting a choice of factorisation that
makes sampling tractable in one of two directions (latent-to-data,
data-to-latent). We evaluate samples from these distributions, assessing their
(marginal) fit to the observed data and our choice of prior, and we also
evaluate samples through a pipeline that connects the two distributions
starting from a data sample, assessing whether together they exploit and reveal
latent factors of variation that are useful to a practitioner. We show that
this methodology offers possibilities for model selection qualitatively beyond
intrinsic evaluation metrics and at a finer granularity than commonly used
statistics can offer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barkhof_C/0/1/0/all/0/1"&gt;Claartje Barkhof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1"&gt;Wilker Aziz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Urban Mobility: using artificial intelligence and new technologies to connect supply and demand. (arXiv:2204.03570v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2204.03570</id>
        <link href="http://arxiv.org/abs/2204.03570"/>
        <updated>2022-04-09T00:48:55.300Z</updated>
        <summary type="html"><![CDATA[As the demand for mobility in our society seems to increase, the various
issues centered on urban mobility are among those that worry most city
inhabitants in this planet. For instance, how to go from A to B in an efficient
(but also less stressful) way? These questions and concerns have not changed
even during the covid-19 pandemic; on the contrary, as the current stand,
people who are avoiding public transportation are only contributing to an
increase in the vehicular traffic. The are of intelligent transportation
systems (ITS) aims at investigating how to employ information and communication
technologies to problems related to transportation. This may mean monitoring
and managing the infrastructure (e.g., traffic roads, traffic signals, etc.).
However, currently, ITS is also targeting the management of demand. In this
panorama, artificial intelligence plays an important role, especially with the
advances in machine learning that translates in the use of computational
vision, connected and autonomous vehicles, agent-based simulation, among
others. In the present work, a survey of several works developed by our group
are discussed in a holistic perspective, i.e., they cover not only the supply
side (as commonly found in ITS works), but also the demand side, and, in an
novel perspective, the integration of both.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bazzan_A/0/1/0/all/0/1"&gt;Ana L. C. Bazzan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Implicit Flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data. (arXiv:2204.03216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03216</id>
        <link href="http://arxiv.org/abs/2204.03216"/>
        <updated>2022-04-09T00:48:55.299Z</updated>
        <summary type="html"><![CDATA[High-dimensional spatio-temporal dynamics can often be encoded in a
low-dimensional subspace. Engineering applications for modeling,
characterization, design, and control of such large-scale systems often rely on
dimensionality reduction to make solutions computationally tractable in
real-time. Common existing paradigms for dimensionality reduction include
linear methods, such as the singular value decomposition (SVD), and nonlinear
methods, such as variants of convolutional autoencoders (CAE). However, these
encoding techniques lack the ability to efficiently represent the complexity
associated with spatio-temporal data, which often requires variable geometry,
non-uniform grid resolution, adaptive meshing, and/or parametric
dependencies.To resolve these practical engineering challenges, we propose a
general framework called Neural Implicit Flow (NIF) that enables a
mesh-agnostic, low-rank representation of large-scale, parametric,
spatial-temporal data. NIF consists of two modified multilayer perceptrons
(MLPs): (i) ShapeNet, which isolates and represents the spatial complexity, and
(ii) ParameterNet, which accounts for any other input complexity, including
parametric dependencies, time, and sensor measurements. We demonstrate the
utility of NIF for parametric surrogate modeling, enabling the interpretable
representation and compression of complex spatio-temporal dynamics, efficient
many-spatial-query tasks, and improved generalization performance for sparse
reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shaowu Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunton_S/0/1/0/all/0/1"&gt;Steven L. Brunton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1"&gt;J. Nathan Kutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results. (arXiv:2204.03475v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03475</id>
        <link href="http://arxiv.org/abs/2204.03475"/>
        <updated>2022-04-09T00:48:55.298Z</updated>
        <summary type="html"><![CDATA[ImageNet serves as the primary dataset for evaluating the quality of
computer-vision models. The common practice today is training each architecture
with a tailor-made scheme, designed and tuned by an expert. In this paper, we
present a unified scheme for training any backbone on ImageNet. The scheme,
named USI (Unified Scheme for ImageNet), is based on knowledge distillation and
modern tricks. It requires no adjustments or hyper-parameters tuning between
different models, and is efficient in terms of training times. We test USI on a
wide variety of architectures, including CNNs, Transformers, Mobile-oriented
and MLP-only. On all models tested, USI outperforms previous state-of-the-art
results. Hence, we are able to transform training on ImageNet from an
expert-oriented task to an automatic seamless routine. Since USI accepts any
backbone and trains it to top results, it also enables to perform methodical
comparisons, and identify the most efficient backbones along the speed-accuracy
Pareto curve. Implementation is available
at:https://github.com/Alibaba-MIIL/Solving_ImageNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawen_H/0/1/0/all/0/1"&gt;Hussam Lawen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing the Long-Term Behaviour of Deep Reinforcement Learning for Pushing and Grasping. (arXiv:2204.03487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03487</id>
        <link href="http://arxiv.org/abs/2204.03487"/>
        <updated>2022-04-09T00:48:55.298Z</updated>
        <summary type="html"><![CDATA[We investigate the "Visual Pushing for Grasping" (VPG) system by Zeng et al.
and the "Hourglass" system by Ewerton et al., an evolution of the former. The
focus of our work is the investigation of the capabilities of both systems to
learn long-term rewards and policies. Zeng et al. original task only needs a
limited amount of foresight. Ewerton et al. attain their best performance using
an agent which only takes the most immediate action under consideration. We are
interested in the ability of their models and training algorithms to accurately
predict long-term Q-Values. To evaluate this ability, we design a new bin
sorting task and reward function. Our task requires agents to accurately
estimate future rewards and therefore use high discount factors in their
Q-Value calculation. We investigate the behaviour of an adaptation of the VPG
training algorithm on our task. We show that this adaptation can not accurately
predict the required long-term action sequences. In addition to the limitations
identified by Ewerton et al., it suffers from the known Deep Q-Learning problem
of overestimated Q-Values. In an effort to solve our task, we turn to the
Hourglass models and combine them with the Double Q-Learning approach. We show
that this approach enables the models to accurately predict long-term action
sequences when trained with large discount factors. Our results show that the
Double Q-Learning technique is essential for training with very high discount
factors, as the models Q-Value predictions diverge otherwise. We also
experiment with different approaches for discount factor scheduling, loss
calculation and exploration procedures. Our results show that the latter
factors do not visibly influence the model's performance for our task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chau_R/0/1/0/all/0/1"&gt;Rodrigo Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03574</id>
        <link href="http://arxiv.org/abs/2204.03574"/>
        <updated>2022-04-09T00:48:55.298Z</updated>
        <summary type="html"><![CDATA[We introduce compositional soft prompting (CSP), a parameter-efficient
learning technique to improve the zero-shot compositionality of large-scale
pretrained vision-language models (VLMs) without the overhead of fine-tuning
the entire model. VLMs can represent arbitrary classes as natural language
prompts in their flexible text encoders but they underperform state-of-the-art
methods on compositional zero-shot benchmark tasks. To improve VLMs, we propose
a novel form of soft prompting. We treat the attributes and objects that are
composed to define classes as learnable tokens of vocabulary and tune them on
multiple prompt compositions. During inference, we recompose the learned
attribute-object vocabulary in new combinations and show that CSP outperforms
the original VLM on benchmark datasets by an average of 14.7 percentage points
of accuracy. CSP also achieves new state-of-the-art accuracies on two out of
three benchmark datasets, while only fine-tuning a small number of parameters.
Further, we show that CSP improves generalization to higher-order
attribute-attribute-object compositions and combinations of pretrained
attributes and fine-tuned objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1"&gt;Nihal V. Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Peilin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1"&gt;Stephen H. Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk-based regulation for all: The need and a method for a wide adoption solution for data-driven inspection targeting. (arXiv:2204.03583v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03583</id>
        <link href="http://arxiv.org/abs/2204.03583"/>
        <updated>2022-04-09T00:48:55.298Z</updated>
        <summary type="html"><![CDATA[Access to data and data processing, including the use of machine learning
techniques, has become significantly easier and cheaper in recent years.
Nevertheless, solutions that can be widely adopted by regulators for market
monitoring and inspection targeting in a data-driven way have not been
frequently discussed by the scientific community. This article discusses the
need and the difficulties for the development of such solutions, presents an
effective method to address regulation planning, and illustrates its use to
account for the most important and common subject for the majority of
regulators: the consumer. This article hopes to contribute to increase the
awareness of the regulatory community to the need for data processing methods
that are objective, impartial, transparent, explainable, simple to implement
and with low computational cost, aiming to the implementation of risk-based
regulation in the world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ribas_C/0/1/0/all/0/1"&gt;Celso H. H. Ribas&lt;/a&gt; (1,2), &lt;a href="http://arxiv.org/find/cs/1/au:+Bermudez_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; C. M. Bermudez&lt;/a&gt; (1) ((1) Digital Signal Processing Research Laboratory, Federal University of Santa Catarina, Santa Catarina, Brazil, (2) Superintendence of Inspection, National Telecommunications Agency, Amazonas, Brazil)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTuit: Understanding Spanish language in Twitter through a native transformer. (arXiv:2204.03465v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03465</id>
        <link href="http://arxiv.org/abs/2204.03465"/>
        <updated>2022-04-09T00:48:55.297Z</updated>
        <summary type="html"><![CDATA[The appearance of complex attention-based language models such as BERT,
Roberta or GPT-3 has allowed to address highly complex tasks in a plethora of
scenarios. However, when applied to specific domains, these models encounter
considerable difficulties. This is the case of Social Networks such as Twitter,
an ever-changing stream of information written with informal and complex
language, where each message requires careful evaluation to be understood even
by humans given the important role that context plays. Addressing tasks in this
domain through Natural Language Processing involves severe challenges. When
powerful state-of-the-art multilingual language models are applied to this
scenario, language specific nuances use to get lost in translation. To face
these challenges we present \textbf{BERTuit}, the larger transformer proposed
so far for Spanish language, pre-trained on a massive dataset of 230M Spanish
tweets using RoBERTa optimization. Our motivation is to provide a powerful
resource to better understand Spanish Twitter and to be used on applications
focused on this social network, with special emphasis on solutions devoted to
tackle the spreading of misinformation in this platform. BERTuit is evaluated
on several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very
competitive multilingual transformers. The utility of our approach is shown
with applications, in this case: a zero-shot methodology to visualize groups of
hoaxes and profiling authors spreading disinformation.

Misinformation spreads wildly on platforms such as Twitter in languages other
than English, meaning performance of transformers may suffer when transferred
outside English speaking communities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1"&gt;Javier Huertas-Tato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1"&gt;Alejandro Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1"&gt;David Camacho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Solve Travelling Salesman Problem with Hardness-adaptive Curriculum. (arXiv:2204.03236v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03236</id>
        <link href="http://arxiv.org/abs/2204.03236"/>
        <updated>2022-04-09T00:48:55.295Z</updated>
        <summary type="html"><![CDATA[Various neural network models have been proposed to tackle combinatorial
optimization problems such as the travelling salesman problem (TSP). Existing
learning-based TSP methods adopt a simple setting that the training and testing
data are independent and identically distributed. However, the existing
literature fails to solve TSP instances when training and testing data have
different distributions. Concretely, we find that different training and
testing distribution will result in more difficult TSP instances, i.e., the
solution obtained by the model has a large gap from the optimal solution. To
tackle this problem, in this work, we study learning-based TSP methods when
training and testing data have different distributions using adaptive-hardness,
i.e., how difficult a TSP instance can be for a solver. This problem is
challenging because it is non-trivial to (1) define hardness measurement
quantitatively; (2) efficiently and continuously generate sufficiently hard TSP
instances upon model training; (3) fully utilize instances with different
levels of hardness to learn a more powerful TSP solver. To solve these
challenges, we first propose a principled hardness measurement to quantify the
hardness of TSP instances. Then, we propose a hardness-adaptive generator to
generate instances with different hardness. We further propose a curriculum
learner fully utilizing these instances to train the TSP solver. Experiments
show that our hardness-adaptive generator can generate instances ten times
harder than the existing methods, and our proposed method achieves significant
improvement over state-of-the-art models in terms of the optimality gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zeyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning. (arXiv:2204.03230v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03230</id>
        <link href="http://arxiv.org/abs/2204.03230"/>
        <updated>2022-04-09T00:48:55.291Z</updated>
        <summary type="html"><![CDATA[We investigate and leverage a connection between Differential Privacy (DP)
and the recently proposed notion of Distributional Generalization (DG).
Applying this connection, we introduce new conceptual tools for designing
deep-learning methods that bypass "pathologies" of standard stochastic gradient
descent (SGD). First, we prove that differentially private methods satisfy a
"What You See Is What You Get (WYSIWYG)" generalization guarantee: whatever a
model does on its train data is almost exactly what it will do at test time.
This guarantee is formally captured by distributional generalization. WYSIWYG
enables principled algorithm design in deep learning by reducing
$\textit{generalization}$ concerns to $\textit{optimization}$ ones: in order to
mitigate unwanted behavior at test time, it is provably sufficient to mitigate
this behavior on the train data. This is notably false for standard (non-DP)
methods, hence this observation has applications even when privacy is not
required. For example, importance sampling is known to fail for standard SGD,
but we show that it has exactly the intended effect for DP-trained models.
Thus, with DP-SGD, unlike with SGD, we can influence test-time behavior by
making principled train-time interventions. We use these insights to construct
simple algorithms which match or outperform SOTA in several distributional
robustness applications, and to significantly improve the privacy vs. disparate
impact trade-off of DP-SGD. Finally, we also improve on known theoretical
bounds relating differential privacy, stability, and distributional
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1"&gt;Bogdan Kulynych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yao-Yuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blasiok_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw B&amp;#x142;asiok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1"&gt;Preetum Nakkiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalised Latent Assimilation in Heterogeneous Reduced Spaces with Machine Learning Surrogate Models. (arXiv:2204.03497v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03497</id>
        <link href="http://arxiv.org/abs/2204.03497"/>
        <updated>2022-04-09T00:48:55.291Z</updated>
        <summary type="html"><![CDATA[Reduced-order modelling and low-dimensional surrogate models generated using
machine learning algorithms have been widely applied in high-dimensional
dynamical systems to improve the algorithmic efficiency. In this paper, we
develop a system which combines reduced-order surrogate models with a novel
data assimilation (DA) technique used to incorporate real-time observations
from different physical spaces. We make use of local smooth surrogate functions
which link the space of encoded system variables and the one of current
observations to perform variational DA with a low computational cost. The new
system, named Generalised Latent Assimilation can benefit both the efficiency
provided by the reduced-order modelling and the accuracy of data assimilation.
A theoretical analysis of the difference between surrogate and original
assimilation cost function is also provided in this paper where an upper bound,
depending on the size of the local training set, is given. The new approach is
tested on a high-dimensional CFD application of a two-phase liquid flow with
non-linear observation operators that current Latent Assimilation methods can
not handle. Numerical results demonstrate that the proposed assimilation
approach can significantly improve the reconstruction and prediction accuracy
of the deep learning surrogate model which is nearly 1000 times faster than the
CFD simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sibo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianhua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasiou_C/0/1/0/all/0/1"&gt;Charitos Anastasiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angeli_P/0/1/0/all/0/1"&gt;Panagiota Angeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matar_O/0/1/0/all/0/1"&gt;Omar K. Matar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yi-Ke Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1"&gt;Christopher C. Pain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1"&gt;Rossella Arcucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explicit Feature Interaction-aware Graph Neural Networks. (arXiv:2204.03225v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03225</id>
        <link href="http://arxiv.org/abs/2204.03225"/>
        <updated>2022-04-09T00:48:55.282Z</updated>
        <summary type="html"><![CDATA[Graph neural networks are powerful methods to handle graph-structured data.
However, existing graph neural networks only learn higher-order feature
interactions implicitly. Thus, they cannot capture information that occurred in
low-order feature interactions. To overcome this problem, we propose Explicit
Feature Interaction-aware Graph Neural Network (EFI-GNN), which explicitly
learns arbitrary-order feature interactions. EFI-GNN can jointly learn with any
other graph neural network. We demonstrate that the joint learning method
always enhances performance on the various node classification tasks.
Furthermore, since EFI-GNN is inherently a linear model, we can interpret the
prediction result of EFI-GNN. With the computation rule, we can obtain an
any-order feature's effect on the decision. By that, we visualize the effects
of the first-order and second-order features as a form of a heatmap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minkyu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hyun-Soo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinho Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Half-sibling regression meets exoplanet imaging: PSF modeling and subtraction using a flexible, domain knowledge-driven, causal framework. (arXiv:2204.03439v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2204.03439</id>
        <link href="http://arxiv.org/abs/2204.03439"/>
        <updated>2022-04-09T00:48:55.280Z</updated>
        <summary type="html"><![CDATA[High-contrast imaging of exoplanets hinges on powerful post-processing
methods to denoise the data and separate the signal of a companion from its
host star, which is typically orders of magnitude brighter. Existing
post-processing algorithms do not use all prior domain knowledge that is
available about the problem. We propose a new method that builds on our
understanding of the systematic noise and the causal structure of the
data-generating process. Our algorithm is based on a modified version of
half-sibling regression (HSR), a flexible denoising framework that combines
ideas from the fields of machine learning and causality. We adapt the method to
address the specific requirements of high-contrast exoplanet imaging data
obtained in pupil tracking mode. The key idea is to estimate the systematic
noise in a pixel by regressing the time series of this pixel onto a set of
causally independent, signal-free predictor pixels. We use regularized linear
models in this work; however, other (non-linear) models are also possible. In a
second step, we demonstrate how the HSR framework allows us to incorporate
observing conditions such as wind speed or air temperature as additional
predictors. When we apply our method to four data sets from the VLT/NACO
instrument, our algorithm provides a better false-positive fraction than
PCA-based PSF subtraction, a popular baseline method in the field.
Additionally, we find that the HSR-based method provides direct and accurate
estimates for the contrast of the exoplanets without the need to insert
artificial companions for calibration in the data sets. Finally, we present
first evidence that using the observing conditions as additional predictors can
improve the results. Our HSR-based method provides an alternative, flexible and
promising approach to the challenge of modeling and subtracting the stellar PSF
and systematic noise in exoplanet imaging data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gebhard_T/0/1/0/all/0/1"&gt;Timothy D. Gebhard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bonse_M/0/1/0/all/0/1"&gt;Markus J. Bonse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Quanz_S/0/1/0/all/0/1"&gt;Sascha P. Quanz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[mulEEG: A Multi-View Representation Learning on EEG Signals. (arXiv:2204.03272v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03272</id>
        <link href="http://arxiv.org/abs/2204.03272"/>
        <updated>2022-04-09T00:48:55.279Z</updated>
        <summary type="html"><![CDATA[Modeling effective representations using multiple views that positively
influence each other is challenging, and the existing methods perform poorly on
Electroencephalogram (EEG) signals for sleep-staging tasks. In this paper, we
propose a novel multi-view self-supervised method (mulEEG) for unsupervised EEG
representation learning. Our method attempts to effectively utilize the
complementary information available in multiple views to learn better
representations. We introduce diverse loss that further encourages
complementary information across multiple views. Our method with no access to
labels beats the supervised training while outperforming multi-view baseline
methods on transfer learning experiments carried out on sleep-staging tasks. We
posit that our method was able to learn better representations by using
complementary multi-views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vamsi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_L/0/1/0/all/0/1"&gt;Likith Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Shivam Kumar Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dadi_K/0/1/0/all/0/1"&gt;Kamalakar Dadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yarra_C/0/1/0/all/0/1"&gt;Chiranjeevi Yarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raju_B/0/1/0/all/0/1"&gt;Bapi S. Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajendran_S/0/1/0/all/0/1"&gt;Srijithesh Rajendran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceive, Represent, Generate: Translating Multimodal Information to Robotic Motion Trajectories. (arXiv:2204.03051v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2204.03051</id>
        <link href="http://arxiv.org/abs/2204.03051"/>
        <updated>2022-04-09T00:48:55.278Z</updated>
        <summary type="html"><![CDATA[We present Perceive-Represent-Generate (PRG), a novel three-stage framework
that maps perceptual information of different modalities (e.g., visual or
sound), corresponding to a sequence of instructions, to an adequate sequence of
movements to be executed by a robot. In the first stage, we perceive and
pre-process the given inputs, isolating individual commands from the complete
instruction provided by a human user. In the second stage we encode the
individual commands into a multimodal latent space, employing a deep generative
model. Finally, in the third stage we convert the multimodal latent values into
individual trajectories and combine them into a single dynamic movement
primitive, allowing its execution in a robotic platform. We evaluate our
pipeline in the context of a novel robotic handwriting task, where the robot
receives as input a word through different perceptual modalities (e.g., image,
sound), and generates the corresponding motion trajectory to write it, creating
coherent and readable handwritten words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vital_F/0/1/0/all/0/1"&gt;F&amp;#xe1;bio Vital&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasco_M/0/1/0/all/0/1"&gt;Miguel Vasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardinha_A/0/1/0/all/0/1"&gt;Alberto Sardinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melo_F/0/1/0/all/0/1"&gt;Francisco Melo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects. (arXiv:2204.03139v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2204.03139</id>
        <link href="http://arxiv.org/abs/2204.03139"/>
        <updated>2022-04-09T00:48:55.278Z</updated>
        <summary type="html"><![CDATA[Research in manipulation of deformable objects is typically conducted on a
limited range of scenarios, because handling each scenario on hardware takes
significant effort. Realistic simulators with support for various types of
deformations and interactions have the potential to speed up experimentation
with novel tasks and algorithms. However, for highly deformable objects it is
challenging to align the output of a simulator with the behavior of real
objects. Manual tuning is not intuitive, hence automated methods are needed. We
view this alignment problem as a joint perception-inference challenge and
demonstrate how to use recent neural network architectures to successfully
perform simulation parameter inference from real point clouds. We analyze the
performance of various architectures, comparing their data and training
requirements. Furthermore, we propose to leverage differentiable point cloud
sampling and differentiable simulation to significantly reduce the time to
achieve the alignment. We employ an efficient way to propagate gradients from
point clouds to simulated meshes and further through to the physical simulation
parameters, such as mass and stiffness. Experiments with highly deformable
objects show that our method can achieve comparable or better alignment with
real object behavior, while reducing the time needed to achieve this by more
than an order of magnitude. Videos and supplementary material are available at
https://tinyurl.com/diffcloud.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1"&gt;Priya Sundaresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antonova_R/0/1/0/all/0/1"&gt;Rika Antonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1"&gt;Jeannette Bohg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-Enabled IoT Security: Open Issues and Challenges Under Advanced Persistent Threats. (arXiv:2204.03433v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2204.03433</id>
        <link href="http://arxiv.org/abs/2204.03433"/>
        <updated>2022-04-09T00:48:55.278Z</updated>
        <summary type="html"><![CDATA[Despite its technological benefits, Internet of Things (IoT) has cyber
weaknesses due to the vulnerabilities in the wireless medium. Machine learning
(ML)-based methods are widely used against cyber threats in IoT networks with
promising performance. Advanced persistent threat (APT) is prominent for
cybercriminals to compromise networks, and it is crucial to long-term and
harmful characteristics. However, it is difficult to apply ML-based approaches
to identify APT attacks to obtain a promising detection performance due to an
extremely small percentage among normal traffic. There are limited surveys to
fully investigate APT attacks in IoT networks due to the lack of public
datasets with all types of APT attacks. It is worth to bridge the
state-of-the-art in network attack detection with APT attack detection in a
comprehensive review article. This survey article reviews the security
challenges in IoT networks and presents the well-known attacks, APT attacks,
and threat models in IoT systems. Meanwhile, signature-based, anomaly-based,
and hybrid intrusion detection systems are summarized for IoT networks. The
article highlights statistical insights regarding frequently applied ML-based
methods against network intrusion alongside the number of attacks types
detected. Finally, open issues and challenges for common network intrusion and
APT attacks are presented for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinxin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1"&gt;Murat Simsek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1"&gt;Burak Kantarci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouftah_H/0/1/0/all/0/1"&gt;Hussein T. Mouftah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Djukic_P/0/1/0/all/0/1"&gt;Petar Djukic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Forecasting of Time-Series with Heterogeneous Channels. (arXiv:2204.03456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03456</id>
        <link href="http://arxiv.org/abs/2204.03456"/>
        <updated>2022-04-09T00:48:55.276Z</updated>
        <summary type="html"><![CDATA[Learning complex time series forecasting models usually requires a large
amount of data, as each model is trained from scratch for each task/data set.
Leveraging learning experience with similar datasets is a well-established
technique for classification problems called few-shot classification. However,
existing approaches cannot be applied to time-series forecasting because i)
multivariate time-series datasets have different channels and ii) forecasting
is principally different from classification. In this paper we formalize the
problem of few-shot forecasting of time-series with heterogeneous channels for
the first time. Extending recent work on heterogeneous attributes in vector
data, we develop a model composed of permutation-invariant deep set-blocks
which incorporate a temporal embedding. We assemble the first meta-dataset of
40 multivariate time-series datasets and show through experiments that our
model provides a good generalization, outperforming baselines carried over from
simpler scenarios that either fail to learn across tasks or miss temporal
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brinkmeyer_L/0/1/0/all/0/1"&gt;Lukas Brinkmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drumond_R/0/1/0/all/0/1"&gt;Rafael Rego Drumond&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burchert_J/0/1/0/all/0/1"&gt;Johannes Burchert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift. (arXiv:2204.03342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03342</id>
        <link href="http://arxiv.org/abs/2204.03342"/>
        <updated>2022-04-09T00:48:55.275Z</updated>
        <summary type="html"><![CDATA[The performance of a machine learning model degrades when it is applied to
data from a similar but different domain than the data it has initially been
trained on. To mitigate this domain shift problem, domain adaptation (DA)
techniques search for an optimal transformation that converts the (current)
input data from a source domain to a target domain to learn a domain-invariant
representations that reduces domain discrepancy.

This paper proposes a novel supervised domain adaptation based on two steps.
First, we search for an optimal class-dependent transformation from the source
to the target domain from a few samples. We consider optimal transport methods
such as the earth mover distance with Laplacian regularization, Sinkhorn
transport and correlation alignment. Second, we use embedding similarity
techniques to select the corresponding transformation at inference. We use
correlation metrics and maximum mean discrepancy with higher-order moment
matching techniques. We conduct an extensive evaluation on time-series datasets
with domain shift including simulated and various online handwriting datasets
to demonstrate the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1"&gt;Felix Ott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1"&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heublein_L/0/1/0/all/0/1"&gt;Lucas Heublein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1"&gt;Christopher Mutschler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Efficient Adaptive Machine Learning on IoT End-Nodes With Class-Dependent Confidence. (arXiv:2204.03431v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03431</id>
        <link href="http://arxiv.org/abs/2204.03431"/>
        <updated>2022-04-09T00:48:55.274Z</updated>
        <summary type="html"><![CDATA[Energy-efficient machine learning models that can run directly on edge
devices are of great interest in IoT applications, as they can reduce network
pressure and response latency, and improve privacy. An effective way to obtain
energy-efficiency with small accuracy drops is to sequentially execute a set of
increasingly complex models, early-stopping the procedure for "easy" inputs
that can be confidently classified by the smallest models. As a stopping
criterion, current methods employ a single threshold on the output
probabilities produced by each model. In this work, we show that such a
criterion is sub-optimal for datasets that include classes of different
complexity, and we demonstrate a more general approach based on per-classes
thresholds. With experiments on a low-power end-node, we show that our method
can significantly reduce the energy consumption compared to the
single-threshold approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daghero_F/0/1/0/all/0/1"&gt;Francesco Daghero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1"&gt;Alessio Burrello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1"&gt;Daniele Jahier Pagliari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1"&gt;Enrico Macii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poncino_M/0/1/0/all/0/1"&gt;Massimo Poncino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03276</id>
        <link href="http://arxiv.org/abs/2204.03276"/>
        <updated>2022-04-09T00:48:55.273Z</updated>
        <summary type="html"><![CDATA[Currently, pre-trained models can be considered the default choice for a wide
range of NLP tasks. Despite their SoTA results, there is practical evidence
that these models may require a different number of computing layers for
different input sequences, since evaluating all layers leads to overconfidence
on wrong predictions (namely overthinking). This problem can potentially be
solved by implementing adaptive computation time approaches, which were first
designed to improve inference speed. Recently proposed PonderNet may be a
promising solution for performing an early exit by treating the exit layers
index as a latent variable. However, the originally proposed exit criterion,
relying on sampling from trained posterior distribution on the probability of
exiting from i-th layer, introduces major variance in model outputs,
significantly reducing the resulting models performance. In this paper, we
propose Ponder ALBERT (PALBERT): an improvement to PonderNet with a novel
deterministic Q-exit criterion and a revisited model architecture. We compared
PALBERT with recent methods for performing an early exit. We observed that the
proposed changes can be considered significant improvements on the original
PonderNet architecture and outperform PABEE on a wide range of GLUE tasks. In
addition, we also performed an in-depth ablation study of the proposed
architecture to further understand Lambda layers and their performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1"&gt;Nikita Balagansky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1"&gt;Daniil Gavrilov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Inference: A Library for Efficient Online Inference with Deep Neural Networks in PyTorch. (arXiv:2204.03418v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03418</id>
        <link href="http://arxiv.org/abs/2204.03418"/>
        <updated>2022-04-09T00:48:55.272Z</updated>
        <summary type="html"><![CDATA[We present Continual Inference, a Python library for implementing Continual
Inference Networks (CINs) in PyTorch, a class of Neural Networks designed
specifically for efficient inference in both online and batch processing
scenarios. We offer a comprehensive introduction and guide to CINs and their
implementation in practice, and provide best-practices and code examples for
composing complex modules for modern Deep Learning. Continual Inference is
readily downloadable via the Python Package Index and at
\url{www.github.com/lukashedegaard/continual-inference}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hedegaard_L/0/1/0/all/0/1"&gt;Lukas Hedegaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Decision Tree as Local Interpretable Model in Autoencoder-based LIME. (arXiv:2204.03321v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03321</id>
        <link href="http://arxiv.org/abs/2204.03321"/>
        <updated>2022-04-09T00:48:55.271Z</updated>
        <summary type="html"><![CDATA[Nowadays, deep neural networks are being used in many domains because of
their high accuracy results. However, they are considered as "black box", means
that they are not explainable for humans. On the other hand, in some tasks such
as medical, economic, and self-driving cars, users want the model to be
interpretable to decide if they can trust these results or not. In this work,
we present a modified version of an autoencoder-based approach for local
interpretability called ALIME. The ALIME itself is inspired by a famous method
called Local Interpretable Model-agnostic Explanations (LIME). LIME generates a
single instance level explanation by generating new data around the instance
and training a local linear interpretable model. ALIME uses an autoencoder to
weigh the new data around the sample. Nevertheless, the ALIME uses a linear
model as the interpretable model to be trained locally, just like the LIME.
This work proposes a new approach, which uses a decision tree instead of the
linear model, as the interpretable model. We evaluate the proposed model in
case of stability, local fidelity, and interpretability on different datasets.
Compared to ALIME, the experiments show significant results on stability and
local fidelity and improved results on interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranjbar_N/0/1/0/all/0/1"&gt;Niloofar Ranjbar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Safabakhsh_R/0/1/0/all/0/1"&gt;Reza Safabakhsh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing finetuned models for better pretraining. (arXiv:2204.03044v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03044</id>
        <link href="http://arxiv.org/abs/2204.03044"/>
        <updated>2022-04-09T00:48:55.270Z</updated>
        <summary type="html"><![CDATA[Pretrained models are the standard starting point for training. This approach
consistently outperforms the use of a random initialization. However,
pretraining is a costly endeavour that few can undertake.

In this paper, we create better base models at hardly any cost, by fusing
multiple existing fine tuned models into one. Specifically, we fuse by
averaging the weights of these models. We show that the fused model results
surpass the pretrained model ones. We also show that fusing is often better
than intertraining.

We find that fusing is less dependent on the target task. Furthermore, weight
decay nullifies intertraining effects but not those of fusing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1"&gt;Leshem Choshen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1"&gt;Elad Venezian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1"&gt;Noam Slonim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1"&gt;Yoav Katz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators. (arXiv:2204.03243v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03243</id>
        <link href="http://arxiv.org/abs/2204.03243"/>
        <updated>2022-04-09T00:48:55.270Z</updated>
        <summary type="html"><![CDATA[We present a new framework AMOS that pretrains text encoders with an
Adversarial learning curriculum via a Mixture Of Signals from multiple
auxiliary generators. Following ELECTRA-style pretraining, the main encoder is
trained as a discriminator to detect replaced tokens generated by auxiliary
masked language models (MLMs). Different from ELECTRA which trains one MLM as
the generator, we jointly train multiple MLMs of different sizes to provide
training signals at various levels of difficulty. To push the discriminator to
learn better with challenging replaced tokens, we learn mixture weights over
the auxiliary MLMs' outputs to maximize the discriminator loss by
backpropagating the gradient from the discriminator via Gumbel-Softmax. For
better pretraining efficiency, we propose a way to assemble multiple MLMs into
one unified auxiliary model. AMOS outperforms ELECTRA and recent
state-of-the-art pretrained models by about 1 point on the GLUE benchmark for
BERT base-sized models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1"&gt;Payal Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1"&gt;Saurabh Tiwary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Joint Learning Approach for Semi-supervised Neural Topic Modeling. (arXiv:2204.03208v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.03208</id>
        <link href="http://arxiv.org/abs/2204.03208"/>
        <updated>2022-04-09T00:48:55.269Z</updated>
        <summary type="html"><![CDATA[Topic models are some of the most popular ways to represent textual data in
an interpret-able manner. Recently, advances in deep generative models,
specifically auto-encoding variational Bayes (AEVB), have led to the
introduction of unsupervised neural topic models, which leverage deep
generative models as opposed to traditional statistics-based topic models. We
extend upon these neural topic models by introducing the Label-Indexed Neural
Topic Model (LI-NTM), which is, to the extent of our knowledge, the first
effective upstream semi-supervised neural topic model. We find that LI-NTM
outperforms existing neural topic models in document reconstruction benchmarks,
with the most notable results in low labeled data regimes and for data-sets
with informative labels; furthermore, our jointly learned classifier
outperforms baseline classifiers in ablation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1"&gt;Jeffrey Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1"&gt;Rajat Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tumma_N/0/1/0/all/0/1"&gt;Neehal Tumma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abhishek Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Statistical Min-Max Learning in the Presence of Byzantine Agents. (arXiv:2204.03187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03187</id>
        <link href="http://arxiv.org/abs/2204.03187"/>
        <updated>2022-04-09T00:48:55.268Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed a growing interest in the topic of min-max
optimization, owing to its relevance in the context of generative adversarial
networks (GANs), robust control and optimization, and reinforcement learning.
Motivated by this line of work, we consider a multi-agent min-max learning
problem, and focus on the emerging challenge of contending with worst-case
Byzantine adversarial agents in such a setup. By drawing on recent results from
robust statistics, we design a robust distributed variant of the extra-gradient
algorithm - a popular algorithmic approach for min-max optimization. Our main
contribution is to provide a crisp analysis of the proposed robust
extra-gradient algorithm for smooth convex-concave and smooth strongly
convex-strongly concave functions. Specifically, we establish statistical rates
of convergence to approximate saddle points. Our rates are near-optimal, and
reveal both the effect of adversarial corruption and the benefit of
collaboration among the non-faulty agents. Notably, this is the first paper to
provide formal theoretical guarantees for large-scale distributed min-max
learning in the presence of adversarial agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adibi_A/0/1/0/all/0/1"&gt;Arman Adibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1"&gt;Aritra Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1"&gt;Hamed Hassani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-Based Language Models for Software Vulnerability Detection: Performance, Model's Security and Platforms. (arXiv:2204.03214v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2204.03214</id>
        <link href="http://arxiv.org/abs/2204.03214"/>
        <updated>2022-04-09T00:48:55.266Z</updated>
        <summary type="html"><![CDATA[The large transformer-based language models demonstrate excellent performance
in natural language processing. By considering the closeness of natural
languages to the high-level programming language such as C/C++, this work
studies how good are the large transformer-based language models detecting
software vulnerabilities. Our results demonstrate the well performance of these
models on software vulnerability detection. The answer enables extending
transformer-based language models to vulnerability detection and leveraging
superior performance beyond the natural language processing domain. Besides, we
perform the model's security check using Microsoft's Counterfit, a command-line
tool to assess the model's security. Our results find that these models are
vulnerable to adversarial examples. In this regard, we present a simple
countermeasure and its result. Experimenting with large models is always a
challenge due to the requirement of computing resources and platforms/libraries
& dependencies. Based on the experiences and difficulties we faced during this
work, we present our recommendation while choosing the platforms to run these
large models. Moreover, the popular platforms are surveyed thoroughly in this
paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1"&gt;Chandra Thapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1"&gt;Seung Ick Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1"&gt;Muhammad Ejaz Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camtepe_S/0/1/0/all/0/1"&gt;Seyit Camtepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pieprzyk_J/0/1/0/all/0/1"&gt;Josef Pieprzyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1"&gt;Surya Nepal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes. (arXiv:2204.03376v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03376</id>
        <link href="http://arxiv.org/abs/2204.03376"/>
        <updated>2022-04-09T00:48:55.265Z</updated>
        <summary type="html"><![CDATA[Hybrid closed loop systems represent the future of care for people with type
1 diabetes (T1D). These devices usually utilise simple control algorithms to
select the optimal insulin dose for maintaining blood glucose levels within a
healthy range. Online reinforcement learning (RL) has been utilised as a method
for further enhancing glucose control in these devices. Previous approaches
have been shown to reduce patient risk and improve time spent in the target
range when compared to classical control algorithms, but are prone to
instability in the learning process, often resulting in the selection of unsafe
actions. This work presents an evaluation of offline RL as a means for
developing clinically effective dosing policies without the need for patient
interaction. This paper examines the utility of BCQ, CQL and TD3-BC in managing
the blood glucose of nine virtual patients within the UVA/Padova glucose
dynamics simulator. When trained on less than a tenth of the data required by
online RL approaches, this work shows that offline RL can significantly
increase time in the healthy blood glucose range when compared to the strongest
state-of-art baseline. This is achieved without any associated increase in low
blood glucose events. Offline RL is also shown to be able to correct for common
and challenging scenarios such as incorrect bolus dosing, irregular meal
timings and sub-optimal training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emerson_H/0/1/0/all/0/1"&gt;Harry Emerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guy_M/0/1/0/all/0/1"&gt;Matt Guy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1"&gt;Ryan McConville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks Designed for Different Graph Types: A Survey. (arXiv:2204.03080v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03080</id>
        <link href="http://arxiv.org/abs/2204.03080"/>
        <updated>2022-04-09T00:48:55.264Z</updated>
        <summary type="html"><![CDATA[Graphs are ubiquitous in nature and can therefore serve as models for many
practical but also theoretical problems. Based on this, the young research
field of Graph Neural Networks (GNNs) has emerged. Despite the youth of the
field and the speed in which new models are developed, many good surveys have
been published in the last years. Nevertheless, an overview on which graph
types can be modeled by GNNs is missing. In this survey, we give a detailed
overview of already existing GNNs and, unlike previous surveys, categorize them
according to their ability to handle different graph types. We consider GNNs
operating on static as well as on dynamic graphs of different structural
constitutions, with or without node or edge attributes. Moreover in the dynamic
case, we separate the models in discrete-time and continuous-time dynamic
graphs based on their architecture. According to our findings, there are still
graph types, that are not covered by existing GNN models. Specifically, models
concerning heterogeneity in attributes are missing and the deletion of nodes
and edges is only covered rarely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_J/0/1/0/all/0/1"&gt;Josephine M. Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moallemy_Oureh_A/0/1/0/all/0/1"&gt;Alice Moallemy-Oureh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beddar_Wiesing_S/0/1/0/all/0/1"&gt;Silvia Beddar-Wiesing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holzhuter_C/0/1/0/all/0/1"&gt;Clara Holzh&amp;#xfc;ter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jacobian Norm for Unsupervised Source-Free Domain Adaptation. (arXiv:2204.03467v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03467</id>
        <link href="http://arxiv.org/abs/2204.03467"/>
        <updated>2022-04-09T00:48:55.263Z</updated>
        <summary type="html"><![CDATA[Unsupervised Source (data) Free domain adaptation (USFDA) aims to transfer
knowledge from a well-trained source model to a related but unlabeled target
domain. In such a scenario, all conventional adaptation methods that require
source data fail. To combat this challenge, existing USFDAs turn to transfer
knowledge by aligning the target feature to the latent distribution hidden in
the source model. However, such information is naturally limited. Thus, the
alignment in such a scenario is not only difficult but also insufficient, which
degrades the target generalization performance. To relieve this dilemma in
current USFDAs, we are motivated to explore a new perspective to boost their
performance. For this purpose and gaining necessary insight, we look back upon
the origin of the domain adaptation and first theoretically derive a new-brand
target generalization error bound based on the model smoothness. Then,
following the theoretical insight, a general and model-smoothness-guided
Jacobian norm (JN) regularizer is designed and imposed on the target domain to
mitigate this dilemma. Extensive experiments are conducted to validate its
effectiveness. In its implementation, just with a few lines of codes added to
the existing USFDAs, we achieve superior results on various benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weikai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1"&gt;Meng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Songcan Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation. (arXiv:2204.03500v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03500</id>
        <link href="http://arxiv.org/abs/2204.03500"/>
        <updated>2022-04-09T00:48:55.247Z</updated>
        <summary type="html"><![CDATA[The widespread application of artificial intelligence in health research is
currently hampered by limitations in data availability. Distributed learning
methods such as federated learning (FL) and shared learning (SL) are introduced
to solve this problem as well as data management and ownership issues with
their different strengths and weaknesses. The recent proposal of federated
split task-agnostic (FeSTA) learning tries to reconcile the distinct merits of
FL and SL by enabling the multi-task collaboration between participants through
Vision Transformer (ViT) architecture, but they suffer from higher
communication overhead. To address this, here we present a multi-task
distributed learning using ViT with random patch permutation. Instead of using
a CNN based head as in FeSTA, p-FeSTA adopts a randomly permuting simple patch
embedder, improving the multi-task learning performance without sacrificing
privacy. Experimental results confirm that the proposed method significantly
enhances the benefit of multi-task collaboration, communication efficiency, and
privacy preservation, shedding light on practical multi-task distributed
learning in the field of medical imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangjoon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning to Prove Equivalence Between Programs via Semantics-Preserving Rewrite Rules. (arXiv:2109.10476v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.10476</id>
        <link href="http://arxiv.org/abs/2109.10476"/>
        <updated>2022-04-09T00:48:55.246Z</updated>
        <summary type="html"><![CDATA[We target the problem of automatically synthesizing proofs of semantic
equivalence between two programs made of sequences of statements. We represent
programs using abstract syntax trees (AST), where a given set of
semantics-preserving rewrite rules can be applied on a specific AST pattern to
generate a transformed and semantically equivalent program. In our system, two
programs are equivalent if there exists a sequence of application of these
rewrite rules that leads to rewriting one program into the other. We propose a
neural network architecture based on a transformer model to generate proofs of
equivalence between program pairs. The system outputs a sequence of rewrites,
and the validity of the sequence is simply checked by verifying it can be
applied. If no valid sequence is produced by the neural network, the system
reports the programs as non-equivalent, ensuring by design no programs may be
incorrectly reported as equivalent. Our system is fully implemented for a given
grammar. To efficiently train the system to generate such sequences, we develop
an original incremental training technique, named self-supervised sample
selection. We extensively study the effectiveness of this novel training
approach on proofs of increasing complexity and length. Our system, S4Eq,
achieves 97% proof success on a curated dataset of 10,000 pairs of equivalent
programs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1"&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1"&gt;Martin Monperrus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1"&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RF Signal Transformation and Classification using Deep Neural Networks. (arXiv:2204.03564v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2204.03564</id>
        <link href="http://arxiv.org/abs/2204.03564"/>
        <updated>2022-04-09T00:48:55.190Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) designed for computer vision and natural language
processing tasks cannot be directly applied to the radio frequency (RF)
datasets. To address this challenge, we propose to convert the raw RF data to
data types that are suitable for off-the-shelf DNNs by introducing a
convolutional transform technique. In addition, we propose a simple 5-layer
convolutional neural network architecture (CONV-5) that can operate with raw RF
I/Q data without any transformation. Further, we put forward an RF dataset,
referred to as RF1024, to facilitate future RF research. RF1024 consists of 8
different RF modulation classes with each class having 1000/200 training/test
samples. Each sample of the RF1024 dataset contains 1024 complex I/Q values.
Lastly, the experiments are performed on the RadioML2016 and RF1024 datasets to
demonstrate the improved classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khalid_U/0/1/0/all/0/1"&gt;Umar Khalid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karim_N/0/1/0/all/0/1"&gt;Nazmul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahnavard_N/0/1/0/all/0/1"&gt;Nazanin Rahnavard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis. (arXiv:2204.03105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03105</id>
        <link href="http://arxiv.org/abs/2204.03105"/>
        <updated>2022-04-09T00:48:55.189Z</updated>
        <summary type="html"><![CDATA[In this paper, we address the problem of texture representation for 3D shapes
for the challenging and underexplored tasks of texture transfer and synthesis.
Previous works either apply spherical texture maps which may lead to large
distortions, or use continuous texture fields that yield smooth outputs lacking
details. We argue that the traditional way of representing textures with images
and linking them to a 3D mesh via UV mapping is more desirable, since
synthesizing 2D images is a well-studied problem. We propose AUV-Net which
learns to embed 3D surfaces into a 2D aligned UV space, by mapping the
corresponding semantic parts of different 3D shapes to the same location in the
UV space. As a result, textures are aligned across objects, and can thus be
easily synthesized by generative models of images. Texture alignment is learned
in an unsupervised manner by a simple yet effective texture alignment module,
taking inspiration from traditional works on linear subspace learning. The
learned UV mapping and aligned texture representations enable a variety of
applications including texture transfer, texture synthesis, and textured single
view 3D reconstruction. We conduct experiments on multiple datasets to
demonstrate the effectiveness of our method. Project page:
https://nv-tlabs.github.io/AUV-NET.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiqin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kangxue Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedADMM: A Robust Federated Deep Learning Framework with Adaptivity to System Heterogeneity. (arXiv:2204.03529v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03529</id>
        <link href="http://arxiv.org/abs/2204.03529"/>
        <updated>2022-04-09T00:48:55.189Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is an emerging framework for distributed processing
of large data volumes by edge devices subject to limited communication
bandwidths, heterogeneity in data distributions and computational resources, as
well as privacy considerations. In this paper, we introduce a new FL protocol
termed FedADMM based on primal-dual optimization. The proposed method leverages
dual variables to tackle statistical heterogeneity, and accommodates system
heterogeneity by tolerating variable amount of work performed by clients.
FedADMM maintains identical communication costs per round as FedAvg/Prox, and
generalizes them via the augmented Lagrangian. A convergence proof is
established for nonconvex objectives, under no restrictions in terms of data
dissimilarity or number of participants per round of the algorithm. We
demonstrate the merits through extensive experiments on real datasets, under
both IID and non-IID data distributions across clients. FedADMM consistently
outperforms all baseline methods in terms of communication efficiency, with the
number of rounds needed to reach a prescribed accuracy reduced by up to 87%.
The algorithm effectively adapts to heterogeneous data distributions through
the use of dual variables, without the need for hyperparameter tuning, and its
advantages are more pronounced in large-scale systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yonghai Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yichuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freris_N/0/1/0/all/0/1"&gt;Nikolaos M. Freris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MTI-Net: A Multi-Target Speech Intelligibility Prediction Model. (arXiv:2204.03310v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2204.03310</id>
        <link href="http://arxiv.org/abs/2204.03310"/>
        <updated>2022-04-09T00:48:55.188Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning (DL)-based non-intrusive speech assessment models
have attracted great attention. Many studies report that these DL-based models
yield satisfactory assessment performance and good flexibility, but their
performance in unseen environments remains a challenge. Furthermore, compared
to quality scores, fewer studies elaborate deep learning models to estimate
intelligibility scores. This study proposes a multi-task speech intelligibility
prediction model, called MTI-Net, for simultaneously predicting human and
machine intelligibility measures. Specifically, given a speech utterance,
MTI-Net is designed to predict subjective listening test results and word error
rate (WER) scores. We also investigate several methods that can improve the
prediction performance of MTI-Net. First, we compare different features
(including low-level features and embeddings from self-supervised learning
(SSL) models) and prediction targets of MTI-Net. Second, we explore the effect
of transfer learning and multi-tasking learning on training MTI-Net. Finally,
we examine the potential advantages of fine-tuning SSL embeddings. Experimental
results demonstrate the effectiveness of using cross-domain features,
multi-task learning, and fine-tuning SSL embeddings. Furthermore, it is
confirmed that the intelligibility and WER scores predicted by MTI-Net are
highly correlated with the ground-truth scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zezario_R/0/1/0/all/0/1"&gt;Ryandhimas E. Zezario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1"&gt;Szu-wei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuh_C/0/1/0/all/0/1"&gt;Chiou-Shann Fuh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariate-assisted Sparse Tensor Completion. (arXiv:2103.06428v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06428</id>
        <link href="http://arxiv.org/abs/2103.06428"/>
        <updated>2022-04-09T00:48:55.162Z</updated>
        <summary type="html"><![CDATA[We aim to provably complete a sparse and highly-missing tensor in the
presence of covariate information along tensor modes. Our motivation comes from
online advertising where users click-through-rates (CTR) on ads over various
devices form a CTR tensor that has about 96% missing entries and has many zeros
on non-missing entries, which makes the standalone tensor completion method
unsatisfactory. Beside the CTR tensor, additional ad features or user
characteristics are often available. In this paper, we propose
Covariate-assisted Sparse Tensor Completion (COSTCO) to incorporate covariate
information for the recovery of the sparse tensor. The key idea is to jointly
extract latent components from both the tensor and the covariate matrix to
learn a synthetic representation. Theoretically, we derive the error bound for
the recovered tensor components and explicitly quantify the improvements on
both the reveal probability condition and the tensor recovery accuracy due to
covariates. Finally, we apply COSTCO to an advertisement dataset consisting of
a CTR tensor and ad covariate matrix, leading to 23% accuracy improvement over
the baseline. An important by-product is that ad latent components from COSTCO
reveal interesting ad clusters, which are useful for better ad targeting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ibriga_H/0/1/0/all/0/1"&gt;Hilda S Ibriga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1"&gt;Will Wei Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covariance matrix preparation for quantum principal component analysis. (arXiv:2204.03495v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2204.03495</id>
        <link href="http://arxiv.org/abs/2204.03495"/>
        <updated>2022-04-09T00:48:55.140Z</updated>
        <summary type="html"><![CDATA[Principal component analysis (PCA) is a dimensionality reduction method in
data analysis that involves diagonalizing the covariance matrix of the dataset.
Recently, quantum algorithms have been formulated for PCA based on
diagonalizing a density matrix. These algorithms assume that the covariance
matrix can be encoded in a density matrix, but a concrete protocol for this
encoding has been lacking. Our work aims to address this gap. Assuming
amplitude encoding of the data, with the data given by the ensemble $\{p_i,|
\psi_i \rangle\}$, then one can easily prepare the ensemble average density
matrix $\overline{\rho} = \sum_i p_i |\psi_i\rangle \langle \psi_i |$. We first
show that $\overline{\rho}$ is precisely the covariance matrix whenever the
dataset is centered. For quantum datasets, we exploit global phase symmetry to
argue that there always exists a centered dataset consistent with
$\overline{\rho}$, and hence $\overline{\rho}$ can always be interpreted as a
covariance matrix. This provides a simple means for preparing the covariance
matrix for arbitrary quantum datasets or centered classical datasets. For
uncentered classical datasets, our method is so-called "PCA without centering",
which we interpret as PCA on a symmetrized dataset. We argue that this closely
corresponds to standard PCA, and we derive equations and inequalities that
bound the deviation of the spectrum obtained with our method from that of
standard PCA. We numerically illustrate our method for the MNIST handwritten
digit dataset. We also argue that PCA on quantum datasets is natural and
meaningful, and we numerically implement our method for molecular ground-state
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gordon_M/0/1/0/all/0/1"&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1"&gt;M. Cerezo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1"&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1"&gt;Patrick J. Coles&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Optimal Power Flow with Contextual Information. (arXiv:2109.07896v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.07896</id>
        <link href="http://arxiv.org/abs/2109.07896"/>
        <updated>2022-04-09T00:48:55.132Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop a distributionally robust chance-constrained
formulation of the Optimal Power Flow problem (OPF) whereby the system operator
can leverage contextual information. For this purpose, we exploit an ambiguity
set based on probability trimmings and optimal transport through which the
dispatch solution is protected against the incomplete knowledge of the
relationship between the OPF uncertainties and the context that is conveyed by
a sample of their joint probability distribution. We provide a tractable
reformulation of the proposed distributionally robust chance-constrained OPF
problem under the popular conditional-value-at-risk approximation. By way of
numerical experiments run on a modified IEEE-118 bus network with wind
uncertainty, we show how the power system can substantially benefit from taking
into account the well-known statistical dependence between the point forecast
of wind power outputs and its associated prediction error. Furthermore, the
experiments conducted also reveal that the distributional robustness conferred
on the OPF solution by our probability-trimmings-based approach is superior to
that bestowed by alternative approaches in terms of expected cost and system
reliability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Esteban_Perez_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Esteban-P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Morales_J/0/1/0/all/0/1"&gt;Juan M. Morales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A comparison of mixed-variables Bayesian optimization approaches. (arXiv:2111.01533v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.01533</id>
        <link href="http://arxiv.org/abs/2111.01533"/>
        <updated>2022-04-09T00:48:55.116Z</updated>
        <summary type="html"><![CDATA[Most real optimization problems are defined over a mixed search space where
the variables are both discrete and continuous. In engineering applications,
the objective function is typically calculated with a numerically costly
black-box simulation.General mixed and costly optimization problems are
therefore of a great practical interest, yet their resolution remains in a
large part an open scientific question. In this article, costly mixed problems
are approached through Gaussian processes where the discrete variables are
relaxed into continuous latent variables. The continuous space is more easily
harvested by classical Bayesian optimization techniques than a mixed space
would. Discrete variables are recovered either subsequently to the continuous
optimization, or simultaneously with an additional continuous-discrete
compatibility constraint that is handled with augmented Lagrangians. Several
possible implementations of such Bayesian mixed optimizers are compared. In
particular, the reformulation of the problem with continuous latent variables
is put in competition with searches working directly in the mixed space. Among
the algorithms involving latent variables and an augmented Lagrangian, a
particular attention is devoted to the Lagrange multipliers for which a local
and a global estimation techniques are studied. The comparisons are based on
the repeated optimization of three analytical functions and a beam design
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cuesta_Ramirez_J/0/1/0/all/0/1"&gt;Jhouben Cuesta-Ramirez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Riche_R/0/1/0/all/0/1"&gt;Rodolphe Le Riche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Roustant_O/0/1/0/all/0/1"&gt;Olivier Roustant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Perrin_G/0/1/0/all/0/1"&gt;Guillaume Perrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Durantin_C/0/1/0/all/0/1"&gt;Cedric Durantin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gliere_A/0/1/0/all/0/1"&gt;Alain Gliere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-aided Traffic Control Scheme for M2M Communications in the Internet of Vehicles. (arXiv:2204.03504v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2204.03504</id>
        <link href="http://arxiv.org/abs/2204.03504"/>
        <updated>2022-04-09T00:48:55.108Z</updated>
        <summary type="html"><![CDATA[Due to the rapid growth of data transmissions in internet of vehicles (IoV),
finding schemes that can effectively alleviate access congestion has become an
important issue. Recently, many traffic control schemes have been studied.
Nevertheless, the dynamics of traffic and the heterogeneous requirements of
different IoV applications are not considered in most existing studies, which
is significant for the random access resource allocation. In this paper, we
consider a hybrid traffic control scheme and use proximal policy optimization
(PPO) method to tackle it. Firstly, IoV devices are divided into various
classes based on delay characteristics. The target of maximizing the successful
transmission of packets with the success rate constraint is established. Then,
the optimization objective is transformed into a markov decision process (MDP)
model. Finally, the access class barring (ACB) factors are obtained based on
the PPO method to maximize the number of successful access devices. The
performance of the proposal algorithm in respect of successful events and delay
compared to existing schemes is verified by simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Minghui Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangnan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_K/0/1/0/all/0/1"&gt;Keping Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leung_V/0/1/0/all/0/1"&gt;Victor C.M.Leung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Deep Learning for All-in EDGE paradigm. (arXiv:2204.03326v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03326</id>
        <link href="http://arxiv.org/abs/2204.03326"/>
        <updated>2022-04-09T00:48:55.107Z</updated>
        <summary type="html"><![CDATA[Deep Learning-based models have been widely investigated, and they have
demonstrated significant performance on non-trivial tasks such as speech
recognition, image processing, and natural language understanding. However,
this is at the cost of substantial data requirements. Considering the
widespread proliferation of edge devices (e.g. Internet of Things devices) over
the last decade, Deep Learning in the edge paradigm, such as device-cloud
integrated platforms, is required to leverage its superior performance.
Moreover, it is suitable from the data requirements perspective in the edge
paradigm because the proliferation of edge devices has resulted in an explosion
in the volume of generated and collected data. However, there are difficulties
due to other requirements such as high computation, high latency, and high
bandwidth caused by Deep Learning applications in real-world scenarios. In this
regard, this survey paper investigates Deep Learning at the edge, its
architecture, enabling technologies, and model adaption techniques, where edge
servers and edge devices participate in deep learning training and inference.
For simplicity, we call this paradigm the All-in EDGE paradigm. Besides, this
paper presents the key performance metrics for Deep Learning at the All-in EDGE
paradigm to evaluate various deep learning techniques and choose a suitable
design. Moreover, various open challenges arising from the deployment of Deep
Learning at the All-in EDGE paradigm are identified and discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_P/0/1/0/all/0/1"&gt;Praveen Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afli_H/0/1/0/all/0/1"&gt;Haithem Afli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasanuzzaman_M/0/1/0/all/0/1"&gt;Mohammed Hasanuzzaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1"&gt;Chandra Thapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scully_T/0/1/0/all/0/1"&gt;Ted Scully&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Standardized feature extraction from pairwise conflicts applied to the train rescheduling problem. (arXiv:2204.03061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03061</id>
        <link href="http://arxiv.org/abs/2204.03061"/>
        <updated>2022-04-09T00:48:55.105Z</updated>
        <summary type="html"><![CDATA[We propose a train rescheduling algorithm which applies a standardized
feature selection based on pairwise conflicts in order to serve as input for
the reinforcement learning framework. We implement an analytical method which
identifies and optimally solves every conflict arising between two trains, then
we design a corresponding observation space which features the most relevant
information considering these conflicts. The data obtained this way then
translates to actions in the context of the reinforcement learning framework.
We test our preliminary model using the evaluation metrics of the Flatland
Challenge. The empirical results indicate that the suggested feature space
provides meaningful observations, from which a sensible scheduling policy can
be learned.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopacz_A/0/1/0/all/0/1"&gt;Anik&amp;#xf3; Kopacz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mester_A/0/1/0/all/0/1"&gt;&amp;#xc1;gnes Mester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolumban_S/0/1/0/all/0/1"&gt;S&amp;#xe1;ndor Kolumb&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehel_C/0/1/0/all/0/1"&gt;Csat&amp;#xf3; Lehel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization Models and Interpretations for Three Types of Adversarial Perturbations against Support Vector Machines. (arXiv:2204.03154v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03154</id>
        <link href="http://arxiv.org/abs/2204.03154"/>
        <updated>2022-04-09T00:48:55.105Z</updated>
        <summary type="html"><![CDATA[Adversarial perturbations have drawn great attentions in various deep neural
networks. Most of them are computed by iterations and cannot be interpreted
very well. In contrast, little attentions are paid to basic machine learning
models such as support vector machines. In this paper, we investigate the
optimization models and the interpretations for three types of adversarial
perturbations against support vector machines, including sample-adversarial
perturbations (sAP), class-universal adversarial perturbations (cuAP) as well
as universal adversarial perturbations (uAP). For linear binary/multi
classification support vector machines (SVMs), we derive the explicit solutions
for sAP, cuAP and uAP (binary case), and approximate solution for uAP of
multi-classification. We also obtain the upper bound of fooling rate for uAP.
Such results not only increase the interpretability of the three adversarial
perturbations, but also provide great convenience in computation since
iterative process can be avoided. Numerical results show that our method is
fast and effective in calculating three types of adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Wen Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingna Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Chunfeng Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Alignment for History Representation in Reinforcement Learning. (arXiv:2204.03525v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03525</id>
        <link href="http://arxiv.org/abs/2204.03525"/>
        <updated>2022-04-09T00:48:55.105Z</updated>
        <summary type="html"><![CDATA[Environments in Reinforcement Learning are usually only partially observable.
To address this problem, a possible solution is to provide the agent with
information about the past. However, providing complete observations of
numerous steps can be excessive. Inspired by human memory, we propose to
represent history with only important changes in the environment and, in our
approach, to obtain automatically this representation using self-supervision.
Our method (TempAl) aligns temporally-close frames, revealing a general, slowly
varying state of the environment. This procedure is based on contrastive loss,
which pulls embeddings of nearby observations to each other while pushing away
other samples from the batch. It can be interpreted as a metric that captures
the temporal relations of observations. We propose to combine both common
instantaneous and our history representation and we evaluate TempAl on all
available Atari games from the Arcade Learning Environment. TempAl surpasses
the instantaneous-only baseline in 35 environments out of 49. The source code
of the method and of all the experiments is available at
https://github.com/htdt/tempal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ermolov_A/0/1/0/all/0/1"&gt;Aleksandr Ermolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task nonparallel support vector machine for classification. (arXiv:2204.02972v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.02972</id>
        <link href="http://arxiv.org/abs/2204.02972"/>
        <updated>2022-04-09T00:48:55.104Z</updated>
        <summary type="html"><![CDATA[Direct multi-task twin support vector machine (DMTSVM) explores the shared
information between multiple correlated tasks, then it produces better
generalization performance. However, it contains matrix inversion operation
when solving the dual problems, so it costs much running time. Moreover, kernel
trick cannot be directly utilized in the nonlinear case. To effectively avoid
above problems, a novel multi-task nonparallel support vector machine (MTNPSVM)
including linear and nonlinear cases is proposed in this paper. By introducing
epsilon-insensitive loss instead of square loss in DMTSVM, MTNPSVM effectively
avoids matrix inversion operation and takes full advantage of the kernel trick.
Theoretical implication of the model is further discussed. To further improve
the computational efficiency, the alternating direction method of multipliers
(ADMM) is employed when solving the dual problem. The computational complexity
and convergence of the algorithm are provided. In addition, the property and
sensitivity of the parameter in model are further explored. The experimental
results on fifteen benchmark datasets and twelve image datasets demonstrate the
validity of MTNPSVM in comparison with the state-of-the-art algorithms.
Finally, it is applied to real Chinese Wine dataset, and also verifies its
effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zongmin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yitian Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Distributed Spectrum Sensing in NextG Communication Networks. (arXiv:2204.03027v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2204.03027</id>
        <link href="http://arxiv.org/abs/2204.03027"/>
        <updated>2022-04-09T00:48:55.104Z</updated>
        <summary type="html"><![CDATA[NextG networks are intended to provide the flexibility of sharing the
spectrum with incumbent users and support various spectrum monitoring tasks
such as anomaly detection, fault diagnostics, user equipment identification,
and authentication. A network of wireless sensors is needed to monitor the
spectrum for signal transmissions of interest over a large deployment area.
Each sensor receives signals under a specific channel condition depending on
its location and trains an individual model of a deep neural network (DNN)
accordingly to classify signals. To improve the accuracy, individual sensors
may exchange sensing data or sensor results with each other or with a fusion
center (such as in cooperative spectrum sensing). In this paper, distributed
federated learning over a multi-hop wireless network is considered to
collectively train a DNN for signal identification. In distributed federated
learning, each sensor broadcasts its trained model to its neighbors, collects
the DNN models from its neighbors, and aggregates them to initialize its own
model for the next round of training. Without exchanging any spectrum data,
this process is repeated over time such that a common DNN is built across the
network while preserving the privacy associated with signals collected at
different locations. Signal classification accuracy and convergence time are
evaluated for different network topologies (including line, star, ring, grid,
and random networks) and packet loss events. Then, the reduction of
communication overhead and energy consumption is considered with random
participation of sensors in model updates. The results show the feasibility of
extending cooperative spectrum sensing over a general multi-hop wireless
network through federated learning and indicate its robustness to wireless
network effects, thereby sustaining high accuracy with low communication
overhead and energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yi Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1"&gt;Yalin E. Sagduyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erpek_T/0/1/0/all/0/1"&gt;Tugba Erpek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster algorithms for learning to link, align sequences, and price two-part tariffs. (arXiv:2204.03569v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2204.03569</id>
        <link href="http://arxiv.org/abs/2204.03569"/>
        <updated>2022-04-09T00:48:55.104Z</updated>
        <summary type="html"><![CDATA[Data-driven algorithm configuration is a promising, learning-based approach
for beyond worst-case analysis of algorithms with tunable parameters. An
important open problem is the design of efficient data-driven algorithms for
algorithm families with more than one parameter. In this work we provide
algorithms for efficient (output-polynomial) multidimensional parameter tuning,
i.e. for families with a small constant number of parameters, for three very
different combinatorial problems -- linkage-based clustering, dynamic
programming for sequence alignment, and auction design for two-part tariff
schemes. We extend the single-parameter clustering algorithm of Balcan et al.
2020 arXiv:1907.00533 to multiple parameters and to the sequence alignment
problem by proposing an execution graph which compactly represents all the
states the algorithm could attain for all possible parameter values. A key
problem-specific challenge is to efficiently compute how the partition of the
parameter space (into regions with unique algorithmic states) changes with a
single algorithmic step. We give algorithms which improve on the runtime of
previously best known results for linkage-based clustering, sequence alignment
and two-part tariff pricing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1"&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seiler_C/0/1/0/all/0/1"&gt;Christopher Seiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Dravyansh Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effects of Regularization and Data Augmentation are Class Dependent. (arXiv:2204.03632v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03632</id>
        <link href="http://arxiv.org/abs/2204.03632"/>
        <updated>2022-04-09T00:48:55.104Z</updated>
        <summary type="html"><![CDATA[Regularization is a fundamental technique to prevent over-fitting and to
improve generalization performances by constraining a model's complexity.
Current Deep Networks heavily rely on regularizers such as Data-Augmentation
(DA) or weight-decay, and employ structural risk minimization, i.e.
cross-validation, to select the optimal regularization hyper-parameters. In
this study, we demonstrate that techniques such as DA or weight decay produce a
model with a reduced complexity that is unfair across classes. The optimal
amount of DA or weight decay found from cross-validation leads to disastrous
model performances on some classes e.g. on Imagenet with a resnet50, the "barn
spider" classification test accuracy falls from $68\%$ to $46\%$ only by
introducing random crop DA during training. Even more surprising, such
performance drop also appears when introducing uninformative regularization
techniques such as weight decay. Those results demonstrate that our search for
ever increasing generalization performance -- averaged over all classes and
samples -- has left us with models and regularizers that silently sacrifice
performances on some classes. This scenario can become dangerous when deploying
a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on
INaturalist sees its performances fall from $70\%$ to $30\%$ on class \#8889
when introducing random crop DA during the Imagenet pre-training phase. Those
results demonstrate that designing novel regularizers without class-dependent
bias remains an open research question.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1"&gt;Leon Bottou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Infused Decoding. (arXiv:2204.03084v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2204.03084</id>
        <link href="http://arxiv.org/abs/2204.03084"/>
        <updated>2022-04-09T00:48:55.103Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (LMs) have been shown to memorize a substantial
amount of knowledge from the pre-training corpora; however, they are still
limited in recalling factually correct knowledge given a certain context.
Hence, they tend to suffer from counterfactual or hallucinatory generation when
used in knowledge-intensive natural language generation (NLG) tasks. Recent
remedies to this problem focus on modifying either the pre-training or task
fine-tuning objectives to incorporate knowledge, which normally require
additional costly training or architecture modification of LMs for practical
applications. We present Knowledge Infused Decoding (KID) -- a novel decoding
algorithm for generative LMs, which dynamically infuses external knowledge into
each step of the LM decoding. Specifically, we maintain a local knowledge
memory based on the current context, interacting with a dynamically created
external knowledge trie, and continuously update the local memory as a
knowledge-aware constraint to guide decoding via reinforcement learning. On six
diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)
armed with KID outperform many task-optimized state-of-the-art models, and show
particularly strong performance in few-shot scenarios over seven related
knowledge-infusion techniques. Human evaluation confirms KID's ability to
generate more relevant and factual language for the input context when compared
with multiple baselines. Finally, KID also alleviates exposure bias and
provides stable generation quality when generating longer sequences. Code for
KID is available at https://github.com/microsoft/KID.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Shashank Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaonkar_R/0/1/0/all/0/1"&gt;Radhika Gaonkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chongyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1"&gt;Soroush Vosoughi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancement on Model Interpretability and Sleep Stage Scoring Performance with A Novel Pipeline Based on Deep Neural Network. (arXiv:2204.03173v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03173</id>
        <link href="http://arxiv.org/abs/2204.03173"/>
        <updated>2022-04-09T00:48:55.103Z</updated>
        <summary type="html"><![CDATA[Considering the natural frequency characteristics in sleep medicine, this
paper first proposes a time-frequency framework for the representation learning
of the electroencephalogram (EEG) following the definition of the American
Academy of Sleep Medicine. To meet the temporal-random and transient nature of
the defining characteristics of sleep stages, we further design a
context-sensitive flexible pipeline that automatically adapts to the attributes
of data itself. That is, the input EEG spectrogram is partitioned into a
sequence of patches in the time and frequency axes, and then input to a
delicate deep learning network for further representation learning to extract
the stage-dependent features, which are used in the classification step
finally. The proposed pipeline is validated against a large database, i.e., the
Sleep Heart Health Study (SHHS), and the results demonstrate that the
competitive performance for the wake, N2, and N3 stages outperforms the
state-of-art works, with the F1 scores being 0.93, 0.88, and 0.87,
respectively, and the proposed method has a high inter-rater reliability of
0.80 kappa. Importantly, we visualize the stage scoring process of the model
decision with the Layer-wise Relevance Propagation (LRP) method, which shows
that the proposed pipeline is more sensitive and perceivable in the
decision-making process than the baseline pipelines. Therefore, the pipeline
together with the LRP method can provide better model interpretability, which
is important for clinical support.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Ming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamura_T/0/1/0/all/0/1"&gt;Toshiyo Tamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ono_N/0/1/0/all/0/1"&gt;Naoaki Ono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Altaf_Ul_Amin_M/0/1/0/all/0/1"&gt;MD Altaf-Ul-Amin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanaya_S/0/1/0/all/0/1"&gt;Shigehiko Kanaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Diffusion Models. (arXiv:2204.03458v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2204.03458</id>
        <link href="http://arxiv.org/abs/2204.03458"/>
        <updated>2022-04-09T00:48:55.102Z</updated>
        <summary type="html"><![CDATA[Generating temporally coherent high fidelity video is an important milestone
in generative modeling research. We make progress towards this milestone by
proposing a diffusion model for video generation that shows very promising
initial results. Our model is a natural extension of the standard image
diffusion architecture, and it enables jointly training from image and video
data, which we find to reduce the variance of minibatch gradients and speed up
optimization. To generate long and higher resolution videos we introduce a new
conditional sampling technique for spatial and temporal video extension that
performs better than previously proposed methods. We present the first results
on a large text-conditioned video generation task, as well as state-of-the-art
results on an established unconditional video generation benchmark.
Supplementary material is available at https://video-diffusion.github.io/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1"&gt;Alexey Gritsenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1"&gt;William Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mo\"ET: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning. (arXiv:1906.06717v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.06717</id>
        <link href="http://arxiv.org/abs/1906.06717"/>
        <updated>2022-04-09T00:48:55.098Z</updated>
        <summary type="html"><![CDATA[Rapid advancements in deep learning have led to many recent breakthroughs.
While deep learning models achieve superior performance, often statistically
better than humans, their adoption into safety-critical settings, such as
healthcare or self-driving cars is hindered by their inability to provide
safety guarantees or to expose the inner workings of the model in a human
understandable form. We present Mo\"ET, a novel model based on Mixture of
Experts, consisting of decision tree experts and a generalized linear model
gating function. Thanks to such gating function the model is more expressive
than the standard decision tree. To support non-differentiable decision trees
as experts, we formulate a novel training procedure. In addition, we introduce
a hard thresholding version, Mo\"ETH, in which predictions are made solely by a
single expert chosen via the gating function. Thanks to that property, Mo\"ETH
allows each prediction to be easily decomposed into a set of logical rules in a
form which can be easily verified. While Mo\"ET is a general use model, we
illustrate its power in the reinforcement learning setting. By training Mo\"ET
models using an imitation learning procedure on deep RL agents we outperform
the previous state-of-the-art technique based on decision trees while
preserving the verifiability of the models. Moreover, we show that Mo\"ET can
also be used in real-world supervised problems on which it outperforms other
verifiable machine learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasic_M/0/1/0/all/0/1"&gt;Marko Vasic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrovic_A/0/1/0/all/0/1"&gt;Andrija Petrovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolic_M/0/1/0/all/0/1"&gt;Mladen Nikolic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khurshid_S/0/1/0/all/0/1"&gt;Sarfraz Khurshid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amortized Auto-Tuning: Cost-Efficient Bayesian Transfer Optimization for Hyperparameter Recommendation. (arXiv:2106.09179v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09179</id>
        <link href="http://arxiv.org/abs/2106.09179"/>
        <updated>2022-04-09T00:48:55.076Z</updated>
        <summary type="html"><![CDATA[With the surge in the number of hyperparameters and training times of modern
machine learning models, hyperparameter tuning is becoming increasingly
expensive. However, after assessing 40 tuning methods systematically, we find
that each faces certain limitations. In particular, methods that speed up
tuning via knowledge transfer typically require the final performance of
hyperparameters and do not focus on low-fidelity information. As we demonstrate
empirically, this common practice is suboptimal and can incur an unnecessary
use of resources. It is more cost-efficient to instead leverage low-fidelity
tuning observations to measure inter-task similarity and transfer knowledge
from existing to new tasks accordingly. However, performing multi-fidelity
tuning comes with its own challenges in the transfer setting: the noise in
additional observations and the need for performance forecasting. Therefore, we
propose and conduct a thorough analysis of a multi-task multi-fidelity Bayesian
optimization framework, which leads to the best instantiation--amortized
auto-tuning (AT2). We further present an offline-computed 27-task
hyperparameter recommendation (HyperRec) database to serve the community.
Extensive experiments on HyperRec and other real-world databases illustrate the
effectiveness of our AT2 method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yuxin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"&gt;Eric P. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Joint Learning Approach for Semi-supervised Neural Topic Modeling. (arXiv:2204.03208v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2204.03208</id>
        <link href="http://arxiv.org/abs/2204.03208"/>
        <updated>2022-04-09T00:48:55.069Z</updated>
        <summary type="html"><![CDATA[Topic models are some of the most popular ways to represent textual data in
an interpret-able manner. Recently, advances in deep generative models,
specifically auto-encoding variational Bayes (AEVB), have led to the
introduction of unsupervised neural topic models, which leverage deep
generative models as opposed to traditional statistics-based topic models. We
extend upon these neural topic models by introducing the Label-Indexed Neural
Topic Model (LI-NTM), which is, to the extent of our knowledge, the first
effective upstream semi-supervised neural topic model. We find that LI-NTM
outperforms existing neural topic models in document reconstruction benchmarks,
with the most notable results in low labeled data regimes and for data-sets
with informative labels; furthermore, our jointly learned classifier
outperforms baseline classifiers in ablation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1"&gt;Jeffrey Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1"&gt;Rajat Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tumma_N/0/1/0/all/0/1"&gt;Neehal Tumma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abhishek Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1"&gt;Finale Doshi-Velez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Set Union. (arXiv:2002.09745v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09745</id>
        <link href="http://arxiv.org/abs/2002.09745"/>
        <updated>2022-04-09T00:48:55.060Z</updated>
        <summary type="html"><![CDATA[We study the basic operation of set union in the global model of differential
privacy. In this problem, we are given a universe $U$ of items, possibly of
infinite size, and a database $D$ of users. Each user $i$ contributes a subset
$W_i \subseteq U$ of items. We want an ($\epsilon$,$\delta$)-differentially
private algorithm which outputs a subset $S \subset \cup_i W_i$ such that the
size of $S$ is as large as possible. The problem arises in countless real world
applications; it is particularly ubiquitous in natural language processing
(NLP) applications as vocabulary extraction. For example, discovering words,
sentences, $n$-grams etc., from private text data belonging to users is an
instance of the set union problem.

Known algorithms for this problem proceed by collecting a subset of items
from each user, taking the union of such subsets, and disclosing the items
whose noisy counts fall above a certain threshold. Crucially, in the above
process, the contribution of each individual user is always independent of the
items held by other users, resulting in a wasteful aggregation process, where
some item counts happen to be way above the threshold. We deviate from the
above paradigm by allowing users to contribute their items in a
$\textit{dependent fashion}$, guided by a $\textit{policy}$. In this new
setting ensuring privacy is significantly delicate. We prove that any policy
which has certain $\textit{contractive}$ properties would result in a
differentially private algorithm. We design two new algorithms, one using
Laplace noise and other Gaussian noise, as specific instances of policies
satisfying the contractive properties. Our experiments show that the new
algorithms significantly outperform previously known mechanisms for the
problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulhane_P/0/1/0/all/0/1"&gt;Pankaj Gulhane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1"&gt;Janardhan Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Judy Hanwen Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1"&gt;Sergey Yekhanin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03528</id>
        <link href="http://arxiv.org/abs/2204.03528"/>
        <updated>2022-04-09T00:48:55.052Z</updated>
        <summary type="html"><![CDATA[Machine Learning with Deep Neural Networks (DNNs) has become a successful
tool in solving tasks across various fields of application. The success of DNNs
is strongly connected to their high complexity in terms of the number of
network layers or of neurons in each layer, which severely complicates to
understand how DNNs solve their learned task. To improve the explainability of
DNNs, we adapt methods from neuroscience because this field has a rich
experience in analyzing complex and opaque systems. In this work, we draw
inspiration from how neuroscience uses topographic maps to visualize the
activity of the brain when it performs certain tasks. Transferring this
approach to DNNs can help to visualize and understand their internal processes
more intuitively, too. However, the inner structures of brains and DNNs differ
substantially. Therefore, to be able to visualize activations of neurons in
DNNs as topographic maps, we research techniques to layout the neurons in a
two-dimensional space in which neurons of similar activity are in the vicinity
of each other. In this work, we introduce and compare different methods to
obtain a topographic layout of the neurons in a network layer. Moreover, we
demonstrate how to use the resulting topographic activation maps to identify
errors or encoded biases in DNNs or data sets. Our novel visualization
technique improves the transparency of DNN-based algorithmic decision-making
systems and is accessible to a broad audience because topographic maps are
intuitive to interpret without expert-knowledge in Machine Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krug_A/0/1/0/all/0/1"&gt;Andreas Krug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratul_R/0/1/0/all/0/1"&gt;Raihan Kabir Ratul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1"&gt;Sebastian Stober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effects of Regularization and Data Augmentation are Class Dependent. (arXiv:2204.03632v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03632</id>
        <link href="http://arxiv.org/abs/2204.03632"/>
        <updated>2022-04-09T00:48:55.052Z</updated>
        <summary type="html"><![CDATA[Regularization is a fundamental technique to prevent over-fitting and to
improve generalization performances by constraining a model's complexity.
Current Deep Networks heavily rely on regularizers such as Data-Augmentation
(DA) or weight-decay, and employ structural risk minimization, i.e.
cross-validation, to select the optimal regularization hyper-parameters. In
this study, we demonstrate that techniques such as DA or weight decay produce a
model with a reduced complexity that is unfair across classes. The optimal
amount of DA or weight decay found from cross-validation leads to disastrous
model performances on some classes e.g. on Imagenet with a resnet50, the "barn
spider" classification test accuracy falls from $68\%$ to $46\%$ only by
introducing random crop DA during training. Even more surprising, such
performance drop also appears when introducing uninformative regularization
techniques such as weight decay. Those results demonstrate that our search for
ever increasing generalization performance -- averaged over all classes and
samples -- has left us with models and regularizers that silently sacrifice
performances on some classes. This scenario can become dangerous when deploying
a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on
INaturalist sees its performances fall from $70\%$ to $30\%$ on class \#8889
when introducing random crop DA during the Imagenet pre-training phase. Those
results demonstrate that designing novel regularizers without class-dependent
bias remains an open research question.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1"&gt;Leon Bottou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Categorical Distributions of Maximum Entropy under Marginal Constraints. (arXiv:2204.03406v1 [hep-th])]]></title>
        <id>http://arxiv.org/abs/2204.03406</id>
        <link href="http://arxiv.org/abs/2204.03406"/>
        <updated>2022-04-09T00:48:55.049Z</updated>
        <summary type="html"><![CDATA[The estimation of categorical distributions under marginal constraints
summarizing some sample from a population in the most-generalizable way is key
for many machine-learning and data-driven approaches. We provide a
parameter-agnostic theoretical framework that enables this task ensuring (i)
that a categorical distribution of Maximum Entropy under marginal constraints
always exists and (ii) that it is unique. The procedure of iterative
proportional fitting (IPF) naturally estimates that distribution from any
consistent set of marginal constraints directly in the space of probabilities,
thus deductively identifying a least-biased characterization of the population.
The theoretical framework together with IPF leads to a holistic workflow that
enables modeling any class of categorical distributions solely using the
phenomenological information provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Loukas_O/0/1/0/all/0/1"&gt;Orestis Loukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Chung_H/0/1/0/all/0/1"&gt;Ho Ryun Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GFlowNet Foundations. (arXiv:2111.09266v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.09266</id>
        <link href="http://arxiv.org/abs/2111.09266"/>
        <updated>2022-04-09T00:48:55.042Z</updated>
        <summary type="html"><![CDATA[Generative Flow Networks (GFlowNets) have been introduced as a method to
sample a diverse set of candidates in an active learning context, with a
training objective that makes them approximately sample in proportion to a
given reward function. In this paper, we show a number of additional
theoretical properties of GFlowNets. They can be used to estimate joint
probability distributions and the corresponding marginal distributions where
some variables are unspecified and, of particular interest, can represent
distributions over composite objects like sets and graphs. GFlowNets amortize
the work typically done by computationally expensive MCMC methods in a single
but trained generative pass. They could also be used to estimate partition
functions and free energies, conditional probabilities of supersets
(supergraphs) given a subset (subgraph), as well as marginal distributions over
all supersets (supergraphs) of a given set (graph). We introduce variations
enabling the estimation of entropy and mutual information, sampling from a
Pareto frontier, connections to reward-maximizing policies, and extensions to
stochastic environments, continuous actions and modular energy functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deleu_T/0/1/0/all/0/1"&gt;Tristan Deleu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;Edward J. Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahlou_S/0/1/0/all/0/1"&gt;Salem Lahlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1"&gt;Mo Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1"&gt;Emmanuel Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.05845</id>
        <link href="http://arxiv.org/abs/2203.05845"/>
        <updated>2022-04-09T00:48:55.020Z</updated>
        <summary type="html"><![CDATA[Streamlined qBOLD acquisitions enable experimentally straightforward
observations of brain oxygen metabolism. $R_2^\prime$ maps are easily inferred;
however, the Oxygen extraction fraction (OEF) and deoxygenated blood volume
(DBV) are more ambiguously determined from the data. As such, existing
inference methods tend to yield very noisy and underestimated OEF maps, while
overestimating DBV.

This work describes a novel probabilistic machine learning approach that can
infer plausible distributions of OEF and DBV. Initially, we create a model that
produces informative voxelwise prior distribution based on synthetic training
data. Contrary to prior work, we model the joint distribution of OEF and DBV
through a scaled multivariate logit-Normal distribution, which enables the
values to be constrained within a plausible range. The prior distribution model
is used to train an efficient amortized variational Bayesian inference model.
This model learns to infer OEF and DBV by predicting real image data, with few
training data required, using the signal equations as a forward model.

We demonstrate that our approach enables the inference of smooth OEF and DBV
maps, with a physiologically plausible distribution that can be adapted through
specification of an informative prior distribution. Other benefits include
model comparison (via the evidence lower bound) and uncertainty quantification
for identifying image artefacts. Results are demonstrated on a small study
comparing subjects undergoing hyperventilation and at rest. We illustrate that
the proposed approach allows measurement of gray matter differences in OEF and
DBV and enables voxelwise comparison between conditions, where we observe
significant increases in OEF and $R_2^\prime$ during hyperventilation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Simpson_I/0/1/0/all/0/1"&gt;Ivor J.A. Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McManamon_A/0/1/0/all/0/1"&gt;Ashley McManamon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Orzsik_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs &amp;#xd6;rzsik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stone_A/0/1/0/all/0/1"&gt;Alan J. Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Blockley_N/0/1/0/all/0/1"&gt;Nicholas P. Blockley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asllani_I/0/1/0/all/0/1"&gt;Iris Asllani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Colasanti_A/0/1/0/all/0/1"&gt;Alessandro Colasanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cercignani_M/0/1/0/all/0/1"&gt;Mara Cercignani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial Diffusion Geometry. (arXiv:2103.15783v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15783</id>
        <link href="http://arxiv.org/abs/2103.15783"/>
        <updated>2022-04-09T00:48:55.011Z</updated>
        <summary type="html"><![CDATA[Clustering algorithms partition a dataset into groups of similar points. The
primary contribution of this article is the Multiscale Spatially-Regularized
Diffusion Learning (M-SRDL) clustering algorithm, which uses
spatially-regularized diffusion distances to efficiently and accurately learn
multiple scales of latent structure in hyperspectral images. The M-SRDL
clustering algorithm extracts clusterings at many scales from a hyperspectral
image and outputs these clusterings' variation of information-barycenter as an
exemplar for all underlying cluster structure. We show that incorporating
spatial regularization into a multiscale clustering framework results in
smoother and more coherent clusters when applied to hyperspectral data,
yielding more accurate clustering labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1"&gt;Sam L. Polk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1"&gt;James M. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Unsupervised Feature Selection for Dynamic Incomplete Multi-view Data. (arXiv:2204.02973v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.02973</id>
        <link href="http://arxiv.org/abs/2204.02973"/>
        <updated>2022-04-09T00:48:55.002Z</updated>
        <summary type="html"><![CDATA[Multi-view unsupervised feature selection has been proven to be efficient in
reducing the dimensionality of multi-view unlabeled data with high dimensions.
The previous methods assume all of the views are complete. However, in real
applications, the multi-view data are often incomplete, i.e., some views of
instances are missing, which will result in the failure of these methods.
Besides, while the data arrive in form of streams, these existing methods will
suffer the issues of high storage cost and expensive computation time. To
address these issues, we propose an Incremental Incomplete Multi-view
Unsupervised Feature Selection method (I$^2$MUFS) on incomplete multi-view
streaming data. By jointly considering the consistent and complementary
information across different views, I$^2$MUFS embeds the unsupervised feature
selection into an extended weighted non-negative matrix factorization model,
which can learn a consensus clustering indicator matrix and fuse different
latent feature matrices with adaptive view weights. Furthermore, we introduce
the incremental leaning mechanisms to develop an alternative iterative
algorithm, where the feature selection matrix is incrementally updated, rather
than recomputing on the entire updated data from scratch. A series of
experiments are conducted to verify the effectiveness of the proposed method by
comparing with several state-of-the-art methods. The experimental results
demonstrate the effectiveness and efficiency of the proposed method in terms of
the clustering metrics and the computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanyong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1"&gt;Kejun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xiuwen Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianrui Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation. (arXiv:2204.03293v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2204.03293</id>
        <link href="http://arxiv.org/abs/2204.03293"/>
        <updated>2022-04-09T00:48:55.002Z</updated>
        <summary type="html"><![CDATA[Code search aims to retrieve the most semantically relevant code snippet for
a given natural language query. Recently, large-scale code pre-trained models
such as CodeBERT and GraphCodeBERT learn generic representations of source code
and have achieved substantial improvement on code search task. However, the
high-quality sequence-level representations of code snippets have not been
sufficiently explored. In this paper, we propose a new approach with multimodal
contrastive learning and soft data augmentation for code search. Multimodal
contrastive learning is used to pull together the representations of code-query
pairs and push apart the unpaired code snippets and queries. Moreover, data
augmentation is critical in contrastive learning for learning high-quality
representations. However, only semantic-preserving augmentations for source
code are considered in existing work. In this work, we propose to do soft data
augmentation by dynamically masking and replacing some tokens in code sequences
to generate code snippets that are similar but not necessarily
semantic-preserving as positive samples for paired queries. We conduct
extensive experiments to evaluate the effectiveness of our approach on a
large-scale dataset with six programming languages. The experimental results
show that our approach significantly outperforms the state-of-the-art methods.
We also adapt our techniques to several pre-trained models such as RoBERTa and
CodeBERT, and significantly boost their performance on the code search task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_E/0/1/0/all/0/1"&gt;Ensheng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gub_W/0/1/0/all/0/1"&gt;Wenchao Gub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hongbin Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self supervised learning for robust voice cloning. (arXiv:2204.03421v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2204.03421</id>
        <link href="http://arxiv.org/abs/2204.03421"/>
        <updated>2022-04-09T00:48:55.002Z</updated>
        <summary type="html"><![CDATA[Voice cloning is a difficult task which requires robust and informative
features incorporated in a high quality TTS system in order to effectively copy
an unseen speaker's voice. In our work, we utilize features learned in a
self-supervised framework via the Bootstrap Your Own Latent (BYOL) method,
which is shown to produce high quality speech representations when specific
audio augmentations are applied to the vanilla algorithm. We further extend the
augmentations in the training procedure to aid the resulting features to
capture the speaker identity and to make them robust to noise and acoustic
conditions. The learned features are used as pre-trained utterance-level
embeddings and as inputs to a Non-Attentive Tacotron based architecture, aiming
to achieve multispeaker speech synthesis without utilizing additional speaker
features. This method enables us to train our model in an unlabeled
multispeaker dataset as well as use unseen speaker embeddings to copy a
speaker's voice. Subjective and objective evaluations are used to validate the
proposed model, as well as the robustness to the acoustic conditions of the
target utterance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klapsas_K/0/1/0/all/0/1"&gt;Konstantinos Klapsas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1"&gt;Nikolaos Ellinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikitaras_K/0/1/0/all/0/1"&gt;Karolos Nikitaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vamvoukakis_G/0/1/0/all/0/1"&gt;Georgios Vamvoukakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakoulidis_P/0/1/0/all/0/1"&gt;Panos Kakoulidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markopoulos_K/0/1/0/all/0/1"&gt;Konstantinos Markopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raptis_S/0/1/0/all/0/1"&gt;Spyros Raptis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1"&gt;June Sig Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jho_G/0/1/0/all/0/1"&gt;Gunu Jho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1"&gt;Aimilios Chalamandaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1"&gt;Pirros Tsiakoulis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Spike-Like Representation of EEG Signals for Sleep Stages Scoring. (arXiv:2204.03565v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2204.03565</id>
        <link href="http://arxiv.org/abs/2204.03565"/>
        <updated>2022-04-09T00:48:55.001Z</updated>
        <summary type="html"><![CDATA[Recently there has seen promising results on automatic stage scoring by
extracting spatio-temporal features from electroencephalogram (EEG). Such
methods entail laborious manual feature engineering and domain knowledge. In
this study, we propose an adaptive scheme to probabilistically encode, filter
and accumulate the input signals and weight the resultant features by the
half-Gaussian probabilities of signal intensities. The adaptive representations
are subsequently fed into a transformer model to automatically mine the
relevance between features and corresponding stages. Extensive experiments on
the largest public dataset against state-of-the-art methods validate the
effectiveness of our proposed method and reveal promising future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Odani_K/0/1/0/all/0/1"&gt;Koki Odani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_G/0/1/0/all/0/1"&gt;Guang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kan_Y/0/1/0/all/0/1"&gt;Yirong Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Renyuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sliced gradient-enhanced Kriging for high-dimensional function approximation. (arXiv:2204.03562v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03562</id>
        <link href="http://arxiv.org/abs/2204.03562"/>
        <updated>2022-04-09T00:48:54.964Z</updated>
        <summary type="html"><![CDATA[Gradient-enhanced Kriging (GE-Kriging) is a well-established surrogate
modelling technique for approximating expensive computational models. However,
it tends to get impractical for high-dimensional problems due to the large
inherent correlation matrix and the associated high-dimensional hyper-parameter
tuning problem. To address these issues, we propose a new method in this paper,
called sliced GE-Kriging (SGE-Kriging) for reducing both the size of the
correlation matrix and the number of hyper-parameters. Firstly, we perform a
derivative-based global sensitivity analysis to detect the relative importance
of each input variable with respect to model response. Then, we propose to
split the training sample set into multiple slices, and invoke Bayes' theorem
to approximate the full likelihood function via a sliced likelihood function,
in which multiple small correlation matrices are utilized to describe the
correlation of the sample set. Additionally, we replace the original
high-dimensional hyper-parameter tuning problem with a low-dimensional
counterpart by learning the relationship between the hyper-parameters and the
global sensitivity indices. Finally, we validate SGE-Kriging by means of
numerical experiments with several benchmarks problems. The results show that
the SGE-Kriging model features an accuracy and robustness that is comparable to
the standard one but comes at much less training costs. The benefits are most
evident in high-dimensional problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kai Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Ralf Zimmermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data blurring: sample splitting a single sample. (arXiv:2112.11079v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.11079</id>
        <link href="http://arxiv.org/abs/2112.11079"/>
        <updated>2022-04-09T00:48:54.964Z</updated>
        <summary type="html"><![CDATA[Suppose we observe a random vector $X$ from some distribution $P$ in a known
family with unknown parameters. We ask the following question: when is it
possible to split $X$ into two parts $f(X)$ and $g(X)$ such that neither part
is sufficient to reconstruct $X$ by itself, but both together can recover $X$
fully, and the joint distribution of $(f(X),g(X))$ is tractable? As one
example, if $X=(X_1,\dots,X_n)$ and $P$ is a product distribution, then for any
$m<n$, we can split the sample to define $f(X)=(X_1,\dots,X_m)$ and
$g(X)=(X_{m+1},\dots,X_n)$. Rasines and Young (2021) offers an alternative
route of accomplishing this task through randomization of $X$ with additive
Gaussian noise which enables post-selection inference in finite samples for
Gaussian distributed data and asymptotically for non-Gaussian additive models.
In this paper, we offer a more general methodology for achieving such a split
in finite samples by borrowing ideas from Bayesian inference to yield a
(frequentist) solution that can be viewed as a continuous analog of data
splitting. We call our method data blurring, as an alternative to data
splitting, data carving and p-value masking. We exemplify the method on a few
prototypical applications, such as post-selection inference for trend filtering
and other regression problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Leiner_J/0/1/0/all/0/1"&gt;James Leiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duan_B/0/1/0/all/0/1"&gt;Boyan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wasserman_L/0/1/0/all/0/1"&gt;Larry Wasserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning. (arXiv:2108.03706v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.03706</id>
        <link href="http://arxiv.org/abs/2108.03706"/>
        <updated>2022-04-09T00:48:54.961Z</updated>
        <summary type="html"><![CDATA[The recent emergence of reinforcement learning has created a demand for
robust statistical inference methods for the parameter estimates computed using
these algorithms. Existing methods for statistical inference in online learning
are restricted to settings involving independently sampled observations, while
existing statistical inference methods in reinforcement learning (RL) are
limited to the batch setting. The online bootstrap is a flexible and efficient
approach for statistical inference in linear stochastic approximation
algorithms, but its efficacy in settings involving Markov noise, such as RL,
has yet to be explored. In this paper, we study the use of the online bootstrap
method for statistical inference in RL. In particular, we focus on the temporal
difference (TD) learning and Gradient TD (GTD) learning algorithms, which are
themselves special instances of linear stochastic approximation under Markov
noise. The method is shown to be distributionally consistent for statistical
inference in policy evaluation, and numerical experiments are included to
demonstrate the effectiveness of this algorithm at statistical inference tasks
across a range of real RL environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ramprasad_P/0/1/0/all/0/1"&gt;Pratik Ramprasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuantong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1"&gt;Will Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14836</id>
        <link href="http://arxiv.org/abs/2106.14836"/>
        <updated>2022-04-09T00:48:54.953Z</updated>
        <summary type="html"><![CDATA[Representations of the world environment play a crucial role in artificial
intelligence. It is often inefficient to conduct reasoning and inference
directly in the space of raw sensory representations, such as pixel values of
images. Representation learning allows us to automatically discover suitable
representations from raw sensory data. For example, given raw sensory data, a
deep neural network learns nonlinear representations at its hidden layers,
which are subsequently used for classification at its output layer. This
happens implicitly during training through minimizing a supervised or
unsupervised loss. In this paper, we study the dynamics of such implicit
nonlinear representation learning. We identify a pair of a new assumption and a
novel condition, called the common model structure assumption and the
data-architecture alignment condition. Under the common model structure
assumption, the data-architecture alignment condition is shown to be sufficient
for the global convergence and necessary for the global optimality. Moreover,
our theory explains how and when increasing the network size does and does not
improve the training behaviors in the practical regime. Our results provide
practical guidance for designing a model structure: e.g., the common model
structure assumption can be used as a justification for using a particular
model structure instead of others. We also derive a new training framework,
which satisfies the data-architecture alignment condition by automatically
modifying any given training algorithm. Given a standard training algorithm,
the framework running its modified version is empirically shown to maintain
competitive test performances while providing global convergence guarantees for
deep residual neural networks with convolutions, skip connections, and batch
normalization with datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion,
KMNIST and SVHN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhun Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VNIbCReg: VICReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series. (arXiv:2204.02697v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2204.02697</id>
        <link href="http://arxiv.org/abs/2204.02697"/>
        <updated>2022-04-09T00:48:54.916Z</updated>
        <summary type="html"><![CDATA[One of the latest self-supervised learning (SSL) methods, VICReg, showed a
great performance both in the linear evaluation and the fine-tuning evaluation.
However, VICReg is proposed in computer vision and it learns by pulling
representations of random crops of an image while maintaining the
representation space by the variance and covariance loss. However, VICReg would
be ineffective on non-stationary time series where different parts/crops of
input should be differently encoded to consider the non-stationarity. Another
recent SSL proposal, Temporal Neighborhood Coding (TNC) is effective for
encoding non-stationary time series. This study shows that a combination of a
VICReg-style method and TNC is very effective for SSL on non-stationary time
series, where a non-stationary seismic signal time series is used as an
evaluation dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daesoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aune_E/0/1/0/all/0/1"&gt;Erlend Aune&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langet_N/0/1/0/all/0/1"&gt;Nad&amp;#xe8;ge Langet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eidsvik_J/0/1/0/all/0/1"&gt;Jo Eidsvik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidimensional linked matrix factorization for pan-omics pan-cancer analysis. (arXiv:2002.02601v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02601</id>
        <link href="http://arxiv.org/abs/2002.02601"/>
        <updated>2022-04-09T00:48:54.898Z</updated>
        <summary type="html"><![CDATA[Several modern applications require the integration of multiple large data
matrices that have shared rows and/or columns. For example, cancer studies that
integrate multiple omics platforms across multiple types of cancer, pan-omics
pan-cancer analysis, have extended our knowledge of molecular heterogenity
beyond what was observed in single tumor and single platform studies. However,
these studies have been limited by available statistical methodology. We
propose a flexible approach to the simultaneous factorization and decomposition
of variation across such bidimensionally linked matrices, BIDIFAC+. This
decomposes variation into a series of low-rank components that may be shared
across any number of row sets (e.g., omics platforms) or column sets (e.g.,
cancer types). This builds on a growing literature for the factorization and
decomposition of linked matrices, which has primarily focused on multiple
matrices that are linked in one dimension (rows or columns) only. Our objective
function extends nuclear norm penalization, is motivated by random matrix
theory, gives an identifiable decomposition under relatively mild conditions,
and can be shown to give the mode of a Bayesian posterior distribution. We
apply BIDIFAC+ to pan-omics pan-cancer data from TCGA, identifying shared and
specific modes of variability across 4 different omics platforms and 29
different cancer types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lock_E/0/1/0/all/0/1"&gt;Eric F. Lock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_J/0/1/0/all/0/1"&gt;Jun Young Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hoadley_K/0/1/0/all/0/1"&gt;Katherine A. Hoadley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. (arXiv:2004.10888v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.10888</id>
        <link href="http://arxiv.org/abs/2004.10888"/>
        <updated>2022-04-09T00:48:54.883Z</updated>
        <summary type="html"><![CDATA[We present a mean-variance policy iteration (MVPI) framework for risk-averse
control in a discounted infinite horizon MDP optimizing the variance of a
per-step reward random variable. MVPI enjoys great flexibility in that any
policy evaluation method and risk-neutral control method can be dropped in for
risk-averse control off the shelf, in both on- and off-policy settings. This
flexibility reduces the gap between risk-neutral control and risk-averse
control and is achieved by working on a novel augmented MDP directly. We
propose risk-averse TD3 as an example instantiating MVPI, which outperforms
vanilla TD3 and many previous risk-averse control methods in challenging Mujoco
robot simulation tasks under a risk-aware performance metric. This risk-averse
TD3 is the first to introduce deterministic policies and off-policy learning
into risk-averse reinforcement learning, both of which are key to the
performance boost we show in Mujoco domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shangtong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep transfer learning for system identification using long short-term memory neural networks. (arXiv:2204.03125v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2204.03125</id>
        <link href="http://arxiv.org/abs/2204.03125"/>
        <updated>2022-04-09T00:48:54.812Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks (RNNs) have many advantages over more traditional
system identification techniques. They may be applied to linear and nonlinear
systems, and they require fewer modeling assumptions. However, these neural
network models may also need larger amounts of data to learn and generalize.
Furthermore, neural networks training is a time-consuming process. Hence,
building upon long-short term memory neural networks (LSTM), this paper
proposes using two types of deep transfer learning, namely parameter
fine-tuning and freezing, to reduce the data and computation requirements for
system identification. We apply these techniques to identify two dynamical
systems, namely a second-order linear system and a Wiener-Hammerstein nonlinear
system. Results show that compared with direct learning, our method accelerates
learning by 10% to 50%, which also saves data and computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Niu_K/0/1/0/all/0/1"&gt;Kaicheng Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdallah_C/0/1/0/all/0/1"&gt;Chaouki T. Abdallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hayajneh_M/0/1/0/all/0/1"&gt;Mohammad Hayajneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTensor: Low-Rank Tensor Decomposition with Deep Network Priors. (arXiv:2204.03145v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2204.03145</id>
        <link href="http://arxiv.org/abs/2204.03145"/>
        <updated>2022-04-09T00:48:53.175Z</updated>
        <summary type="html"><![CDATA[DeepTensor is a computationally efficient framework for low-rank
decomposition of matrices and tensors using deep generative networks. We
decompose a tensor as the product of low-rank tensor factors (e.g., a matrix as
the outer product of two vectors), where each low-rank tensor is generated by a
deep network (DN) that is trained in a self-supervised manner to minimize the
mean-squared approximation error. Our key observation is that the implicit
regularization inherent in DNs enables them to capture nonlinear signal
structures (e.g., manifolds) that are out of the reach of classical linear
methods like the singular value decomposition (SVD) and principal component
analysis (PCA). Furthermore, in contrast to the SVD and PCA, whose performance
deteriorates when the tensor's entries deviate from additive white Gaussian
noise, we demonstrate that the performance of DeepTensor is robust to a wide
range of distributions. We validate that DeepTensor is a robust and
computationally efficient drop-in replacement for the SVD, PCA, nonnegative
matrix factorization (NMF), and similar decompositions by exploring a range of
real-world applications, including hyperspectral image denoising, 3D MRI
tomography, and image classification. In particular, DeepTensor offers a 6dB
signal-to-noise ratio improvement over standard denoising methods for signals
corrupted by Poisson noise and learns to decompose 3D tensors 60 times faster
than a single DN equipped with 3D convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Saragadam_V/0/1/0/all/0/1"&gt;Vishwanath Saragadam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Balestriero_R/0/1/0/all/0/1"&gt;Randall Balestriero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Veeraraghavan_A/0/1/0/all/0/1"&gt;Ashok Veeraraghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1"&gt;Richard G. Baraniuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiAuto-DeepONet: A Multi-resolution Autoencoder DeepONet for Nonlinear Dimension Reduction, Uncertainty Quantification and Operator Learning of Forward and Inverse Stochastic Problems. (arXiv:2204.03193v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03193</id>
        <link href="http://arxiv.org/abs/2204.03193"/>
        <updated>2022-04-09T00:48:53.167Z</updated>
        <summary type="html"><![CDATA[A new data-driven method for operator learning of stochastic differential
equations(SDE) is proposed in this paper. The central goal is to solve forward
and inverse stochastic problems more effectively using limited data. Deep
operator network(DeepONet) has been proposed recently for operator learning.
Compared to other neural networks to learn functions, it aims at the problem of
learning nonlinear operators. However, it can be challenging by using the
original model to learn nonlinear operators for high-dimensional stochastic
problems. We propose a new multi-resolution autoencoder DeepONet model referred
to as MultiAuto-DeepONet to deal with this difficulty with the aid of
convolutional autoencoder. The encoder part of the network is designed to
reduce the dimensionality as well as discover the hidden features of
high-dimensional stochastic inputs. The decoder is designed to have a special
structure, i.e. in the form of DeepONet. The first DeepONet in decoder is
designed to reconstruct the input function involving randomness while the
second one is used to approximate the solution of desired equations. Those two
DeepONets has a common branch net and two independent trunk nets. This
architecture enables us to deal with multi-resolution inputs naturally. By
adding $L_1$ regularization to our network, we found the outputs from the
branch net and two trunk nets all have sparse structures. This reduces the
number of trainable parameters in the neural network thus making the model more
efficient. Finally, we conduct several numerical experiments to illustrate the
effectiveness of our proposed MultiAuto-DeepONet model with uncertainty
quantification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiahao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel nonconvex, smooth-at-origin penalty for statistical learning. (arXiv:2204.03123v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2204.03123</id>
        <link href="http://arxiv.org/abs/2204.03123"/>
        <updated>2022-04-09T00:48:53.143Z</updated>
        <summary type="html"><![CDATA[Nonconvex penalties are utilized for regularization in high-dimensional
statistical learning algorithms primarily because they yield unbiased or nearly
unbiased estimators for the parameters in the model. Nonconvex penalties
existing in the literature such as SCAD, MCP, Laplace and arctan have a
singularity at origin which makes them useful also for variable selection.
However, in several high-dimensional frameworks such as deep learning, variable
selection is less of a concern. In this paper, we present a nonconvex penalty
which is smooth at origin. The paper includes asymptotic results for ordinary
least squares estimators regularized with the new penalty function, showing
asymptotic bias that vanishes exponentially fast. We also conducted an
empirical study employing deep neural network architecture on three datasets
and convolutional neural network on four datasets. The empirical study showed
better performance for the new regularization approach in five out of the seven
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+John_M/0/1/0/all/0/1"&gt;Majnu John&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vettam_S/0/1/0/all/0/1"&gt;Sujit Vettam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yihren Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Model Criticism of Variational Auto-Encoders. (arXiv:2204.03030v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03030</id>
        <link href="http://arxiv.org/abs/2204.03030"/>
        <updated>2022-04-09T00:48:53.135Z</updated>
        <summary type="html"><![CDATA[We propose a framework for the statistical evaluation of variational
auto-encoders (VAEs) and test two instances of this framework in the context of
modelling images of handwritten digits and a corpus of English text. Our take
on evaluation is based on the idea of statistical model criticism, popular in
Bayesian data analysis, whereby a statistical model is evaluated in terms of
its ability to reproduce statistics of an unknown data generating process from
which we can obtain samples. A VAE learns not one, but two joint distributions
over a shared sample space, each exploiting a choice of factorisation that
makes sampling tractable in one of two directions (latent-to-data,
data-to-latent). We evaluate samples from these distributions, assessing their
(marginal) fit to the observed data and our choice of prior, and we also
evaluate samples through a pipeline that connects the two distributions
starting from a data sample, assessing whether together they exploit and reveal
latent factors of variation that are useful to a practitioner. We show that
this methodology offers possibilities for model selection qualitatively beyond
intrinsic evaluation metrics and at a finer granularity than commonly used
statistics can offer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barkhof_C/0/1/0/all/0/1"&gt;Claartje Barkhof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1"&gt;Wilker Aziz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning. (arXiv:2204.03230v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2204.03230</id>
        <link href="http://arxiv.org/abs/2204.03230"/>
        <updated>2022-04-09T00:48:53.125Z</updated>
        <summary type="html"><![CDATA[We investigate and leverage a connection between Differential Privacy (DP)
and the recently proposed notion of Distributional Generalization (DG).
Applying this connection, we introduce new conceptual tools for designing
deep-learning methods that bypass "pathologies" of standard stochastic gradient
descent (SGD). First, we prove that differentially private methods satisfy a
"What You See Is What You Get (WYSIWYG)" generalization guarantee: whatever a
model does on its train data is almost exactly what it will do at test time.
This guarantee is formally captured by distributional generalization. WYSIWYG
enables principled algorithm design in deep learning by reducing
$\textit{generalization}$ concerns to $\textit{optimization}$ ones: in order to
mitigate unwanted behavior at test time, it is provably sufficient to mitigate
this behavior on the train data. This is notably false for standard (non-DP)
methods, hence this observation has applications even when privacy is not
required. For example, importance sampling is known to fail for standard SGD,
but we show that it has exactly the intended effect for DP-trained models.
Thus, with DP-SGD, unlike with SGD, we can influence test-time behavior by
making principled train-time interventions. We use these insights to construct
simple algorithms which match or outperform SOTA in several distributional
robustness applications, and to significantly improve the privacy vs. disparate
impact trade-off of DP-SGD. Finally, we also improve on known theoretical
bounds relating differential privacy, stability, and distributional
generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1"&gt;Bogdan Kulynych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yao-Yuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blasiok_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw B&amp;#x142;asiok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1"&gt;Preetum Nakkiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composite Spatial Monte Carlo Integration Based on Generalized Least Squares. (arXiv:2204.03248v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2204.03248</id>
        <link href="http://arxiv.org/abs/2204.03248"/>
        <updated>2022-04-09T00:48:49.945Z</updated>
        <summary type="html"><![CDATA[Although evaluation of the expectations on the Ising model is essential in
various applications, this is frequently infeasible because of intractable
multiple summations (or integrations). Spatial Monte Carlo integration (SMCI)
is a sampling-based approximation, and can provide high-accuracy estimations
for such intractable expectations. To evaluate the expectation of a function of
variables in a specific region (called target region), SMCI considers a larger
region containing the target region (called sum region). In SMCI, the multiple
summation for the variables in the sum region is precisely executed, and that
in the outer region is evaluated by the sampling approximation such as the
standard Monte Carlo integration. It is guaranteed that the accuracy of the
SMCI estimator is monotonically improved as the size of the sum region
increases. However, a haphazard expansion of the sum region could cause a
combinatorial explosion. Therefore, we hope to improve the accuracy without
such region expansion. In this study, based on the theory of generalized least
squares, a new effective method is proposed by combining multiple SMCI
estimators. The validity of the proposed method is demonstrated theoretically
and numerically. The results indicate that the proposed method can be effective
in the inverse Ising problem (or Boltzmann machine learning).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sekimoto_K/0/1/0/all/0/1"&gt;Kaiji Sekimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yasuda_M/0/1/0/all/0/1"&gt;Muneki Yasuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] MLOps vs Platform Engineering]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzhbxj/discussion_mlops_vs_platform_engineering/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzhbxj/discussion_mlops_vs_platform_engineering/"/>
        <updated>2022-04-09T00:27:17.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I have the opportunity to either move to the platform engineering team or the freshly created MLOps team within my company. I'm interested in both careers, as I like to build Infra. I'm currently a Data Eng, and I find myself to like building apps and enabling applications to talk to each other, better than cleaning up data. I worked as a data scientist before, but I didn't like the science. I was always into engineering. What would make sense from a career perspective (both long and short term), i.e., money, promotions, attractiveness, etc.
    submitted by    /u/dash2392  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any guesstimates for how much a DALLE 2 generation will eventually cost?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzg9p2/d_any_guesstimates_for_how_much_a_dalle_2/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzg9p2/d_any_guesstimates_for_how_much_a_dalle_2/"/>
        <updated>2022-04-08T23:29:44.000Z</updated>
        <summary type="html"><![CDATA[Just based on the estimated running costs of GPT3, and then whatever profit gets applied on top of that, are there any estimates for what openai will eventually charge for image generation?
    submitted by    /u/EugeneJudo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Problem with CVPR template and arXiv? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzf2r1/problem_with_cvpr_template_and_arxiv_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzf2r1/problem_with_cvpr_template_and_arxiv_d/"/>
        <updated>2022-04-08T22:29:46.000Z</updated>
        <summary type="html"><![CDATA[I don't know what would be the best place to post this. But I am having trouble uploading an Overleaf manuscript to arXiv based on the CVPR 2022 template. I am getting the following error. Does anyone have any ideas?
 ​
 https://preview.redd.it/y9jq0rbysds81.png?width=1614&format=png&auto=webp&s=16be3d32468837f649e846ec8a309dab2854c762
    submitted by    /u/avd4292  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL for dynamic environments]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tzekkb/rl_for_dynamic_environments/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tzekkb/rl_for_dynamic_environments/"/>
        <updated>2022-04-08T22:04:50.000Z</updated>
        <summary type="html"><![CDATA[In their 2019 review article in Nature Machine Intelligence, Neftci and Averbeck point out, “Most work in biological systems has focused on simple learning problems… where flexibility and ongoing learning are important, similar to real-world learning problems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment.”
 Are there RL approaches designed to handle dynamic environments with changing reward functions?
 I did find this earlier post, but thought I'd ask if anyone had other suggested lines of reading.
 Thanks!
    submitted by    /u/Careless-Argument-37  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Observationspace max Size?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tzee5s/observationspace_max_size/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tzee5s/observationspace_max_size/"/>
        <updated>2022-04-08T21:56:32.000Z</updated>
        <summary type="html"><![CDATA[I want to give my AI as many information as possible. Can there be an Issue with a too large observation space?
    submitted by    /u/Willing-Classroom735  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R]Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language - Google Apr 2022]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tze09r/rsocratic_models_composing_zeroshot_multimodal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tze09r/rsocratic_models_composing_zeroshot_multimodal/"/>
        <updated>2022-04-08T21:37:47.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2204.00598
 https://socraticmodels.github.io/
 Twitter: https://twitter.com/andyzengtweets/status/1512089759497269251
 Abstract: " Large foundation models can exhibit unique capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT questions). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue -- in whi…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responsible AI in a Global Context]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzdoaq/responsible_ai_in_a_global_context/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzdoaq/responsible_ai_in_a_global_context/"/>
        <updated>2022-04-08T21:21:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/john133435  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What to do next after the sanity check?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tzcnwb/d_what_to_do_next_after_the_sanity_check/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tzcnwb/d_what_to_do_next_after_the_sanity_check/"/>
        <updated>2022-04-08T20:33:36.000Z</updated>
        <summary type="html"><![CDATA[I have two years of time-series data taken from two sensors which I have split into 80/10/10 non-overlapping train/val/test splits. The task is to denoise one sensor data into another and I am handling it as a regression problem.
 I am following this website and considered an already published model (5 convolutional and 1 fully connected layer) which is trained on a similar dataset and same task.
 For the sake of sanity check, as per the website, I have trained the model on a subset of trainset (3 months) and tried to overfit it (while evaluating on complete val set), which works fine.
 However, I am not sure what to do next from this point on? Shall I just train on the complete trainset now? Or do I increase the layers or play with other hyper params to find more details about my regression problem/data? I would really appreciate your comments. Thank you.
 PS. The target value is sparse i.e. more than 85% of the time it is zero.
    submitted by    /u/muaz_usmani  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI website that transitions photos into video?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzc7xd/ai_website_that_transitions_photos_into_video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzc7xd/ai_website_that_transitions_photos_into_video/"/>
        <updated>2022-04-08T20:12:03.000Z</updated>
        <summary type="html"><![CDATA[Remember using a website like a year ago, where you could put in 2 or more images, and it would sort of make a transition between the two with AI. Then you could export the video and such. You could also very extensively edit human faces and change small features on a scale from 1-100. The features where incredibly specific like brow bone and nasal bridge.
 If anyone has the website I would appreciate it!!
    submitted by    /u/yungbenz0_bajs  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Artificial Intelligence Is Impacting Today’s Businesses]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzai2l/how_artificial_intelligence_is_impacting_todays/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzai2l/how_artificial_intelligence_is_impacting_todays/"/>
        <updated>2022-04-08T18:51:33.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mr_j_b  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alibaba’s AI tool to improve efficiency of China’s waste-to-energy plants]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tzahgt/alibabas_ai_tool_to_improve_efficiency_of_chinas/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tzahgt/alibabas_ai_tool_to_improve_efficiency_of_chinas/"/>
        <updated>2022-04-08T18:50:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mr_j_b  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Leaving ML for Software Engineering?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz9obh/d_leaving_ml_for_software_engineering/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz9obh/d_leaving_ml_for_software_engineering/"/>
        <updated>2022-04-08T18:12:46.000Z</updated>
        <summary type="html"><![CDATA[I'm keen to hear from people who have made the transition from ML Research/Engineering positions to software engineering roles (or who are considering it). What were your reasons for doing it and did you regret it? I see so many articles about transition from software to ML but none about going the opposite direction.
 Context: I've been working as an ML Engineer for a little over a year, and I'm just... not enjoying it. I want to love my job so badly as I like my boss, my colleagues, and the company (and I'm paid quite well for my level), but I just don't. I feel like the type of work I'm doing is not very smart and yet it's extremely draining: I spend so many hours just looking at loss curves, tweaking features and parameters. I'm somehow bored and stressed at the same time, because I don't enjoy the work and yet I feel the pressure to produce good models, and when they don't work as expected I can't help but take it personally as if if I just tried hard enough they would work. I find that the days were I end up having to take care of more purely engineering tasks I just have a lot more fun and I finish the day more satisfied and less drained. I think I just want to build something instead of spending hours banging my head against shit data. I would love to hear from people who feel or have felt the same way because whenever I speak about this with friends who are in ML they look at me like I'm a lunatic for wanting to leave it for software engineering.
 I'm obviously aware that swe roles are not all fun and games, but I just feel like there's been an excessive push for so many people to move to ML as it's "cool" and "smart" when in reality they're just different things who are going to suit different people.
    submitted by    /u/hedy-m  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best GAN for Tabular-data]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tz9gyq/best_gan_for_tabulardata/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tz9gyq/best_gan_for_tabulardata/"/>
        <updated>2022-04-08T18:03:19.000Z</updated>
        <summary type="html"><![CDATA[What in your opinion is the best GAN for tabular-data. Please include any references if you have any.
    submitted by    /u/ily_jk  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Matrix Factorization on TPUs]]></title>
        <id>http://ai.googleblog.com/2022/04/large-scale-matrix-factorization-on-tpus.html</id>
        <link href="http://ai.googleblog.com/2022/04/large-scale-matrix-factorization-on-tpus.html"/>
        <updated>2022-04-08T17:52:00.006Z</updated>
        <summary type="html"><![CDATA[Posted by Harsh Mehta, Software Engineer, Google Research 
Matrix factorization is one of the oldest, yet still widely used, techniques for learning how to recommend items such as songs or movies from user ratings. In its basic form, it approximates a large, sparse (i.e., mostly empty) matrix of user-item interactions with a product of two smaller, denser matrices representing learned item and user features. These dense matrices, in turn, can be used to recommend items to a user with which they haven't interacted before. 
Despite its algorithmic simplicity, matrix factorization can still achieve competitive performance in recommender benchmarks. Alternating least squares (ALS), and especially its implicit variation, is a fundamental algorithm to learn the parameters of matrix factorization…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supercharged UI for MLflow]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tz8xvn/supercharged_ui_for_mlflow/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tz8xvn/supercharged_ui_for_mlflow/"/>
        <updated>2022-04-08T17:38:40.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, we've built a plugin that seamlessly reads MLflow logs and provides a beautiful UI to compare multiple runs with just a few clicks. You can:
  
filter runs with a super versatile fully pythonic search
 group and aggregate your metrics / images
  
We are trying make it work seamlessly with MLflow and complement its other awesome features 🎉
 Here is more info about it https://aimstack.io/aimlflow Would love your feedback!!
    submitted by    /u/ManeSa  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rock On: Scientists Use AI to Improve Sequestering Carbon Underground]]></title>
        <id>https://blogs.nvidia.com/?p=56521</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/08/ai-improves-carbon-sequestration/"/>
        <updated>2022-04-08T17:36:44.000Z</updated>
        <summary type="html"><![CDATA[A team of scientists have created a new AI-based tool to help lock up greenhouse gases like CO2 in porous rock formations faster and more precisely than ever before. Carbon capture technology, also referred to as carbon sequestration, is a climate change mitigation method that redirects CO2 emitted from power plants back underground. While doing Read article >
The post Rock On: Scientists Use AI to Improve Sequestering Carbon Underground appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Isha Salian</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a custom entity recognizer for PDF documents using Amazon Comprehend]]></title>
        <id>0d01c636ffdbb651bb76c8c2f8ff1eb0118a73b1</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-custom-entity-recognizer-for-pdf-documents-using-amazon-comprehend/"/>
        <updated>2022-04-08T17:32:30.000Z</updated>
        <summary type="html"><![CDATA[In many industries, it’s critical to extract custom entities from documents in a timely manner. This can be challenging. Insurance claims, for example, often contain dozens of important attributes (such as dates, names, locations, and reports) sprinkled across lengthy and dense documents. Manually scanning and extracting such information can be error-prone and time-consuming. Rule-based software […]]]></summary>
        <author>
            <name>Joshua Levy</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Takeaways From 3 Years Working In Machine Learning]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tz8izm/takeaways_from_3_years_working_in_machine_learning/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tz8izm/takeaways_from_3_years_working_in_machine_learning/"/>
        <updated>2022-04-08T17:19:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/elcric_krej  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Getting started with the Amazon Kendra Box connector]]></title>
        <id>c460ad7425a2142b35a23d87bcece5f7ff3037a6</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/getting-started-with-the-amazon-kendra-box-connector/"/>
        <updated>2022-04-08T16:10:06.000Z</updated>
        <summary type="html"><![CDATA[Amazon Kendra is a highly accurate and easy-to-use intelligent search service powered by machine learning (ML). Amazon Kendra offers a suite of data source connectors to simplify the process of ingesting and indexing your content, wherever it resides. For many organizations, Box Content Cloud is a core part of their content storage and lifecycle management […]]]></summary>
        <author>
            <name>Bob Strahan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is virtual ICLR 2022 worth paying for]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz6izi/d_is_virtual_iclr_2022_worth_paying_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz6izi/d_is_virtual_iclr_2022_worth_paying_for/"/>
        <updated>2022-04-08T15:49:34.000Z</updated>
        <summary type="html"><![CDATA[The 2022 ICLR conference at the end of this month is virtual and costs $100 to attend. I was thinking of attending for networking opportunities but I’m not sure. Is it a good idea to go for it?
    submitted by    /u/sybar142857  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI 's new model DALL·E 2 is amazing!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tz5xqi/openai_s_new_model_dalle_2_is_amazing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tz5xqi/openai_s_new_model_dalle_2_is_amazing/"/>
        <updated>2022-04-08T15:21:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hilbert transform and Mathematica]]></title>
        <id>https://www.johndcook.com/blog/?p=101985</id>
        <link href="https://www.johndcook.com/blog/2022/04/08/hilbert-transform-and-mathematica/"/>
        <updated>2022-04-08T15:12:50.000Z</updated>
        <summary type="html"><![CDATA[The Hilbert transform of a function f(t) is a function fH(x) defined [1] by The integral must be interpreted in the sense of the Cauchy principal value: The integrand is not absolutely integrable because of the singularity at x and so the value of the integral depends on how you handle the singularity. The Cauchy […]
Hilbert transform and Mathematica first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Triplet vs. Contrastive Loss]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz50is/d_triplet_vs_contrastive_loss/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz50is/d_triplet_vs_contrastive_loss/"/>
        <updated>2022-04-08T14:38:54.000Z</updated>
        <summary type="html"><![CDATA[The online triplet mining strategy is more efficient than the offline one. It implies "getting a batch of n samples and their associated labels, and form triplets on the fly." Here is an article about Triplet vs. Contrastive Loss comparison and its efficient implementation. I would like to know your feedback.
    submitted by    /u/devzaya  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["UC Berkeley’s Pieter Abbeel receives 2021 ACM Prize in Computing" (for DRL robotics)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tz4u57/uc_berkeleys_pieter_abbeel_receives_2021_acm/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tz4u57/uc_berkeleys_pieter_abbeel_receives_2021_acm/"/>
        <updated>2022-04-08T14:30:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Animated Character Generator]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz4dic/p_animated_character_generator/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz4dic/p_animated_character_generator/"/>
        <updated>2022-04-08T14:08:32.000Z</updated>
        <summary type="html"><![CDATA[Hello everybody,
 I'd like to share the latest machine learning project of mine. It allows one to generate animated characters in the style of old video game consoles. Here are some examples. I would appreciate any feedback.
 https://i.redd.it/sig6ilpi8bs81.gif
 https://i.redd.it/xaf906qi8bs81.gif
 https://i.redd.it/8v7lz2qi8bs81.gif
    submitted by    /u/ie9res  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Annotation formats for image annotations?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz3x87/d_annotation_formats_for_image_annotations/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz3x87/d_annotation_formats_for_image_annotations/"/>
        <updated>2022-04-08T13:47:05.000Z</updated>
        <summary type="html"><![CDATA[Hey ML people, what is your favorite annotation format for image bounding boxes/labels? I know coco is very popular, we are rethinking parts of our data infrastructure wondering what everyone is using. Our platform hosts hundreds of millions of images. Ideal format would support running queries on data stored in a Data lake
 If the format supports 3D annotation types that is even better. Thanks for your insights in advance.
    submitted by    /u/mmuppidi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] OpenAI's DALL-E 2 paper "Hierarchical Text-Conditional Image Generation with CLIP Latents" has been updated with added section "Training details" (see Appendix C)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz3qc8/n_openais_dalle_2_paper_hierarchical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz3qc8/n_openais_dalle_2_paper_hierarchical/"/>
        <updated>2022-04-08T13:37:13.000Z</updated>
        <summary type="html"><![CDATA[New version of paper is linked to in the DALL-E 2 blog post and also here (pdf file format).
 Tweet announcing updated paper.
 Older version of paper (pdf file format).
 Original Reddit post.
    submitted by    /u/Wiskkey  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Passage Retriever(DPR) Open-QA System [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz32ub/dense_passage_retrieverdpr_openqa_system_p/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz32ub/dense_passage_retrieverdpr_openqa_system_p/"/>
        <updated>2022-04-08T13:03:32.000Z</updated>
        <summary type="html"><![CDATA[Hi, I made a video explaining Dense Passage Retriever(DPR) paper. We specifically explain the End to End QA system suggested in the latter part of the paper which discusses how to build an Open-QA system using dense retrievers.
 DPR was one of the first papers that discussed building dense retrievers using QA pairs only and didn't require a big pretraining computational setup like ORQA or REALM. It is currently used in a lot of places as a dense retriever. You can find Hugginface and Haystack implementations also.
 This video is part of a series on Open-QA using dense retrievers. We have made 2 videos on DPR. In the latter, we discuss how to build a dense retriever from scratch. Thanks for the support and it would be great if you could give any feedback.
 https://www.youtube.com/watch?v=rvcyyJNjPU0
    submitted by    /u/infiniteakashe  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Passage Retriever(DPR) Open-QA System]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tz32az/dense_passage_retrieverdpr_openqa_system/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tz32az/dense_passage_retrieverdpr_openqa_system/"/>
        <updated>2022-04-08T13:02:42.000Z</updated>
        <summary type="html"><![CDATA[Hi, I made a video explaining Dense Passage Retriever(DPR) paper. We specifically explain the End to End QA system suggested in the latter part of the paper which discusses how to build an Open-QA system using dense retrievers.
 DPR was one of the first papers that discussed building dense retrievers using QA pairs only and didn't require a big pretraining computational setup like ORQA or REALM. It is currently used in a lot of places as a dense retriever. You can find Hugginface and Haystack implementations also.
 This video is part of a series on Open-QA using dense retrievers. We have made 2 videos on DPR. In the latter, we discuss how to build a dense retriever from scratch. Thanks for the support and it would be great if you could give any feedback.
 https://www.youtube.com/watch?v=rvcyyJNjPU0
    submitted by    /u/infiniteakashe  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI in a jar]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tz31sv/the_ai_in_a_jar/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tz31sv/the_ai_in_a_jar/"/>
        <updated>2022-04-08T13:01:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bendee983  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Works that can process variable input resolution of images]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz2o0b/d_works_that_can_process_variable_input/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz2o0b/d_works_that_can_process_variable_input/"/>
        <updated>2022-04-08T12:41:56.000Z</updated>
        <summary type="html"><![CDATA[Hi. I'm looking for existing computer vision papers /networks that can process variable input resolution. Can anyone share me with similar works? For example, a network/layer N can take both inputs with H*W and 2H*2H individually and give correct prediction. One of them I know is ROI pooling used in Faster RCNN. Thanks very much.
    submitted by    /u/vincent341  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Computers Learn Common Sense?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tz1tbc/can_computers_learn_common_sense/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tz1tbc/can_computers_learn_common_sense/"/>
        <updated>2022-04-08T11:54:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/estasfuera  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual integration]]></title>
        <id>https://www.johndcook.com/blog/?p=101943</id>
        <link href="https://www.johndcook.com/blog/2022/04/08/visual-integration/"/>
        <updated>2022-04-08T11:10:02.000Z</updated>
        <summary type="html"><![CDATA[The plot below is of a meromorphic function f(z). That is, the function f(z) is analytic except possibly at poles, and the colors represent the phase angles, the values of θ if you write the function values in polar form. What is the value of the integral where C is the perimeter of the square? […]
Visual integration first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine Learning Engineers - What Does Your Day Involve?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tz05tg/d_machine_learning_engineers_what_does_your_day/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tz05tg/d_machine_learning_engineers_what_does_your_day/"/>
        <updated>2022-04-08T10:06:51.000Z</updated>
        <summary type="html"><![CDATA[Hey, I'm looking to transition from my current role as a data scientist to one that has a machine learning engineering focus. I was wondering if anyone could provide insights into how they plan their day, or what activities you do throughout the day/week. I'd be particularly interested to understand the balance between deploying models/writing production worthy code and your time spent learning/developing given the field is moving so fast.
    submitted by    /u/MenArePigs69  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metaverse weekly digest: Shiba Inu’s metaverse, Alibaba’s $60 million VR investment]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tyyrzl/metaverse_weekly_digest_shiba_inus_metaverse/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tyyrzl/metaverse_weekly_digest_shiba_inus_metaverse/"/>
        <updated>2022-04-08T08:23:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bent_out_of_shape_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Space Dimensional reduction for better convergence]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tyxrzi/action_space_dimensional_reduction_for_better/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tyxrzi/action_space_dimensional_reduction_for_better/"/>
        <updated>2022-04-08T07:08:41.000Z</updated>
        <summary type="html"><![CDATA[I am working on a project in which a robots learns its motion. for example Bipedal robot learns to walk on a straight line by learning to adjust the torque and angular velocity of each joint. However, the robot I am working on has complex architecture. It has 10 joints instead of 2, most importantly all of these joints work simultaneously and coherently to produce a desired motion.
 The Problem I am facing is that, the robot has ten joints and each joint can move between -450 to +450
 for simplicity let me define State and Actions of the system
 State = -450 to +450 -------------> normalization ------------------> -10 to 10
 Actions = choose an angle between -10 to 10 for each joint 
 Total Action space for each State at each time step = 10 (no of joints each can move between -10 to 10 at each time step) * 360 (total time steps for a single motion) = 3600 (Output: No of angles required to generate a motion)
 I am using TD3 to solve this conundrum. The Action space is too large how can I reduce the action space?
    submitted by    /u/SAM_Baloch  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence: Benefits for Automation Testing]]></title>
        <id>https://www.datasciencecentral.com/?p=57118</id>
        <link href="https://www.datasciencecentral.com/artificial-intelligence-benefits-for-automation-testing/"/>
        <updated>2022-04-08T06:38:59.000Z</updated>
        <summary type="html"><![CDATA[AI has been making a lot of noise of late, especially in the context of software development. Of course, this topic is quite wide, but in this article, we shall focus our attention on AI-driven automation testing. Let us start with understanding what is AI and automation testing. Automation testing refers to the process of… Read More »Artificial Intelligence: Benefits for Automation Testing
The post Artificial Intelligence: Benefits for Automation Testing appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ryan Williamson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Bayesian Non-Parametrics for Ranking?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tywuga/d_bayesian_nonparametrics_for_ranking/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tywuga/d_bayesian_nonparametrics_for_ranking/"/>
        <updated>2022-04-08T06:03:54.000Z</updated>
        <summary type="html"><![CDATA[I am currently sitting at a difficult machine-learning problem that I have found no literature on how to solve it. 
 I am given n datapoints x_1,...,x_n that are ordered according to a ranking preference rank(x_1)<rank(x_2)<...<rank(x_n). I am assuming there exists a function f, such, that f(x_i)<f(x_i+1). I am now searching a Bayesian non-parametric model that gives the posterior probability of functions f that abide f(x_i)<f(x_i+1), so that i can estimate the relative rank preferences at new points.
 I have tried out a few things. The naive approach is using a GP prior on f. Unfortunately, computing the posterior distribution p(f(x_1), ... f(x_n)| f(x_1)<...<f(x_n)) has no closed form solution (it is a normal distribution with N linear constraints, which is absolutely terrible to sample from). This makes computing conditional distributions for predictions very challenging. 
 I am currently approximating the solution by using a GP regression model with label y_i = rank(x_i)=i. But this is systematically under-estimating the shape-variation, due to the fact that it adds the assumption that function values between ranks are equidistant. 
 Is there any known approach how to do this?
    submitted by    /u/Ulfgardleo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Video Diffusion Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tywmy0/r_video_diffusion_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tywmy0/r_video_diffusion_models/"/>
        <updated>2022-04-08T05:50:09.000Z</updated>
        <summary type="html"><![CDATA[From the webpage:
 We present results on video generation using diffusion models. We propose an architecture for video diffusion models which is a natural extension of the standard image architecture. We show that this architecture is effective for jointly training from image and video data. To generate long and higher resolution videos we introduce a new conditioning technique that performs better than previously proposed methods. We present results on text-conditioned video generation and state-of-the-art results on an unconditional video generation benchmark.
 Paper: https://arxiv.org/abs/2204.03458
 https://video-diffusion.github.io/
    submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] how to decide publication venue]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyv2tu/d_how_to_decide_publication_venue/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyv2tu/d_how_to_decide_publication_venue/"/>
        <updated>2022-04-08T04:12:23.000Z</updated>
        <summary type="html"><![CDATA[How to decide if a paper is appropriate for a specific venue? Moreover, how would you categorize the difference between a good NiPs publication and a good CvPR or ICCV publication?
    submitted by    /u/LifeguardDismal142  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet ‘ChestLink’, The First Autonomous AI Medical Imaging Application by ‘Oxipit’ That Received CE Mark Approval in the EU]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tyuzhg/meet_chestlink_the_first_autonomous_ai_medical/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tyuzhg/meet_chestlink_the_first_autonomous_ai_medical/"/>
        <updated>2022-04-08T04:06:41.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/e2q3jit3c8s81.png?width=1024&format=png&auto=webp&s=837aa6256647df6fb8777a02b04313a38428f573
 The most common diagnostic imaging test conducted in emergency rooms is chest radiography. Providing automated preliminary read helpers to physicians might speed up surgery, enhance accuracy, and lower healthcare costs.
 An artificial intelligence tool that interprets chest X-rays without the intervention of a radiologist received regulatory approval in the European Union this week, marking a first for a wholly autonomous medical imaging AI, according to ‘Oxipit‘, the developer of this tool. It’s a watershed moment for AI, and it’s more than likely to spark debate, given that radiologists have spent the last few years working to fully automate parts of their jobs.
 Continue Reading
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] LR Warmup for PyTorch]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyu1n6/n_lr_warmup_for_pytorch/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyu1n6/n_lr_warmup_for_pytorch/"/>
        <updated>2022-04-08T03:13:36.000Z</updated>
        <summary type="html"><![CDATA[​
 RadamWarmup + CosineAnnealingLR + StepLR
 Colab Link
 pytorch_warmup v0.1.0 was released.
    submitted by    /u/TonyY_RIMCS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic action space in RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tytrpx/dynamic_action_space_in_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tytrpx/dynamic_action_space_in_rl/"/>
        <updated>2022-04-08T02:58:27.000Z</updated>
        <summary type="html"><![CDATA[I am doing a project and there is a problem with dynamic action space
 A complete action space can be divided into four parts. In each state, the action to be selected is one of them
 For example, the total discrete action space length is 1000, which can be divided into four parts, [0:300], [301:500],[501:900],[901:1000]
 For state 1, action_ space is [0:300], State2, action_ space is [301:500], etc
 For this idea, I have several ideas at present:
  
There is no restriction at all. The legal actions of all States are [1:1000], but it may take longer train time and there is not much innovation
 Soft constraint, for example, if state1 selects an illegal action, such as one action in [251: 500], reward gives a negative value, but it is also not innovative
 Hard constraint, use action space mask in each state, but I don't know how to do it.. Is there any relevant article？
 It is directly divided into four action spaces and uses multi-agent cooperative relationship learning
  
​
 Any suggestions？
 thanks！
    submitted by    /u/RangerWYR  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any paper suggestions??]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tyqv88/any_paper_suggestions/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tyqv88/any_paper_suggestions/"/>
        <updated>2022-04-08T00:22:24.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, i have to define a project for my master degree, so i'm looking for the best papers published since 2018-2019 until now in Reinforcement Learning . Do you have any suggestions, titles or projects that i can check?
    submitted by    /u/acaviedes15  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend the 2022 National Autonomous Vehicle Expo (April 16-17th)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tyqc1f/attend_the_2022_national_autonomous_vehicle_expo/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tyqc1f/attend_the_2022_national_autonomous_vehicle_expo/"/>
        <updated>2022-04-07T23:55:18.000Z</updated>
        <summary type="html"><![CDATA[Interested in the future of autonomous vehicles? Want to know more about the impacts of this technology? Join us on April 16-17th at the 2022 National Autonomous Vehicle Expo to discover the engineering, ethics, and policymaking of this emerging technology. The virtual expo consists of speaker and workshop sessions led by industry-leading companies, such as NVIDIA, Waymo, and Motional, as well as distinguished programs/organizations like MIT Beaverworks and InspiritAI. You will also have the opportunity to compete in our hackathon, where you can win a variety of cool prizes! Even if you don't participate in the hackathon, there will be free merchandise and giveaways throughout the expo! To register and/or view more information about the event, head over to avexpo.org. For hackathon-specific registration, you can visit our devpost at https://autonomous-vehicle-expo.devpost.com/. Hope to see you all there!
 ​
 https://preview.redd.it/qgfx3sv837s81.png?width=1080&format=png&auto=webp&s=ed19d68bdff274de188deaa8f4338c864943b508
 https://preview.redd.it/a4kbsrv837s81.png?width=1080&format=png&auto=webp&s=6a19347b708d822d8dca226beb82cbdc73ffbb87
 https://preview.redd.it/s9nghsv837s81.png?width=1080&format=png&auto=webp&s=cf32712b9985564b455be53e02fd00589725ad2c
    submitted by    /u/avexpo22  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implementation of RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tyog3i/implementation_of_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tyog3i/implementation_of_rl/"/>
        <updated>2022-04-07T22:21:20.000Z</updated>
        <summary type="html"><![CDATA[Hi all! I am a beginner in RL field and am trying to implement the RL algorithm in the following paper :
 [1912.04321] Learning to Code: Coded Caching via Deep Reinforcement Learning (arxiv.org)
 In short, we are trying to achieve the minimum number of transmission of bits from the server to all users
 Now, after 500 episodes of training the number of transmissions does decrease. But when I implement the same actor critic algorithm this does not happen. In fact, the results seem to be completely random. 
 Here is the plot of the same :
 https://preview.redd.it/yve3cay7l6s81.png?width=745&format=png&auto=webp&s=36bf4f0aa813a30024128d797acde3b8adc2df30
 Although my training parameters are slightly different, I can't understand why this would happen.
 I used the parameters and pseudo code from this paper: A Deep Reinforcement Learning Approach for Shared Caching | IEEE Conference Publication | IEEE Xplore - which is an extension of the link at the top.
 ​
 Attaching link to my code: https://www.kaggle.com/samarthtiwari123/rl-for-coded-caching
 Any help would be helpful!!
 Thanks in advance
    submitted by    /u/samt_123  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] self attention visualization]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyo9qd/d_self_attention_visualization/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyo9qd/d_self_attention_visualization/"/>
        <updated>2022-04-07T22:13:00.000Z</updated>
        <summary type="html"><![CDATA[Has anyone ever come across seemingly chaotic self attention maps during visualization. If your model is performing well but no insights can be gleaned from the visualization how do you explain it in a paper?
    submitted by    /u/LifeguardDismal142  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can I extract the direction a specific agent is facing?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tynq4i/how_can_i_extract_the_direction_a_specific_agent/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tynq4i/how_can_i_extract_the_direction_a_specific_agent/"/>
        <updated>2022-04-07T21:47:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] PaLM's (Google's 530B LLM) training costs around $9M to $17M.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyn0yt/n_palms_googles_530b_llm_training_costs_around_9m/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyn0yt/n_palms_googles_530b_llm_training_costs_around_9m/"/>
        <updated>2022-04-07T21:14:22.000Z</updated>
        <summary type="html"><![CDATA[Here's the blogpost estimating the cost.
 What would it cost you to train PaLM using cloud computing (and you're not Google)? Something around $9M to $17M.
    submitted by    /u/cirqe  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Feature selection methods]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tymu9e/d_feature_selection_methods/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tymu9e/d_feature_selection_methods/"/>
        <updated>2022-04-07T21:05:46.000Z</updated>
        <summary type="html"><![CDATA[I'm working on a ML project and I'm working with a dataset with 20 columns, for feature selection I just removed one column one by one and looked at the error of the ML outputs for each, then saw when what column is removed gives a lower error and kept repeating that but that didn't seem to help the model at all and the error went down very little. Is this an okay way of doing feature selection is there another way that gives better results. I tried PCA and LDA and Pearson Correlation method as well in Python and that didn't seem to help or is this the best I could do. Thanks!
    submitted by    /u/ihshosv  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VDTTS: Visually-Driven Text-To-Speech]]></title>
        <id>http://ai.googleblog.com/2022/04/vdtts-visually-driven-text-to-speech.html</id>
        <link href="http://ai.googleblog.com/2022/04/vdtts-visually-driven-text-to-speech.html"/>
        <updated>2022-04-07T20:45:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Tal Remez, Software Engineer, Google Research and Micheal Hassid, Software Engineer Intern, Google Research   
Recent years have seen a tremendous increase in the creation and serving of video content to users across the world in a variety of languages and over numerous platforms. The process of creating high quality content can include several stages from video capturing and captioning to  video and audio editing. In some cases dialogue is re-recorded (referred to as dialog replacement, post-sync or dubbing) in a studio in order to achieve high quality and replace original audio that might have been recorded in noisy conditions. However, the dialog replacement process can be difficult and tedious because the newly recorded audio needs to be well synced with the video, requiring …]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flow of gradients through multiple classes]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tylf8u/flow_of_gradients_through_multiple_classes/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tylf8u/flow_of_gradients_through_multiple_classes/"/>
        <updated>2022-04-07T20:01:03.000Z</updated>
        <summary type="html"><![CDATA[Naive question: if I define my RL model as a combination of different classes (one class that preprocesses the observation, one class that processes the observation, one class that outputs the actions, etc.), is this going to affect the flow of gradients in PyTorch? The alternative would be to create only one class in which I combine everything
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Overfitting a sign high learning capacity?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tykn7y/d_overfitting_a_sign_high_learning_capacity/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tykn7y/d_overfitting_a_sign_high_learning_capacity/"/>
        <updated>2022-04-07T19:24:50.000Z</updated>
        <summary type="html"><![CDATA[This is a two part question:
  
If a neural network can overfit the a large dataset is this a sign that a neural network has high learning capacity? 
 If a neural network can overfit a dataset with substantially less parameters than other neural networks developed for the same learning task is this a sign that the neural network has a high learning capacity relative to other datasets?
  
   submitted by    /u/LifeguardDismal142  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resources about cognitive theories]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tyjqk5/resources_about_cognitive_theories/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tyjqk5/resources_about_cognitive_theories/"/>
        <updated>2022-04-07T18:43:22.000Z</updated>
        <summary type="html"><![CDATA[Hi! I am new to the community, and was wondering what y'all's favorite resources were to learn about cognitive theories and how they will shape future AI advancements.
 YouTube channels would be great.
    submitted by    /u/Apprehensive-Candy97  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] training cnn with synthetic data. Should i mix synth and real and train from the scratch or pretrain the network with synth and finetune with real?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyjj3z/d_training_cnn_with_synthetic_data_should_i_mix/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyjj3z/d_training_cnn_with_synthetic_data_should_i_mix/"/>
        <updated>2022-04-07T18:34:01.000Z</updated>
        <summary type="html"><![CDATA[I'm doing research on the use of synthetic data for a computer vision task and i have generally always tried to train in a mixed setting from scratch, but i have noticed that in similar papers, researchers always pretrain on synth first and then finetune on real data. Is there a logic behind that? Should i expect better results by finetuning?
    submitted by    /u/TheManveru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Andrew Yang & Yuval Noah Harari: Tech, Public Policy & the Future of Work]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tyj9ds/andrew_yang_yuval_noah_harari_tech_public_policy/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tyj9ds/andrew_yang_yuval_noah_harari_tech_public_policy/"/>
        <updated>2022-04-07T18:21:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/john133435  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI News | AI News | Why AI Made 40,000 New Chemical Weapons Compounds in 6 Hours | Cancer Treatment AI Breakthrough]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tyi240/ai_news_ai_news_why_ai_made_40000_new_chemical/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tyi240/ai_news_ai_news_why_ai_made_40000_new_chemical/"/>
        <updated>2022-04-07T17:26:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/getrich_or_diemining  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Sampling in Dirichlet Process Mixture Models for Clustering Streaming Data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyhxl2/r_sampling_in_dirichlet_process_mixture_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyhxl2/r_sampling_in_dirichlet_process_mixture_models/"/>
        <updated>2022-04-07T17:20:31.000Z</updated>
        <summary type="html"><![CDATA[Hi Everyone,
 We have recently published the code for our AISTATS 2022 paper -
 Sampling in Dirichlet Process Mixture Models for Clustering Streaming Data
 ​
 Video Segmentation Example
 In our work, we have proposed a solution for clustering streaming data. Unlike 'standard' clustering scenarios, in the streaming case the data stream is possibly infinite, you cannot backtrack to previously processed points, and the data statistics are dynamic and change over time.
 Our solution is based on the Dirichlet Process Mixture Model (DPMM), can work with different types of observations, and is very fast, outperforming other methods both in the quality of the results and the speed with which it achieves them.
 It can even be distributed across several processes and/or machines!
  
Paper: https://dinarior.github.io/papers/Dinari_AISTATS_streaming.pdf
 Code (Julia Package): https://github.com/BGU-CS-VIL/DPMMSubClustersStreaming.jl
 Code (Python wrapper): https://github.com/BGU-CS-VIL/dpmmpythonStreaming
 Notebook (Julia) for creating the video: https://nbviewer.org/github/BGU-CS-VIL/DPMMSubClustersStreaming.jl/blob/main/examples/VideoSeg.ipynb
  
   submitted by    /u/dinarior  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to create a BOT for a existing game?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tygoz1/how_to_create_a_bot_for_a_existing_game/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tygoz1/how_to_create_a_bot_for_a_existing_game/"/>
        <updated>2022-04-07T16:21:40.000Z</updated>
        <summary type="html"><![CDATA[I wanna create a bot for a game which basically is: get resources, craft itens, sell then.
 The problem is, some itens has different qualities, and I wanna automatize this process, to identify the good stuff to keep, and sell the bad stuff.
 What's the best way to do that? 
 I work with desktop systems, so i'm not familiar with this kind of stuff, but I usually read about python and some frameworks, what do you guys recommend me to start?
    submitted by    /u/AbbathDoom  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best way to handle encoding disconnected graphs at the graph level.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyfltn/d_best_way_to_handle_encoding_disconnected_graphs/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyfltn/d_best_way_to_handle_encoding_disconnected_graphs/"/>
        <updated>2022-04-07T15:31:10.000Z</updated>
        <summary type="html"><![CDATA[I am thinking of building a graph classifier that takes in graphs and labels the incoming graph.
 The dataset of interest to me is RadGraph: https://arxiv.org/abs/2106.14463
 The issue I am having is that the graphs in RadGraph are disconnected in nature (on average 20 disconnected components), making it difficult for the various graph encoders I am aware of to do a good job classifying the graphs.
    submitted by    /u/AICoderGamer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does someone know how much faster deepspeed's transformer implementation is?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tyfj7d/d_does_someone_know_how_much_faster_deepspeeds/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tyfj7d/d_does_someone_know_how_much_faster_deepspeeds/"/>
        <updated>2022-04-07T15:27:53.000Z</updated>
        <summary type="html"><![CDATA[Implementation here
 Looks like they manually calculate the gradient? I'm very curious how much of a difference this makes!
    submitted by    /u/fasttosmile  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DALL·E 2: A new AI system to create realistic images and art from natural language commands]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tyfdj8/dalle_2_a_new_ai_system_to_create_realistic/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tyfdj8/dalle_2_a_new_ai_system_to_create_realistic/"/>
        <updated>2022-04-07T15:20:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/alien128  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Observability: Cracking the Code]]></title>
        <id>https://www.datasciencecentral.com/?p=57114</id>
        <link href="https://www.datasciencecentral.com/data-observability-cracking-the-code/"/>
        <updated>2022-04-07T15:12:45.000Z</updated>
        <summary type="html"><![CDATA[‍What is the shortest distance between two points? A straight line of course. What if there are multiple points? Then, it depends.  A job executed in response to a user action – refreshing a dashboard, aggregating data, building a report, developing an ML algorithm, performing analytics – all require multiple hops through the data ecosystem.… Read More »Data Observability: Cracking the Code
The post Data Observability: Cracking the Code appeared first on Data Science Central.]]></summary>
        <author>
            <name>Sameer Narkhede</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NN from Scratch: #2 Initializing parameters | Kolbenkraft]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tye21i/nn_from_scratch_2_initializing_parameters/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tye21i/nn_from_scratch_2_initializing_parameters/"/>
        <updated>2022-04-07T14:17:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/cjmodi306  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular expressions and successive approximation]]></title>
        <id>https://www.johndcook.com/blog/?p=101905</id>
        <link href="https://www.johndcook.com/blog/2022/04/07/regex-approximation/"/>
        <updated>2022-04-07T14:00:43.000Z</updated>
        <summary type="html"><![CDATA[Regular expressions can do a lot of tasks in practice that they cannot do in theory. That’s because a particular application of regular expressions comes with context and with error tolerance. For example, much has been said about how regular expressions cannot parse HTML. This is strictly true, but it says nothing about how well […]
Regular expressions and successive approximation first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Try This Out: GFN Thursday Delivers Instant-Play Game Demos on GeForce NOW]]></title>
        <id>https://blogs.nvidia.com/?p=56490</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/07/geforce-now-thursday-april-7/"/>
        <updated>2022-04-07T13:00:01.000Z</updated>
        <summary type="html"><![CDATA[GeForce NOW is about bringing new experiences to gamers. This GFN Thursday introduces game demos to GeForce NOW. Members can now try out some of the hit games streaming on the service before purchasing the full PC version — including some finalists from the 2021 Epic MegaJam. Plus, look for six games ready to stream Read article >
The post Try This Out: GFN Thursday Delivers Instant-Play Game Demos on GeForce NOW appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can KL divergence be used as a metric to see the learning progress in PPO?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tycj9d/can_kl_divergence_be_used_as_a_metric_to_see_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tycj9d/can_kl_divergence_be_used_as_a_metric_to_see_the/"/>
        <updated>2022-04-07T12:59:23.000Z</updated>
        <summary type="html"><![CDATA[In the hyper parameter section of the paper, it is written that step size of Adam is varied according to KL divergence. So I wanted to know is KL divergence the correct metric to be used for observing the learning progress because we have many states for which probabilites of a particular action is either increased or decreased thus taking average KL mixes up a lot of things.
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are other "technology" fields that is good to learn while studying AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty99as/what_are_other_technology_fields_that_is_good_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty99as/what_are_other_technology_fields_that_is_good_to/"/>
        <updated>2022-04-07T09:31:30.000Z</updated>
        <summary type="html"><![CDATA[Hello! What do you guys think are other "technology" fields that would be good to study with AI? It is okay as long as it is "tech." What would be the tech field that would be beneficial in the future? My goal is to make a self-aware AI (AGI). I was always fascinated about AI since my childhood, that's why I'm going to pursue this field. Also, I am currently studying Game Development to make a VR Game that hopefully will have humanlike AI in it. I have read a LOT of articles about the future of AI, and Cybersecurity keeps popping up because superintelligent AI needs to be CONTROLLED from hackers (based on the articles) otherwise it is over. What do you guys think would be the tech field that will bring the most changes in the future?
    submitted by    /u/ThatOneEpicAstronaut  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does this artificial intelligence think like a human?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty964g/does_this_artificial_intelligence_think_like_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty964g/does_this_artificial_intelligence_think_like_a/"/>
        <updated>2022-04-07T09:25:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/qptbook  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial intelligence Courses for Healthcare]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty95tr/artificial_intelligence_courses_for_healthcare/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty95tr/artificial_intelligence_courses_for_healthcare/"/>
        <updated>2022-04-07T09:24:33.000Z</updated>
        <summary type="html"><![CDATA[We keep on hearing about how artificial intelligence and machine learning is going to revolutionise Medicine.
 But what’s hype, and what’s realistic? And how can you get involved?
 The first step is to understand the technology - where it’s well-suited to healthcare (and where it isn’t).
 When it comes to health care, especially for life and death situations AI has made things very easy for us. However, it is still expected to drastically change the way medicine is practised. It will also replace the surgeries done by the doctors with the surgeries done using Artificial intelligence, making diagnosing complex diseases, genetic issues and many other health problems extremely easy in the future. Here are the best Artificial Intelligence courses for healthcare you can learn in 2022.
    submitted by    /u/maneesh123456  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] My research group is publicly sharing its paper presentations! Check it out!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty8oi7/r_my_research_group_is_publicly_sharing_its_paper/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty8oi7/r_my_research_group_is_publicly_sharing_its_paper/"/>
        <updated>2022-04-07T08:47:55.000Z</updated>
        <summary type="html"><![CDATA[https://outsystems-ai-reading-group.github.io/
    submitted by    /u/JClub  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] On-the-fly Strategy Adaptation for ad-hoc Agent Coordination]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty7h2p/r_onthefly_strategy_adaptation_for_adhoc_agent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty7h2p/r_onthefly_strategy_adaptation_for_adhoc_agent/"/>
        <updated>2022-04-07T07:15:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any good free to use DALL-E style datasets?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty6hee/d_any_good_free_to_use_dalle_style_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty6hee/d_any_good_free_to_use_dalle_style_datasets/"/>
        <updated>2022-04-07T06:06:51.000Z</updated>
        <summary type="html"><![CDATA[Are there any free to use datasets that contain image/annotation pairs in the style OpenAI used to train the DALL-E models? Pretty inspired by DALL-E 2 and think it would be cool to create a tiny less powerful replication
    submitted by    /u/puppet_pals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] TensorFlow tf.range() vs range()]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty6g23/d_tensorflow_tfrange_vs_range/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty6g23/d_tensorflow_tfrange_vs_range/"/>
        <updated>2022-04-07T06:04:17.000Z</updated>
        <summary type="html"><![CDATA[TLDR: TensorFlow AutoGraph unwraps native Python ranges, baking each value into the graph. This can be an unexpected cause of graph size explosion. 
 This recently caused an issue in my project, so I thought I'd share some more details:
 https://lukewood.xyz/blog/to-unroll-or-to-not-unroll
    submitted by    /u/puppet_pals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares: Smithing Stone 6 || Clip Guided Diffusion AI Art Video [4K 20 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty4l6x/artificial_nightmares_smithing_stone_6_clip/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty4l6x/artificial_nightmares_smithing_stone_6_clip/"/>
        <updated>2022-04-07T04:08:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] A benchmarking framework for time-series unsupervised domain adaptation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty4gw4/r_a_benchmarking_framework_for_timeseries/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty4gw4/r_a_benchmarking_framework_for_timeseries/"/>
        <updated>2022-04-07T04:01:52.000Z</updated>
        <summary type="html"><![CDATA[Our work "AdaTime: A Systematic Evaluation of Domain Adaptation Algorithms on Time Series Data" is now public. We provide a benchmarking framework named "AdaTime" to fairly evaluate Unsupervised domain adaptation (UDA) approaches on time-series data. We find that UDA approaches proposed for visual data can be directly applied to time-series data, and still achieve excellent performance, even better than methods specially proposed for time-series UDA. Se were impressed by the consistently superior performance of "DIRT-T" method on all the datasets. We provide the code publicly on github https://github.com/emadeldeen24/AdaTime
    submitted by    /u/emad_eldeen  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing MindSpore 1.6 New Features]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty48ka/introducing_mindspore_16_new_features/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty48ka/introducing_mindspore_16_new_features/"/>
        <updated>2022-04-07T03:49:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Creative_Habit_6868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI's DALL·E 2 ! Text-to-Image Generation Explained]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty3xvn/openais_dalle_2_texttoimage_generation_explained/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty3xvn/openais_dalle_2_texttoimage_generation_explained/"/>
        <updated>2022-04-07T03:32:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][R] Announcing: Dataset & Denoising Shabby Pages Competition]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty3w48/pr_announcing_dataset_denoising_shabby_pages/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty3w48/pr_announcing_dataset_denoising_shabby_pages/"/>
        <updated>2022-04-07T03:29:34.000Z</updated>
        <summary type="html"><![CDATA[Into machine learning? Want a chance to earn a new MacBook Pro? Check out the Denoising ShabbyPages competition! The ShabbyPages dataset is being produced as a way to help train, test, and calibrate computer vision machine learning algorithms designed for working with documents. Enter the competition by training a model to remove the noise, and be awarded a MacBook Pro or some swag in the process! Check out the short paper introducing the dataset, and learn more about the competition at denoising-shabby.com.
    submitted by    /u/proofconstruct  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do I get into the field of AI policy and strategy?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty3vz2/how_do_i_get_into_the_field_of_ai_policy_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty3vz2/how_do_i_get_into_the_field_of_ai_policy_and/"/>
        <updated>2022-04-07T03:29:19.000Z</updated>
        <summary type="html"><![CDATA[I've read online that a career in AI policy and strategy is heavily needed and is actually ranked as the number one problem in the future by 80,000 hours. I am choosing which undergraduate degree to pursue in the fall and I'm not sure the best pathway to pursue to work in this field in an extremely high level position. an economics degree? Computer science degree? AI degree? should I pursue one subject until I get a PhD in it or mix with other degrees/certificates? is it a straight forward pathway focused on one subject where I only work in one subject field or is it necessary to pursue and work in other fields as well, what are the typical steps? Also if there is anything else that would be helpful on the pathway or anything you would recommend please let me know.
    submitted by    /u/Key-Lawyer-7586  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] FaceSigns: Semi-Fragile Neural Watermarks for Media Authentication and Countering Deepfakes]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty3jae/r_facesigns_semifragile_neural_watermarks_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty3jae/r_facesigns_semifragile_neural_watermarks_for/"/>
        <updated>2022-04-07T03:09:27.000Z</updated>
        <summary type="html"><![CDATA[Hi Everyone! 
 We have released the preprint and google colab demo for our paper FaceSigns. FaceSigns embeds a secret bit-string as a semi-fragile watermark in the image pixels. The message is recoverable if benign image operations such as color/contrast adjustment, JPEG compression, Instagram filters are applied. However, the message cannot be decoded if the image is facially tampered (eg. DeepFake manipulation) . This selective fragility allows reliable detection of DeepFake manipulations applied on images signed using FaceSigns. 
 Try out our google colab demo to see message encoding and decoding using FaceSigns!
 Paper: https://arxiv.org/abs/2204.01960
 Project Webpage: https://shehzeen.github.io/facesigns
 Demo: https://github.com/paarthneekhara/FaceSignsDemo
    submitted by    /u/LynxCompetitive7637  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weight decay in policy network for Discrete SAC?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ty2dt2/weight_decay_in_policy_network_for_discrete_sac/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ty2dt2/weight_decay_in_policy_network_for_discrete_sac/"/>
        <updated>2022-04-07T02:09:30.000Z</updated>
        <summary type="html"><![CDATA[We’re finding that our network is returning a tensor of NaNs towards the end of training. Adding weight decay solves this issue but reduces learning, was wondering if anyone else had experience with vanishing gradients in off-policy methods or any insight?
    submitted by    /u/TerrificJam  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine learning models / ideas for Google search ads?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ty25fi/d_machine_learning_models_ideas_for_google_search/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ty25fi/d_machine_learning_models_ideas_for_google_search/"/>
        <updated>2022-04-07T01:57:24.000Z</updated>
        <summary type="html"><![CDATA[Hi guys!
 I work in house and I’m part of our Google search team. Our ad spend is pretty large (9 figures per year, in USD). We build/manage stuff at scale using SQL, R, Javascript, and so on. So everything is pretty much “big data” in flavour.
 Lately I’ve been more and more interested in data science, and I’m looking to take things to the next level by incorporating machine learning into our workflow. I’d really love to build some useful machine learning models using popular Python libraries such as Pandas, SciKit Learn, NumPy, TensorFlow, PyTorch, and so on.
 Any suggestions on cool, and most importantly useful machine learning models I could build? (By “useful”, I mean something that could help increase the profits.) I think some classification, predictive, or recommender models would be great to start with. Cheers! 😄
    submitted by    /u/TropicalBound111  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Five Google Chrome Extensions that every Machine Learning / Data Science professional should know about 🚀💯]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ty030w/five_google_chrome_extensions_that_every_machine/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ty030w/five_google_chrome_extensions_that_every_machine/"/>
        <updated>2022-04-07T00:09:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/MLtinkerer  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about Model Predictive Control (MPC) cost function]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txyv62/question_about_model_predictive_control_mpc_cost/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txyv62/question_about_model_predictive_control_mpc_cost/"/>
        <updated>2022-04-06T23:07:19.000Z</updated>
        <summary type="html"><![CDATA[To my understanding, the cost function is the error between predicted state value and real state value.
 So if I use a neural network as my dynamics model(unknown true dynamics), the MPC cost function is equivalent to NN’s loss function?
    submitted by    /u/Blasphemer666  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Using Gamma Distribution to Improve Long-Tail Event Predictions at Doordash]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txy6il/r_using_gamma_distribution_to_improve_longtail/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txy6il/r_using_gamma_distribution_to_improve_longtail/"/>
        <updated>2022-04-06T22:34:20.000Z</updated>
        <summary type="html"><![CDATA[Predicting longtail events can be one of the more challenging ML tasks. Last year my team published a blog article where we improved DoorDash’s ETA predictions by 10% by tweaking the loss function with historical and real-time features. I thought members of the community would be interested in learning how we improved the model even more by using Gamma distribution-based inverse sampling approach to loss function tunning. Please check out the new article for all the technical details and let us know your feedback on our approach.
 https://doordash.engineering/2022/04/06/using-gamma-distribution-to-improve-long-tail-event-predictions/
    submitted by    /u/pmp-dash1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning To Play "Settlers of Catan" With Deep RL - code and write-up]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txxore/learning_to_play_settlers_of_catan_with_deep_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txxore/learning_to_play_settlers_of_catan_with_deep_rl/"/>
        <updated>2022-04-06T22:10:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/henrythepaw  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quick Little Keras Question]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/txw0eh/quick_little_keras_question/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/txw0eh/quick_little_keras_question/"/>
        <updated>2022-04-06T20:54:46.000Z</updated>
        <summary type="html"><![CDATA[I tried posting this on stackoverflow with no response.
 Im trying to use model.save() and keras.models.load_model()
 on this chunk of code. But, unlike some of the other keras examples I've played with, this one seems to crash.
 I'm super new to this, any Ideas why? I can post the error message if it helps.
    submitted by    /u/HoneyBunchsOGoats  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ICML author response. What reviewers expect.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txvlf0/d_icml_author_response_what_reviewers_expect/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txvlf0/d_icml_author_response_what_reviewers_expect/"/>
        <updated>2022-04-06T20:36:00.000Z</updated>
        <summary type="html"><![CDATA[Hi, we submitted to ICML for the first time. We got 4 reviews and 3 of them are mostly positive. Major comments by the reviewers include: more justification on the assumptions, discussion on choices of parameters, and experiments in more complex and different environments. 
 We want to address all the major and minor comments as best as we can but given that the response is limited to one page we cannot explain everything in detail. I am not sure what is the acceptable norm here. Do reviewers expect the authors to conduct some experiments during the rebuttal and provide sample results or just explain what additional experiment we will conduct and how we will do it. Justification and reasoning should be in details or a brief explanation with an assurance to add a detailed discussion in the final version suffices.
 TIA
    submitted by    /u/srvsinha186  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Questions for a TPM for ML interview at Google]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txvky1/d_questions_for_a_tpm_for_ml_interview_at_google/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txvky1/d_questions_for_a_tpm_for_ml_interview_at_google/"/>
        <updated>2022-04-06T20:35:25.000Z</updated>
        <summary type="html"><![CDATA[Hey all,
 I have a technical program manager interview soon for an ML team at google and I want to know if anyone has any sample role-related questions I can gauge myself with.
 I have a strong data science & statistics background but that doesn't always translate to deep ML knowledge like an ML Engineer might have.
 Any resources or sample questions? I have not found adequate results from google regarding this team area specifically.
    submitted by    /u/math_is_my_religion  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which environments do you use for benchmarking?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txvc52/which_environments_do_you_use_for_benchmarking/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txvc52/which_environments_do_you_use_for_benchmarking/"/>
        <updated>2022-04-06T20:24:16.000Z</updated>
        <summary type="html"><![CDATA[Hey guys I'm curious which environments you use to benchmark your standard RL algorithms.
 I typically use some environments from the OpenAI Gym or the DM control suite but benchmarking all my implementations against all environments for multiple seeds would take forever. Are there some of their environments you particularly like for benchmarking?
    submitted by    /u/NiconiusX  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Anyone knows any high accuracy models on UCI adult dataset?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txv5o3/d_anyone_knows_any_high_accuracy_models_on_uci/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txv5o3/d_anyone_knows_any_high_accuracy_models_on_uci/"/>
        <updated>2022-04-06T20:16:14.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone. This is my first-time post here, and I hope I did not break any sub rules.
 Currently, I am doing some research with the UCI Adult dataset(https://archive.ics.uci.edu/ml/datasets/adult). This first step is to build a high-accuracy classifier model. 
 Does anyone know any high accuracy model on this dataset (more than 90%)? I use many machine learning models like logistic regression and neural network. But no matter how complex the model is, I can only get an accuracy of about 85% on the test set. I tried to google but I found many others also have similar results of about 85%.
 Any posts or papers will be helpful! Thanks in advance for your help!
    submitted by    /u/Akasakura888  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weekly China AI News: Slime Robot Grabs Swallowed Objects; SenseTime Revenue Grows Despite $2.7B Net Loss; Transformer Architecture Search Without Training]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txulan/weekly_china_ai_news_slime_robot_grabs_swallowed/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txulan/weekly_china_ai_news_slime_robot_grabs_swallowed/"/>
        <updated>2022-04-06T19:51:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/trcytony  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Luxurious: The Intelligent NIO ET7 EV Built on NVIDIA DRIVE Orin Arrives]]></title>
        <id>https://blogs.nvidia.com/?p=56511</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/06/nio-et7-drive-orin-arrives/"/>
        <updated>2022-04-06T19:41:03.000Z</updated>
        <summary type="html"><![CDATA[Meet the electric vehicle that’s quick-witted and fully outfitted. Last week, NIO began deliveries of its highly anticipated ET7 fully electric vehicle, in Hefei, China. The full-size luxury sedan is the first production vehicle built on the NIO Adam supercomputer, powered by four NVIDIA DRIVE Orin systems-on-a-chip (SoCs). The production launch of its flagship sedan Read article >
The post Fast and Luxurious: The Intelligent NIO ET7 EV Built on NVIDIA DRIVE Orin Arrives appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Jurgen Ferchau</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How does advantage estimation is done when episodes are of variable length in PPO?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txtzv6/how_does_advantage_estimation_is_done_when/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txtzv6/how_does_advantage_estimation_is_done_when/"/>
        <updated>2022-04-06T19:24:05.000Z</updated>
        <summary type="html"><![CDATA[In the PPO paper it is stated that we have to collect trajectories of length T from N different workers. Suppose I am not using multiple workers then I have to collect episodes N times of fixed length T. But these episode lengths are variable i.e. some episodes end much before T and some much after T. So my question is how do we calculated advantage because according to the PPO paper, for generalized advantage estimate, we have to observe the reward of terminal state. 
 ​
 So how should I calculate GAE in this ?
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Using Gamma Distribution to Improve Long-Tail Event Predictions]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txtp5u/r_using_gamma_distribution_to_improve_longtail/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txtp5u/r_using_gamma_distribution_to_improve_longtail/"/>
        <updated>2022-04-06T19:10:10.000Z</updated>
        <summary type="html"><![CDATA[Predicting longtail events can be one of the more challenging ML tasks. Last year my team published a blog article where we improved DoorDash’s ETA predictions by 10% by tweaking the loss function with historical and real-time features. I thought members of the community would be interested in learning how we improved the model even more by using Gamma distribution-based inverse sampling approach to loss function tuning. Please check out the new article for all the technical details and let us know your feedback on our approach.
 ​
 https://doordash.engineering/2022/04/06/using-gamma-distribution-to-improve-long-tail-event-predictions/
    submitted by    /u/pmp-dash1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Reading the Tea Leaves: Expert End-Users Explaining the Unexplainable]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txth9e/d_reading_the_tea_leaves_expert_endusers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txth9e/d_reading_the_tea_leaves_expert_endusers/"/>
        <updated>2022-04-06T19:00:19.000Z</updated>
        <summary type="html"><![CDATA[Hey there, just a heads up we at The Gradient just published a new article discussing explainability - 
  
"This article uses the common backdrop of competitive games to explore the ways in which domain experts adapt to new technologies that lack explainability. I illustrate how interpretations vary based on user experience and model architecture, and how special care must be taken when adapting models to human-centric problems."
  
Check it out here if you think it's interesting / worth discussing:
 Reading the Tea Leaves: Expert End-Users Explaining the Unexplainable
    submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reading the Tea Leaves: Expert End-Users Explaining the Unexplainable]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txtfc5/reading_the_tea_leaves_expert_endusers_explaining/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txtfc5/reading_the_tea_leaves_expert_endusers_explaining/"/>
        <updated>2022-04-06T18:58:03.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do we know that A.I hasn't already taken over our worlds ? How do we know this isn't the matrix ? #simulation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txt853/how_do_we_know_that_ai_hasnt_already_taken_over/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txt853/how_do_we_know_that_ai_hasnt_already_taken_over/"/>
        <updated>2022-04-06T18:49:06.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Individual-Fly-610  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A silly question from a new beginner]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txs2jh/a_silly_question_from_a_new_beginner/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txs2jh/a_silly_question_from_a_new_beginner/"/>
        <updated>2022-04-06T17:56:48.000Z</updated>
        <summary type="html"><![CDATA[I could not find an answer to a question hanging around in my head for a while. Suppose we have some data, if we build up an MDP to capture actions + state dynamics. Would the optimal policy win state-of-art RL algorithms?
 Edit: If that is the case, why would the community bothers with learning algorithm since finding the model of dynamics is the key?
 ​
    submitted by    /u/musicinthedark  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Orin Leaps Ahead in Edge AI, Boosting Leadership in MLPerf Tests]]></title>
        <id>https://blogs.nvidia.com/?p=56403</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/06/mlperf-edge-ai-inference-orin/"/>
        <updated>2022-04-06T17:00:38.000Z</updated>
        <summary type="html"><![CDATA[In its debut in the industry MLPerf benchmarks, NVIDIA Orin, a low-power system-on-chip based on the NVIDIA Ampere architecture, set new records in AI inference, raising the bar in per-accelerator performance at the edge. Overall, NVIDIA with its partners continued to show the highest performance and broadest ecosystem for running all machine-learning workloads and scenarios Read article >
The post NVIDIA Orin Leaps Ahead in Edge AI, Boosting Leadership in MLPerf Tests appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Dave Salvator</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project] Learning to Play "Settlers of Catan" With Deep RL - Writeup and Code]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txqkin/project_learning_to_play_settlers_of_catan_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txqkin/project_learning_to_play_settlers_of_catan_with/"/>
        <updated>2022-04-06T16:49:47.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I just wanted to share a project I've been working on for the past year - using deep RL to learn to play the board game Settlers of Catan.
 I expect everyone is aware of the results that DeepMind/OpenAI have got recently on Go, DOTA 2, Starcraft 2 etc, but I was motivated to see how much progress could be made with existing RL techniques on a reasonably complex game - but with access to significantly less computational resources.
 Whilst I didn't end up with an agent that performs at a super-human level, there was clear learning progress and the results were quite interesting. I decided to do a full write-up of the project here, which I figured could be useful for anyone else who is interested in trying to apply DRL to a new, complicated environment. I also open-sourced all the code here for anyone interested.
 If anyone has any feedback or any questions at all that'd be great!
    submitted by    /u/henrythepaw  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Receive notifications for image analysis with Amazon Rekognition Custom Labels and analyze predictions]]></title>
        <id>9770db1640db4792d317e78fe4a612d13b6f8025</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/receive-notifications-for-image-analysis-with-amazon-rekognition-custom-labels-and-analyze-predictions/"/>
        <updated>2022-04-06T16:46:32.000Z</updated>
        <summary type="html"><![CDATA[Amazon Rekognition Custom Labels is a fully managed computer vision service that allows developers to build custom models to classify and identify objects in images that are specific and unique to your business. Rekognition Custom Labels doesn’t require you to have any prior computer vision expertise. You can get started by simply uploading tens of […]]]></summary>
        <author>
            <name>Jay Rao</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DALL·E 2]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txqh23/dalle_2/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txqh23/dalle_2/"/>
        <updated>2022-04-06T16:45:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/roblox22y  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ICML rebuttals optional or semi-mandatory?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txqapw/d_icml_rebuttals_optional_or_semimandatory/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txqapw/d_icml_rebuttals_optional_or_semimandatory/"/>
        <updated>2022-04-06T16:37:16.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 We just submitted to ICML 2022 and got our reviews back. We were excited to see that 4/4 reviews were positive and acknowledged the contribution of the paper. However, there were some minor criticisms (e.g. didn't do good enough lit reviews, could use a few more experiments) across several reviews.
 I was wondering if it is ever acceptable to not submit a rebuttal? Can a rebuttal in this case actually hurt us by rocking the boat---or for ICML is the norm that you should always submit a rebuttal that addresses all the reviewers' criticisms.
 We were wondering what the norm is for ICML specifically?
    submitted by    /u/optimistic313  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Hierarchical Text-Conditional Image Generation with CLIP Latents. This is the paper for OpenAI's DALL-E 2]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txples/r_hierarchical_textconditional_image_generation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txples/r_hierarchical_textconditional_image_generation/"/>
        <updated>2022-04-06T16:04:57.000Z</updated>
        <summary type="html"><![CDATA[Blog post.
 Paper (pdf file format). The paper is also linked to in the above blog post.
 Abstract
  
Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.
  
OpenAI's Sam Altman used DALL-E 2 to generate ~20 text prompt requests from Twitter users. The results are here, with individual result links and other samples in this comment from another Reddit user in a different post.
 Twitter thread about the paper (not from the paper authors).
 Sam Altman's blog post about DALL-E 2.
  
Hopefully this summer, we’ll do a product launch and people will be able to use it for all sorts of things.
  
   submitted by    /u/Wiskkey  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An optimized solution for face recognition]]></title>
        <id>https://news.mit.edu/2022/optimized-solution-face-recognition-0406</id>
        <link href="https://news.mit.edu/2022/optimized-solution-face-recognition-0406"/>
        <updated>2022-04-06T15:25:00.000Z</updated>
        <summary type="html"><![CDATA[When artificial intelligence is tasked with visually identifying objects and faces, it assigns specific components of its network to face recognition — just like the human brain.]]></summary>
        <author>
            <name>Jennifer Michalowski | McGovern Institute for Brain Research</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly Digest: Moving Time]]></title>
        <id>https://www.datasciencecentral.com/?p=57050</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-digest-moving-time/"/>
        <updated>2022-04-06T15:01:18.000Z</updated>
        <summary type="html"><![CDATA[For nine years, my family and I have lived in a house in Issaquah, a little community about twenty minutes east of Seattle. The town still retains its charms — a downtown area about three blocks long that includes a vintage (and long since decommissioned) gas station, numerous restaurants, a live theater, the library, and… Read More »DSC Weekly Digest: Moving Time
The post DSC Weekly Digest: Moving Time appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kurt Cagle</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficiently Initializing Reinforcement Learning With Prior Policies]]></title>
        <id>http://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html</id>
        <link href="http://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html"/>
        <updated>2022-04-06T14:38:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Ikechukwu Uchendu, AI Resident and Ted Xiao, Software Engineer, Robotics at Google 
Reinforcement learning (RL) can be used to train a policy to perform a task via trial and error, but a major challenge in RL is learning policies from scratch in environments with hard exploration challenges. For example, consider the setting depicted in the door-binary-v0 environment from the adroit manipulation suite, where an RL agent must control a hand in 3D space to open a door placed in front of it. 
   


An RL agent must control a hand in 3D space to open a door placed in front of it. The agent receives a reward signal only when the door is completely open.

   
Since the agent receives no intermediary rewards, it cannot measure how close it is to completing the task, and so must explore …]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DALL·E 2]]></title>
        <id>624d5b837ce26d004d92d14d</id>
        <link href="https://openai.com/blog/dall-e-2/"/>
        <updated>2022-04-06T13:42:00.000Z</updated>
        <summary type="html"><![CDATA[DALL·E 2 is a new AI system that can create realistic images and art from a description in natural language.]]></summary>
        <author>
            <name>OpenAI</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is the 'first boss attempt' phenomenon know to occur amongst NN playing games, or is this learning trajectory unique to human players?[D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txm40w/is_the_first_boss_attempt_phenomenon_know_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txm40w/is_the_first_boss_attempt_phenomenon_know_to/"/>
        <updated>2022-04-06T13:24:35.000Z</updated>
        <summary type="html"><![CDATA[I'm curious about wether this unusual learning trajectory observed in humans has also been observed in artificial neural nets. A well known phenomenon in the 'dark souls' video game series is that ones first attempt at a boss is often much better than subsequent attempts. Boss hp at time of death by attempt might go something like: 35%, 55%, 85%, 87%, 75%, 54%, , 60%, 43%, 27%, 38%, 12%, 0%. This sounds very anecdotal, but its know to the community of these games to be a real thing. See this thread for evidence. Have NN playing games been known to exhibit a similar pattern, with peak in success early on, followed by a step descent , then a slow gradual climb? Or is this a purely human phenomenon?
 My hypothesis as to why this happens is that over the course of the first couple attempts, the player learns a bunch of bad strategies which must be slowly unlearned, whereas on attempt one, the player has no defined strategies good or bad.
    submitted by    /u/Greenface1998  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sum the zeros of an analytic function without finding them first]]></title>
        <id>https://www.johndcook.com/blog/?p=101812</id>
        <link href="https://www.johndcook.com/blog/2022/04/06/sum-analytic-function-zeros/"/>
        <updated>2022-04-06T13:22:57.000Z</updated>
        <summary type="html"><![CDATA[A couple days ago I wrote about how Vieta’s formulas let you sum the zeros of a polynomial without having to first compute the zeros. This is especially handy for high-order polynomials since there is no explicit formula for the zeros. Most functions that arise in applications are not polynomials. How could you find the […]
Sum the zeros of an analytic function without finding them first first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn how GANs work with a cool Toonify example!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txl7aq/learn_how_gans_work_with_a_cool_toonify_example/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txl7aq/learn_how_gans_work_with_a_cool_toonify_example/"/>
        <updated>2022-04-06T12:37:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txktli/r_disentangling_abstraction_from_statistical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txktli/r_disentangling_abstraction_from_statistical/"/>
        <updated>2022-04-06T12:16:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/papajan18  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Project][P] Who invented Graph Neural Networks?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txkl43/projectp_who_invented_graph_neural_networks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txkl43/projectp_who_invented_graph_neural_networks/"/>
        <updated>2022-04-06T12:03:35.000Z</updated>
        <summary type="html"><![CDATA[Just a side project (only for me) in which I try to sum up some history of DL. Can't be 100% sure this is the first article in which they appear: Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2008). The graph neural network model. IEEE transactions on neural networks, 20(1), 61-80. Would appreciate any help. Thanks
    submitted by    /u/Siddh__  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txkh8m/r_gpbart_a_novel_bayesian_additive_regression/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txkh8m/r_gpbart_a_novel_bayesian_additive_regression/"/>
        <updated>2022-04-06T11:57:53.000Z</updated>
        <summary type="html"><![CDATA[(not my paper)
 paper: https://arxiv.org/abs/2204.02112
 abstract: "The Bayesian additive regression trees (BART) model is an ensemble method extensively and successfully used in regression tasks due to its consistently strong predictive performance and its ability to quantify uncertainty. BART combines "weak" tree models through a set of shrinkage priors, whereby each tree explains a small portion of the variability in the data. However, the lack of smoothness and the absence of a covariance structure over the observations in standard BART can yield poor performance in cases where such assumptions would be necessary. We propose Gaussian processes Bayesian additive regression trees (GP-BART) as an extension of BART which assumes Gaussian process (GP) priors for the predictions of each terminal node among all trees. We illustrate our model on simulated and real data and compare its performance to traditional modelling approaches, outperforming them in many scenarios. An implementation of our method is available in the \textsf{R} package \texttt{rGPBART} available at: https://github.com/MateusMaiaDS/gpbart."
    submitted by    /u/bikeskata  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence, Machine Learning and the Higgs boson - Live talk with Dr. David Rousseau]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txjowl/artificial_intelligence_machine_learning_and_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txjowl/artificial_intelligence_machine_learning_and_the/"/>
        <updated>2022-04-06T11:10:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aair_x  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driving a robot with a neural network - use case study]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/txj3ng/driving_a_robot_with_a_neural_network_use_case/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/txj3ng/driving_a_robot_with_a_neural_network_use_case/"/>
        <updated>2022-04-06T10:32:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/KamilBugnoKrk  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Anyone know about any interesting recent improvements with SNNs?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txijls/d_anyone_know_about_any_interesting_recent/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txijls/d_anyone_know_about_any_interesting_recent/"/>
        <updated>2022-04-06T09:54:31.000Z</updated>
        <summary type="html"><![CDATA[I’m currently writing a research paper for my MSc on neuromorphic sensing and spike neural networks and most good papers are from around 2015 and was looking for something more recent.
 Anyone here heard of any interesting upgrades in architecture or applications?
 Cheers!
    submitted by    /u/GandhisLittleHelper  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does it look like the output of a reinforcement learning agent/algorithm in practice?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txhq5z/what_does_it_look_like_the_output_of_a/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txhq5z/what_does_it_look_like_the_output_of_a/"/>
        <updated>2022-04-06T08:55:08.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am relatively new in the area of machine learning/reinforcement learning. I have this basic question regarding practical implementations. I just want to know, what does it look like the output of a reinforcement learning agent/algorithm in practice? Is it like a 'look-up table' that will set the weights/parameters of the ML model based on the input data? 
 Note that I am asking after the offline training of the agent. How to implement the trained agent in practice, like in an embedded system? Do you guys have references or clues to help me to clarify?
 BR
    submitted by    /u/b0bzera  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[multi-discrete action space in SAC(Soft Actor-Critic)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txhgbe/multidiscrete_action_space_in_sacsoft_actorcritic/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txhgbe/multidiscrete_action_space_in_sacsoft_actorcritic/"/>
        <updated>2022-04-06T08:33:16.000Z</updated>
        <summary type="html"><![CDATA[Hello!
 I am using SAC(Soft Actor-Critic) to complete a reinforcement learning task with only four steps, each action is from one of four different action spaces. These four action spaces are essentially the same, and they are all chemical compound. I just want the agent to take different types of compounds at each step.
 I have the following questions：
  
Whether the different four steps can be trained? In fact, there is a paper that only has four steps in a reinforcement learning process, but his action space only has one discrete action space.
 Is there any article I can learn from? because I know that for the handle control of the game, there are usually multiple discrete action spaces, but each discrete space dimension of my task is larger, such as [800, 700, 500, 600]
  
Thanks！
    submitted by    /u/RangerWYR  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noticing that profs focus on male student’s goals and female student’s capabilities, any weigh-in? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txha3z/noticing_that_profs_focus_on_male_students_goals/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txha3z/noticing_that_profs_focus_on_male_students_goals/"/>
        <updated>2022-04-06T08:19:25.000Z</updated>
        <summary type="html"><![CDATA[Hello, I’m currently a graduate student. I do different projects and for some I get to decide on what I want the scope to be. I do have to get the scope/ plan/ idea approved first. I pitch my ideas to profs who aren’t directly my profs and normally 5-6 other students will pitch ideas to the same group of profs at the same time….. I noticed that i get really different questions and feedback in comparison to my peers. I’m a female and my peers are male… I didn’t start out with this outlook but I’m starting to search for reasons why I often get questioned about my capabilities to preform a project ( which is normal enough but I get questioned to the point where explaining my approach isn’t enough and they ask me for examples of codes) and my peers definitely do not get asked about there capabilities, rather they tell them what they can do and they don’t get questioned. …………… really frustrating.
    submitted by    /u/tyger-lily  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to write a ML+Healthcare paper where the research was a framework with pre-trained models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txgmti/d_how_to_write_a_mlhealthcare_paper_where_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txgmti/d_how_to_write_a_mlhealthcare_paper_where_the/"/>
        <updated>2022-04-06T07:30:09.000Z</updated>
        <summary type="html"><![CDATA[As a project in the course of my PhD, I had to create a prototype for a project. My PhD is application of machine learning in health care. The project definition and scope was faaaar too wide. However, I managed to create a working demo which encompasses some use cases of the project. At best, it can be called a framework, where I have put in different DL components and it works okay for those use cases only. Most of the components, I have used are pre-trained language models (maybe fine tuned them to my use case). However, there is no active training or learning involved. This is because I created this for a demo only. I also created a very small dataset and tested the framework over the dataset and the results were ok. However, my supervisor now wants me to write a paper, as he is confident, that the use case is rather unique and my working framework is a good first step. I believe, his aim is to get me started on the paper writing process, which I appreciate. However, I am not confident about it at all.
 My question is, can a 'framework' composed of pre-trained models with the end goal of solving a problem in health care is good enough? Are there precedents of any such paper? And if I trust my supervisor's instincts, are there any fancy ways to frame the framework so that it does not look so basic?
    submitted by    /u/Complex_State9960  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Building a knowledge based recommender system]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txfq52/p_building_a_knowledge_based_recommender_system/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txfq52/p_building_a_knowledge_based_recommender_system/"/>
        <updated>2022-04-06T06:26:17.000Z</updated>
        <summary type="html"><![CDATA[I am trying to build a knowledge based recommender system but do not have prior knowledge. 
 We first take in user inputs such as occasion, weather, top wear and bottom wear, color. Based on this we want to create a knowledge base and recommend clothes. 
 Can anyone help me on how to go about on doing this process step by step and what algorithms and technology to be used?
    submitted by    /u/bills70  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is multi bandit on policy or off policy？]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txfewf/is_multi_bandit_on_policy_or_off_policy/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txfewf/is_multi_bandit_on_policy_or_off_policy/"/>
        <updated>2022-04-06T06:04:14.000Z</updated>
        <summary type="html"><![CDATA[quick question
    submitted by    /u/Asleep_Donut1382  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are your thoughts about AI teachers?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txelh6/what_are_your_thoughts_about_ai_teachers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txelh6/what_are_your_thoughts_about_ai_teachers/"/>
        <updated>2022-04-06T05:12:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/curiosityVeil  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How wrong is it to use sampling at inference time ?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txe8rj/how_wrong_is_it_to_use_sampling_at_inference_time/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txe8rj/how_wrong_is_it_to_use_sampling_at_inference_time/"/>
        <updated>2022-04-06T04:51:21.000Z</updated>
        <summary type="html"><![CDATA[At my company we use RL to solve our problem. The thing is : our problem is rather complex, and this is the core of our product (so clients rely on the results produced). 
 In order to reach satisfying results despite an agent that doesn't learn very well, we use sampling at inference time : instead of taking the best trajectory according to the agent, we take X trajectories and keep only the one with the best reward.
 This seems completely fine at first (similar things are done in NLP for example, with beam search), but in our case the sampling size is huge : 1024. Usually when using beam search, we use maybe a beam size of 6. Maybe 10 if you have good hardware ?
 ​
 Now, the agent seems to be learning : the mean return is slightly increasing over time, the entropies for the actions are steadily decreasing, etc...
 Now the goal of the ML team is to improve agent's learning to decrease the sampling size at inference time (because it's costly to run 1024 trajectories through the environment...).
 But whatever we try, the improvements are not reflected (we compare all our experiments with 1024 sampling in order to see what the customers will see).
 ​
 IMO this is because our sampling size is way too huge, even a random agent can produce okay-ish results...
 Is my intuition the right one ?
    submitted by    /u/dummy-gummy  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does a master's thesis/doctoral dissertation need to have implications down the line for it to be good?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txduuq/does_a_masters_thesisdoctoral_dissertation_need/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txduuq/does_a_masters_thesisdoctoral_dissertation_need/"/>
        <updated>2022-04-06T04:27:30.000Z</updated>
        <summary type="html"><![CDATA[I'm in the last semester of my undergraduate degree; over the past couple of weeks, I've been trying to brainstorm ideas that I would like to pursue in my graduate research career. I'm interested in the emergence of language in multi-agent reinforcement environments but I can't see how this would be important down the line when there are large language models that are completely dominating language and communication. 
 Should this stop me from pursuing this idea or should I let my interest in the idea take precedence?
    submitted by    /u/clarky103  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ICML 2022 Paper Reviews]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txdhxt/d_icml_2022_paper_reviews/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txdhxt/d_icml_2022_paper_reviews/"/>
        <updated>2022-04-06T04:06:11.000Z</updated>
        <summary type="html"><![CDATA[ICML 2022 paper reviews are supposed to be released soon. Creating a discussion thread for this year's reviews.
    submitted by    /u/zy415  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does this artificial intelligence think like a human?]]></title>
        <id>https://news.mit.edu/2022/does-this-artificial-intelligence-think-human-0406</id>
        <link href="https://news.mit.edu/2022/does-this-artificial-intelligence-think-human-0406"/>
        <updated>2022-04-06T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[A new technique compares the reasoning of a machine-learning model to that of a human, so the user can see patterns in the model’s behavior.]]></summary>
        <author>
            <name>Adam Zewe | MIT News Office</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Here's an intuitive explanation to Singular Value Decomposition. 👇]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/txd4pu/heres_an_intuitive_explanation_to_singular_value/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/txd4pu/heres_an_intuitive_explanation_to_singular_value/"/>
        <updated>2022-04-06T03:45:33.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mr-minion  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Here's an intuitive explanation to Singular Value Decomposition. 👇]]></title>
        <id>https://www.reddit.com/r/artificial/comments/txd3gn/heres_an_intuitive_explanation_to_singular_value/</id>
        <link href="https://www.reddit.com/r/artificial/comments/txd3gn/heres_an_intuitive_explanation_to_singular_value/"/>
        <updated>2022-04-06T03:43:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mr-minion  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] In general, should you let the model find interactions between many basic features, or should you use feature engineering to ‘help’ the model find the interaction?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/txbyy7/d_in_general_should_you_let_the_model_find/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/txbyy7/d_in_general_should_you_let_the_model_find/"/>
        <updated>2022-04-06T02:41:45.000Z</updated>
        <summary type="html"><![CDATA[I’ll give an example to better explain my question (don’t get hung up on the numbers, it’s all made up). Say you are using a tree based model trying to project how many points a player will score in a given basketball game.
 Most players shoot free throws at a slightly lower percentage on the road, than they do at home. However, the magnitude varies player to player. Let’s assume for 95% of players with significant data, the ratio of home free throw percentage to away is 1 to 1.15. Generally speaking, older players are closer to 1 and younger players are around 1.1 (since older players get used to the opposing crowd).
 Now also say it takes 100 home and 100 away free throws to get a stable reliable ratio.
 Now say a young player only has 50 home, and 50 away free throws. With this amount of data he has a ratio of 1, however the sample size is not enough to be fully stable.
 Which would be better…
 5 features into this model, his home away ratio, average ratio for players his age, home free throw count, and away free throw attempts.
 1 feature. His ‘projected’ home away ratio, which is a weighted average of his ratio with the average for plaeyrs his age. Since he’s 50% of the way to significance, 0.5 * 1 + 0.5 * 1.1 = 1.05
 The benefit of the of the first choice is that it may find other interactions that I never conceived of, however, it could incorporate noise. Is there a general consensus, or is this just a try both and see what works?
    submitted by    /u/irndk10  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How long would it take you to implement a MARL PPO agent with joint attention architecture?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/txbvya/how_long_would_it_take_you_to_implement_a_marl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/txbvya/how_long_would_it_take_you_to_implement_a_marl/"/>
        <updated>2022-04-06T02:37:14.000Z</updated>
        <summary type="html"><![CDATA[Out of curiosity, how long would it take to implement a paper like this one? https://arxiv.org/abs/2104.07750 
 It has PPO agents in MARL, all of them with multihead attention performed on the observation, in such a way that an attention map is created for each agent. This attention map has information about how strongly each agent is attending to various elements of the environment. With KL divergence, the agents are rewarded for minimizing the difference between their attention maps.
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares: Beauty Parlor || Clip Guided Diffusion AI Art Video [4K 20 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tx99v0/artificial_nightmares_beauty_parlor_clip_guided/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tx99v0/artificial_nightmares_beauty_parlor_clip_guided/"/>
        <updated>2022-04-06T00:21:20.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI: AI improves algae for biofuel and carbon capture, more AI decision-making in the military, and more!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tx7x0d/last_week_in_ai_ai_improves_algae_for_biofuel_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tx7x0d/last_week_in_ai_ai_improves_algae_for_biofuel_and/"/>
        <updated>2022-04-05T23:13:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why aren't new LLMs using the Perceiver architecture?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tx7e34/d_why_arent_new_llms_using_the_perceiver/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tx7e34/d_why_arent_new_llms_using_the_perceiver/"/>
        <updated>2022-04-05T22:48:26.000Z</updated>
        <summary type="html"><![CDATA[Perceiver and PerceiverIO (https://arxiv.org/abs/2107.14795) appear to offer significantly improved FLOP efficiency, but new LLMs (including Deepmind's own Gopher) don't use it.
 What gives? Is it still too new, or is the Perceiver architecture not appropriate for LLMs?
    submitted by    /u/deeceeo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Meta-Learning Machines in a Single Lifelong Trial: lecture video (24 min) presented at meta-learning workshops at ICML 2020 and NeurIPS 2021 (Schmidhuber YouTube Talk)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tx59tj/r_metalearning_machines_in_a_single_lifelong/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tx59tj/r_metalearning_machines_in_a_single_lifelong/"/>
        <updated>2022-04-05T21:11:28.000Z</updated>
        <summary type="html"><![CDATA[Saw this posted on Schmidhuber's Twitter:
 Meta-Learning Machines in a Single Lifelong Trial: lecture video (24 min) presented at meta-learning workshops at ICML 2020 and NeurIPS 2021. URL of talk: https://youtu.be/2GgGVdkq2bU
 Abstract
 The most widely used machine learning algorithms were designed by humans and thus are hindered by our cognitive biases and limitations. Can we also construct meta-learning algorithms that can learn better learning algorithms so that our self-improving AIs have no limits other than those inherited from computability and physics? This question has been a main driver of my research since I wrote a thesis on it in 1987. In the past decade, it has become a driver of many other people's research as well. Here I summarize our work starting in 1994 on meta-reinforcement learning with self-modifying policies in a single lifelong trial, and - since 2003 - mathematically optimal meta-learning through the self-referential Gödel Machine. This talk was previously presented at meta-learning workshops at ICML 2020 and NeurIPS 2021. Many additional publications on meta-learning can be found at https://people.idsia.ch/~juergen/metalearning.html
    submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Win tickets to The AI Summit London 2022]]></title>
        <id>https://machinelearningmastery.com/?p=13440</id>
        <link href="https://machinelearningmastery.com/win-tickets-to-the-ai-summit-london-2022/"/>
        <updated>2022-04-05T21:04:02.000Z</updated>
        <summary type="html"><![CDATA[Sponsored Post Join the UK’s most forward-thinking technologists and business professionals this June in a celebration of emerging technology. Machine […]
The post Win tickets to The AI Summit London 2022 appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Mitch Bartlett</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIT has trained AI to generate new molecular materials]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tx4n1h/mit_has_trained_ai_to_generate_new_molecular/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tx4n1h/mit_has_trained_ai_to_generate_new_molecular/"/>
        <updated>2022-04-05T20:44:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aidev2040  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a Data Products-centric Business Model]]></title>
        <id>https://www.datasciencecentral.com/?p=57061</id>
        <link href="https://www.datasciencecentral.com/datastrategist-datamanagement-ai-iot-ml/"/>
        <updated>2022-04-05T20:31:31.000Z</updated>
        <summary type="html"><![CDATA[When I was the Vice President of Advertiser Analytics at Yahoo!, I painfully learned that my targeted user personas (Media Planners & Buyers and Campaign Managers) didn’t want more data in helping them optimize their marketing, campaign, and advertising spend across the Yahoo! Ad Network.  Heck, they didn’t even want analytics!  The aspirations for these… Read More »Building a Data Products-centric Business Model
The post Building a Data Products-centric Business Model appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Hyperparameter Tuning: does it even work?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tx42h9/d_hyperparameter_tuning_does_it_even_work/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tx42h9/d_hyperparameter_tuning_does_it_even_work/"/>
        <updated>2022-04-05T20:19:19.000Z</updated>
        <summary type="html"><![CDATA[Hi *,
 I've been working for the last 5 years as Data Scientist. During this time I have tried dozens of times to improve my models via hyperparameter tuning, but I've never got improvements from there. I've tried all the possible approaches: grid search, random search, bayesian search, etc. But in no case did I get satisfactory results.
 Does this happen to anyone else? Have you ever got robust improvements via HP tuning?
    submitted by    /u/AM_DS  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Autoregressive model for graph generation?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tx3f5g/d_autoregressive_model_for_graph_generation/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tx3f5g/d_autoregressive_model_for_graph_generation/"/>
        <updated>2022-04-05T19:51:08.000Z</updated>
        <summary type="html"><![CDATA[Autoregressive models like GPT-2 do fairly well in text generation. Is it possible to do the same for graph data? A transformer based model Graphormer has recently shown its effectiveness in graph representation learning. Is there any way I can train Graphormer or any other model to generate graphs from an initial graph context?
    submitted by    /u/ratt_m  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need project suggestions]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tx2zbs/need_project_suggestions/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tx2zbs/need_project_suggestions/"/>
        <updated>2022-04-05T19:31:36.000Z</updated>
        <summary type="html"><![CDATA[I’ve been running circles in tutorial purgatory and I want to get out of it with sone projects. Anyone has any suggestions? Guided ones would be nice. For unguided ones, could you please provide source links/hints?
    submitted by    /u/HellVollhart  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Discovery for ML Engineers]]></title>
        <id>https://www.datasciencecentral.com/?p=57077</id>
        <link href="https://www.datasciencecentral.com/data-discovery-for-ml-engineers/"/>
        <updated>2022-04-05T19:14:14.000Z</updated>
        <summary type="html"><![CDATA[Real-world production ML systems consist of two main components: data and code. Data is clearly the leader, and rapidly taking center stage. Data defines the quality of almost any ML-based product, more so than code or any other aspect. In Feature Store as a Foundation for Machine Learning, we have discussed how feature stores are… Read More »Data Discovery for ML Engineers
The post Data Discovery for ML Engineers appeared first on Data Science Central.]]></summary>
        <author>
            <name>German Osin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers From Allen Institute for AI Introduce ‘MERLOT Reserve’: A Novel Multimodal Video Question Answering Model]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tx2gbr/researchers_from_allen_institute_for_ai_introduce/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tx2gbr/researchers_from_allen_institute_for_ai_introduce/"/>
        <updated>2022-04-05T19:08:12.000Z</updated>
        <summary type="html"><![CDATA[We humans navigate the environment using all of our senses. Allen Institute researchers propose MERLOT Reserve, a model that learns to represent videos over time and across several modalities, including audio, subtitles, and video frames. It was trained using a new learning objective and more than 20 million YouTube videos.
 MERLOT Reserve is a unique, cutting-edge methodology for solving video-related inquiries. MERLOT Reserve can dependably choose the correct answer from a selection of multiple-choice answers when given a video and a question. This forecast is made by MERLOT Reserve jointly reasoning over the visual frames of the video, the video subtitles, and the audio in the movie.
 Continue reading this cool research update from AI2
 Paper: https://arxiv.org/pdf/2201.02639.pdf
 Demo: https://merlot-reserve.apps.allenai.org/
 Project: https://rowanzellers.com/merlotreserve/
 Github: https://github.com/rowanz/merlot\_reserve
 ​
 https://preview.redd.it/031i6ty6err81.png?width=1920&format=png&auto=webp&s=299569e12160eb991f35a2c6b41c5758ff027235
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content Metrics That Can Help You To Write  Dissertations]]></title>
        <id>https://www.datasciencecentral.com/?p=57035</id>
        <link href="https://www.datasciencecentral.com/content-metrics-that-can-help-you-to-write-dissertations/"/>
        <updated>2022-04-05T18:24:50.000Z</updated>
        <summary type="html"><![CDATA[Many things can help you to write good dissertations. One of the most important is to use content metrics. It is necessary for all of the students to understand content metrics in detail. A clear understanding of its types and measuring strategies help you to evaluate things in a precise way. Whatever is your topic… Read More »Content Metrics That Can Help You To Write  Dissertations
The post Content Metrics That Can Help You To Write  Dissertations appeared first on Data Science Central.]]></summary>
        <author>
            <name>EdwardNick</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EndlessVN open alpha today]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tx1gpx/endlessvn_open_alpha_today/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tx1gpx/endlessvn_open_alpha_today/"/>
        <updated>2022-04-05T18:24:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/roblox22y  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robots dress humans without the full picture]]></title>
        <id>https://news.mit.edu/2022/robots-dress-humans-without-full-picture-0405</id>
        <link href="https://news.mit.edu/2022/robots-dress-humans-without-full-picture-0405"/>
        <updated>2022-04-05T18:20:00.000Z</updated>
        <summary type="html"><![CDATA[MIT researchers design a robot that has a trick or two up its sleeve.]]></summary>
        <author>
            <name>Steve Nadis | MIT CSAIL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do you guys hear about the latest papers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tx1bjv/d_how_do_you_guys_hear_about_the_latest_papers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tx1bjv/d_how_do_you_guys_hear_about_the_latest_papers/"/>
        <updated>2022-04-05T18:18:42.000Z</updated>
        <summary type="html"><![CDATA[Hi! I'm a first-year Grad Student in Computer Vision and I am trying to get caught up on the latest research in my field. It seems like everyone in CS has heard about all of the latest papers but I just have no idea how. My knowledge is limited to general ideas and doesn't know any specific papers unless they have like 20000+ citations.
 So my question is: how do you hear about these papers and get caught up? Is there a reference somewhere that puts together a list of all the "must-read" papers that have come out? I feel like I am already 5 years behind in my knowledge. It would be great if there was something like "Top 5 papers of the week" that I could read to stay on top of things.
 Also, this doesn't just apply to Vision. I would like to have an idea of the other major developments in other fields (like NLP, general ML/DL, etc.) since I think that can carry over to my field.
 Thanks! Looking forward to your replies
    submitted by    /u/TobusFire  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative analysis of an Intel and AMD Processor]]></title>
        <id>https://www.datasciencecentral.com/?p=57075</id>
        <link href="https://www.datasciencecentral.com/comparative-analysis-of-an-intel-and-amd-processor/"/>
        <updated>2022-04-05T18:16:54.000Z</updated>
        <summary type="html"><![CDATA[The need of a highly functional and fast processing Central Processing Unit (CPU) in today’s world is not just mostly desired, but also mostly required due to the rapid digitalization across the globe. Whether you work on a personal computer (PC) unit or laptop, the necessity of a highly advanced processor is indispensable.  This is… Read More »Comparative analysis of an Intel and AMD Processor
The post Comparative analysis of an Intel and AMD Processor appeared first on Data Science Central.]]></summary>
        <author>
            <name>Karen Anthony</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Fake authors and paper riders]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tx0z89/d_fake_authors_and_paper_riders/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tx0z89/d_fake_authors_and_paper_riders/"/>
        <updated>2022-04-05T18:03:17.000Z</updated>
        <summary type="html"><![CDATA[Based on my experiences in both academia and industry, I see that many researchers get listed as authors on papers solely for having attended the relevant project meetings, despite not contributing anything substantial to the work. I know of several people who've gotten on dozens of papers this way, despite not being able to explain the main details behind many of the papers they "co-authored." Of course, they can then claim credit for the work publicly as well as have their academic profile benefit from the citations accrued by the work.
 I've noticed that typically, these people are initially invited onto the project because they are on chummy terms with someone on the project. Concerningly, the more someone successfully "paper-rides" this way, the stronger their publication record looks, which makes it easier for them to find their way onto more projects to paper ride in the future.
 It seems that the obsessive focus on paper counts and citations has encouraged the rise of intellectually dishonest strategies for maximizing one's academic footprint. The huge research scientist salaries at top industry labs, which similarly obsess over paper counts and citations in their hiring process, only amplifies the incentive for paper riding.
 The reason I think it is bad: As more people paper ride, co-authorship on a paper gradually becomes a worse indication of expertise. Not to mention, paper riders are intellectually dishonest, by claiming credit for research that they didn't significantly contribute to. In a sense, it seems like a roundabout form of plagiarism.
 I know some might disagree with this take, as some people believe in being as generous about co-authorship as possible. I find that mindset to create the perfect environment for paper riders to flourish. I'm wondering if you've also seen paper riding happen and whether you think this behavior is good or bad.
    submitted by    /u/alwayshumming  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducibility in Deep Learning and Smooth Activations]]></title>
        <id>http://ai.googleblog.com/2022/04/reproducibility-in-deep-learning-and.html</id>
        <link href="http://ai.googleblog.com/2022/04/reproducibility-in-deep-learning-and.html"/>
        <updated>2022-04-05T17:41:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Gil Shamir and Dong Lin, Research Software Engineers, Google Research 
Ever queried a recommender system and found that the same search only a few moments later or on a different device yields very different results? This is not uncommon and can be frustrating if a person is looking for something specific. As a designer of such a system, it is also not uncommon for the metrics measured to change from design and testing to deployment, bringing into question the utility of the experimental testing phase. Some level of such irreproducibility can be expected as the world changes and new models are deployed. However, this also happens regularly as requests hit duplicates of the same model or models are being refreshed.  
Lack of replicability, where researchers are unable to reproduce…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agents learns policy when sampling last episode from replay buffer, but don't when randomly sampling from the replay buffer]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tx0cmf/agents_learns_policy_when_sampling_last_episode/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tx0cmf/agents_learns_policy_when_sampling_last_episode/"/>
        <updated>2022-04-05T17:33:13.000Z</updated>
        <summary type="html"><![CDATA[Hi all. I've been stuck on this problem for a while and I thought I might be able to find some help here. Any kind of assistance would be greatly appreciated. 
 My setup is as follows. I have an environment with 3 agents. All 3 agents have a single policy network, and it is based on CommNet. My goal is to implement a replay buffer for this environment. I verified that my replay buffer logic is good. I tried running 3 different types of runs:
  
Normal on-policy run: The agents perform an episode, and at the end of each episode the data (such as the states, actions, etc) from this episode are used to calculate the loss
 Using just the last episode from the replay buffer: The agents perform an episode, and the data is stored in the replay buffer. At the end of each episode, the last episode is sampled from the replay buffer (which is the episode that was just performed). This is just to confirm that my replay buffer is working properly, and the reward curve for this case matches that from (1).
 Using 1 random episode from the replay buffer: The agents perform an episode, and the data is stored in the replay buffer. At the end of each episode, a random episode is sampled from the replay buffer and used to calculate the loss. The performance is terrible in this case, and the environment times out each time
  
For some reason, as soon as I turn on random sampling, progress is really bad. I'm sorry to pose such an open-ended question, but what are some things I could check to pinpoint the source of this problem? What could be a reason as to why performance is as expected when just sampling the last episode, whereas it is terrible when randomly sampling episodes? I've tried some things thus far but nothing has worked, and I turned to this community in hopes of getting some help. I'm new to the area of reinforcement learning, so I would be very grateful for any kind of help you can offer. Thanks in advance
    submitted by    /u/lebr0n99  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customize the Amazon SageMaker XGBoost algorithm container]]></title>
        <id>0e8423c3b0ca2b8aa2fd2cab118be4196def0bbd</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/customize-the-amazon-sagemaker-xgboost-algorithm-container/"/>
        <updated>2022-04-05T17:24:58.000Z</updated>
        <summary type="html"><![CDATA[The built-in Amazon SageMaker XGBoost algorithm provides a managed container to run the popular XGBoost machine learning (ML) framework, with added convenience of supporting advanced training or inference features like distributed training, dataset sharding for large-scale datasets, A/B model testing, or multi-model inference endpoints. You can also extend this powerful algorithm to accommodate different requirements. […]]]></summary>
        <author>
            <name>Peyman Razaghi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect adversarial inputs using Amazon SageMaker Model Monitor and Amazon SageMaker Debugger]]></title>
        <id>b427fe772702e9e6bb92c140946b1bcd9c5c6dc0</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/detect-adversarial-inputs-using-amazon-sagemaker-model-monitor-and-amazon-sagemaker-debugger/"/>
        <updated>2022-04-05T17:19:32.000Z</updated>
        <summary type="html"><![CDATA[Research over the past few years has shown that machine learning (ML) models are vulnerable to adversarial inputs, where an adversary can craft inputs to strategically alter the model’s output (in image classification, speech recognition, or fraud detection). For example, imagine you have deployed a model that identifies your employees based on images of their […]]]></summary>
        <author>
            <name>Nathalie Rauschmayr</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][R] Generate random sample for exponentiated Weibull distribution]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twzqsk/dr_generate_random_sample_for_exponentiated/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twzqsk/dr_generate_random_sample_for_exponentiated/"/>
        <updated>2022-04-05T17:06:57.000Z</updated>
        <summary type="html"><![CDATA[Hi there experts, I have a real distribution for which I had run this scipy script to detect the best fit:
 However, the script outputs 4 parameter values and the best fit is actually a Exponentiated Weibull distribution.
 Now I am clueless how to generate a sample list of data of n-size. I know for sure about the normal distribution after getting these params as mean and sigma. How to I generate such list. Please help.
 ​
 ​
 https://preview.redd.it/79n28icmsqr81.png?width=1141&format=png&auto=webp&s=d9478691c06f5cdfe03af4f82db8293443e91f1e
    submitted by    /u/GoldenDew9  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI And Its Impact On Diversity And Inclusion]]></title>
        <id>https://www.datasciencecentral.com/?p=57089</id>
        <link href="https://www.datasciencecentral.com/ai-and-its-impact-on-diversity-and-inclusion/"/>
        <updated>2022-04-05T17:04:16.000Z</updated>
        <summary type="html"><![CDATA[How does artificial intelligence Diversity, Equity, and Inclusion (DEI) fit into the technological stack of daily companies? Fostering a diverse workforce is a very human problem. The cry for a halt to race prejudice has become deafening, and it’s increasingly a decisive factor for talent when weighing job offers and purchases. To stay up with the… Read More »AI And Its Impact On Diversity And Inclusion
The post AI And Its Impact On Diversity And Inclusion appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][D] VAE Embedding Space - Can we force it to learn a metric?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twznq8/rd_vae_embedding_space_can_we_force_it_to_learn_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twznq8/rd_vae_embedding_space_can_we_force_it_to_learn_a/"/>
        <updated>2022-04-05T17:03:16.000Z</updated>
        <summary type="html"><![CDATA[I understand that certain AE types such as B-VAE disentangle certain aspects of variation in the data, and those such as Conditional AE or VAE allow us to separate these aspects with labels.
 However, what I have seen is that the embedding space doesn't cluster the images as well as some contrastive methods. However contrastive methods require non-elegant negative sampling etc. 
 Can we somehow force the VAE to learn both the variational lower bound as well as learn a good metric between samples such as visually similar samples are better clustered together?
    submitted by    /u/jim_from_truckistan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Jetson AGX Orin dev kit as a stand-alone training platform]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twylnl/d_jetson_agx_orin_dev_kit_as_a_standalone/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twylnl/d_jetson_agx_orin_dev_kit_as_a_standalone/"/>
        <updated>2022-04-05T16:14:45.000Z</updated>
        <summary type="html"><![CDATA[The Jetson Orin 64gb model has "275 Sparse|138 Dense INT8 TOPS", and I am a little confused about how to compare this to something like the RTX a6000's performance. I am looking to do deep rl training and am new to the field. What metrics make a difference for deep rl? Any thoughts on the Orin dev kit's ability to train deep rl?
    submitted by    /u/here_to_create  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] With the rise of AutoML, what are the important skills for a ML career?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twxmk9/d_with_the_rise_of_automl_what_are_the_important/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twxmk9/d_with_the_rise_of_automl_what_are_the_important/"/>
        <updated>2022-04-05T15:30:36.000Z</updated>
        <summary type="html"><![CDATA[Some time down the road, when AutoML becomes more established, it can help us determine the best ML model and hyperparameters for a particular problem. This will not replace data scientist, as we still need data scientists for their domain knowledge, which is critical for scoping business problems, pre-processing data, and deriving business insights from the trained model. However, since data scientists no longer need to deal with the technicalities of a model in the near future (i.e. they no longer have to tune hyperparameters, determine the best opitmistion function etc), is there still a need for aspiring data scientists to learn about the intricacies and nuances behind the various models (maybe by coding the model from scratch)? Or is it enough for them to learn how to operate an AutoML system? (My question is referring to the corporate world in general and not to academia) Thanks in advance for your answers :)
    submitted by    /u/smart_oinker  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unreal Engine and NVIDIA: From One Generation to the Next]]></title>
        <id>https://blogs.nvidia.com/?p=56436</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/05/unreal-engine-5/"/>
        <updated>2022-04-05T15:10:12.000Z</updated>
        <summary type="html"><![CDATA[Square/Enix presents the fictional city of Midgar in Final Fantasy VII Remake at a filmic level of detail. Epic’s Fortnite bathes its environments in ray-traced sunlight, simulating how light bounces in the real world. And artists at Lucasfilm revolutionized virtual production techniques in The Mandalorian, using synchronized NVIDIA RTX GPUs to drive pixels on LED Read article >
The post Unreal Engine and NVIDIA: From One Generation to the Next appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Ethan Einhorn</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Green Teams Achieve the Dream: NVIDIA Announces NPN Americas Partners of the Year]]></title>
        <id>https://blogs.nvidia.com/?p=56434</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/05/americas-npn-award-winners-2021/"/>
        <updated>2022-04-05T15:00:15.000Z</updated>
        <summary type="html"><![CDATA[A dozen companies today received NVIDIA’s highest award for partners, recognizing their impact on AI education and adoption across such industries as education, federal, healthcare and technology. The winners of the 2021 NPN Americas Partner of the Year Awards have created a profound impact on AI by helping customers meet the demands of recommender systems, Read article >
The post Green Teams Achieve the Dream: NVIDIA Announces NPN Americas Partners of the Year appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Craig Weinstein</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bounding zeros of an analytic function]]></title>
        <id>https://www.johndcook.com/blog/?p=101725</id>
        <link href="https://www.johndcook.com/blog/2022/04/05/analytic-zeros/"/>
        <updated>2022-04-05T14:33:39.000Z</updated>
        <summary type="html"><![CDATA[The previous post looked at the problem of finding the zeros of a cubic polynomial. Assuming we’re going to use a numerical method to calculate the zero, the hard part is knowing where to tell the numerical method to look. That post showed how to use a change of variables to guarantee that the polynomial […]
Bounding zeros of an analytic function first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Meets Quantum Technology in New Google Spinoff, Sandbox AQ - News]]></title>
        <id>https://www.reddit.com/r/artificial/comments/twvsua/ai_meets_quantum_technology_in_new_google_spinoff/</id>
        <link href="https://www.reddit.com/r/artificial/comments/twvsua/ai_meets_quantum_technology_in_new_google_spinoff/"/>
        <updated>2022-04-05T14:08:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/allaboutcircuits  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPO sample correlation?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twvl67/ppo_sample_correlation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twvl67/ppo_sample_correlation/"/>
        <updated>2022-04-05T13:59:05.000Z</updated>
        <summary type="html"><![CDATA[Hi, I'm wondering if the PPO algorithm can solve the sample correlation problem of on-policy algorithm in training. PPO uses successive samples to compute GAE, doesn't the sample correlation occurring here interfere with learning?
    submitted by    /u/noisemastar  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AutoML-Conf Competition: DAC4AutoML]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twu26i/p_automlconf_competition_dac4automl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twu26i/p_automlconf_competition_dac4automl/"/>
        <updated>2022-04-05T12:42:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/catsortion  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AutoML-Conf Competition: DAC4AutoML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twu1z5/p_automlconf_competition_dac4automl/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twu1z5/p_automlconf_competition_dac4automl/"/>
        <updated>2022-04-05T12:42:17.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone!
 We've just launched a competition at the AutoML-Conf 2022, the DAC4AutoML competition. It has two tracks, one for configuring a Computer Vision model and one for a RL pipeline: https://automl.github.io/dac4automlcomp/ 
 And what is DAC exactly? It means we want to find well-performing hyperparameter configurations like in Algorithm Configuration, but we do it dynamically - thus DAC, Dynamic Algorithm Configuration. As to how that is supposed to happen? We don't put any restrictions on the solutions for the competitions, so you can submit your hand-tuned static hyperparameter setting if you want. Or you can use some sort of heuristic, a regression model, reinforcement learning, ... whatever works. 
 If you're interested in participating, you can submit from now on until the 18.06. AOE, the winners will be announced at the AutoML-Conf.
    submitted by    /u/catsortion  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Difference Learning for Model Predictive Control]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twu0ai/temporal_difference_learning_for_model_predictive/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twu0ai/temporal_difference_learning_for_model_predictive/"/>
        <updated>2022-04-05T12:39:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bendee983  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why is there no rollout monitoring for this CustomEnv (on the right) ?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twtuwp/why_is_there_no_rollout_monitoring_for_this/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twtuwp/why_is_there_no_rollout_monitoring_for_this/"/>
        <updated>2022-04-05T12:31:27.000Z</updated>
        <summary type="html"><![CDATA[​
 Output from using model.learn(env) on both Envs
 On the left I have a simple dummy CustomEnv (Using Stable-Baselines3 with Gym) for testing, and on the right I have my actual CustomEnv that I am working on in a project.
 As you can see, the dummy environment gives me the rollout monitoring, whereas there is no rollout monitoring for the actual environment (just time + train statistics/monitoring). 
 I am using very similar code when setting up the training of the model, however the complexity of the actual model is significantly higher than the dummy. In theory, the complexity of the environment shouldnt make a big difference to the monitoring right? All of the key parts are still there (reward function, step function, reset function etc.).
 In both cases it says that the environments are being wrapped by the 'Moniter' wrapper so that cant be it.
 Does anyone know why this might be happening?
    submitted by    /u/C_BearHill  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Imagenet Original Pictures]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twtg6d/d_imagenet_original_pictures/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twtg6d/d_imagenet_original_pictures/"/>
        <updated>2022-04-05T12:07:50.000Z</updated>
        <summary type="html"><![CDATA[As I understood it Imagenet got generated from internet images, but I am unable to to find the originals using naive image search. Is there any mapping? I wonder if imagenet data is a cropped versions of original pictures or not, i don't see it in the paper.
    submitted by    /u/LeanderKu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerically finding roots of a cubic]]></title>
        <id>https://www.johndcook.com/blog/?p=101681</id>
        <link href="https://www.johndcook.com/blog/2022/04/05/cubic/"/>
        <updated>2022-04-05T12:00:16.000Z</updated>
        <summary type="html"><![CDATA[The analog of the quadratic formula for cubic equations is cumbersome. A lot of people naturally say “Forget all that. If I need to find the roots of a cubic, I’ll just use a numerical method like Newton’s method.” Sounds good. Where to start? But how do you know where to look for the roots? […]
Numerically finding roots of a cubic first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] UFO Lands on Highway! Or Depth Estimation using ML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twt98b/p_ufo_lands_on_highway_or_depth_estimation_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twt98b/p_ufo_lands_on_highway_or_depth_estimation_using/"/>
        <updated>2022-04-05T11:57:51.000Z</updated>
        <summary type="html"><![CDATA[Article describing depth estimation using machine learning models and 3D visualization of depth maps using three.js.
 https://www.storminthecastle.com/posts/ufos_and_depth/
    submitted by    /u/CakeStandard3577  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Could Stylegan-XL be great for out-of-domain generation?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twspe4/d_could_styleganxl_be_great_for_outofdomain/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twspe4/d_could_styleganxl_be_great_for_outofdomain/"/>
        <updated>2022-04-05T11:25:38.000Z</updated>
        <summary type="html"><![CDATA[In the context of text-to-image generation, I'd say one of the reasons VQGAN is so used in popular notebooks is that it can deal with many concepts, while stylegan used to be limited to the domain it was trained for. 
 That may be about to change with the rollout release of Stylegan-XL weights trained on Imagenet. This notebook (https://github.com/CasualGANPapers/StyleGANXL-CLIP) has had nice results with objects never seen by the model, such as "apple" and "ant", as well as scenes such as "judo athletes fighting"
 Please note that the Stylegan-XL weights are currently available for 128x128 pixels. ETA for the 256 resolution is 14.04.22
    submitted by    /u/HrodRuck  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Value Iteration in Car Racing V1]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twrz5x/value_iteration_in_car_racing_v1/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twrz5x/value_iteration_in_car_racing_v1/"/>
        <updated>2022-04-05T10:38:55.000Z</updated>
        <summary type="html"><![CDATA[I’m working on Q table learning model for OpenAI’s. I have everything done in regards to a basic agent, but I’m unsure how I’m supposed to use the box data for action space and observance space, to populate a q table?
 Or is this approach incorrect? Car Racing doesn’t have a P (probability) call so I’m not sure how else I would do value iteration.
    submitted by    /u/Dzartovian94  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best undergraduate major besides computer science for pursuing a career in artificial intelligence?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/twrsgf/best_undergraduate_major_besides_computer_science/</id>
        <link href="https://www.reddit.com/r/artificial/comments/twrsgf/best_undergraduate_major_besides_computer_science/"/>
        <updated>2022-04-05T10:26:35.000Z</updated>
        <summary type="html"><![CDATA[Hi, all. I got accepted into my top choice of college as an undecided major. Recently, I have decided to pursue artificial intelligence! Unfortunately, it is near impossible to transfer into computer science at my particular university. I was wondering if I can still pursue AI as a career if I complete one of the following majors:
 -Mathematics
 -Information or Data Science
 -Statistics
 -Linguistics
 Additionally, I could pursue one of these and minor in another. I should be able to minor in computer science as well if necessary. Hopefully, my choice of major would allow me to pursue research or an internship in artificial intelligence. I am willing to take additional summer courses and pursue relevant certifications to ensure that I am up to par with my computer science colleagues. 
 (posted on behalf of a family member)
    submitted by    /u/runelagoon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing old and new AI voices from Replica Studios (new in second half)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/twqf43/comparing_old_and_new_ai_voices_from_replica/</id>
        <link href="https://www.reddit.com/r/artificial/comments/twqf43/comparing_old_and_new_ai_voices_from_replica/"/>
        <updated>2022-04-05T08:47:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/autumns  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Support Vector Machines... in 2022]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twqel5/discussion_support_vector_machines_in_2022/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twqel5/discussion_support_vector_machines_in_2022/"/>
        <updated>2022-04-05T08:46:32.000Z</updated>
        <summary type="html"><![CDATA[My post is inspired by this discussion.
 In that thread, OP asked why support vector machines are still taught. People offered several thoughts: they're easier to think about, they're still perfectly good for some real-world problems, and for some problems they apparently rival deep networks.
 I did a project for a class around six years ago using an SVM as implemented in scikit-learn. I was pretty satisfied with the project, but I also experienced some frustrations, and came away with some questions. I started working with Tensorflow and DNNs in earnest soon after finishing that project, and I largely stopped thinking about SVM. I would like to revive the questions I asked, but never answered, here.
  
A DNN with multiple outputs can potentially use a single neuron in the prediction of more than one output. For multiple, mutually-exclusive categories, this makes good sense. An SVM with multiple outputs in scikit-learn was implemented as pairs of one-vs-one SVMs, each of which was independently fit to data. This gets inefficient quickly. Has this changed? Can it be changed?
 DNN training at scale is a problem that many people have worked hard to make practical. Even non-experts like myself use our home GPUs to accelerate training of DNNs on large data sets. In scikit-learn, SVM training was implemented in a single thread on one CPU core. If you are performing cross-validation or a hyperparameter optimization study, it might be practical to parallelize fitting; one thread for each distinct condition. But can you parallelize the SVM fitting algorithm for a single condition? I went looking for software, but I couldn't find anything.
  
Over to you folks. Cheers.
    submitted by    /u/aotus_trivirgatus  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Restormer: Efficient Transformer for High-Resolution Image Restoration (CVPR 2022--ORAL) + Colab Demo + Gradio Web Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twqeei/r_restormer_efficient_transformer_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twqeei/r_restormer_efficient_transformer_for/"/>
        <updated>2022-04-05T08:46:08.000Z</updated>
        <summary type="html"><![CDATA[​
 Visual Results
 With Restormer, you can remove noise, motion blur, defocus blur, and rain streaks from your own images.
 Paper: https://arxiv.org/abs/2111.09881
 Github: https://github.com/swz30/Restormer
 Colab Demo: https://colab.research.google.com/drive/1C2818h7KnjNv4R1sabe14_AYL7lWhmu6?usp=sharing
 Gradio Web Demo: https://huggingface.co/spaces/swzamir/Restormer
    submitted by    /u/swz30  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Spaces all landing to zero probability in few steps]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twp4x8/action_spaces_all_landing_to_zero_probability_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twp4x8/action_spaces_all_landing_to_zero_probability_in/"/>
        <updated>2022-04-05T07:13:26.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I am new to RL and walking through the Keras implementation of Actor Critic.
 ​
 As a variant of it, I am trying to learn the strategy for WORDLE. However, after a few runs, my action spaces all go down to zero. Not sure what's happening. Could someone have any insights or pointers?
 ​
 Attaching my code for reference.
 ​
 Thanks
 import pandas as pd import numpy as np import random import string import random import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers # Configuration parameters for the whole setup gamma = 0.9 # Discount factor for past rewards max_runs = 10000 eps = np.finfo(np.float32).eps.item() # Smallest number such that 1.0 + eps != 1.0 my_file = open("<wordle set of words data path>", "r") content = my_file.read() content = lis…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Data Intelligence Platforms Promote Business Success]]></title>
        <id>https://www.datasciencecentral.com/?p=57056</id>
        <link href="https://www.datasciencecentral.com/how-data-intelligence-platforms-promote-business-success/"/>
        <updated>2022-04-05T06:12:10.000Z</updated>
        <summary type="html"><![CDATA[Understanding consumer behavior is becoming more and more critical as businesses seek to find innovative ways to survive and thrive in a period of constant change. In the last few years, the market has seen significant changes in the way people shop, travel, dine and purchase goods. As a business, when it comes to understanding… Read More »How Data Intelligence Platforms Promote Business Success
The post How Data Intelligence Platforms Promote Business Success appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kerry Pearce</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring AI labeling for children’s products]]></title>
        <id>https://www.datasciencecentral.com/?p=57040</id>
        <link href="https://www.datasciencecentral.com/exploring-ai-labeling-for-childrens-products/"/>
        <updated>2022-04-05T06:06:34.000Z</updated>
        <summary type="html"><![CDATA[I read an article from the world economic forum which proposed an AI labeling system for AI products designed for children Today, for the first time, children are growing up in a world shaped by artificial intelligence (AI) and decisions are being made for children implicitly by AI.  Algorithms need data that is collected and… Read More »Exploring AI labeling for children’s products
The post Exploring AI labeling for children’s products appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Super-res model/program comparison]]></title>
        <id>https://www.reddit.com/r/artificial/comments/twmyc3/superres_modelprogram_comparison/</id>
        <link href="https://www.reddit.com/r/artificial/comments/twmyc3/superres_modelprogram_comparison/"/>
        <updated>2022-04-05T04:50:22.000Z</updated>
        <summary type="html"><![CDATA[I upscaled an image with a few different superres models and programs, pick your favorite!
 https://files.botbox.dev/superrestestcollage.png
 Because of how reddit is, I can't make this as a poll, so comment your pick.
 Animated original version: https://www.youtube.com/watch?v=zRaTwVuqd70 (I will also make an animated version upscaled with the most voted model/program)
    submitted by    /u/Recent_Coffee_2551  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] [D] Seq2seq model hyperparameters tuning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twmnol/r_d_seq2seq_model_hyperparameters_tuning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twmnol/r_d_seq2seq_model_hyperparameters_tuning/"/>
        <updated>2022-04-05T04:32:59.000Z</updated>
        <summary type="html"><![CDATA[Does anyone have any advices or research papers on what hyperparameters do researchers use for their seq2seq model? 
 I am interested in knowing whether hyperparameters such as dropout, or recurrent dropout, batchnorm, etc etc, are even necessary in the usage of seq2seq model, but couldn’t find anything on it for weeks.
 In the case, let’s say, using gridsearchCV, what hyperparameters do you tweak for ur seq2seq model? (Other than the usual stuff like number of neurons, etc). There is absolutely zero information for that on seq2seq model, and everyone just assumes that putting an attention mechanism solves everything without hyperparameters tunings. I have also looked up on codes on seq2seq, and no hyperparameters tunings were shown whatsoever. 
 FYI, this is in the context of time series data, using seq2seq, if that matters.
 Thanks
    submitted by    /u/plsendfast  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Has anyone seen any papers related to GANs which prove that the optimum remains unchanged when adding supervised loss (e.g. L1, L2)?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twmfom/d_has_anyone_seen_any_papers_related_to_gans/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twmfom/d_has_anyone_seen_any_papers_related_to_gans/"/>
        <updated>2022-04-05T04:19:49.000Z</updated>
        <summary type="html"><![CDATA[I’ve been reading many papers lately pertaining to GANs, with more and more introducing supervised loss into the generator’s objective function. However, no one ever seems to show that the optimum remains undisturbed. Results seem to be strictly empirical most of the time. 
 Has anyone seen any papers where it is shown that the disruption to the generator’s loss doesn’t harm convergence?
    submitted by    /u/king_of_walrus  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any RL-related conferences right after NeurIPS 22’?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twlxdl/any_rlrelated_conferences_right_after_neurips_22/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twlxdl/any_rlrelated_conferences_right_after_neurips_22/"/>
        <updated>2022-04-05T03:51:12.000Z</updated>
        <summary type="html"><![CDATA[In case my NeurIPS submission rejected, lol.
    submitted by    /u/Blasphemer666  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[solo voiceovers]]></title>
        <id>https://www.reddit.com/r/artificial/comments/twl2bd/solo_voiceovers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/twl2bd/solo_voiceovers/"/>
        <updated>2022-04-05T03:02:48.000Z</updated>
        <summary type="html"><![CDATA[I am looking for something to change my voice in a way that is more satisfactory and more convincingly varied than what simple voice modulation software can achieve and as cheaply as is possible (preferably free).
 Use case: I have been working on an animated movie to which I am the sole contributor. Though I have been putting it off while looking for an appropriate solution, the time has come to voice my various characters, who are a range of ages, both male and female. For several reasons, I am interested in voicing them all myself while doing the facial motion captures as well. What I am in need of is, essentially, something that does exactly what Respeecher does, but without the $200/month sub fee. I would love to be in a position to simply pay them what they are asking for in exchange…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares: Frenzied Flame || Clip Guided Diffusion AI Art Video [4K 20 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/twkptb/artificial_nightmares_frenzied_flame_clip_guided/</id>
        <link href="https://www.reddit.com/r/artificial/comments/twkptb/artificial_nightmares_frenzied_flame_clip_guided/"/>
        <updated>2022-04-05T02:44:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI that takes multiple songs as input, and then generates a similar song or song with similar elements?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/twjoqd/ai_that_takes_multiple_songs_as_input_and_then/</id>
        <link href="https://www.reddit.com/r/artificial/comments/twjoqd/ai_that_takes_multiple_songs_as_input_and_then/"/>
        <updated>2022-04-05T01:52:00.000Z</updated>
        <summary type="html"><![CDATA[I have been searching for a music AI that takes input as mp3 or midi files, yet haven't been successful yet. Is there such a thing? If not, is such a thing feasible?
    submitted by    /u/16pxl  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is your experience with Fake results or overfitted results being sold as awesome?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twidsn/d_what_is_your_experience_with_fake_results_or/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twidsn/d_what_is_your_experience_with_fake_results_or/"/>
        <updated>2022-04-05T00:48:43.000Z</updated>
        <summary type="html"><![CDATA[I am curious what is everyones experience with completely faked, falsified, or fabricated results in the area? Another aspect of this I think is people taking heavily overfitted results and finding one decent example that is from the test set and claiming their method is awesome. How much of this have you seen and how much of the research out there fits into this category?
    submitted by    /u/LifeguardDismal142  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mathematics and piano tuning]]></title>
        <id>https://www.johndcook.com/blog/?p=101654</id>
        <link href="https://www.johndcook.com/blog/2022/04/04/mathemacs-and-piano-tuning/"/>
        <updated>2022-04-04T23:19:24.000Z</updated>
        <summary type="html"><![CDATA[The following is a slightly edited version of a Twitter thread on @AlgebraFact. The lowest C on a piano is called C1 in scientific pitch notation. The C one octave up is C2 and so forth. Middle C is C4. The frequency of Cn is approximately 2n+4 Hz. This would be exact if C0 were […]
Mathematics and piano tuning first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PPO Alg confusion]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twg36r/ppo_alg_confusion/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twg36r/ppo_alg_confusion/"/>
        <updated>2022-04-04T23:10:05.000Z</updated>
        <summary type="html"><![CDATA[As I read the paper and several tutorials, I am quite confused about the details.
  
I see many implementations scale the running cumulative discounted reward in a certain way. However, each of them does it in a different way. let R be the running cumulative discounted reward, which is considered the best? Or is there a source of the method to use? implementations I saw from different places include: 
  
 directly use R to calculate the advantage and training value network (most PPO tutorials use this)
 use (R / std(R)), where std is the mini-batch standard deviation
 use (R / std(R)), where std is the running standard deviation
 use ((R - mean(R)) / std(R)), where both mean and std are mini-batch wise
 use ((R - mean(R)) / std(R)), where both mean and std are running stats.
 do the above and clip to a certain range ([-10, 10] or [-1, 1])
  
 I also see several different ways for the value network, let V be the output of the value network:
  
 output raw logit, without any scaling/output activation (most PPO tutorials use this)
 output raw logit, but use the same scaling as discussed above for running cumulative discounted reward, for example, if return value is (R / std(R)), value output will be (V / std(R))
 do the same as 2, but use the stat of V instead of R for scaling, for example, if the return value is (R / std(R)), the value output will be (V / std(V))
 output with tanh activation at the last layer
 output with tanh activation at the last layer, and multiply by a constant to match the range of the return
  
any help would be appreciated, thanks!
    submitted by    /u/seermer  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why do we still teach support vector machines?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twcdt2/d_why_do_we_still_teach_support_vector_machines/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twcdt2/d_why_do_we_still_teach_support_vector_machines/"/>
        <updated>2022-04-04T20:38:01.000Z</updated>
        <summary type="html"><![CDATA[Honest question: are there any applications for which SVMs are the best choice? In my experience, no one seems to use this methodology anymore, though maybe I'm wrong. It just kinda feels like teaching people how to use a slide rule when everyone has calculators.
    submitted by    /u/WartimeHotTot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Paper Explained - Continual Backprop: Stochastic Gradient Descent with Persistent Randomness]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/twc4or/d_paper_explained_continual_backprop_stochastic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/twc4or/d_paper_explained_continual_backprop_stochastic/"/>
        <updated>2022-04-04T20:27:50.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/zEMOX3Di2Tc
 This paper finds what seems to be a new phenomenon when working in the continual learning/life-long learning domain. If new tasks are continually introduced to an agent, it seems to loose it's ability to learn the more time progresses. Intuitively it's similar to this idea that "an old dog can't learn new tricks". They propose a fairly simple method of overcoming this limitation that involves resetting weights that are not contributing much to the outcome of the network. They call the method Continual Backprop.
 ​
 Outline:
 0:00 - Overview
 2:00 - Paper Intro
 2:53 - Problems & Environments
 8:11 - Plasticity Decay Experiments
 11:45 - Continual Backprop Explained
 15:54 - Continual Backprop Experiments
 22:00 - Extra Interesting Experiments
 25:34 - Summary 
 ​
 Paper link: https://arxiv.org/abs/2108.06325
    submitted by    /u/SlickBlueML  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I’m completely new to RL and will be building my first model as part of my degree-ending project. Do you have any tips you can provide?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twbnb2/im_completely_new_to_rl_and_will_be_building_my/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twbnb2/im_completely_new_to_rl_and_will_be_building_my/"/>
        <updated>2022-04-04T20:07:52.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 As the title describes, I’ll be making my first model as part of my final project. I still have a pretty high-level understanding of everything, so forgive any inaccuracies as I describe what I’m going for.
 The problem I’m attempting to solve is known as the traveling salesman problem. Essentially, a route needs to be formulated that stops at n given locations. Finding the most efficient route with many stops algorithmically is impractical because the number of possible routes increases exponentially with each added location. 
 The environment will simulate travel on city roads. Speed will be a constant, set to whatever the roads speed limit is. I am using .pbf format vector GIS data from OSM so that the environment consists of real-world pathways. I’m using GeoPandas and Pyrosm to work with the data, and I’m collecting nodes for the location of gas stations so that the environment can simulate needing to fuel the vehicle. Gas price will be a constant, as well as vehicle fuel-efficiency.
 Scoring will be based on the calculated time it would take to complete a route and the calculated cost (in gas). The goal will be to find the most efficient route to take when n = some large number. 
 I’ve never worked with spatial data either, so I’m not sure what kind of challenges that poses. I worry that adding nodes for the locations of gas stations might be difficult. I’m also wondering if I’m better off using Tensorflow and Keras for this, but I’m not really aware of all the technical considerations I should be making before deciding on that.
 Do you have any tips that might help me out? Solutions to problems I haven’t hit just yet, but likely will? 
 Thanks for your help!
    submitted by    /u/professorDissociate  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best implementations for extensibility?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twbn40/best_implementations_for_extensibility/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twbn40/best_implementations_for_extensibility/"/>
        <updated>2022-04-04T20:07:40.000Z</updated>
        <summary type="html"><![CDATA[As far as I am aware, StableBaselines3 is the gold standard for reliable implementations of most popular / SOTA deep RL methods. However working with them in the past, I don't find them to be the most usable when looking for extensibility (making changes to the provided implementations) due to how the code base is structured in the behind the scenes (inheritance, lots of helper methods & utilities, etc.).
 For example, if I wish to change some portion of a method's training update with SB3 it would probably involve overloading a class method before initialization, making sure al the untouched portions of the original method are carried over, etc.
 Could anyone point me in the direction of any implementations that are more workable from the perspective of extensibility? Ideally implementations that are largely self contained to a single class / file, aren't heavily abstracted aware across multiple interfaces, don't rely heavily on utility functions, etc.
    submitted by    /u/Farconion  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logging in Python]]></title>
        <id>https://machinelearningmastery.com/?p=13385</id>
        <link href="https://machinelearningmastery.com/logging-in-python/"/>
        <updated>2022-04-04T20:00:58.000Z</updated>
        <summary type="html"><![CDATA[Logging is a way to store information about your script and track events that occur. When writing any complex script […]
The post Logging in Python appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Daniel Chung</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build an MLOps sentiment analysis pipeline using Amazon SageMaker Ground Truth and Databricks MLflow]]></title>
        <id>093bc4f91e188e1a2e2f385311b63204025f2c19</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-an-mlops-sentiment-analysis-pipeline-using-amazon-sagemaker-ground-truth-and-databricks-mlflow/"/>
        <updated>2022-04-04T19:43:42.000Z</updated>
        <summary type="html"><![CDATA[As more organizations move to machine learning (ML) to drive deeper insights, two key stumbling blocks they run into are labeling and lifecycle management. Labeling is the identification of data and adding labels to provide context so an ML model can learn from it. Labels might indicate a phrase in an audio file, a car […]]]></summary>
        <author>
            <name>Rumi Olsen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[School of Engineering welcomes Thomas Tull as visiting innovation scholar]]></title>
        <id>https://news.mit.edu/2022/school-engineering-welcomes-thomas-tull-visiting-innovation-scholar-0404</id>
        <link href="https://news.mit.edu/2022/school-engineering-welcomes-thomas-tull-visiting-innovation-scholar-0404"/>
        <updated>2022-04-04T19:40:00.000Z</updated>
        <summary type="html"><![CDATA[Primary focus will be to advance and promote technology, innovation, and entrepreneurship across the school.]]></summary>
        <author>
            <name>Lori LoTurco | School of Engineering</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it possible to use inspect.getcallargs to convert *args and **kwargs to a canonical kwarg representation in RL?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/twaicu/is_it_possible_to_use_inspectgetcallargs_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/twaicu/is_it_possible_to_use_inspectgetcallargs_to/"/>
        <updated>2022-04-04T19:21:36.000Z</updated>
        <summary type="html"><![CDATA[Given a NN class, is there something specific we need to care of when converting *args and **kwargs to a canonical kwarg representation? I ask this because in this code from Google (https://github.com/google-research/google-research/blob/c56b47713b08c95ad427d5f93ee0dbb9ad008964/social_rl/multiagent_tfagents/joint_attention/attention_networks.py#L557) they use a TFDecorator-aware replacement for inspect.getcallargs, instead of using getcallargs directly. So my questions are: 
 - Is it possible to use inspect.getcallargs to convert *args and **kwargs to a canonical kwarg representation?
 - If no, is there an equivalent in PyTorch? I couldn't find any, so I was wondering how people go about that.
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Google's 540B (Dense) model Pathways LLM, "Unlocks" new tasks proportional to scale]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tw9jp5/r_googles_540b_dense_model_pathways_llm_unlocks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tw9jp5/r_googles_540b_dense_model_pathways_llm_unlocks/"/>
        <updated>2022-04-04T18:42:07.000Z</updated>
        <summary type="html"><![CDATA[Blog: https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html
 Paper: https://goo.gle/palm-paper
 - AFAIK from the Blogpost, Scaling laws still hold up (i.e not yet plateaued)
 - New transfer learning capabilities, outperforms fine-tuned models with 50x less data (Codex-12B)
 - The interesting part is how it meta-learns techy geeky jokes and is able to correlate concepts, and explain jokes suggesting starting doing a bit more meta-learning than GPT3 ever could.... But still not enough to generate decent ones (though the joke wasn't particularly humorous, so I may be underestimating)
 SoTA on various tasks, chain-of-thought-reasoning still holds up to scaling and outperforms some reasoning benchmarks, BIG-bench sees a huge improvement and general LLM thingys :)
    submitted by    /u/Competitive-Rub-1958  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft Researchers Introduce ‘Jigsaw’: An AI Tool To Augment Large Language Models (GPT-3, Codex, etc.) By Deploying Post-Processing Techniques That Understand The Programs’ Syntax And Semantics]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tw91fr/microsoft_researchers_introduce_jigsaw_an_ai_tool/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tw91fr/microsoft_researchers_introduce_jigsaw_an_ai_tool/"/>
        <updated>2022-04-04T18:21:08.000Z</updated>
        <summary type="html"><![CDATA[GPT-3, Codex, and other sizable pre-trained language models can be adjusted to create code from natural language descriptions of programmer intent. Every developer in the world might benefit from these automated models, which have the potential to increase productivity. However, because the models may fail to understand program semantics, the quality of the generated code cannot be guaranteed.
 Microsoft researchers introduce Jigsaw, a new tool that can help these big language models perform better. Jigsaw is a Python Pandas API code generator that accepts multi-modal inputs. Jigsaw uses post-processing techniques to decipher the syntax and semantics of programs and then uses user feedback to improve future performance.
 Continue Reading
 Paper: https://arxiv.org/pdf/2112.02969.pdf
 Dataset: https://github.com/microsoft/JigsawDataset
 ​
 https://i.redd.it/x223r5qu0kr81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enable Amazon Kendra search for a scanned or image-based text document]]></title>
        <id>44ec1f237a527c498cfe53252dabc3cb1f64d1ce</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/enable-amazon-kendra-search-for-a-scanned-or-image-based-text-document/"/>
        <updated>2022-04-04T18:07:45.000Z</updated>
        <summary type="html"><![CDATA[Amazon Kendra is an intelligent search service powered by machine learning (ML). Amazon Kendra reimagines search for your websites and applications so your employees and customers can easily find the content they’re looking for, even when it’s scattered across multiple locations and content repositories within your organization. Amazon Kendra supports a variety of document formats, […]]]></summary>
        <author>
            <name>Sanjay Tiwary</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tw8fae/pathways_language_model_palm_scaling_to_540/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tw8fae/pathways_language_model_palm_scaling_to_540/"/>
        <updated>2022-04-04T17:56:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nick7566  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmented Healthcare Part 2]]></title>
        <id>https://www.datasciencecentral.com/?p=57058</id>
        <link href="https://www.datasciencecentral.com/data-augmented-healthcare-part-2/"/>
        <updated>2022-04-04T17:41:15.000Z</updated>
        <summary type="html"><![CDATA[In the previous part, we discussed the current state of data imaging tools in healthcare and the future applications of these technologies. While increased access to information is invaluable to physicians, they can still be limited by their own ability to interpret, or the physical limitations of their surgical ability. In addition to augmenting the… Read More »Data Augmented Healthcare Part 2
The post Data Augmented Healthcare Part 2 appeared first on Data Science Central.]]></summary>
        <author>
            <name>Scott Thompson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative AI+Alex Grey = xxxxxoooooooo (Disco Diffusion)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tw79so/generative_aialex_grey_xxxxxoooooooo_disco/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tw79so/generative_aialex_grey_xxxxxoooooooo_disco/"/>
        <updated>2022-04-04T17:09:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/JoshGrambo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[openAI gym return done==True but not seeing goal is reached]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tw75kp/openai_gym_return_donetrue_but_not_seeing_goal_is/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tw75kp/openai_gym_return_donetrue_but_not_seeing_goal_is/"/>
        <updated>2022-04-04T17:04:39.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I am running some starter code from openAI(FetchReach-v1, FetchPush-v1) gym with env.action_space.sample(). But I don't see the goal is actually achieved when done returned is True. I copied the code from here (https://openai.com/blog/ingredients-for-robotics-research/). I even let it sleep every step to watch more closely. Another related thing that I can't explain is that it always returns done==True rather quickly with very few sampled actions. These all make me worried about using it as my task environment.
    submitted by    /u/AnimatorRemarkable20  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UiPath extract Tables from PDF (use case) (PDF table)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tw6zcv/uipath_extract_tables_from_pdf_use_case_pdf_table/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tw6zcv/uipath_extract_tables_from_pdf_use_case_pdf_table/"/>
        <updated>2022-04-04T16:58:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Cristi_UiPath  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpret caller input using grammar slot types in Amazon Lex]]></title>
        <id>2aeebc95eb280c09dba0b3c96cc6c229198abe7f</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/interpret-caller-input-using-grammar-slot-types-in-amazon-lex/"/>
        <updated>2022-04-04T16:45:41.000Z</updated>
        <summary type="html"><![CDATA[Customer service calls require customer agents to have the customer’s account information to process the caller’s request. For example, to provide a status on an insurance claim, the support agent needs policy holder information such as the policy ID and claim number. Such information is often collected in the interactive voice response (IVR) flow at […]]]></summary>
        <author>
            <name>Kai Loreck</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance]]></title>
        <id>http://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html</id>
        <link href="http://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html"/>
        <updated>2022-04-04T16:01:00.007Z</updated>
        <summary type="html"><![CDATA[Posted by Sharan Narang and Aakanksha Chowdhery, Software Engineers, Google Research 
In recent years, large neural networks trained for language understanding and generation have achieved impressive results across a wide range of tasks. GPT-3 first showed that large language models (LLMs) can be used for few-shot learning and can achieve impressive results without large-scale task-specific data collection or model parameter updating. More recent LLMs, such as GLaM, LaMDA, Gopher, and Megatron-Turing NLG, achieved state-of-the-art few-shot results on many tasks by scaling model size, using sparsely activated modules, and training on larger datasets from more diverse sources. Yet much work remains in understanding the capabilities that emerge with few-shot learning as we push the limits of …]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Minimum Description Length Recurrent Neural Networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tw4wyj/r_minimum_description_length_recurrent_neural/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tw4wyj/r_minimum_description_length_recurrent_neural/"/>
        <updated>2022-04-04T15:33:42.000Z</updated>
        <summary type="html"><![CDATA[https://arxiv.org/abs/2111.00600
 https://preview.redd.it/l6dni0007jr81.png?width=4888&format=png&auto=webp&s=82c7c9b9433b79c66318090ff85e4535c35ddb18
    submitted by    /u/inland-1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet the Omnivore: Videographer Makes Digital Walls, Virtual Homes Pop With NVIDIA Omniverse]]></title>
        <id>https://blogs.nvidia.com/?p=56447</id>
        <link href="https://blogs.nvidia.com/blog/2022/04/04/pekka-varis-omniverse-creator/"/>
        <updated>2022-04-04T15:00:58.000Z</updated>
        <summary type="html"><![CDATA[Pekka Varis’s artistry has come a long way from his early days as a self-styled “punk activist” who spray painted during the “old school days of hip hop in Finland.”
The post Meet the Omnivore: Videographer Makes Digital Walls, Virtual Homes Pop With NVIDIA Omniverse appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Angie Lee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composing Music with Neural Networks]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tw3wz6/composing_music_with_neural_networks/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tw3wz6/composing_music_with_neural_networks/"/>
        <updated>2022-04-04T14:51:18.000Z</updated>
        <summary type="html"><![CDATA[Hey guys,
 ​
 I really love creating music algorithmically, which is why I have dedicated my master’s thesis to the generation of music patterns by the use of artificial intelligence.
 In the course of the past 12 months, I have programmed a deep recurrent neural network in Python, which I have trained on 200 self-made music patterns in order to generate somehow novel motifs.
 ​
 In order to evaluate my model, I have set up a short online listening experiment.
 I’m looking for test subjects right now, so if you are interested in participating, I would really appreciate it.
 The listening experiment will take you just about 5 to 8 minutes to complete and the only thing you need is a pair of headphones.
 You can partake on your computer as well as on your smartphone or tablet.
 ​
 Here is the link which gets you to the listening experiment:
 https://forms.gle/rx1FUQ7RgpjMu1xx9
 ​
 Thank you very much for taking the time to help me reach my goal.
 Really appreciate it.
    submitted by    /u/JosephdeLaquinta  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computing functions of roots without computing roots]]></title>
        <id>https://www.johndcook.com/blog/?p=101658</id>
        <link href="https://www.johndcook.com/blog/2022/04/04/vieta/"/>
        <updated>2022-04-04T14:42:33.000Z</updated>
        <summary type="html"><![CDATA[Once in a while it’s necessary to calculate some function of the roots of a polynomial, and it may be possible to do this without first calculating the roots. Quadratics The quadratic formula gives explicit solutions to the equation The two solutions for x are where The awkward part is taking the square root of […]
Computing functions of roots without computing roots first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which elective: Monte Carlo Simulation or Computational Learning Theory?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tw3fii/which_elective_monte_carlo_simulation_or/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tw3fii/which_elective_monte_carlo_simulation_or/"/>
        <updated>2022-04-04T14:30:21.000Z</updated>
        <summary type="html"><![CDATA[Hello /r/reinforcementlearning. I have to choose electives pretty soon, and as i am interested in reinforcement learning, I wanted to know which of these would be the most beneficial. 
  
Monte Carlo Simulation
 Computational Learning Theory
  
The year after I will also take a course on Reinforcement Learning, but it has not been created yet. 
 Note: I can also take both if recommended, if I do so, I will take one of the courses before taking the RL course, and the other would be at the same time. 
 Some further thought I've had:
  
CLT includes Bandits, which is surely were useful to know, but it seems to be only a rather small part, and I'm unsure whether all the other topics like PAC Learning and Rademacher Bounds are useful?
 
MC is more practical while CLT is more theoretic (Apparently VERY theoretic according to the course description above). I am not afraid of theoretic courses, but I struggle more with them than more practical courses.
 
The sentiment around the MC course, is that it is pretty good. I don't know anyone who have taken the CLT course.
 
If I choose both, which order would you take them in?
 
    submitted by    /u/John_Hitler  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ray RL lib observations normalized?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tw2v97/ray_rl_lib_observations_normalized/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tw2v97/ray_rl_lib_observations_normalized/"/>
        <updated>2022-04-04T14:06:31.000Z</updated>
        <summary type="html"><![CDATA[Hey i am using the RL lib from ray and i don't know if the observations automatically normalized by the lib or not?
 By creating a costum environment ray wants you to create an observationspace. That would be a gym box in my case. Anyway idk the exact high and low values. My values lay between -1 and 1 more or less. 
 My fear is now that ray would normalize the Observation values to a new range although they are already processed. Does ray normalized observationspace? If yes how can i turn it off?
 Thanks!
    submitted by    /u/Willing-Classroom735  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] CfP EvoRL @ GECCO 2022. One week before the deadline!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tw2ec1/r_cfp_evorl_gecco_2022_one_week_before_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tw2ec1/r_cfp_evorl_gecco_2022_one_week_before_the/"/>
        <updated>2022-04-04T13:45:43.000Z</updated>
        <summary type="html"><![CDATA[CALL FOR PAPERS
 EvoRL 2022
 Evolutionary Reinforcement Learning workshop at GECCO 2022, July 9-13, Boston, USA
 
 In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks. At the same time, multiple recent papers, notably work from OpenAI, have shown that evolution strategies (ES) can be competitive with standard RL algorithms on some problems while being simpler and more scalable. Similar results were obtained by researchers from Uber, this time using a gradient-free genetic algorithm (GA) to train deep neural networks on complex control tasks. Moreover, recent research in the field of evolutionary algorithms (EA) has led to the development of algorithms like Novelty Search and Quality Diversity, capable of…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[CfP] EvoRL @ GECCO 2022. One week before the deadline!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tw2dla/cfp_evorl_gecco_2022_one_week_before_the_deadline/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tw2dla/cfp_evorl_gecco_2022_one_week_before_the_deadline/"/>
        <updated>2022-04-04T13:44:45.000Z</updated>
        <summary type="html"><![CDATA[CALL FOR PAPERS
 EvoRL 2022
 Evolutionary Reinforcement Learning workshop at GECCO 2022, July 9-13, Boston, USA
 
 In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks. At the same time, multiple recent papers, notably work from OpenAI, have shown that evolution strategies (ES) can be competitive with standard RL algorithms on some problems while being simpler and more scalable. Similar results were obtained by researchers from Uber, this time using a gradient-free genetic algorithm (GA) to train deep neural networks on complex control tasks. Moreover, recent research in the field of evolutionary algorithms (EA) has led to the development of algorithms like Novelty Search and Quality Diversity, capable of…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New RL technique achieves superior performance in control tasks]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tw1nzj/new_rl_technique_achieves_superior_performance_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tw1nzj/new_rl_technique_achieves_superior_performance_in/"/>
        <updated>2022-04-04T13:11:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bendee983  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FWHM for a quadratic]]></title>
        <id>https://www.johndcook.com/blog/?p=101533</id>
        <link href="https://www.johndcook.com/blog/2022/04/04/fwhm-quadratic/"/>
        <updated>2022-04-04T12:59:26.000Z</updated>
        <summary type="html"><![CDATA[This post contains a derives a result I needed recently. The derivation is simple but a little tedious, so I wanted to save it in case I need it again. Full width half maximum A common way to measure the width of a function peak in a function f(x) is to find the place x0 […]
FWHM for a quadratic first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Number slang and numbered lists]]></title>
        <id>https://www.johndcook.com/blog/?p=101648</id>
        <link href="https://www.johndcook.com/blog/2022/04/04/number-slang-and-numbered-lists/"/>
        <updated>2022-04-04T12:39:43.000Z</updated>
        <summary type="html"><![CDATA[Here’s a list of five numbers used as slang in various contexts. Location (CB and police radio) End of column (journalism) Best wishes (ham radio) All aircraft in area (US Navy) I love you (text messages) The motivation for this post was an article Those HTML attributes you never use. I wanted to make a […]
Number slang and numbered lists first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How PPO deals with episodes of Variable lengths?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tw10gn/how_ppo_deals_with_episodes_of_variable_lengths/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tw10gn/how_ppo_deals_with_episodes_of_variable_lengths/"/>
        <updated>2022-04-04T12:39:24.000Z</updated>
        <summary type="html"><![CDATA[In the paper it is written to collect trajectories of length T. Then calculate advantage and then train the Actor and Critic Network. My question is suppose one episode ends much before T. If I run that episode upto lenth T then it will only collect negative rewards in each timestep which in turn makes the training impossible as the return if very big negative number. So what can be done instead of this?
 I might be getting it wrong, so please correct me by commenting.
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Looking for a dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tw0c1o/p_looking_for_a_dataset/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tw0c1o/p_looking_for_a_dataset/"/>
        <updated>2022-04-04T12:02:44.000Z</updated>
        <summary type="html"><![CDATA[Hey! New Here. I logged back into Reddit after years just to ask this question on this forum. I need to test a model, based loosely on BERT, that classifies a piece of text as having right or left political ideology leaning and whether it promotes any racial or religious stereotypes.
 For training purpose we used SBIC, IBC, and Stereoset. Though these only contain short sentences which are labeled as belonging to only one of the above categories.
 Is anyone aware of any other Dataset which can be used for this purpose, which hopefully contains text labeled as promoting or containing a political leaning (left/right, conservative/liberal, neutral) and further either any racial or religious stereotypes?
 Very thankful in adv
    submitted by    /u/Fee_Imaginary  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metrics: Matthew's correlation coefficient]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tvz2cs/metrics_matthews_correlation_coefficient/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tvz2cs/metrics_matthews_correlation_coefficient/"/>
        <updated>2022-04-04T10:51:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TheTesseractAcademy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need help with OpenAI gym custom environment, state representation as "observation"]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tvyv0w/need_help_with_openai_gym_custom_environment/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tvyv0w/need_help_with_openai_gym_custom_environment/"/>
        <updated>2022-04-04T10:38:08.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I'm making a custom openAI gym environment to train various algorithms on it. I have encountered some issues. 
 My .flatten() method on the state class returns a large integer which can be converted back into the same state as the object. However when I try to do this as the returned observation for environment.reset() and environment.step(), when testing it I get: "AssertionError: The observation returned by the `reset()` method does not match the given observation space" which I can fix by having it just return a 0. How do I go about resolving this? and are there any better approaches for wanting to train RL agents on an environment? ty!
    submitted by    /u/snaredrum_merchant  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[12 Graphs That Explain the State of AI in 2022]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tvy3mp/12_graphs_that_explain_the_state_of_ai_in_2022/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tvy3mp/12_graphs_that_explain_the_state_of_ai_in_2022/"/>
        <updated>2022-04-04T09:49:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Tao_Dragon  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is WhatsApp Business API? How can it Help your Business?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tvx311/what_is_whatsapp_business_api_how_can_it_help/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tvx311/what_is_whatsapp_business_api_how_can_it_help/"/>
        <updated>2022-04-04T08:38:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mihircontra20  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Random Relational Graph Convolutional Networks (RR-GCN)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tvwnwy/p_random_relational_graph_convolutional_networks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tvwnwy/p_random_relational_graph_convolutional_networks/"/>
        <updated>2022-04-04T08:07:48.000Z</updated>
        <summary type="html"><![CDATA[📑 The Random R-GCN code has just been released!
 📝 With just a few lines of code, you can now create embeddings of entities in a Knowledge Graph.
 ​
 Minimal example on how to create embeddings with RR-GCN
 ​
 💡 RR-GCN does not require training and is competitive to fully trained R-GCNs.
 👉 https://github.com/predict-idlab/RR-GCN
    submitted by    /u/givdwiel  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] DiffusionCLIP: Text-Guided Diffusion Models for "Robust" Image Manipulation (CVPR 2022)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tvug94/r_diffusionclip_textguided_diffusion_models_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tvug94/r_diffusionclip_textguided_diffusion_models_for/"/>
        <updated>2022-04-04T05:38:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ImBradleyKim  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Transformers for Software Engineers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tvr2ib/p_transformers_for_software_engineers/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tvr2ib/p_transformers_for_software_engineers/"/>
        <updated>2022-04-04T02:36:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voice copying/cloning]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tvqnw5/voice_copyingcloning/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tvqnw5/voice_copyingcloning/"/>
        <updated>2022-04-04T02:17:04.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 Don't know if this is the right subreddit, but here goes....
 I'm looking to voice clone my father. He has passed recently, and despite being difficult for all, it's been especially hard for my mother, married early to him and together for 50 years. Her birthday is coming up, I'd love to be able to create a 5-10 second sound byte of him for her.
 Fortunately, there's likely to be lots of his voice recording around, part of his job was speaking and instructing.
 So, is there any way this is possible, to be done without great difficulty, and produce an accurate result?
 I am understanding the moralities of crafting something with his deceased voice. I thought about it quite a bit. However, I feel that it's for his soulmate who's struggling, who he had no qualms spending his life with and travelling abroad with, spent his last days with. I'm certain he would want to help.
    submitted by    /u/mininggotboring  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Unconventional computer vision problems that are intrinsically different from classifying ordinary stuff]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tvntfs/d_unconventional_computer_vision_problems_that/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tvntfs/d_unconventional_computer_vision_problems_that/"/>
        <updated>2022-04-03T23:58:19.000Z</updated>
        <summary type="html"><![CDATA[Given that most benchmarks for image classification are based on regular, everyday world objects RGB images (or grayscale), what are some unconventional science cases where 2D inputs are substantially different from what we are used to perceive by eye? 
 For example, I'm interested in cases where spatial information can't be constrained to narrow pixel value ranges, such as exponential signals. Or that any standard normalisation (say min-max, zscore) and normalisation layers are not applicable and could lead to the loss of information.
 One of these cases is Astronomy. However, most practitioners try to to adapt the problem to established standards (say fake RGB images, log scaling flux images, etc). What are other cases out there where the nature of the 2D inputs are very distinct to what we are used to parse through our eyes and what deep nets are benchmarked on? I'm curious about tailored solutions that would intrinsically change the way the deep nets are constructed to solve the research question.
    submitted by    /u/astroferreira  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blog: Let’s manually approximate a simple function with a ReLU neural network]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tvneyx/blog_lets_manually_approximate_a_simple_function/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tvneyx/blog_lets_manually_approximate_a_simple_function/"/>
        <updated>2022-04-03T23:38:37.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/rhkibria  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top Ways in Which AI Impacts Grocery Retail]]></title>
        <id>https://www.datasciencecentral.com/?p=57029</id>
        <link href="https://www.datasciencecentral.com/top-ways-in-which-ai-impacts-grocery-retail/"/>
        <updated>2022-04-03T21:47:26.000Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence has been long making waves globally, empowering companies from across the broad spectrum of industries to take their businesses to the next level. So it is no surprise that this technology is making inroads in the grocery retail space, helping grocers deliver personalized and irreproachable experiences across different channels, establishing improved customer loyalty,… Read More »Top Ways in Which AI Impacts Grocery Retail
The post Top Ways in Which AI Impacts Grocery Retail appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ryan Williamson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A different take on business intelligence]]></title>
        <id>https://www.datasciencecentral.com/?p=57026</id>
        <link href="https://www.datasciencecentral.com/a-different-take-on-business-intelligence/"/>
        <updated>2022-04-03T21:05:49.000Z</updated>
        <summary type="html"><![CDATA[Data is useless if it doesn’t shed light. The more light it sheds on the most acute problems businesses face, the better. Within this context, data synergy–data from multiple sources and disciplines that is more valuable than the sum of its parts–is often underappreciated. With data synergy, the light can be in many more places,… Read More »A different take on business intelligence
The post A different take on business intelligence appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research][Project][Library] Dog-feeding a new Machine Learning data tool]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tvjw43/researchprojectlibrary_dogfeeding_a_new_machine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tvjw43/researchprojectlibrary_dogfeeding_a_new_machine/"/>
        <updated>2022-04-03T21:03:32.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone!
 I'm Atindriyo Sanyal, one of the founders of the ML company Galileo (https://rungalileo.io/). We're building a cool new tool/framework for ML practitioners that helps shine a light on the data you are training your models with.
 I'd love to get some feedback on the product, and since we're still in private beta, I'm looking for folks to try out the product on their datasets and models. It's easy to use and hooks into popular frameworks such as pyTorch, Tensorflow, Keras, SpaCy etc. 
 Caveat: Currently the tool only works for NLP use cases (think text classification, NER etc).
 I'll be giving $100 to folks who are willing to give some time to this and provide feedback on the usability of the product. If you're interested, here's a really tiny form (should take <1 minute to fill) for you to fill out. I'll review the applications and send you an email for a follow up Zoom chat where I'll share the software artifacts with you!
 https://docs.google.com/forms/d/11V20C_J_SyNaX7QL6DasnTe7f0UiueUyaKdmt3xL1oI/edit
 Look forward and happy (machine) learning!
 - Atindriyo
 P.S. If you have any questions or want to chat personally, send me an email at [atin@rungalileo.io](mailto:atin@rungalileo.io).
    submitted by    /u/atindriyo_galileo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blockchain Won’t Save The Metaverse]]></title>
        <id>https://www.datasciencecentral.com/?p=57017</id>
        <link href="https://www.datasciencecentral.com/blockchain-wont-save-the-metaverse/"/>
        <updated>2022-04-03T20:31:44.000Z</updated>
        <summary type="html"><![CDATA[Blockchain is widely touted as a mechanism for securing digital property. Multiple problems exist for driving metaverse transactions. A new review highlights the challenges, some of which may be insurmountable. Blockchain has been touted as a potential solution to securing users’ digital content and data due to its decentralization, immutability, and transparency. However, there are… Read More »Blockchain Won’t Save The Metaverse
The post Blockchain Won’t Save The Metaverse appeared first on Data Science Central.]]></summary>
        <author>
            <name>Stephanie Glen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How does the ACER algorithm work?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tvipcq/how_does_the_acer_algorithm_work/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tvipcq/how_does_the_acer_algorithm_work/"/>
        <updated>2022-04-03T20:14:25.000Z</updated>
        <summary type="html"><![CDATA[I am currently writing a report on reinforcement learning, where I am trying to describe how the ACER algorithm works. I have read the arxiv paper on the sample actor-critic with experienced replay, but I don't understand where the experience replay comes in. Is this part of the policy gradient? where the policy is updated every episode it's trained on from the previous knowledge it gathers in previous episodes.
 https://arxiv.org/pdf/1611.01224.pdf
 ​
    submitted by    /u/beepingwater_neko  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What’s the best way to implement tree bases function approximators for RL/Control?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tvif3g/whats_the_best_way_to_implement_tree_bases/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tvif3g/whats_the_best_way_to_implement_tree_bases/"/>
        <updated>2022-04-03T20:03:04.000Z</updated>
        <summary type="html"><![CDATA[Sorry if this post is not appropriate here, but I have been wondering how can I implement and learn a decision tree or any other non differentiable function approximators for the Value Function.
 It’s relatively easy to formulate and use DQN type algorithms by using neural network and say pytorch + stochastic optimization but I want to try out some tree based methods. (at least to reproduce papers which claim to use them)
 But I don’t know 1) If we have to design the structures and learning algorithms by hand or is there any package I can use? 
 2) How should the learning be done? We obviously can’t go regression type learning because of the bootstrapping nature of the Bellman equation?
 Thanks
    submitted by    /u/Htaseht  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Pain points when using GPU instance platforms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tvfx9r/d_pain_points_when_using_gpu_instance_platforms/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tvfx9r/d_pain_points_when_using_gpu_instance_platforms/"/>
        <updated>2022-04-03T18:23:04.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, I just launched a GPU compute instance platform (think lambdalabs, fluidstack, aws EC2, vast), and I was wondering what pain points everyone has with existing solutions. I'm not trying to sell anyone anything, but I want to look for feedback that will help me to build a better product.
 My current thoughts are
  
Ease of getting data into the platform
 Ease of getting data off of the platform
 Automation for spinning up and down instances
 Availability of the type of instance you want
 Price too high
 Not enough/too many abstractions
  
TIA and I look forward to some good discussions!
    submitted by    /u/runpod-io  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI News | ALS Brain Computer Interface 1 Year Human Trial Results | Skin Cancer Detection | New IBM AI Hardware]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tvd9w0/ai_news_als_brain_computer_interface_1_year_human/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tvd9w0/ai_news_als_brain_computer_interface_1_year_human/"/>
        <updated>2022-04-03T16:31:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/getrich_or_diemining  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your Next Teacher Will be a Machine: Why the Future of Education is Automation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tvctg8/your_next_teacher_will_be_a_machine_why_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tvctg8/your_next_teacher_will_be_a_machine_why_the/"/>
        <updated>2022-04-03T16:11:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/itsallshit-eatup  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Efficient-VDVAE: An open-source memory-efficient and stable very deep hierarchical VAE]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tvc7xl/r_efficientvdvae_an_opensource_memoryefficient/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tvc7xl/r_efficientvdvae_an_opensource_memoryefficient/"/>
        <updated>2022-04-03T15:46:10.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone :)
 We have released last week our paper "Efficient-VDVAE: Less is more" with code!
 We present simple modifications to the Very Deep VAE to make it converge up to 2.6x times faster and save up to 20x times memory load. We also introduce a gradient smoothing technique to improve stability during training. Our model achieves comparable or better negative log-likelihood (NLL) on 7 commonly used datasets.
 Additionally, we make an argument against existing 5-bit benchmarks. We empirically show as well that 3% of the latent space is enough to encode the data information without any performance loss. Thus, indicating the potential to efficiently leverage the Hierarchical VAE's latent space in downstream tasks.
  
Paper: https://arxiv.org/abs/2203.13751
 Code: https://github.com/Rayhane-mamah/Efficient-VDVAE
 Paperswithcode: https://paperswithcode.com/paper/efficient-vdvae-less-is-more
  
Feedback is very much appreciated!
 https://preview.redd.it/tjua1xpq3cr81.png?width=878&format=png&auto=webp&s=718bd91fd648acd673ddab1ad5342207e8be09e7
    submitted by    /u/Louay-AI  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi!, Im wondering if anyone could help me🇦🇷🇦🇷]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tvaoe1/hi_im_wondering_if_anyone_could_help_me/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tvaoe1/hi_im_wondering_if_anyone_could_help_me/"/>
        <updated>2022-04-03T14:36:41.000Z</updated>
        <summary type="html"><![CDATA[Im a 19yo guy from Argentina that studies system ingeneer, I like my career, beeing an ingeneer is great, but coding and AI is greater, Im tired of courses like Free code academy, or basics things, im looking for a more professional, useful and deeper courses, that will really teach me, im currently with python(pandas,numpy,matplotlib,tensorflow) basics, and wanna to be better in that field that i love❤
    submitted by    /u/Sasulanda  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I'm working on a DQN agent using the Keras RL library to play Atari games, however a weird thing keeps happening where every episode is the same length but it's a random number each time.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tv9iif/im_working_on_a_dqn_agent_using_the_keras_rl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tv9iif/im_working_on_a_dqn_agent_using_the_keras_rl/"/>
        <updated>2022-04-03T13:40:29.000Z</updated>
        <summary type="html"><![CDATA[The episode step count is the same for training and testing.
    submitted by    /u/Gleann_na_nGealt  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] DeepDPM: Deep Clustering With an Unknown Number of Clusters]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tv9fuv/r_deepdpm_deep_clustering_with_an_unknown_number/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tv9fuv/r_deepdpm_deep_clustering_with_an_unknown_number/"/>
        <updated>2022-04-03T13:36:53.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone :)
 We've just released the code for our paper (accepted to CVPR2022) 
 DeepDPM is a nonparametric deep-clustering method which unlike most deep clustering methods, does not require knowing the number of clusters, K; rather, it infers it as a part of the overall learning. Using a split/merge framework to change the clusters number adaptively and a novel loss, our proposed method outperforms existing (both classical and deep) nonparametric methods.
 While the few existing deep nonparametric methods lack scalability, we show ours by being the first such method that reports its performance on ImageNet.
 ​
  
Paper: https://arxiv.org/abs/2203.14309
 Code: https://github.com/BGU-CS-VIL/DeepDPM/
  
Below are some examples of clusters our method found in ImageNet.
 https://preview.redd.it/jw5kvcuzfbr81.jpg?width=737&format=pjpg&auto=webp&s=5b61cdd0efdea7c92aba611171e5dc7f4276c892
    submitted by    /u/shahaff32  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active non-ML research areas?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tv829c/active_nonml_research_areas/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tv829c/active_nonml_research_areas/"/>
        <updated>2022-04-03T12:22:42.000Z</updated>
        <summary type="html"><![CDATA[What are the most active non-ML/statistical research areas in AI?
 Are there any recent books published that give an overview of such areas?
 Seems like AI is now either ML or people saying that ML won’t work, but vague on alternatives.
    submitted by    /u/spookyplatypus  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heard about Github Copilot? Now Meet Salesforce's 'CodeGen’ : An AI Model That Turns Simple Natural Language Requests Into Executable Code]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tv133y/heard_about_github_copilot_now_meet_salesforces/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tv133y/heard_about_github_copilot_now_meet_salesforces/"/>
        <updated>2022-04-03T04:40:09.000Z</updated>
        <summary type="html"><![CDATA[Imagine being able to tell a machine to write an app simply by telling it what the app does. As far-fetched as it may appear, this scenario is already a reality.
 According to Salesforce AI Research, conversational AI programming is a new paradigm that brings this vision to life, thanks to an AI system that builds software.
 Introducing CodeGen: Creating Programs from Prompts
 The large-scale language model, CodeGen, which converts simple English prompts into executable code, is the first step toward this objective. The person doesn’t write any code; instead, (s)he describes what (s)he wants the code to perform in normal language, and the computer does the rest.
 Conversational AI refers to technologies that allow a human and a computer to engage naturally through a conversation. Chatbots, voice assistants, and virtual agents are examples of conversational AI.
 Continue Reading
 Paper: https://arxiv.org/pdf/2203.13474.pdf
 Github: https://github.com/salesforce/CodeGen
 https://i.redd.it/dbyba3dct8r81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dan Huttenlocher ponders our human future in an age of artificial intelligence]]></title>
        <id>https://news.mit.edu/2022/huttenlocher-age-of-ai-0403</id>
        <link href="https://news.mit.edu/2022/huttenlocher-age-of-ai-0403"/>
        <updated>2022-04-03T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[For the MIT Schwarzman College of Computing dean, bringing disciplines together is the best way to address challenges and opportunities posed by rapid advancements in computing.]]></summary>
        <author>
            <name>Adam Zewe | MIT News Office</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any real life projects I could do with this? How do I get ideas to use this?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tuzq36/are_there_any_real_life_projects_i_could_do_with/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tuzq36/are_there_any_real_life_projects_i_could_do_with/"/>
        <updated>2022-04-03T03:22:28.000Z</updated>
        <summary type="html"><![CDATA[I tried using RL for some work at my university, it did not really work all that well. 
 I'm wondering if there are some real life scenarios that I could use to create my own personal projects. Otherwise, I'd be fine with games. 
 I want to try all of it out from Dynamic Programming to crazy ass stuff like A3C, PPO and so on. I like RL, more so than any other form of ML, and I want to play around with it as a hobby. 
 This is really the area of ML for me. For starters, there are fundamentals behind it, so you can mostly explain why agents do one thing or another. Additionally, there isn't a need to have massive amounts of data. It's also the only type of ML that I've been able to successfully use with software engineering. 
 Designing the agent and the API it uses to take actions in an environment is as much a software engineering project as is creating a REST API. 
 I feel there is a lot of potential for me to go crazy with this, and I was wondering if people have any cool suggestions. Anything that is real time is anything that I want to do. Real time systems and RL are exactly the sort of thing I love to do.
    submitted by    /u/HesperusIII  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why are confidence regions elliptic?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tuzn4m/d_why_are_confidence_regions_elliptic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tuzn4m/d_why_are_confidence_regions_elliptic/"/>
        <updated>2022-04-03T03:18:05.000Z</updated>
        <summary type="html"><![CDATA[Confidence regions are the 2D version of a confidence interval. Almost everywhere in the literature, the shape is elliptic, but no justification is provided. You would think that a confidence region of level γ is defined as the domain of minimum area, covering a mass γ of the underlying probability distribution. That sounds perfectly logical, but it is mentioned nowhere. Based on this definition, the boundary of a confidence region is obtained by solving an optimization problem: it is a problem in calculus of variations -- finding a boundary curve encompassing a domain of minimum area. These problems are usually hard to solve, but in this case, the solution seems trivial: it must be a contour line. And if the underlying distribution is Gaussian, contour lines are obviously ellipses. This would be a solid justification as to why ellipses are so widespread.
 https://preview.redd.it/42mr1t1je8r81.png?width=1072&format=png&auto=webp&s=2fb9cedbbb15895827ed00edc4912ac39fad0b71
 My question here is whether or not my argumentation makes sense, or if there is something faulty in my math. I discuss it in more details in one of my articles, here. If you need clarifications, please reply on Reddit, I will do my best to explain.
    submitted by    /u/MLRecipes  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Satellite Imagery using Deep Learning for the Sensor To Shooter Timeline. (arXiv:2203.00116v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.00116</id>
        <link href="http://arxiv.org/abs/2203.00116"/>
        <updated>2022-04-03T00:52:40.822Z</updated>
        <summary type="html"><![CDATA[The sensor to shooter timeline is affected by two main variables: satellite
positioning and asset positioning. Speeding up satellite positioning by adding
more sensors or by decreasing processing time is important only if there is a
prepared shooter, otherwise the main source of time is getting the shooter into
position. However, the intelligence community should work towards the
exploitation of sensors to the highest speed and effectiveness possible.
Achieving a high effectiveness while keeping speed high is a tradeoff that must
be considered in the sensor to shooter timeline. In this paper we investigate
two main ideas, increasing the effectiveness of satellite imagery through image
manipulation and how on-board image manipulation would affect the sensor to
shooter timeline. We cover these ideas in four scenarios: Discrete Event
Simulation of onboard processing versus ground station processing, quality of
information with cloud cover removal, information improvement with super
resolution, and data reduction with image to caption. This paper will show how
image manipulation techniques such as Super Resolution, Cloud Removal, and
Image to Caption will improve the quality of delivered information in addition
to showing how those processes effect the sensor to shooter timeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1"&gt;Matthew Ciolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hambrick_D/0/1/0/all/0/1"&gt;Dominick Hambrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1"&gt;David Noever&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]New Scaling Laws for Large Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tux0p3/dnew_scaling_laws_for_large_language_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tux0p3/dnew_scaling_laws_for_large_language_models/"/>
        <updated>2022-04-03T00:01:56.000Z</updated>
        <summary type="html"><![CDATA[https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models
    submitted by    /u/Singularian2501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AgentZero: Ray & PyTorch based light-weight Distributed Fast Reinforcement Learning Framework]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tuwhbp/agentzero_ray_pytorch_based_lightweight/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tuwhbp/agentzero_ray_pytorch_based_lightweight/"/>
        <updated>2022-04-02T23:35:17.000Z</updated>
        <summary type="html"><![CDATA[AgentZero
 https://github.com/zhoubin-me/agent0
 This is my personal project developed two years ago. It covers major DRL algorithms like:
 - [DQN](https://arxiv.org/abs/1312.5602)- [Double DQN](https://arxiv.org/abs/1509.06461)- [Dueling DQN](https://arxiv.org/abs/1511.06581)- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)- [Noisy Network](https://arxiv.org/abs/1706.10295)- [C51](https://arxiv.org/abs/1707.06887)- [Rainbow](https://arxiv.org/abs/1710.02298)- [QR-DQN](https://arxiv.org/abs/1710.10044)- [IQR](https://arxiv.org/abs/1806.06923)- [FQF](https://arxiv.org/abs/1911.02140)- [DDPG](https://arxiv.org/abs/1509.02971)- [SAC](https://arxiv.org/abs/1801.01290)- [TD3](https://arxiv.org/abs/1802.09477)- [MDQN](https://arxiv.org/abs/2007.14430)
 What is amazing is its speed and memory efficiency after some optimization:
 With a single 2080Ti GPU and a 8 core AMD CPU, the training speed of rainbow for Atari could achieve 3000FPS, which means it can finish training of 10M frames within 1 hour. With compression of image frames, replay memory's RAM usage is down by 20%.
 I have tested several algorithms and games on Atari and get some initial result. Welcome to use and contribute!
    submitted by    /u/zhoubin-me  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization for DRL: reward or objective function?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tuv1t5/regularization_for_drl_reward_or_objective/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tuv1t5/regularization_for_drl_reward_or_objective/"/>
        <updated>2022-04-02T22:27:26.000Z</updated>
        <summary type="html"><![CDATA[I am searching for regularization methods applied to DRL algorithms (either value or policy-based) to understand what has been done so far in the field. I cannot find any valid reference that studies the effect of applying a soft constraint to the reward function instead of to the policy objective. This may seem useless for some applications, but it is highly relevant for finance, which is my domain of application,
 The idea I have so far is that if you constrain the reward, it is like you are imposing limits on the agent's behavior. On the other hand, if you constrain the objective, you are not limiting the behavior, but you are correcting the ex-post the undesired behavior. The latter way does not allow the agent to learn not to behave in a certain way.
 ​
 Did anyone ever think about it? Are they good references that analyze the different effects of a constraint to whatever DRL algorithm?
    submitted by    /u/alebrini  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building Trust with Responsible AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tuu59j/building_trust_with_responsible_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tuu59j/building_trust_with_responsible_ai/"/>
        <updated>2022-04-02T21:46:33.000Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence is being used in almost every aspect of life. AI symbolizes growth and productivity in the minds of some, but it is raising questions as well on the fairness, privacy, and security of these systems. Many legitimate issues exist, including biased choices, labor replacement, and a lack of security. When it comes to robots, this is very frightening. Self-driving automobiles, for example, can cause injury or death if they make mistakes. Responsible AI addresses these difficulties and makes AI systems more accountable.
 Responsible AI should fulfill the following aims:
  
Interpretability: We obtain an explanation for how a model makes predictions when we interpret it. An AI system makes predictions for a user. Even if these selections are correct, a user is likely to seek an explanation. Responsible AI can describe how we create interpretable models.
 Fairness: AI systems have the potential to make judgments that are biased towards particular groups of people. Bias in the training data is the source of this bias. The easier it is to assure fairness and rectify any bias in a model, the more interpretable it is. As a result, we need a Responsible AI framework to explain how we evaluate fairness and what to do if a model makes unjust predictions.
 Safety and Security: AI systems aren’t deterministic. When confronted with new situations, they are prone to making poor choices. The systems can even be tampered with to make unwise decisions. Therefore, we need to ensure safety and security in these systems.
 Data Governance: The data used must be of high quality. If the data used by AI has errors, the system may make wrong decisions.
  
Continue Reading The Article Here
 ​
 https://preview.redd.it/9iivp31ir6r81.png?width=1024&format=png&auto=webp&s=207409694b68a1e985ad1dfcf3b466ac25916da2
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's unbelievable what ML can do! Disco+RIFE= 7hr Colab Run...]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tusmv0/its_unbelievable_what_ml_can_do_discorife_7hr/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tusmv0/its_unbelievable_what_ml_can_do_discorife_7hr/"/>
        <updated>2022-04-02T20:38:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/JoshGrambo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What does it mean to feed the "network state" in an LSTM in the actor network?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/turvmf/what_does_it_mean_to_feed_the_network_state_in_an/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/turvmf/what_does_it_mean_to_feed_the_network_state_in_an/"/>
        <updated>2022-04-02T20:05:26.000Z</updated>
        <summary type="html"><![CDATA[I am looking at this code from Google (https://github.com/google-research/google-research/blob/master/social_rl/multiagent_tfagents/joint_attention/attention_networks.py). 
 At line 639, the LSTM is called. The first two inputs are the state and the network state, but I don't understand what the latter is.
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating A Chatbot with transformers and Gradio]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tuov1l/creating_a_chatbot_with_transformers_and_gradio/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tuov1l/creating_a_chatbot_with_transformers_and_gradio/"/>
        <updated>2022-04-02T17:53:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oscillations in RLC circuits]]></title>
        <id>https://www.johndcook.com/blog/?p=101499</id>
        <link href="https://www.johndcook.com/blog/2022/04/02/rlc-circuits/"/>
        <updated>2022-04-02T17:16:43.000Z</updated>
        <summary type="html"><![CDATA[Electrical and mechanical oscillations satisfy analogous equations. This is the basis of using the word “analog” in electronics. You could study a mechanical system by building an analogous circuit and measuring that circuit in a lab. Mass, dashpot, spring Years ago I wrote a series of four posts about mechanical vibrations: Free, undamped vibrations Free, […]
Oscillations in RLC circuits first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers Develop Parking Analytics Framework Using Deep Learning]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tunb06/researchers_develop_parking_analytics_framework/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tunb06/researchers_develop_parking_analytics_framework/"/>
        <updated>2022-04-02T16:48:24.000Z</updated>
        <summary type="html"><![CDATA[Artificial Intelligence and deep learning in video analytics are gaining popularity. It has enabled a wide range of industrial applications, including surveillance and public safety, robotics perception, medical intervention, and facial recognition. According to Markets & Markets, the global market for video analytics was valued at USD 5.9 billion in 2021 and is predicted to reach USD 14.9 billion by 2026.
 Unmanned aerial vehicles (UAVs) have also enabled a wide range of video analytics applications (e.g., aerial surveys) since they provide aerial views of the environment, allowing for collecting aerial photos and processing with deep learning algorithms. Parking analytics is one of these critical smart city applications that uses deep learning and UAVs to collect real-time data and analyze it in order to maximize parking revenue, enhance parking resource allocations, and better manage public space.
 Continue Reading
 Paper: https://arxiv.org/pdf/2203.07792.pdf
 ​
 https://i.redd.it/u5th7z0ja5r81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How Logistic Regression nomogram is constructed from binary classifier?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tul97t/d_how_logistic_regression_nomogram_is_constructed/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tul97t/d_how_logistic_regression_nomogram_is_constructed/"/>
        <updated>2022-04-02T15:19:36.000Z</updated>
        <summary type="html"><![CDATA[Well I' ve been reading some scientific works and I don't understand how nomograms are constructed from logistic regression models.
 In this example I have:
 https://ieeexplore-1ieee-1org-1000007l206e9.han.bg.pg.edu.pl/document/9514609
 And they train LR model on Covid19 dataset [death/ didn't die] so it's binary classification problem However later on, they construct nomogram, which determines whether there is low/moderate/high risk of covid19 mortality. What I don't undestand is how they calculate the score the establish chances of death. E.G. If the score is <0.05 there is low possibility that patient will die.
 My general question is, how they constructed this nomogram from the binary classifier they had?
    submitted by    /u/s168501  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepRL and Rubik’s Cube]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tukbpx/deeprl_and_rubiks_cube/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tukbpx/deeprl_and_rubiks_cube/"/>
        <updated>2022-04-02T14:38:04.000Z</updated>
        <summary type="html"><![CDATA[I'm part of a group of researchers from top ML institutions and industry, our goal is to figure out how improve efficiency in DeepRL.
 We are looking at Rubik’s Cube as target problem, and kicking off a project which will start from https://github.com/forestagostinelli/DeepCubeA and go from there.
 Prior works require hand crafted curriculum and billion of interactions to solve a cube, we believe that order of magnitude more compute that it should take.
 Is anyone interested to collaborate? I'm happy to dedicate a few hours a week to help a newcomer like I was a few years ago with the RL stuff given some basics of machine learning and programming skills, and this could be the golden opportunity for someone to see RL at scale.
    submitted by    /u/mind_library  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Neural Head Avatars from Monocular RGB Videos (CVPR 2022)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tujtva/r_neural_head_avatars_from_monocular_rgb_videos/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tujtva/r_neural_head_avatars_from_monocular_rgb_videos/"/>
        <updated>2022-04-02T14:15:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Mandelmus100  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is an Experimentation program and Who is Involved? (Experimentation Program Series: Guide 02)]]></title>
        <id>https://mlinproduction.com/?p=936</id>
        <link href="https://mlinproduction.com/experimentation-program-stakeholders/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=experimentation-program-stakeholders"/>
        <updated>2022-04-02T14:03:58.000Z</updated>
        <summary type="html"><![CDATA[In my previous post, I briefly described how leading companies use experimentation to optimize their products and services and evolve them to the point of feeling elegant, efficient, and magical. These companies have developed mature experimentation programs (ExPrs), including the… Read More 
The post What is an Experimentation program and Who is Involved? (Experimentation Program Series: Guide 02) appeared first on ML in Production.]]></summary>
        <author>
            <name>Luigi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is an Experimentation program and Who is Involved? (Experimentation Program Series: Guide 02)]]></title>
        <id>https://mlinproduction.com/?p=936</id>
        <link href="https://mlinproduction.com/experimentation-program-stakeholders/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=experimentation-program-stakeholders"/>
        <updated>2022-04-02T14:03:58.000Z</updated>
        <summary type="html"><![CDATA[In my previous post, I briefly described how leading companies use experimentation to optimize their products and services and evolve them to the point of feeling elegant, efficient, and magical. These companies have developed mature experimentation programs (ExPrs), including the… Read More 
The post What is an Experimentation program and Who is Involved? (Experimentation Program Series: Guide 02) appeared first on ML in Production.]]></summary>
        <author>
            <name>Luigi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi agent reinforcement learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tujiup/multi_agent_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tujiup/multi_agent_reinforcement_learning/"/>
        <updated>2022-04-02T14:00:39.000Z</updated>
        <summary type="html"><![CDATA[I'm absolutely new to machine learning, let alone reinforcement learning. I've been tasked to replicate and if possible improve upon the paper linked in the post. I don't know what platform to use and how to create the custom environment. if anybody could share any resources it would be tremendously helpful.
 https://drive.google.com/file/d/1fIT43hKi61WUIvoTh2a3AWlRsphi-L98/view?usp=sharing
    submitted by    /u/Lazarus_07  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Predicting hard properties of graphs using Machine Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tuhgid/d_predicting_hard_properties_of_graphs_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tuhgid/d_predicting_hard_properties_of_graphs_using/"/>
        <updated>2022-04-02T12:11:38.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 there is a lot of work in the field of geometric deep learning for combinatorial optimization that yields good approximation algorithms for "hard" problems on graphs (see here), with the most prominent example being the TSP problem. However, as far as I can see, all these considered problems share the fact that the computed solution is a subset of the vertices/edges of the original graph. In my field (graph drawing), one of the most important considered properties is the crossing number). Hence, the solution would not consists of a labeling of the edges/vertices, but is rather a regression task on the whole graph. I have a dataset that consists of roughly 10000 graphs together with their crossing number. Treating the above problem as a supervised regression task and simply inserting the graph into a GNN does not work for me at all - is this a problem of my choice of architecture or is this sort of "function" that maps a graph to its crossing number something we can expect no current architecture to find.
 I appreciate any comment, even if it is just your intuition on the problem.
 Best regards,
 MrLemming
    submitted by    /u/MrLemming2  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NN from Scratch: #1 Data Preprocessing | Kolbenkraft]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tugj08/nn_from_scratch_1_data_preprocessing_kolbenkraft/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tugj08/nn_from_scratch_1_data_preprocessing_kolbenkraft/"/>
        <updated>2022-04-02T11:16:08.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/cjmodi306  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Announcement: Call for Papers for our ICML ShiftHappens Workshop!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tugeo8/n_announcement_call_for_papers_for_our_icml/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tugeo8/n_announcement_call_for_papers_for_our_icml/"/>
        <updated>2022-04-02T11:08:24.000Z</updated>
        <summary type="html"><![CDATA[Dear community, I hope I do not violate rules by advertizing our Call for Papers here. In a nutshell, submissions can be robustness or OOD datasets and new metrics which we will consolidate in one benchmark. More infos on our website.
 I am happy to answer any questions in regards to the call.
    submitted by    /u/helavisa4  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] OpenAI Codex helping to write shell commands]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tuf0vv/p_openai_codex_helping_to_write_shell_commands/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tuf0vv/p_openai_codex_helping_to_write_shell_commands/"/>
        <updated>2022-04-02T09:36:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/tomd_96  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How will AI impact games]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tuelaw/how_will_ai_impact_games/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tuelaw/how_will_ai_impact_games/"/>
        <updated>2022-04-02T09:05:20.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/GravermanYT  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[C++ Machine Learning Book]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tuecc2/c_machine_learning_book/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tuecc2/c_machine_learning_book/"/>
        <updated>2022-04-02T08:48:09.000Z</updated>
        <summary type="html"><![CDATA[Hey, guys. Just want to ask if anybody's interested with a C++ machine learning book, "Hands-on Machine Learning with C++" by Kirill Kolodiazhnyi. 
 If you are, send me a DM.
    submitted by    /u/edmondgrasa  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets + Gradio Web Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tucu1d/rp_styleganxl_scaling_stylegan_to_large_diverse/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tucu1d/rp_styleganxl_scaling_stylegan_to_large_diverse/"/>
        <updated>2022-04-02T07:02:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient net vs resnet]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tua0x3/efficient_net_vs_resnet/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tua0x3/efficient_net_vs_resnet/"/>
        <updated>2022-04-02T04:05:56.000Z</updated>
        <summary type="html"><![CDATA[As the title says, I would like to know/get some direction to the question when in general does and effnet is preferred to a resnet? I understand that the paper compares performances and it shows a higher performance wrt every network. So my question would be is that always the case or is there a specific situation where it would be better?
 Sorry for the typos (on my mobile)
    submitted by    /u/johnyj01  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] PetaFLOPS as a Unit of Measure in Machine Learning Applications]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tu8szc/d_petaflops_as_a_unit_of_measure_in_machine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tu8szc/d_petaflops_as_a_unit_of_measure_in_machine/"/>
        <updated>2022-04-02T02:59:28.000Z</updated>
        <summary type="html"><![CDATA[I was looking at this paper (https://arxiv.org/pdf/2005.14165.pdf) and came across this graph:
 ​
 https://preview.redd.it/49ydy9bo61r81.png?width=665&format=png&auto=webp&s=729743449d6a99d2b84b81610e7e32d87ea4dfeb
 I am trying to understand the following two things about this graph:
 ​
  
What is PetaFLOP/s-days? I read that a PetaFLOP is 1,000,000,000,000,000 calculations (e.g. addition, subtraction). I am guessing that 10^2 would imply 100 * 1,000,000,000,000,000 calculations per day - is this correct? Is there any difference between PetaFLOP/days and PetaFLOP/s-days? (I also find it interesting they are probably referring to "computer resources" as simply "compute")
 What does "C" stand for in L = 2.57 * C^-0.048? I am guessing that the "dotted line" probably refers to the "average loss" for different Neural Networks with differing amounts of Parameters - but what exactly does "C" stand for?
 Finally, is there a reason that "Validation Loss" is not expressed as a percentage? For instance, what is a Validation Loss of 3? Is a Validation Loss of 3 the same as a Loss of 30%? Or does Validation Loss simply refer to the value of the Loss Function obtained during the Validation stage of Cross Validation?
  
Thank you!
    submitted by    /u/blueest  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[q-learning vs. policy gradient]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tu85b5/qlearning_vs_policy_gradient/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tu85b5/qlearning_vs_policy_gradient/"/>
        <updated>2022-04-02T02:24:42.000Z</updated>
        <summary type="html"><![CDATA[Trying to wrap my head around the RL essentials. Would it be correct to say that Q-learning attempts to select the best available policy by optimizing the Q-function, while policy gradient methods work directly to optimize a pre-determined policy's parameters?
    submitted by    /u/JimBeanery  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to use a deep model for DRL?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tu7499/how_to_use_a_deep_model_for_drl/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tu7499/how_to_use_a_deep_model_for_drl/"/>
        <updated>2022-04-02T01:31:43.000Z</updated>
        <summary type="html"><![CDATA[I noticed most DRL papers use very shallow models like three or four layers. However, when I try to do DRL tasks that have relatively complicated scenes (for example, some modern video game), shallow models become way too weak.
 Are there papers, blogs, articles etc. that use more complex/deep models? Or maybe some methods that can deal with complicated scenes without deep models?
 Thanks
    submitted by    /u/seermer  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MyStyle: A Personalized Generative Prior. (arXiv:2203.17272v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17272</id>
        <link href="http://arxiv.org/abs/2203.17272"/>
        <updated>2022-04-02T00:47:22.631Z</updated>
        <summary type="html"><![CDATA[We introduce MyStyle, a personalized deep generative prior trained with a few
shots of an individual. MyStyle allows to reconstruct, enhance and edit images
of a specific person, such that the output is faithful to the person's key
facial characteristics. Given a small reference set of portrait images of a
person (~100), we tune the weights of a pretrained StyleGAN face generator to
form a local, low-dimensional, personalized manifold in the latent space. We
show that this manifold constitutes a personalized region that spans latent
codes associated with diverse portrait images of the individual. Moreover, we
demonstrate that we obtain a personalized generative prior, and propose a
unified approach to apply it to various ill-posed image enhancement problems,
such as inpainting and super-resolution, as well as semantic editing. Using the
personalized generative prior we obtain outputs that exhibit high-fidelity to
the input images and are also faithful to the key facial characteristics of the
individual in the reference set. We demonstrate our method with fair-use images
of numerous widely recognizable individuals for whom we have the prior
knowledge for a qualitative evaluation of the expected outcome. We evaluate our
approach against few-shots baselines and show that our personalized prior,
quantitatively and qualitatively, outperforms state-of-the-art alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1"&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1"&gt;Kfir Aberman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1"&gt;Qiurui He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liba_O/0/1/0/all/0/1"&gt;Orly Liba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1"&gt;Michal Yarom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1"&gt;Yossi Gandelsman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1"&gt;Inbar Mosseri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1"&gt;Yael Pritch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_or_D/0/1/0/all/0/1"&gt;Daniel Cohen-or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Muesli: Combining Improvements in Policy Optimization. (arXiv:2104.06159v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06159</id>
        <link href="http://arxiv.org/abs/2104.06159"/>
        <updated>2022-04-02T00:47:22.555Z</updated>
        <summary type="html"><![CDATA[We propose a novel policy update that combines regularized policy
optimization with model learning as an auxiliary loss. The update (henceforth
Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli
does so without using deep search: it acts directly with a policy network and
has computation speed comparable to model-free baselines. The Atari results are
complemented by extensive ablations, and by additional results on continuous
control and 9x9 Go.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_M/0/1/0/all/0/1"&gt;Matteo Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danihelka_I/0/1/0/all/0/1"&gt;Ivo Danihelka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viola_F/0/1/0/all/0/1"&gt;Fabio Viola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guez_A/0/1/0/all/0/1"&gt;Arthur Guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1"&gt;Simon Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1"&gt;Laurent Sifre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_T/0/1/0/all/0/1"&gt;Theophane Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1"&gt;David Silver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1"&gt;Hado van Hasselt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Relation Networks for End-to-End Speaker Verification and Identification. (arXiv:2203.17218v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.17218</id>
        <link href="http://arxiv.org/abs/2203.17218"/>
        <updated>2022-04-02T00:47:22.447Z</updated>
        <summary type="html"><![CDATA[Speaker identification systems in a real-world scenario are tasked to
identify a speaker amongst a set of enrolled speakers given just a few samples
for each enrolled speaker. This paper demonstrates the effectiveness of
meta-learning and relation networks for this use case. We propose improved
relation networks for speaker verification and few-shot (unseen) speaker
identification. The use of relation networks facilitates joint training of the
frontend speaker encoder and the backend model. Inspired by the use of
prototypical networks in speaker verification and to increase the
discriminability of the speaker embeddings, we train the model to classify
samples in the current episode amongst all speakers present in the training
set. Furthermore, we propose a new training regime for faster model convergence
by extracting more information from a given meta-learning episode with
negligible extra computation. We evaluate the proposed techniques on VoxCeleb,
SITW and VCTK datasets on the tasks of speaker verification and unseen speaker
identification. The proposed approach outperforms the existing approaches
consistently on both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chaubey_A/0/1/0/all/0/1"&gt;Ashutosh Chaubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sinha_S/0/1/0/all/0/1"&gt;Sparsh Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghose_S/0/1/0/all/0/1"&gt;Susmita Ghose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.04488</id>
        <link href="http://arxiv.org/abs/2110.04488"/>
        <updated>2022-04-02T00:47:22.395Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) models are one of the most frequently
used deep learning networks, and extensively used in both academia and
industry. Recent studies demonstrated that adversarial attacks against such
models can maintain their effectiveness even when used on models other than the
one targeted by the attacker. This major property is known as transferability,
and makes CNNs ill-suited for security applications. In this paper, we provide
the first comprehensive study which assesses the robustness of CNN-based models
for computer networks against adversarial transferability. Furthermore, we
investigate whether the transferability property issue holds in computer
networks applications. In our experiments, we first consider five different
attacks: the Iterative Fast Gradient Method (I-FGSM), the Jacobian-based
Saliency Map (JSMA), the Limited-memory Broyden Fletcher Goldfarb Shanno BFGS
(L- BFGS), the Projected Gradient Descent (PGD), and the DeepFool attack. Then,
we perform these attacks against three well- known datasets: the Network-based
Detection of IoT (N-BaIoT) dataset, the Domain Generating Algorithms (DGA)
dataset, and the RIPE Atlas dataset. Our experimental results show clearly that
the transferability happens in specific use cases for the I- FGSM, the JSMA,
and the LBFGS attack. In such scenarios, the attack success rate on the target
network range from 63.00% to 100%. Finally, we suggest two shielding strategies
to hinder the attack transferability, by considering the Most Powerful Attacks
(MPAs), and the mismatch LSTM architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1"&gt;Ehsan Nowroozi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mekdad_Y/0/1/0/all/0/1"&gt;Yassine Mekdad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berenjestanaki_M/0/1/0/all/0/1"&gt;Mohammad Hajian Berenjestanaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Mauro Conti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fergougui_A/0/1/0/all/0/1"&gt;Abdeslam EL Fergougui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge Computing. (arXiv:2110.01863v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.01863</id>
        <link href="http://arxiv.org/abs/2110.01863"/>
        <updated>2022-04-02T00:47:22.386Z</updated>
        <summary type="html"><![CDATA[The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamansavascilar_B/0/1/0/all/0/1"&gt;Baris Yamansavascilar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baktir_A/0/1/0/all/0/1"&gt;Ahmet Cihat Baktir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonmez_C/0/1/0/all/0/1"&gt;Cagatay Sonmez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozgovde_A/0/1/0/all/0/1"&gt;Atay Ozgovde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ersoy_C/0/1/0/all/0/1"&gt;Cem Ersoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based Reinforcement Learning: A Survey. (arXiv:2006.16712v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16712</id>
        <link href="http://arxiv.org/abs/2006.16712"/>
        <updated>2022-04-02T00:47:22.362Z</updated>
        <summary type="html"><![CDATA[Sequential decision making, commonly formalized as Markov Decision Process
(MDP) optimization, is a important challenge in artificial intelligence. Two
key approaches to this problem are reinforcement learning (RL) and planning.
This paper presents a survey of the integration of both fields, better known as
model-based reinforcement learning. Model-based RL has two main steps. First,
we systematically cover approaches to dynamics model learning, including
challenges like dealing with stochasticity, uncertainty, partial observability,
and temporal abstraction. Second, we present a systematic categorization of
planning-learning integration, including aspects like: where to start planning,
what budgets to allocate to planning and real data collection, how to plan, and
how to integrate planning in the learning and acting loop. After these two
sections, we also discuss implicit model-based RL as an end-to-end alternative
for model learning and planning, and we cover the potential benefits of
model-based RL. Along the way, the survey also draws connections to several
related RL fields, like hierarchical RL and transfer learning. Altogether, the
survey presents a broad conceptual overview of the combination of planning and
learning for MDP optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1"&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broekens_J/0/1/0/all/0/1"&gt;Joost Broekens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1"&gt;Catholijn M. Jonker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing. (arXiv:2203.17266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17266</id>
        <link href="http://arxiv.org/abs/2203.17266"/>
        <updated>2022-04-02T00:47:22.354Z</updated>
        <summary type="html"><![CDATA[Recent advances like StyleGAN have promoted the growth of controllable facial
editing. To address its core challenge of attribute decoupling in a single
latent space, attempts have been made to adopt dual-space GAN for better
disentanglement of style and content representations. Nonetheless, these
methods are still incompetent to obtain plausible editing results with high
controllability, especially for complicated attributes. In this study, we
highlight the importance of interaction in a dual-space GAN for more
controllable editing. We propose TransEditor, a novel Transformer-based
framework to enhance such interaction. Besides, we develop a new dual-space
editing and inversion strategy to provide additional editing flexibility.
Extensive experiments demonstrate the superiority of the proposed framework in
image quality and editing capability, suggesting the effectiveness of
TransEditor for highly controllable facial editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yanbo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yueqin Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qianyi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chengyao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wayne Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unifying Framework for Reinforcement Learning and Planning. (arXiv:2006.15009v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15009</id>
        <link href="http://arxiv.org/abs/2006.15009"/>
        <updated>2022-04-02T00:47:22.346Z</updated>
        <summary type="html"><![CDATA[Sequential decision making, commonly formalized as optimization of a Markov
Decision Process, is a key challenge in artificial intelligence. Two successful
approaches to MDP optimization are reinforcement learning and planning, which
both largely have their own research communities. However, if both research
fields solve the same problem, then we might be able to disentangle the common
factors in their solution approaches. Therefore, this paper presents a unifying
algorithmic framework for reinforcement learning and planning (FRAP), which
identifies underlying dimensions on which MDP planning and learning algorithms
have to decide. At the end of the paper, we compare a variety of well-known
planning, model-free and model-based RL algorithms along these dimensions.
Altogether, the framework may help provide deeper insight in the algorithmic
design space of planning and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1"&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broekens_J/0/1/0/all/0/1"&gt;Joost Broekens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1"&gt;Catholijn M. Jonker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11821</id>
        <link href="http://arxiv.org/abs/2106.11821"/>
        <updated>2022-04-02T00:47:22.338Z</updated>
        <summary type="html"><![CDATA[In this paper we study data augmentation for opcode sequence based Android
malware detection. Data augmentation has been successfully used in many areas
of deep-learning to significantly improve model performance. Typically, data
augmentation simulates realistic variations in data to increase the apparent
diversity of the training-set. However, for opcode-based malware analysis it is
not immediately clear how to apply data augmentation. Hence we first study the
use of fixed transformations, then progress to adaptive methods. We propose a
novel data augmentation method -- Self-Embedding Language Model Augmentation --
that uses a malware detection network's own opcode embedding layer to measure
opcode similarity for adaptive augmentation. To the best of our knowledge this
is the first paper to carry out a systematic study of different augmentation
methods for opcode sequence based Android malware classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1"&gt;Niall McLaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1"&gt;Jesus Martinez del Rincon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommender Systems meet Mechanism Design. (arXiv:2110.12558v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.12558</id>
        <link href="http://arxiv.org/abs/2110.12558"/>
        <updated>2022-04-02T00:47:22.316Z</updated>
        <summary type="html"><![CDATA[Machine learning has developed a variety of tools for learning and
representing high-dimensional distributions with structure. Recent years have
also seen big advances in designing multi-item mechanisms. Akin to overfitting,
however, these mechanisms can be extremely sensitive to the Bayesian prior that
they target, which becomes problematic when that prior is only approximately
known. At the same time, even if access to the exact Bayesian prior is given,
it is known that optimal or even approximately optimal multi-item mechanisms
run into sample, computational, representation and communication intractability
barriers.

We consider a natural class of multi-item mechanism design problems with very
large numbers of items, but where the bidders' value distributions can be
well-approximated by a topic model akin to those used in recommendation systems
with very large numbers of possible recommendations. We propose a mechanism
design framework for this setting, building on a recent robustification
framework by Brustle et al., which disentangles the statistical challenge of
estimating a multi-dimensional prior from the task of designing a good
mechanism for it, and robustifies the performance of the latter against the
estimation error of the former. We provide an extension of this framework
appropriate for our setting, which allows us to exploit the expressive power of
topic models to reduce the effective dimensionality of the mechanism design
problem and remove the dependence of its computational, communication and
representation complexity on the number of items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1"&gt;Constantinos Daskalakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11956</id>
        <link href="http://arxiv.org/abs/2107.11956"/>
        <updated>2022-04-02T00:47:22.309Z</updated>
        <summary type="html"><![CDATA[Automatically mining sentiment tendency contained in natural language is a
fundamental research to some artificial intelligent applications, where
solutions alternate with challenges. Transfer learning and multi-task learning
techniques have been leveraged to mitigate the supervision sparsity and
collaborate multiple heterogeneous domains correspondingly. Recent years, the
sensitive nature of users' private data raises another challenge for sentiment
classification, i.e., data privacy protection. In this paper, we resort to
federated learning for multiple domain sentiment classification under the
constraint that the corpora must be stored on decentralized devices. In view of
the heterogeneous semantics across multiple parties and the peculiarities of
word embedding, we pertinently provide corresponding solutions. First, we
propose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for
better model aggregation and personalization in federated sentiment
classification. Second, we propose KTEPS$^\star$ with the consideration of the
rich semantic and huge embedding size properties of word vectors, utilizing
Projection-based Dimension Reduction (PDR) methods for privacy protection and
efficient transmission simultaneously. We propose two federated sentiment
classification scenes based on public benchmarks, and verify the superiorities
of our proposed methods with abundant experimental investigations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin-Chun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingshuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shaoming Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Feature Selection for Algorithmic Fairness. (arXiv:2006.06053v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06053</id>
        <link href="http://arxiv.org/abs/2006.06053"/>
        <updated>2022-04-02T00:47:22.302Z</updated>
        <summary type="html"><![CDATA[The use of machine learning (ML) in high-stakes societal decisions has
encouraged the consideration of fairness throughout the ML lifecycle. Although
data integration is one of the primary steps to generate high quality training
data, most of the fairness literature ignores this stage. In this work, we
consider fairness in the integration component of data management, aiming to
identify features that improve prediction without adding any bias to the
dataset. We work under the causal interventional fairness paradigm. Without
requiring the underlying structural causal model a priori, we propose an
approach to identify a sub-collection of features that ensure the fairness of
the dataset by performing conditional independence tests between different
subsets of features. We use group testing to improve the complexity of the
approach. We theoretically prove the correctness of the proposed algorithm to
identify features that ensure interventional fairness and show that sub-linear
conditional independence tests are sufficient to identify these variables. A
detailed empirical evaluation is performed on real-world datasets to
demonstrate the efficacy and efficiency of our technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1"&gt;Sainyam Galhotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1"&gt;Prasanna Sattigeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1"&gt;Kush R. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?. (arXiv:2110.04184v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.04184</id>
        <link href="http://arxiv.org/abs/2110.04184"/>
        <updated>2022-04-02T00:47:22.294Z</updated>
        <summary type="html"><![CDATA[Multi-agent reinforcement learning has made substantial empirical progresses
in solving games with a large number of players. However, theoretically, the
best known sample complexity for finding a Nash equilibrium in general-sum
games scales exponentially in the number of players due to the size of the
joint action space, and there is a matching exponential lower bound. This paper
investigates what learning goals admit better sample complexities in the
setting of $m$-player general-sum Markov games with $H$ steps, $S$ states, and
$A_i$ actions per player. First, we design algorithms for learning an
$\epsilon$-Coarse Correlated Equilibrium (CCE) in
$\widetilde{\mathcal{O}}(H^5S\max_{i\le m} A_i / \epsilon^2)$ episodes, and an
$\epsilon$-Correlated Equilibrium (CE) in
$\widetilde{\mathcal{O}}(H^6S\max_{i\le m} A_i^2 / \epsilon^2)$ episodes. This
is the first line of results for learning CCE and CE with sample complexities
polynomial in $\max_{i\le m} A_i$. Our algorithm for learning CE integrates an
adversarial bandit subroutine which minimizes a weighted swap regret, along
with several novel designs in the outer loop. Second, we consider the important
special case of Markov Potential Games, and design an algorithm that learns an
$\epsilon$-approximate Nash equilibrium within
$\widetilde{\mathcal{O}}(S\sum_{i\le m} A_i / \epsilon^3)$ episodes (when only
highlighting the dependence on $S$, $A_i$, and $\epsilon$), which only depends
linearly in $\sum_{i\le m} A_i$ and significantly improves over existing
efficient algorithm in the $\epsilon$ dependence. Overall, our results shed
light on what equilibria or structural assumptions on the game may enable
sample-efficient learning with many players.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Ziang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1"&gt;Song Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TUNet: A Block-online Bandwidth Extension Model based on Transformers and Self-supervised Pretraining. (arXiv:2110.13492v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.13492</id>
        <link href="http://arxiv.org/abs/2110.13492"/>
        <updated>2022-04-02T00:47:22.286Z</updated>
        <summary type="html"><![CDATA[We introduce a block-online variant of the temporal feature-wise linear
modulation (TFiLM) model to achieve bandwidth extension. The proposed
architecture simplifies the UNet backbone of the TFiLM to reduce inference time
and employs an efficient transformer at the bottleneck to alleviate performance
degradation. We also utilize self-supervised pretraining and data augmentation
to enhance the quality of bandwidth extended signals and reduce the sensitivity
with respect to downsampling methods. Experiment results on the VCTK dataset
show that the proposed method outperforms several recent baselines in both
intrusive and non-intrusive metrics. Pretraining and filter augmentation also
help stabilize and enhance the overall performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet-Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh H. T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khong_A/0/1/0/all/0/1"&gt;Andy W. H. Khong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance Weighted Incremental Evolution Strategies for Reinforcement Learning in Dynamic Environments. (arXiv:2010.04605v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04605</id>
        <link href="http://arxiv.org/abs/2010.04605"/>
        <updated>2022-04-02T00:47:22.258Z</updated>
        <summary type="html"><![CDATA[Evolution strategies (ES), as a family of black-box optimization algorithms,
recently emerge as a scalable alternative to reinforcement learning (RL)
approaches such as Q-learning or policy gradient, and are much faster when many
central processing units (CPUs) are available due to better parallelization. In
this paper, we propose a systematic incremental learning method for ES in
dynamic environments. The goal is to adjust previously learned policy to a new
one incrementally whenever the environment changes. We incorporate an instance
weighting mechanism with ES to facilitate its learning adaptation, while
retaining scalability of ES. During parameter updating, higher weights are
assigned to instances that contain more new knowledge, thus encouraging the
search distribution to move towards new promising areas of parameter space. We
propose two easy-to-implement metrics to calculate the weights: instance
novelty and instance quality. Instance novelty measures an instance's
difference from the previous optimum in the original environment, while
instance quality corresponds to how well an instance performs in the new
environment. The resulting algorithm, Instance Weighted Incremental Evolution
Strategies (IW-IES), is verified to achieve significantly improved performance
on challenging RL tasks ranging from robot navigation to locomotion. This paper
thus introduces a family of scalable ES algorithms for RL domains that enables
rapid learning adaptation to dynamic environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chunlin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1"&gt;Daoyi Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.01661</id>
        <link href="http://arxiv.org/abs/2110.01661"/>
        <updated>2022-04-02T00:47:22.250Z</updated>
        <summary type="html"><![CDATA[Iterating with new and improved OCR solutions enforces decision making when
it comes to targeting the right candidates for reprocessing. This especially
applies when the underlying data collection is of considerable size and rather
diverse in terms of fonts, languages, periods of publication and consequently
OCR quality. This article captures the efforts of the National Library of
Luxembourg to support those targeting decisions. They are crucial in order to
guarantee low computational overhead and reduced quality degradation risks,
combined with a more quantifiable OCR improvement. In particular, this work
explains the methodology of the library with respect to text block level
quality assessment. Through extension of this technique, a regression model,
that is able to take into account the enhancement potential of a new OCR
engine, is also presented. They both mark promising approaches, especially for
cultural institutions dealing with historical data of lower quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1"&gt;Pit Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_Y/0/1/0/all/0/1"&gt;Yves Maurer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Speaker Adaptation for Text-to-Speech Synthesis. (arXiv:2103.14512v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14512</id>
        <link href="http://arxiv.org/abs/2103.14512"/>
        <updated>2022-04-02T00:47:22.243Z</updated>
        <summary type="html"><![CDATA[Training a multi-speaker Text-to-Speech (TTS) model from scratch is
computationally expensive and adding new speakers to the dataset requires the
model to be re-trained. The naive solution of sequential fine-tuning of a model
for new speakers can lead to poor performance of older speakers. This
phenomenon is known as catastrophic forgetting. In this paper, we look at TTS
modeling from a continual learning perspective, where the goal is to add new
speakers without forgetting previous speakers. Therefore, we first propose an
experimental setup and show that serial fine-tuning for new speakers can cause
the forgetting of the earlier speakers. Then we exploit two well-known
techniques for continual learning, namely experience replay and weight
regularization. We reveal how one can mitigate the effect of degradation in
speech synthesis diversity in sequential training of new speakers using these
methods. Finally, we present a simple extension to experience replay to improve
the results in extreme setups where we have access to very small buffers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hemati_H/0/1/0/all/0/1"&gt;Hamed Hemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1"&gt;Damian Borth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Examples in Random Neural Networks with General Activations. (arXiv:2203.17209v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17209</id>
        <link href="http://arxiv.org/abs/2203.17209"/>
        <updated>2022-04-02T00:47:22.235Z</updated>
        <summary type="html"><![CDATA[A substantial body of empirical work documents the lack of robustness in deep
learning models to adversarial examples. Recent theoretical work proved that
adversarial examples are ubiquitous in two-layers networks with sub-exponential
width and ReLU or smooth activations, and multi-layer ReLU networks with
sub-exponential width. We present a result of the same type, with no
restriction on width and for general locally Lipschitz continuous activations.

More precisely, given a neural network $f(\,\cdot\,;{\boldsymbol \theta})$
with random weights ${\boldsymbol \theta}$, and feature vector ${\boldsymbol
x}$, we show that an adversarial example ${\boldsymbol x}'$ can be found with
high probability along the direction of the gradient $\nabla_{{\boldsymbol
x}}f({\boldsymbol x};{\boldsymbol \theta})$. Our proof is based on a Gaussian
conditioning technique. Instead of proving that $f$ is approximately linear in
a neighborhood of ${\boldsymbol x}$, we characterize the joint distribution of
$f({\boldsymbol x};{\boldsymbol \theta})$ and $f({\boldsymbol x}';{\boldsymbol
\theta})$ for ${\boldsymbol x}' = {\boldsymbol x}-s({\boldsymbol
x})\nabla_{{\boldsymbol x}}f({\boldsymbol x};{\boldsymbol \theta})$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1"&gt;Andrea Montanari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuchen Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A 23 MW data centre is all you need. (arXiv:2203.17265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17265</id>
        <link href="http://arxiv.org/abs/2203.17265"/>
        <updated>2022-04-02T00:47:22.214Z</updated>
        <summary type="html"><![CDATA[The field of machine learning has achieved striking progress in recent years,
witnessing breakthrough results on language modelling, protein folding and
nitpickingly fine-grained dog breed classification. Some even succeeded at
playing computer games and board games, a feat both of engineering and of
setting their employers' expectations. The central contribution of this work is
to carefully examine whether this progress, and technology more broadly, can be
expected to continue indefinitely. Through a rigorous application of
statistical theory and failure to extrapolate beyond the training data, we
answer firmly in the negative and provide details: technology will peak at 3:07
am (BST) on 20th July, 2032. We then explore the implications of this finding,
discovering that individuals awake at this ungodly hour with access to a
sufficiently powerful computer possess an opportunity for myriad forms of
long-term linguistic 'lock in'. All we need is a large (>> 1W) data centre to
seize this pivotal moment. By setting our analogue alarm clocks, we propose a
tractable algorithm to ensure that, for the future of humanity, the British
spelling of colour becomes the default spelling across more than 80% of the
global word processing software market.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1"&gt;Samuel Albanie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1"&gt;Dylan Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities. (arXiv:2203.17242v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.17242</id>
        <link href="http://arxiv.org/abs/2203.17242"/>
        <updated>2022-04-02T00:47:20.632Z</updated>
        <summary type="html"><![CDATA[We present a novel feasibility study on the automatic recognition of
Expressed Emotion (EE), a family environment concept based on caregivers
speaking freely about their relative/family member. We describe an automated
approach for determining the \textit{degree of warmth}, a key component of EE,
from acoustic and text features acquired from a sample of 37 recorded
interviews. These recordings, collected over 20 years ago, are derived from a
nationally representative birth cohort of 2,232 British twin children and were
manually coded for EE. We outline the core steps of extracting usable
information from recordings with highly variable audio quality and assess the
efficacy of four machine learning approaches trained with different
combinations of acoustic and text features. Despite the challenges of working
with this legacy data, we demonstrated that the degree of warmth can be
predicted with an $F_{1}$-score of \textbf{61.5\%}. In this paper, we summarise
our learning and provide recommendations for future work using real-world
speech samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirheidari_B/0/1/0/all/0/1"&gt;Bahman Mirheidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bittar_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Bittar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummins_N/0/1/0/all/0/1"&gt;Nicholas Cummins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Downs_J/0/1/0/all/0/1"&gt;Johnny Downs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_H/0/1/0/all/0/1"&gt;Helen L. Fisher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christensen_H/0/1/0/all/0/1"&gt;Heidi Christensen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17247</id>
        <link href="http://arxiv.org/abs/2203.17247"/>
        <updated>2022-04-02T00:47:20.632Z</updated>
        <summary type="html"><![CDATA[Breakthroughs in transformer-based models have revolutionized not only the
NLP field, but also vision and multimodal systems. However, although
visualization and interpretability tools have become available for NLP models,
internal mechanisms of vision and multimodal transformers remain largely
opaque. With the success of these transformers, it is increasingly critical to
understand their inner workings, as unraveling these black-boxes will lead to
more capable and trustworthy models. To contribute to this quest, we propose
VL-InterpreT, which provides novel interactive visualizations for interpreting
the attentions and hidden representations in multimodal transformers.
VL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety
of statistics in attention heads throughout all layers for both vision and
language components, (2) visualizes cross-modal and intra-modal attentions
through easily readable heatmaps, and (3) plots the hidden representations of
vision and language tokens as they pass through the transformer layers. In this
paper, we demonstrate the functionalities of VL-InterpreT through the analysis
of KD-VLP, an end-to-end pretraining vision-language multimodal
transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and
WebQA, two visual question answering benchmarks. Furthermore, we also present a
few interesting findings about multimodal transformer behaviors that were
learned through our tool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aflalo_E/0/1/0/all/0/1"&gt;Estelle Aflalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Meng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1"&gt;Shao-Yen Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1"&gt;Vasudev Lal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wind Farm Layout Optimisation using Set Based Multi-objective Bayesian Optimisation. (arXiv:2203.17065v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.17065</id>
        <link href="http://arxiv.org/abs/2203.17065"/>
        <updated>2022-04-02T00:47:20.631Z</updated>
        <summary type="html"><![CDATA[Wind energy is one of the cleanest renewable electricity sources and can help
in addressing the challenge of climate change. One of the drawbacks of
wind-generated energy is the large space necessary to install a wind farm; this
arises from the fact that placing wind turbines in a limited area would hinder
their productivity and therefore not be economically convenient. This naturally
leads to an optimisation problem, which has three specific challenges: (1)
multiple conflicting objectives (2) computationally expensive simulation models
and (3) optimisation over design sets instead of design vectors. The first and
second challenges can be addressed by using surrogate-assisted e.g.\ Bayesian
multi-objective optimisation. However, the traditional Bayesian optimisation
cannot be applied as the optimisation function in the problem relies on design
sets instead of design vectors. This paper extends the applicability of
Bayesian multi-objective optimisation to set based optimisation for solving the
wind farm layout problem. We use a set-based kernel in Gaussian process to
quantify the correlation between wind farms (with a different number of
turbines). The results on the given data set of wind energy and direction
clearly show the potential of using set-based Bayesian multi-objective
optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chugh_T/0/1/0/all/0/1"&gt;Tinkle Chugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ymeraj_E/0/1/0/all/0/1"&gt;Endi Ymeraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization. (arXiv:2106.03721v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03721</id>
        <link href="http://arxiv.org/abs/2106.03721"/>
        <updated>2022-04-02T00:47:20.388Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved tremendous success with independent and
identically distributed (i.i.d.) data. However, the performance of neural
networks often degenerates drastically when encountering out-of-distribution
(OoD) data, i.e., when training and test data are sampled from different
distributions. While a plethora of algorithms have been proposed for OoD
generalization, our understanding of the data used to train and evaluate these
algorithms remains stagnant. In this work, we first identify and measure two
distinct kinds of distribution shifts that are ubiquitous in various datasets.
Next, through extensive experiments, we compare OoD generalization algorithms
across two groups of benchmarks, each dominated by one of the distribution
shifts, revealing their strengths on one shift as well as limitations on the
other shift. Overall, we position existing datasets and algorithms from
different research areas seemingly unconnected into the same coherent picture.
It may serve as a foothold that can be resorted to by future OoD generalization
research. Our code is available at https://github.com/ynysjtu/ood_bench.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1"&gt;Nanyang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kaican Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoyue Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Runpeng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Fengwei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Audio Deepfake Detection Generalize?. (arXiv:2203.16263v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16263</id>
        <link href="http://arxiv.org/abs/2203.16263"/>
        <updated>2022-04-02T00:47:20.194Z</updated>
        <summary type="html"><![CDATA[Current text-to-speech algorithms produce realistic fakes of human voices,
making deepfake detection a much-needed area of research. While researchers
have presented various techniques for detecting audio spoofs, it is often
unclear exactly why these architectures are successful: Preprocessing steps,
hyperparameter settings, and the degree of fine-tuning are not consistent
across related work. Which factors contribute to success, and which are
accidental? In this work, we address this problem: We systematize audio
spoofing detection by re-implementing and uniformly evaluating architectures
from related work. We identify overarching features for successful audio
deepfake detection, such as using cqtspec or logspec features instead of
melspec features, which improves performance by 37% EER on average, all other
factors constant. Additionally, we evaluate generalization capabilities: We
collect and publish a new dataset consisting of 37.9 hours of found audio
recordings of celebrities and politicians, of which 17.2 hours are deepfakes.
We find that related work performs poorly on such real-world data (performance
degradation of up to one thousand percent). This may suggest that the community
has tailored its solutions too closely to the prevailing ASVSpoof benchmark and
that deepfakes are much harder to detect outside the lab than previously
thought.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1"&gt;Nicolas M. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czempin_P/0/1/0/all/0/1"&gt;Pavel Czempin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dieckmann_F/0/1/0/all/0/1"&gt;Franziska Dieckmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Froghyar_A/0/1/0/all/0/1"&gt;Adam Froghyar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1"&gt;Konstantin B&amp;#xf6;ttinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimisation-free Classification and Density Estimation with Quantum Circuits. (arXiv:2203.14452v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.14452</id>
        <link href="http://arxiv.org/abs/2203.14452"/>
        <updated>2022-04-02T00:47:20.189Z</updated>
        <summary type="html"><![CDATA[We demonstrate the implementation of a novel machine learning framework for
probability density estimation and classification using quantum circuits. The
framework maps a training data set or a single data sample to the quantum state
of a physical system through quantum feature maps. The quantum state of the
arbitrarily large training data set summarises its probability distribution in
a finite-dimensional quantum wave function. By projecting the quantum state of
a new data sample onto the quantum state of the training data set, one can
derive statistics to classify or estimate the density of the new data sample.
Remarkably, the implementation of our framework on a real quantum device does
not require any optimisation of quantum circuit parameters. Nonetheless, we
discuss a variational quantum circuit approach that could leverage quantum
advantage for our framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vargas_Calderon_V/0/1/0/all/0/1"&gt;Vladimir Vargas-Calder&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gonzalez_F/0/1/0/all/0/1"&gt;Fabio A. Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vinck_Posada_H/0/1/0/all/0/1"&gt;Herbert Vinck-Posada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.15588</id>
        <link href="http://arxiv.org/abs/2203.15588"/>
        <updated>2022-04-02T00:47:20.170Z</updated>
        <summary type="html"><![CDATA[The rapid development of diagnostic technologies in healthcare is leading to
higher requirements for physicians to handle and integrate the heterogeneous,
yet complementary data that are produced during routine practice. For instance,
the personalized diagnosis and treatment planning for a single cancer patient
relies on the various images (e.g., radiological, pathological, and camera
images) and non-image data (e.g., clinical data and genomic data). However,
such decision-making procedures can be subjective, qualitative, and have large
inter-subject variabilities. With the recent advances in multi-modal deep
learning technologies, an increasingly large number of efforts have been
devoted to a key question: how do we extract and aggregate multi-modal
information to ultimately provide more objective, quantitative computer-aided
clinical decision making? This paper reviews the recent studies on dealing with
such a question. Briefly, this review will include the (1) overview of current
multi-modal learning workflows, (2) summarization of multi-modal fusion
methods, (3) discussion of the performance, (4) applications in disease
diagnosis and prognosis, and (5) challenges and future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Can Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haichun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaohong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shilin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asad_Z/0/1/0/all/0/1"&gt;Zuhayr Asad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coburn_L/0/1/0/all/0/1"&gt;Lori A. Coburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_K/0/1/0/all/0/1"&gt;Keith T. Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TraHGR: Transformer for Hand Gesture Recognition via ElectroMyography. (arXiv:2203.16336v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16336</id>
        <link href="http://arxiv.org/abs/2203.16336"/>
        <updated>2022-04-02T00:47:20.170Z</updated>
        <summary type="html"><![CDATA[Deep learning-based Hand Gesture Recognition (HGR) via surface Electromyogram
(sEMG) signals has recently shown significant potential for development of
advanced myoelectric-controlled prosthesis. Existing deep learning approaches,
typically, include only one model as such can hardly maintain acceptable
generalization performance in changing scenarios. In this paper, we aim to
address this challenge by capitalizing on the recent advances of hybrid models
and transformers. In other words, we propose a hybrid framework based on the
transformer architecture, which is a relatively new and revolutionizing deep
learning model. The proposed hybrid architecture, referred to as the
Transformer for Hand Gesture Recognition (TraHGR), consists of two parallel
paths followed by a linear layer that acts as a fusion center to integrate the
advantage of each module and provide robustness over different scenarios. We
evaluated the proposed architecture TraHGR based on the commonly used second
Ninapro dataset, referred to as the DB2. The sEMG signals in the DB2 dataset
are measured in the real-life conditions from 40 healthy users, each performing
49 gestures. We have conducted extensive set of experiments to test and
validate the proposed TraHGR architecture, and have compared its achievable
accuracy with more than five recently proposed HGR classification algorithms
over the same dataset. We have also compared the results of the proposed TraHGR
architecture with each individual path and demonstrated the distinguishing
power of the proposed hybrid architecture. The recognition accuracies of the
proposed TraHGR architecture are 86.18%, 88.91%, 81.44%, and 93.84%, which are
2.48%, 5.12%, 8.82%, and 4.30% higher than the state-ofthe-art performance for
DB2 (49 gestures), DB2-B (17 gestures), DB2-C (23 gestures), and DB2-D (9
gestures), respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zabihi_S/0/1/0/all/0/1"&gt;Soheil Zabihi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahimian_E/0/1/0/all/0/1"&gt;Elahe Rahimian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1"&gt;Amir Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1"&gt;Arash Mohammadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. (arXiv:2112.03028v2 [cs.RO] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2112.03028</id>
        <link href="http://arxiv.org/abs/2112.03028"/>
        <updated>2022-04-02T00:47:20.170Z</updated>
        <summary type="html"><![CDATA[We introduce the dynamic grasp synthesis task: given an object with a known
6D pose and a grasp reference, our goal is to generate motions that move the
object to a target 6D pose. This is challenging, because it requires reasoning
about the complex articulation of the human hand and the intricate physical
interaction with the object. We propose a novel method that frames this problem
in the reinforcement learning framework and leverages a physics simulation,
both to learn and to evaluate such dynamic interactions. A hierarchical
approach decomposes the task into low-level grasping and high-level motion
synthesis. It can be used to generate novel hand sequences that approach,
grasp, and move an object to a desired location, while retaining
human-likeness. We show that our approach leads to stable grasps and generates
a wide range of motions. Furthermore, even imperfect labels can be corrected by
our method to generate dynamic interaction sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Christen_S/0/1/0/all/0/1"&gt;Sammy Christen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1"&gt;Muhammed Kocabas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aksan_E/0/1/0/all/0/1"&gt;Emre Aksan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwangbo_J/0/1/0/all/0/1"&gt;Jemin Hwangbo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v1 [eess.AS] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.15937</id>
        <link href="http://arxiv.org/abs/2203.15937"/>
        <updated>2022-04-02T00:47:20.169Z</updated>
        <summary type="html"><![CDATA[Current leading mispronunciation detection and diagnosis (MDD) systems
achieve promising performance via end-to-end phoneme recognition. One challenge
of such end-to-end solutions is the scarcity of human-annotated phonemes on
natural L2 speech. In this work, we leverage unlabeled L2 speech via a
pseudo-labeling (PL) procedure and extend the fine-tuning approach based on
pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec
2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples
plus the created pseudo-labeled L2 speech samples. Our pseudo labels are
dynamic and are produced by an ensemble of the online model on-the-fly, which
ensures that our model is robust to pseudo label noise. We show that
fine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and
2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning
baseline. The proposed PL method is also shown to outperform conventional
offline PL methods. Compared to the state-of-the-art MDD systems, our MDD
solution achieves a more accurate and consistent phonetic error diagnosis. In
addition, we conduct an open test on a separate UTD-4Accents dataset, where our
system recognition outputs show a strong correlation with human perception,
based on accentedness and intelligibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hirschi_K/0/1/0/all/0/1"&gt;Kevin Hirschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Looney_S/0/1/0/all/0/1"&gt;Stephen D. Looney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kang_O/0/1/0/all/0/1"&gt;Okim Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hansen_J/0/1/0/all/0/1"&gt;John H. L. Hansen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.14542</id>
        <link href="http://arxiv.org/abs/2203.14542"/>
        <updated>2022-04-02T00:47:20.167Z</updated>
        <summary type="html"><![CDATA[Supervised deep learning methods require a large repository of annotated
data; hence, label noise is inevitable. Training with such noisy data
negatively impacts the generalization performance of deep neural networks. To
combat label noise, recent state-of-the-art methods employ some sort of sample
selection mechanism to select a possibly clean subset of data. Next, an
off-the-shelf semi-supervised learning method is used for training where
rejected samples are treated as unlabeled data. Our comprehensive analysis
shows that current selection methods disproportionately select samples from
easy (fast learnable) classes while rejecting those from relatively harder
ones. This creates class imbalance in the selected clean set and in turn,
deteriorates performance under high label noise. In this work, we propose
UNICON, a simple yet effective sample selection method which is robust to high
label noise. To address the disproportionate selection of easy and hard
samples, we introduce a Jensen-Shannon divergence based uniform selection
mechanism which does not require any probabilistic modeling and hyperparameter
tuning. We complement our selection method with contrastive learning to further
combat the memorization of noisy labels. Extensive experimentation on multiple
benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%
improvement over the current state-of-the-art on CIFAR100 dataset with a 90%
noise rate. Our code is publicly available]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1"&gt;Nazmul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1"&gt;Mamshad Nayeem Rizve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1"&gt;Nazanin Rahnavard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.06537</id>
        <link href="http://arxiv.org/abs/2110.06537"/>
        <updated>2022-04-02T00:47:20.159Z</updated>
        <summary type="html"><![CDATA[The conventional wisdom behind learning deep classification models is to
focus on bad-classified examples and ignore well-classified examples that are
far from the decision boundary. For instance, when training with cross-entropy
loss, examples with higher likelihoods (i.e., well-classified examples)
contribute smaller gradients in back-propagation. However, we theoretically
show that this common practice hinders representation learning, energy
optimization, and margin growth. To counteract this deficiency, we propose to
reward well-classified examples with additive bonuses to revive their
contribution to the learning process. This counterexample theoretically
addresses these three issues. We empirically support this claim by directly
verifying the theoretical results or significant performance improvement with
our counterexample on diverse tasks, including image classification, graph
classification, and machine translation. Furthermore, this paper shows that we
can deal with complex scenarios, such as imbalanced classification, OOD
detection, and applications under adversarial attacks because our idea can
solve these three issues. Code is available at:
https://github.com/lancopku/well-classified-examples-are-underestimated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenkai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunfang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.08377</id>
        <link href="http://arxiv.org/abs/2201.08377"/>
        <updated>2022-04-02T00:47:20.158Z</updated>
        <summary type="html"><![CDATA[Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1"&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mannat Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1"&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1"&gt;Laurens van der Maaten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speaker Embedding-aware Neural Diarization: an Efficient Framework for Overlapping Speech Diarization in Meeting Scenarios. (arXiv:2203.09767v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.09767</id>
        <link href="http://arxiv.org/abs/2203.09767"/>
        <updated>2022-04-02T00:47:20.157Z</updated>
        <summary type="html"><![CDATA[Overlapping speech diarization has been traditionally treated as a
multi-label classification problem. In this paper, we reformulate this task as
a single-label prediction problem by encoding multiple binary labels into a
single label with the power set, which represents the possible combinations of
target speakers. This formulation has two benefits. First, the overlaps of
target speakers are explicitly modeled. Second, threshold selection is no
longer needed. Through this formulation, we propose the speaker embedding-aware
neural diarization (SEND) framework, where a speech encoder, a speaker encoder,
two similarity scorers, and a post-processing network are jointly optimized to
predict the encoded labels according to the similarities between speech
features and speaker embeddings. Experimental results show that SEND has a
stable learning process and can be trained on highly overlapped data without
extra initialization. More importantly, our method achieves the
state-of-the-art performance in real meeting scenarios with fewer model
parameters and lower computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhihao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Siqi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhijie Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16297</id>
        <link href="http://arxiv.org/abs/2203.16297"/>
        <updated>2022-04-02T00:47:20.155Z</updated>
        <summary type="html"><![CDATA[Object detection and forecasting are fundamental components of embodied
perception. These two problems, however, are largely studied in isolation by
the community. In this paper, we propose an end-to-end approach for detection
and motion forecasting based on raw sensor measurement as opposed to ground
truth tracks. Instead of predicting the current frame locations and forecasting
forward in time, we directly predict future object locations and backcast to
determine where each trajectory began. Our approach not only improves overall
accuracy compared to other modular or end-to-end baselines, it also prompts us
to rethink the role of explicit tracking for embodied perception. Additionally,
by linking future and current locations in a many-to-one manner, our approach
is able to reason about multiple futures, a capability that was previously
considered difficult for end-to-end approaches. We conduct extensive
experiments on the popular nuScenes dataset and demonstrate the empirical
effectiveness of our approach. In addition, we investigate the appropriateness
of reusing standard forecasting metrics for an end-to-end setup, and find a
number of limitations which allow us to build simple baselines to game these
metrics. We address this issue with a novel set of joint forecasting and
detection metrics that extend the commonly used AP metrics from the detection
community to measuring forecasting accuracy. Our code is available at
https://github.com/neeharperi/FutureDet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1"&gt;Neehar Peri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luiten_J/0/1/0/all/0/1"&gt;Jonathon Luiten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mengtian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1"&gt;Aljo&amp;#x161;a O&amp;#x161;ep&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taix&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1"&gt;Deva Ramanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks in IoT: A Survey. (arXiv:2203.15935v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.15935</id>
        <link href="http://arxiv.org/abs/2203.15935"/>
        <updated>2022-04-02T00:47:20.154Z</updated>
        <summary type="html"><![CDATA[The Internet of Things (IoT) boom has revolutionized almost every corner of
people's daily lives: healthcare, home, transportation, manufacturing, supply
chain, and so on. With the recent development of sensor and communication
technologies, IoT devices including smart wearables, cameras, smartwatches, and
autonomous vehicles can accurately measure and perceive their surrounding
environment. Continuous sensing generates massive amounts of data and presents
challenges for machine learning. Deep learning models (e.g., convolution neural
networks and recurrent neural networks) have been extensively employed in
solving IoT tasks by learning patterns from multi-modal sensory data. Graph
Neural Networks (GNNs), an emerging and fast-growing family of neural network
models, can capture complex interactions within sensor topology and have been
demonstrated to achieve state-of-the-art results in numerous IoT learning
tasks. In this survey, we present a comprehensive review of recent advances in
the application of GNNs to the IoT field, including a deep dive analysis of GNN
design in various IoT sensing environments, an overarching list of public data
and source code from the collected publications, and future research
directions. To keep track of newly published works, we collect representative
papers and their open-source implementations and create a Github repository at
https://github.com/GuiminDong/GNN4IoT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1"&gt;Guimin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingyue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiechao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Sikun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1"&gt;Lihua Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1"&gt;Robert Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_B/0/1/0/all/0/1"&gt;Bradford Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1"&gt;Laura E. Barnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boukhechba_M/0/1/0/all/0/1"&gt;Mehdi Boukhechba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longitudinal Fairness with Censorship. (arXiv:2203.16024v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16024</id>
        <link href="http://arxiv.org/abs/2203.16024"/>
        <updated>2022-04-02T00:47:20.144Z</updated>
        <summary type="html"><![CDATA[Recent works in artificial intelligence fairness attempt to mitigate
discrimination by proposing constrained optimization programs that achieve
parity for some fairness statistic. Most assume availability of the class
label, which is impractical in many real-world applications such as precision
medicine, actuarial analysis and recidivism prediction. Here we consider
fairness in longitudinal right-censored environments, where the time to event
might be unknown, resulting in censorship of the class label and
inapplicability of existing fairness studies. We devise applicable fairness
measures, propose a debiasing algorithm, and provide necessary theoretical
constructs to bridge fairness with and without censorship for these important
and socially-sensitive tasks. Our experiments on four censored datasets confirm
the utility of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_J/0/1/0/all/0/1"&gt;Jeremy C. Weiss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.15173</id>
        <link href="http://arxiv.org/abs/2203.15173"/>
        <updated>2022-04-02T00:47:20.144Z</updated>
        <summary type="html"><![CDATA[Word embedding is a modern distributed word representations approach widely
used in many natural language processing tasks. Converting the vocabulary in a
legal document into a word embedding model facilitates subjecting legal
documents to machine learning, deep learning, and other algorithms and
subsequently performing the downstream tasks of natural language processing
vis-\`a-vis, for instance, document classification, contract review, and
machine translation. The most common and practical approach of accuracy
evaluation with the word embedding model uses a benchmark set with linguistic
rules or the relationship between words to perform analogy reasoning via
algebraic calculation. This paper proposes establishing a 1,134 Legal
Analogical Reasoning Questions Set (LARQS) from the 2,388 Chinese Codex corpus
using five kinds of legal relations, which are then used to evaluate the
accuracy of the Chinese word embedding model. Moreover, we discovered that
legal relations might be ubiquitous in the word embedding model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chun-Hsien Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pu-Jen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization. (arXiv:2107.12438v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12438</id>
        <link href="http://arxiv.org/abs/2107.12438"/>
        <updated>2022-04-02T00:47:20.143Z</updated>
        <summary type="html"><![CDATA[Motivated by the poor performance of cross-validation in settings where data
are scarce, we propose a novel estimator of the out-of-sample performance of a
policy in data-driven optimization.Our approach exploits the optimization
problem's sensitivity analysis to estimate the gradient of the optimal
objective value with respect to the amount of noise in the data and uses the
estimated gradient to debias the policy's in-sample performance. Unlike
cross-validation techniques, our approach avoids sacrificing data for a test
set, utilizes all data when training and, hence, is well-suited to settings
where data are scarce. We prove bounds on the bias and variance of our
estimator for optimization problems with uncertain linear objectives but known,
potentially non-convex, feasible regions. For more specialized optimization
problems where the feasible region is "weakly-coupled" in a certain sense, we
prove stronger results. Specifically, we provide explicit high-probability
bounds on the error of our estimator that hold uniformly over a policy class
and depends on the problem's dimension and policy class's complexity. Our
bounds show that under mild conditions, the error of our estimator vanishes as
the dimension of the optimization problem grows, even if the amount of
available data remains small and constant. Said differently, we prove our
estimator performs well in the small-data, large-scale regime. Finally, we
numerically compare our proposed method to state-of-the-art approaches through
a case-study on dispatching emergency medical response services using real
data. Our method provides more accurate estimates of out-of-sample performance
and learns better-performing policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vishal Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_M/0/1/0/all/0/1"&gt;Michael Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rusmevichientong_P/0/1/0/all/0/1"&gt;Paat Rusmevichientong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STICC: A multivariate spatial clustering method for repeated geographic pattern discovery with consideration of spatial contiguity. (arXiv:2203.09611v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.09611</id>
        <link href="http://arxiv.org/abs/2203.09611"/>
        <updated>2022-04-02T00:47:20.143Z</updated>
        <summary type="html"><![CDATA[Spatial clustering has been widely used for spatial data mining and knowledge
discovery. An ideal multivariate spatial clustering should consider both
spatial contiguity and aspatial attributes. Existing spatial clustering
approaches may face challenges for discovering repeated geographic patterns
with spatial contiguity maintained. In this paper, we propose a Spatial
Toeplitz Inverse Covariance-Based Clustering (STICC) method that considers both
attributes and spatial relationships of geographic objects for multivariate
spatial clustering. A subregion is created for each geographic object serving
as the basic unit when performing clustering. A Markov random field is then
constructed to characterize the attribute dependencies of subregions. Using a
spatial consistency strategy, nearby objects are encouraged to belong to the
same cluster. To test the performance of the proposed STICC algorithm, we apply
it in two use cases. The comparison results with several baseline methods show
that the STICC outperforms others significantly in terms of adjusted rand index
and macro-F1 score. Join count statistics is also calculated and shows that the
spatial contiguity is well preserved by STICC. Such a spatial clustering method
may benefit various applications in the fields of geography, remote sensing,
transportation, and urban planning, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yuhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kunlin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Song Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_I/0/1/0/all/0/1"&gt;Ignavier Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1"&gt;Jinmeng Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_T/0/1/0/all/0/1"&gt;Teng Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Compression and Actionable Intelligence With Deep Neural Networks. (arXiv:2203.13686v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.13686</id>
        <link href="http://arxiv.org/abs/2203.13686"/>
        <updated>2022-04-02T00:47:20.143Z</updated>
        <summary type="html"><![CDATA[If a unit cannot receive intelligence from a source due to external factors,
we consider them disadvantaged users. We categorize this as a preoccupied unit
working on a low connectivity device on the edge. This case requires that we
use a different approach to deliver intelligence, particularly satellite
imagery information, than normally employed. To address this, we propose a
survey of information reduction techniques to deliver the information from a
satellite image in a smaller package. We investigate four techniques to aid in
the reduction of delivered information: traditional image compression, neural
network image compression, object detection image cutout, and image to caption.
Each of these mechanisms have their benefits and tradeoffs when considered for
a disadvantaged user.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1"&gt;Matthew Ciolino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Unsupervised Anomaly Detection in Continuous Auditing of Financial Accounting Data. (arXiv:2112.13215v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.13215</id>
        <link href="http://arxiv.org/abs/2112.13215"/>
        <updated>2022-04-02T00:47:20.142Z</updated>
        <summary type="html"><![CDATA[International audit standards require the direct assessment of a financial
statement's underlying accounting journal entries. Driven by advances in
artificial intelligence, deep-learning inspired audit techniques emerged to
examine vast quantities of journal entry data. However, in regular audits, most
of the proposed methods are applied to learn from a comparably stationary
journal entry population, e.g., of a financial quarter or year. Ignoring
situations where audit relevant distribution changes are not evident in the
training data or become incrementally available over time. In contrast, in
continuous auditing, deep-learning models are continually trained on a stream
of recorded journal entries, e.g., of the last hour. Resulting in situations
where previous knowledge interferes with new information and will be entirely
overwritten. This work proposes a continual anomaly detection framework to
overcome both challenges and designed to learn from a stream of journal entry
data experiences. The framework is evaluated based on deliberately designed
audit scenarios and two real-world datasets. Our experimental results provide
initial evidence that such a learning scheme offers the ability to reduce
false-positive alerts and false-negative decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hemati_H/0/1/0/all/0/1"&gt;Hamed Hemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreyer_M/0/1/0/all/0/1"&gt;Marco Schreyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1"&gt;Damian Borth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radial Autoencoders for Enhanced Anomaly Detection. (arXiv:2203.15884v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.15884</id>
        <link href="http://arxiv.org/abs/2203.15884"/>
        <updated>2022-04-02T00:47:20.142Z</updated>
        <summary type="html"><![CDATA[In classification problems, supervised machine-learning methods outperform
traditional algorithms, thanks to the ability of neural networks to learn
complex patterns. However, in two-class classification tasks like anomaly or
fraud detection, unsupervised methods could do even better, because their
prediction is not limited to previously learned types of anomalies. An
intuitive approach of anomaly detection can be based on the distances from the
centers of mass of the two respective classes. Autoencoders, although trained
without supervision, can also detect anomalies: considering the center of mass
of the normal points, reconstructions have now radii, with largest radii most
likely indicating anomalous points. Of course, radii-based classification were
already possible without interposing an autoencoder. In any space, radial
classification can be operated, to some extent. In order to outperform it, we
proceed to radial deformations of data (i.e. centric compression or expansions
of axes) and autoencoder training. Any autoencoder that makes use of a data
center is here baptized a centric autoencoder (cAE). A special type is the cAE
trained with a uniformly compressed dataset, named the centripetal autoencoder
(cpAE). The new concept is studied here in relation with a schematic artificial
dataset, and the derived methods show consistent score improvements. But tested
on real banking data, our radial deformation supervised algorithms alone still
perform better that cAEs, as expected from most supervised methods;
nonetheless, in hybrid approaches, cAEs can be combined with a radial
deformation of space, improving its classification score. We expect that
centric autoencoders will become irreplaceable objects in anomaly live
detection based on geometry, thanks to their ability to stem naturally on
geometrical algorithms and to their native capability of detecting unknown
anomaly types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1"&gt;Mihai-Cezar Augustin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonvin_V/0/1/0/all/0/1"&gt;Vivien Bonvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houssou_R/0/1/0/all/0/1"&gt;Regis Houssou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rappos_E/0/1/0/all/0/1"&gt;Efstratios Rappos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robert_Nicoud_S/0/1/0/all/0/1"&gt;Stephan Robert-Nicoud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Resource Constrained Multiclass Scheduling in Wireless Networks. (arXiv:2011.13634v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13634</id>
        <link href="http://arxiv.org/abs/2011.13634"/>
        <updated>2022-04-02T00:47:20.141Z</updated>
        <summary type="html"><![CDATA[The problem of resource constrained scheduling in a dynamic and heterogeneous
wireless setting is considered here. In our setup, the available limited
bandwidth resources are allocated in order to serve randomly arriving service
demands, which in turn belong to different classes in terms of payload data
requirement, delay tolerance, and importance/priority. In addition to
heterogeneous traffic, another major challenge stems from random service rates
due to time-varying wireless communication channels. Various approaches for
scheduling and resource allocation can be used, ranging from simple greedy
heuristics and constrained optimization to combinatorics. Those methods are
tailored to specific network or application configuration and are usually
suboptimal. To this purpose, we resort to deep reinforcement learning (DRL) and
propose a distributional Deep Deterministic Policy Gradient (DDPG) algorithm
combined with Deep Sets to tackle the aforementioned problem. Furthermore, we
present a novel way to use a Dueling Network, which leads to further
performance improvement. Our proposed algorithm is tested on both synthetic and
real data, showing consistent gains against state-of-the-art conventional
methods from combinatorics, optimization, and scheduling metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avranas_A/0/1/0/all/0/1"&gt;Apostolos Avranas&lt;/a&gt; (EURECOM), &lt;a href="http://arxiv.org/find/cs/1/au:+Kountouris_M/0/1/0/all/0/1"&gt;Marios Kountouris&lt;/a&gt; (EURECOM), &lt;a href="http://arxiv.org/find/cs/1/au:+Ciblat_P/0/1/0/all/0/1"&gt;Philippe Ciblat&lt;/a&gt; (T&amp;#xe9;l&amp;#xe9;com Paris)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.15547</id>
        <link href="http://arxiv.org/abs/2203.15547"/>
        <updated>2022-04-02T00:47:20.141Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks need the construction of informative features,
which are determined by channel-wise and spatial-wise information at the
network's layers. In this research, we focus on bringing in a novel solution
that uses sophisticated optimization for enhancing both the spatial and channel
components inside each layer's receptive field. Capsule Networks were used to
understand the spatial association between features in the feature map.
Standalone capsule networks have shown good results on comparatively simple
datasets than on complex datasets as a result of the inordinate amount of
feature information. Thus, to tackle this issue, we have proposed ME-CapsNet by
introducing deeper convolutional layers to extract important features before
passing through modules of capsule layers strategically to improve the
performance of the network significantly. The deeper convolutional layer
includes blocks of Squeeze-Excitation networks which use a stochastic sampling
approach for progressively reducing the spatial size thereby dynamically
recalibrating the channels by reconstructing their interdependencies without
much loss of important feature information. Extensive experimentation was done
using commonly used datasets demonstrating the efficiency of the proposed
ME-CapsNet, which clearly outperforms various research works by achieving
higher accuracy with minimal model complexity in complex datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bright_J/0/1/0/all/0/1"&gt;Jerrin Bright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajkumar_S/0/1/0/all/0/1"&gt;Suryaprakash Rajkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doss_A/0/1/0/all/0/1"&gt;Arockia Selvakumar Arockia Doss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.01522</id>
        <link href="http://arxiv.org/abs/2203.01522"/>
        <updated>2022-04-02T00:47:20.141Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep neural networks, there are still many challenges
in deep representation learning due to the data scarcity issues such as data
imbalance, unseen distribution, and domain shift. To address the
above-mentioned issues, a variety of methods have been devised to explore the
sample relationships in a vanilla way (i.e., from the perspectives of either
the input or the loss function), failing to explore the internal structure of
deep neural networks for learning with sample relationships. Inspired by this,
we propose to enable deep neural networks themselves with the ability to learn
the sample relationships from each mini-batch. Specifically, we introduce a
batch transformer module or BatchFormer, which is then applied into the batch
dimension of each mini-batch to implicitly explore sample relationships during
training. By doing this, the proposed method enables the collaboration of
different samples, e.g., the head-class samples can also contribute to the
learning of the tail classes for long-tailed recognition. Furthermore, to
mitigate the gap between training and testing, we share the classifier between
with or without the BatchFormer during training, which can thus be removed
during testing. We perform extensive experiments on over ten datasets and the
proposed method achieves significant improvements on different data scarcity
applications without any bells and whistles, including the tasks of long-tailed
recognition, compositional zero-shot learning, domain generalization, and
contrastive learning. Code will be made publicly available at
https://github.com/zhihou7/BatchFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1"&gt;Zhi Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis. (arXiv:2203.17263v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17263</id>
        <link href="http://arxiv.org/abs/2203.17263"/>
        <updated>2022-04-02T00:47:20.131Z</updated>
        <summary type="html"><![CDATA[Since facial actions such as lip movements contain significant information
about speech content, it is not surprising that audio-visual speech enhancement
methods are more accurate than their audio-only counterparts. Yet,
state-of-the-art approaches still struggle to generate clean, realistic speech
without noise artifacts and unnatural distortions in challenging acoustic
environments. In this paper, we propose a novel audio-visual speech enhancement
framework for high-fidelity telecommunications in AR/VR. Our approach leverages
audio-visual speech cues to generate the codes of a neural speech codec,
enabling efficient synthesis of clean, realistic speech from noisy signals.
Given the importance of speaker-specific cues in speech, we focus on developing
personalized models that work well for individual speakers. We demonstrate the
efficacy of our approach on a new audio-visual speech dataset collected in an
unconstrained, large vocabulary setting, as well as existing audio-visual
datasets, outperforming speech enhancement baselines on both quantitative
metrics and human evaluation studies. Please see the supplemental video for
qualitative results at
https://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Karren Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1"&gt;Dejan Markovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krenn_S/0/1/0/all/0/1"&gt;Steven Krenn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1"&gt;Vasu Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1"&gt;Alexander Richard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CTA-RNN: Channel and Temporal-wise Attention RNN Leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition. (arXiv:2203.17023v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.17023</id>
        <link href="http://arxiv.org/abs/2203.17023"/>
        <updated>2022-04-02T00:47:20.119Z</updated>
        <summary type="html"><![CDATA[Previous research has looked into ways to improve speech emotion recognition
(SER) by utilizing both acoustic and linguistic cues of speech. However, the
potential association between state-of-the-art ASR models and the SER task has
yet to be investigated. In this paper, we propose a novel channel and
temporal-wise attention RNN (CTA-RNN) architecture based on the intermediate
representations of pre-trained ASR models. Specifically, the embeddings of a
large-scale pre-trained end-to-end ASR encoder contain both acoustic and
linguistic information, as well as the ability to generalize to different
speakers, making them well suited for downstream SER task. To further exploit
the embeddings from different layers of the ASR encoder, we propose a novel
CTA-RNN architecture to capture the emotional salient parts of embeddings in
both the channel and temporal directions. We evaluate our approach on two
popular benchmark datasets, IEMOCAP and MSP-IMPROV, using both within-corpus
and cross-corpus settings. Experimental results show that our proposed method
can achieve excellent performance in terms of accuracy and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chengxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengyuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using IPA-Based Tacotron for Data Efficient Cross-Lingual Speaker Adaptation and Pronunciation Enhancement. (arXiv:2011.06392v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06392</id>
        <link href="http://arxiv.org/abs/2011.06392"/>
        <updated>2022-04-02T00:47:20.118Z</updated>
        <summary type="html"><![CDATA[Recent neural Text-to-Speech (TTS) models have been shown to perform very
well when enough data is available. However, fine-tuning them for new speakers
or languages is not straightforward in a low-resource setup. In this paper, we
show that by applying minor modifications to a Tacotron model, one can transfer
an existing TTS model for new speakers from the same or a different language
using only 20 minutes of data. For this purpose, we first introduce a base
multi-lingual Tacotron with language-agnostic input, then demonstrate how
transfer learning is done for different scenarios of speaker adaptation without
exploiting any pre-trained speaker encoder or code-switching technique. We
evaluate the transferred model in both subjective and objective ways.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hemati_H/0/1/0/all/0/1"&gt;Hamed Hemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1"&gt;Damian Borth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.05885</id>
        <link href="http://arxiv.org/abs/2108.05885"/>
        <updated>2022-04-02T00:47:20.118Z</updated>
        <summary type="html"><![CDATA[Obtaining human-like performance in NLP is often argued to require
compositional generalisation. Whether neural networks exhibit this ability is
usually studied by training models on highly compositional synthetic data.
However, compositionality in natural language is much more complex than the
rigid, arithmetic-like version such data adheres to, and artificial
compositionality tests thus do not allow us to determine how neural models deal
with more realistic forms of compositionality. In this work, we re-instantiate
three compositionality tests from the literature and reformulate them for
neural machine translation (NMT). Our results highlight that: i) unfavourably,
models trained on more data are more compositional; ii) models are sometimes
less compositional than expected, but sometimes more, exemplifying that
different levels of compositionality are required, and models are not always
able to modulate between them correctly; iii) some of the non-compositional
behaviours are mistakes, whereas others reflect the natural variation in data.
Apart from an empirical study, our work is a call to action: we should rethink
the evaluation of compositionality in neural networks and develop benchmarks
using real data to evaluate compositionality on natural language, where
composing meaning is not as straightforward as doing the math.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1"&gt;Verna Dankers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1"&gt;Elia Bruni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1"&gt;Dieuwke Hupkes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.13366</id>
        <link href="http://arxiv.org/abs/2203.13366"/>
        <updated>2022-04-02T00:47:20.118Z</updated>
        <summary type="html"><![CDATA[For a long period, different recommendation tasks typically require designing
task-specific architectures and training objectives. As a result, it is hard to
transfer the learned knowledge and representations from one task to another,
thus restricting the generalization ability of existing recommendation
approaches, e.g., a sequential recommendation model can hardly be applied or
transferred to a review generation method. To deal with such issues,
considering that language grounding is a powerful medium to describe and
represent various problems or tasks, we present a flexible and unified
text-to-text paradigm called "Pretrain, Personalized Prompt, and Predict
Paradigm" (P5) for recommendation, which unifies various recommendation tasks
in a shared framework. In P5, all data such as user-item interactions, item
metadata, and user reviews are converted to a common format -- natural language
sequences. The rich information from natural language assist P5 to capture
deeper semantics for recommendation. P5 learns different tasks with the same
language modeling objective during pretraining. Thus, it possesses the
potential to serve as the foundation model for downstream recommendation tasks,
allows easy integration with other modalities, and enables instruction-based
recommendation, which will revolutionize the technical form of recommender
system towards universal recommendation engine. With adaptive personalized
prompt for different users, P5 is able to make predictions in a zero-shot or
few-shot manner and largely reduces the necessity for extensive fine-tuning. On
several recommendation benchmarks, we conduct experiments to show the
effectiveness of our generative approach. We will release our prompts and
pretrained P5 language model to help advance future research on Recommendation
as Language Processing (RLP) and Personalized Foundation Models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1"&gt;Shijie Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuchang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zuohui Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10811</id>
        <link href="http://arxiv.org/abs/2106.10811"/>
        <updated>2022-04-02T00:47:20.117Z</updated>
        <summary type="html"><![CDATA[Shape implicit neural representations (INRs) have recently shown to be
effective in shape analysis and reconstruction tasks. Existing INRs require
point coordinates to learn the implicit level sets of the shape. When a normal
vector is available for each point, a higher fidelity representation can be
learned, however normal vectors are often not provided as raw data.
Furthermore, the method's initialization has been shown to play a crucial role
for surface reconstruction. In this paper, we propose a divergence guided shape
representation learning approach that does not require normal vectors as input.
We show that incorporating a soft constraint on the divergence of the distance
function favours smooth solutions that reliably orients gradients to match the
unknown normal at each point, in some cases even better than approaches that
use ground truth normal vectors directly. Additionally, we introduce a novel
geometric initialization method for sinusoidal INRs that further improves
convergence to the desired solution. We evaluate the effectiveness of our
approach on the task of surface reconstruction and shape space learning and
show SOTA performance compared to other unoriented methods. Code and model
parameters available at our project page https://chumbyte.github.io/DiGS-Site/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Shabat_Y/0/1/0/all/0/1"&gt;Yizhak Ben-Shabat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koneputugodage_C/0/1/0/all/0/1"&gt;Chamin Hewa Koneputugodage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schema matching using Gaussian mixture models with Wasserstein distance. (arXiv:2111.14244v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.14244</id>
        <link href="http://arxiv.org/abs/2111.14244"/>
        <updated>2022-04-02T00:47:20.117Z</updated>
        <summary type="html"><![CDATA[Gaussian mixture models find their place as a powerful tool, mostly in the
clustering problem, but with proper preparation also in feature extraction,
pattern recognition, image segmentation and in general machine learning. When
faced with the problem of schema matching, different mixture models computed on
different pieces of data can maintain crucial information about the structure
of the dataset. In order to measure or compare results from mixture models, the
Wasserstein distance can be very useful, however it is not easy to calculate
for mixture distributions. In this paper we derive one of possible
approximations for the Wasserstein distance between Gaussian mixture models and
reduce it to linear problem. Furthermore, application examples concerning real
world data are shown.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Przyborowski_M/0/1/0/all/0/1"&gt;Mateusz Przyborowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pabis_M/0/1/0/all/0/1"&gt;Mateusz Pabi&amp;#x15b;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janusz_A/0/1/0/all/0/1"&gt;Andrzej Janusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1"&gt;Dominik &amp;#x15a;l&amp;#x119;zak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools. (arXiv:2203.17275v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17275</id>
        <link href="http://arxiv.org/abs/2203.17275"/>
        <updated>2022-04-02T00:47:20.110Z</updated>
        <summary type="html"><![CDATA[We consider the problem of sequential robotic manipulation of deformable
objects using tools. Previous works have shown that differentiable physics
simulators provide gradients to the environment state and help trajectory
optimization to converge orders of magnitude faster than model-free
reinforcement learning algorithms for deformable object manipulation. However,
such gradient-based trajectory optimization typically requires access to the
full simulator states and can only solve short-horizon, single-skill tasks due
to local optima. In this work, we propose a novel framework, named DiffSkill,
that uses a differentiable physics simulator for skill abstraction to solve
long-horizon deformable object manipulation tasks from sensory observations. In
particular, we first obtain short-horizon skills using individual tools from a
gradient-based optimizer, using the full state information in a differentiable
simulator; we then learn a neural skill abstractor from the demonstration
trajectories which takes RGBD images as input. Finally, we plan over the skills
by finding the intermediate goals and then solve long-horizon tasks. We show
the advantages of our method in a new set of sequential deformable object
manipulation tasks compared to previous reinforcement learning algorithms and
compared to the trajectory optimizer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xingyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunzhu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1"&gt;David Held&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A statistical framework for efficient out of distribution detection in deep neural networks. (arXiv:2102.12967v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12967</id>
        <link href="http://arxiv.org/abs/2102.12967"/>
        <updated>2022-04-02T00:47:20.110Z</updated>
        <summary type="html"><![CDATA[Background. Commonly, Deep Neural Networks (DNNs) generalize well on samples
drawn from a distribution similar to that of the training set. However, DNNs'
predictions are brittle and unreliable when the test samples are drawn from a
dissimilar distribution. This is a major concern for deployment in real-world
applications, where such behavior may come at a considerable cost, such as
industrial production lines, autonomous vehicles, or healthcare applications.
Contributions. We frame Out Of Distribution (OOD) detection in DNNs as a
statistical hypothesis testing problem. Tests generated within our proposed
framework combine evidence from the entire network. Unlike previous OOD
detection heuristics, this framework returns a $p$-value for each test sample.
It is guaranteed to maintain the Type I Error (T1E - incorrectly predicting OOD
for an actual in-distribution sample) for test data. Moreover, this allows to
combine several detectors while maintaining the T1E. Building on this
framework, we suggest a novel OOD procedure based on low-order statistics. Our
method achieves comparable or better results than state-of-the-art methods on
well-accepted OOD benchmarks, without retraining the network parameters or
assuming prior knowledge on the test distribution -- and at a fraction of the
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haroush_M/0/1/0/all/0/1"&gt;Matan Haroush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frostig_T/0/1/0/all/0/1"&gt;Tzviel Frostig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heller_R/0/1/0/all/0/1"&gt;Ruth Heller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1"&gt;Daniel Soudry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighted Programming. (arXiv:2202.07577v2 [cs.PL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07577</id>
        <link href="http://arxiv.org/abs/2202.07577"/>
        <updated>2022-04-02T00:47:20.110Z</updated>
        <summary type="html"><![CDATA[We study weighted programming, a programming paradigm for specifying
mathematical models. More specifically, the weighted programs we investigate
are like usual imperative programs with two additional features: (1)
nondeterministic branching and (2) weighting execution traces. Weights can be
numbers but also other objects like words from an alphabet, polynomials, formal
power series, or cardinal numbers. We argue that weighted programming as a
paradigm can be used to specify mathematical models beyond probability
distributions (as is done in probabilistic programming).

We develop weakest-precondition- and weakest-liberal-precondition-style
calculi \`{a} la Dijkstra for reasoning about mathematical models specified by
weighted programs. We present several case studies. For instance, we use
weighted programming to model the ski rental problem - an optimization problem.
We model not only the optimization problem itself, but also the best
deterministic online algorithm for solving this problem as weighted programs.
By means of weakest-precondition-style reasoning, we can determine the
competitive ratio of the online algorithm on source code level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batz_K/0/1/0/all/0/1"&gt;Kevin Batz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallus_A/0/1/0/all/0/1"&gt;Adrian Gallus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaminski_B/0/1/0/all/0/1"&gt;Benjamin Lucien Kaminski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katoen_J/0/1/0/all/0/1"&gt;Joost-Pieter Katoen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winkler_T/0/1/0/all/0/1"&gt;Tobias Winkler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs. (arXiv:2203.16939v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16939</id>
        <link href="http://arxiv.org/abs/2203.16939"/>
        <updated>2022-04-02T00:47:20.109Z</updated>
        <summary type="html"><![CDATA[As a powerful tool for modeling complex relationships, hypergraphs are
gaining popularity from the graph learning community. However, commonly used
frameworks in deep hypergraph learning focus on hypergraphs with
\textit{edge-independent vertex weights}(EIVWs), without considering
hypergraphs with \textit{edge-dependent vertex weights} (EDVWs) that have more
modeling power. To compensate for this, in this paper, we present General
Hypergraph Spectral Convolution(GHSC), a general learning framework that not
only can handle EDVW and EIVW hypergraphs, but more importantly, enables
theoretically explicitly utilizing the existing powerful Graph Convolutional
Neural Networks (GCNNs) such that largely ease the design of Hypergraph Neural
Networks. In this framework, the graph Laplacian of the given undirected GCNNs
is replaced with a unified hypergraph Laplacian that incorporates vertex weight
information from a random walk perspective by equating our defined generalized
hypergraphs with simple undirected graphs. Extensive experiments from various
domains including social network analysis, visual objective classification,
protein learning demonstrate that the proposed framework can achieve
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fuyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xi Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tingyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1"&gt;Yatao Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Agnostic Defence against Backdoor Attacks in Machine Learning. (arXiv:1908.02203v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.02203</id>
        <link href="http://arxiv.org/abs/1908.02203"/>
        <updated>2022-04-02T00:47:20.106Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) has automated a multitude of our day-to-day decision
making domains such as education, employment and driving automation. The
continued success of ML largely depends on our ability to trust the model we
are using. Recently, a new class of attacks called Backdoor Attacks have been
developed. These attacks undermine the user's trust in ML models. In this work,
we present NEO, a model agnostic framework to detect and mitigate such backdoor
attacks in image classification ML models. For a given image classification
model, our approach analyses the inputs it receives and determines if the model
is backdoored. In addition to this feature, we also mitigate these attacks by
determining the correct predictions of the poisoned images. An appealing
feature of NEO is that it can, for the first time, isolate and reconstruct the
backdoor trigger. NEO is also the first defence methodology, to the best of our
knowledge that is completely blackbox.

We have implemented NEO and evaluated it against three state of the art
poisoned models. These models include highly critical applications such as
traffic sign detection (USTS) and facial detection. In our evaluation, we show
that NEO can detect $\approx$88% of the poisoned inputs on average and it is as
fast as 4.4 ms per input image. We also reconstruct the poisoned input for the
user to effectively test their systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1"&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shanshan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_G/0/1/0/all/0/1"&gt;Gerald Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loh_L/0/1/0/all/0/1"&gt;Lionell Loh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawshan_L/0/1/0/all/0/1"&gt;Louth Rawshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Sudipta Chattopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Does Pre-trained Wav2Vec2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. (arXiv:2203.16822v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16822</id>
        <link href="http://arxiv.org/abs/2203.16822"/>
        <updated>2022-04-02T00:47:20.105Z</updated>
        <summary type="html"><![CDATA[Recent work on self-supervised pre-training focus on leveraging large-scale
unlabeled speech data to build robust end-to-end (E2E) acoustic models (AM)
that can be later fine-tuned on downstream tasks e.g., automatic speech
recognition (ASR). Yet, few works investigated the impact on performance when
the data substantially differs between the pre-training and downstream
fine-tuning phases (i.e., domain shift). We target this scenario by analyzing
the robustness of Wav2Vec2.0 and XLS-R models on downstream ASR for a
completely unseen domain, i.e., air traffic control (ATC) communications. We
benchmark the proposed models on four challenging ATC test sets
(signal-to-noise ratio varies between 5 to 20 dB). Relative word error rate
(WER) reduction between 20% to 40% are obtained in comparison to hybrid-based
state-of-the-art ASR baselines by fine-tuning E2E acoustic models with a small
fraction of labeled data. We also study the impact of fine-tuning data size on
WERs, going from 5 minutes (few-shot) to 15 hours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1"&gt;Juan Zuluaga-Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1"&gt;Amrutha Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1"&gt;Iuliia Nigmatulina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1"&gt;Saeed Sarfjoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1"&gt;Petr Motlicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kleinert_M/0/1/0/all/0/1"&gt;Matthias Kleinert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1"&gt;Hartmut Helmke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1"&gt;Oliver Ohneiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhan_Q/0/1/0/all/0/1"&gt;Qingran Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart. (arXiv:2105.14785v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14785</id>
        <link href="http://arxiv.org/abs/2105.14785"/>
        <updated>2022-04-02T00:47:20.105Z</updated>
        <summary type="html"><![CDATA[Correctly classifying adversarial examples is an essential but challenging
requirement for safely deploying machine learning models. As reported in
RobustBench, even the state-of-the-art adversarially trained models struggle to
exceed 67% robust test accuracy on CIFAR-10, which is far from practical. A
complementary way towards robustness is to introduce a rejection option,
allowing the model to not return predictions on uncertain inputs, where
confidence is a commonly used certainty proxy. Along with this routine, we find
that confidence and a rectified confidence (R-Con) can form two coupled
rejection metrics, which could provably distinguish wrongly classified inputs
from correctly classified ones. This intriguing property sheds light on using
coupling strategies to better detect and reject adversarial examples. We
evaluate our rectified rejection (RR) module on CIFAR-10, CIFAR-10-C, and
CIFAR-100 under several attacks including adaptive ones, and demonstrate that
the RR module is compatible with different adversarial training frameworks on
improving robustness, with little extra computation. The code is available at
https://github.com/P2333/Rectified-Rejection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Single-Timescale Method for Stochastic Bilevel Optimization. (arXiv:2102.04671v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04671</id>
        <link href="http://arxiv.org/abs/2102.04671"/>
        <updated>2022-04-02T00:47:20.103Z</updated>
        <summary type="html"><![CDATA[Stochastic bilevel optimization generalizes the classic stochastic
optimization from the minimization of a single objective to the minimization of
an objective function that depends the solution of another optimization
problem. Recently, stochastic bilevel optimization is regaining popularity in
emerging machine learning applications such as hyper-parameter optimization and
model-agnostic meta learning. To solve this class of stochastic optimization
problems, existing methods require either double-loop or two-timescale updates,
which are sometimes less efficient. This paper develops a new optimization
method for a class of stochastic bilevel problems that we term Single-Timescale
stochAstic BiLevEl optimization (STABLE) method. STABLE runs in a single loop
fashion, and uses a single-timescale update with a fixed batch size. To achieve
an $\epsilon$-stationary point of the bilevel problem, STABLE requires ${\cal
O}(\epsilon^{-2})$ samples in total; and to achieve an $\epsilon$-optimal
solution in the strongly convex case, STABLE requires ${\cal O}(\epsilon^{-1})$
samples. To the best of our knowledge, this is the first bilevel optimization
algorithm achieving the same order of sample complexity as the stochastic
gradient descent method for the single-level stochastic optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_Q/0/1/0/all/0/1"&gt;Quan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepFry: Identifying Vocal Fry Using Deep Neural Networks. (arXiv:2203.17019v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.17019</id>
        <link href="http://arxiv.org/abs/2203.17019"/>
        <updated>2022-04-02T00:47:20.102Z</updated>
        <summary type="html"><![CDATA[Vocal fry or creaky voice refers to a voice quality characterized by
irregular glottal opening and low pitch. It occurs in diverse languages and is
prevalent in American English, where it is used not only to mark phrase
finality, but also sociolinguistic factors and affect. Due to its irregular
periodicity, creaky voice challenges automatic speech processing and
recognition systems, particularly for languages where creak is frequently used.

This paper proposes a deep learning model to detect creaky voice in fluent
speech. The model is composed of an encoder and a classifier trained together.
The encoder takes the raw waveform and learns a representation using a
convolutional neural network. The classifier is implemented as a multi-headed
fully-connected network trained to detect creaky voice, voicing, and pitch,
where the last two are used to refine creak prediction. The model is trained
and tested on speech of American English speakers, annotated for creak by
trained phoneticians.

We evaluated the performance of our system using two encoders: one is
tailored for the task, and the other is based on a state-of-the-art
unsupervised representation. Results suggest our best-performing system has
improved recall and F1 scores compared to previous methods on unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chernyak_B/0/1/0/all/0/1"&gt;Bronya R. Chernyak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simon_T/0/1/0/all/0/1"&gt;Talia Ben Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Segal_Y/0/1/0/all/0/1"&gt;Yael Segal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steffman_J/0/1/0/all/0/1"&gt;Jeremy Steffman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chodroff_E/0/1/0/all/0/1"&gt;Eleanor Chodroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cole_J/0/1/0/all/0/1"&gt;Jennifer S. Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keshet_J/0/1/0/all/0/1"&gt;Joseph Keshet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain. (arXiv:2203.17004v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.17004</id>
        <link href="http://arxiv.org/abs/2203.17004"/>
        <updated>2022-04-02T00:47:20.085Z</updated>
        <summary type="html"><![CDATA[Score-based generative models (SGMs) have recently shown impressive results
for difficult generative tasks such as the unconditional and conditional
generation of natural images and audio signals. In this work, we extend these
models to the complex short-time Fourier transform (STFT) domain, proposing a
novel training task for speech enhancement using a complex-valued deep neural
network. We derive this training task within the formalism of stochastic
differential equations, thereby enabling the use of predictor-corrector
samplers. We provide alternative formulations inspired by previous publications
on using SGMs for speech enhancement, avoiding the need for any prior
assumptions on the noise distribution and making the training task purely
generative which, as we show, results in improved enhancement performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Welker_S/0/1/0/all/0/1"&gt;Simon Welker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Richter_J/0/1/0/all/0/1"&gt;Julius Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1"&gt;Timo Gerkmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation and Simulation of Synthetic Datasets with Copulas. (arXiv:2203.17250v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17250</id>
        <link href="http://arxiv.org/abs/2203.17250"/>
        <updated>2022-04-02T00:47:20.085Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new method to generate synthetic data sets based on
copula models. Our goal is to produce surrogate data resembling real data in
terms of marginal and joint distributions. We present a complete and reliable
algorithm for generating a synthetic data set comprising numeric or categorical
variables. Applying our methodology to two datasets shows better performance
compared to other methods such as SMOTE and autoencoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Houssou_R/0/1/0/all/0/1"&gt;Regis Houssou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1"&gt;Mihai-Cezar Augustin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rappos_E/0/1/0/all/0/1"&gt;Efstratios Rappos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonvin_V/0/1/0/all/0/1"&gt;Vivien Bonvin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robert_Nicoud_S/0/1/0/all/0/1"&gt;Stephan Robert-Nicoud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Scene Representations for Embodied AI. (arXiv:2203.17251v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17251</id>
        <link href="http://arxiv.org/abs/2203.17251"/>
        <updated>2022-04-02T00:47:20.084Z</updated>
        <summary type="html"><![CDATA[We propose Continuous Scene Representations (CSR), a scene representation
constructed by an embodied agent navigating within a space, where objects and
their relationships are modeled by continuous valued embeddings. Our method
captures feature relationships between objects, composes them into a graph
structure on-the-fly, and situates an embodied agent within the representation.
Our key insight is to embed pair-wise relationships between objects in a latent
space. This allows for a richer representation compared to discrete relations
(e.g., [support], [next-to]) commonly used for building scene representations.
CSR can track objects as the agent moves in a scene, update the representation
accordingly, and detect changes in room configurations. Using CSR, we
outperform state-of-the-art approaches for the challenging downstream task of
visual room rearrangement, without any task specific training. Moreover, we
show the learned embeddings capture salient spatial details of the scene and
show applicability to real world data. A summery video and code is available at
https://prior.allenai.org/projects/csr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1"&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1"&gt;Kiana Ehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuran Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LEAD1.0: A Large-scale Annotated Dataset for Energy Anomaly Detection in Commercial Buildings. (arXiv:2203.17256v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17256</id>
        <link href="http://arxiv.org/abs/2203.17256"/>
        <updated>2022-04-02T00:47:20.082Z</updated>
        <summary type="html"><![CDATA[Modern buildings are densely equipped with smart energy meters, which
periodically generate a massive amount of time-series data yielding few million
data points every day. This data can be leveraged to discover the underlying
loads, infer their energy consumption patterns, inter-dependencies on
environmental factors, and the building's operational properties. Furthermore,
it allows us to simultaneously identify anomalies present in the electricity
consumption profiles, which is a big step towards saving energy and achieving
global sustainability. However, to date, the lack of large-scale annotated
energy consumption datasets hinders the ongoing research in anomaly detection.
We contribute to this effort by releasing a well-annotated version of a
publicly available ASHRAE Great Energy Predictor III data set containing 1,413
smart electricity meter time series spanning over one year. In addition, we
benchmark the performance of eight state-of-the-art anomaly detection methods
on our dataset and compare their performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gulati_M/0/1/0/all/0/1"&gt;Manoj Gulati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arjunan_P/0/1/0/all/0/1"&gt;Pandarasamy Arjunan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from many trajectories. (arXiv:2203.17193v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17193</id>
        <link href="http://arxiv.org/abs/2203.17193"/>
        <updated>2022-04-02T00:47:20.077Z</updated>
        <summary type="html"><![CDATA[We initiate a study of supervised learning from many independent sequences
("trajectories") of non-independent covariates, reflecting tasks in sequence
modeling, control, and reinforcement learning. Conceptually, our
multi-trajectory setup sits between two traditional settings in statistical
learning theory: learning from independent examples and learning from a single
auto-correlated sequence. Our conditions for efficient learning generalize the
former setting--trajectories must be non-degenerate in ways that extend
standard requirements for independent examples. They do not require that
trajectories be ergodic, long, nor strictly stable.

For linear least-squares regression, given $n$-dimensional examples produced
by $m$ trajectories, each of length $T$, we observe a notable change in
statistical efficiency as the number of trajectories increases from a few
(namely $m \lesssim n$) to many (namely $m \gtrsim n$). Specifically, we
establish that the worst-case error rate this problem is $\Theta(n / m T)$
whenever $m \gtrsim n$. Meanwhile, when $m \lesssim n$, we establish a (sharp)
lower bound of $\Omega(n^2 / m^2 T)$ on the worst-case error rate, realized by
a simple, marginally unstable linear dynamical system. A key upshot is that, in
domains where trajectories regularly reset, the error rate eventually behaves
as if all of the examples were independent altogether, drawn from their
marginals. As a corollary of our analysis, we also improve guarantees for the
linear system identification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1"&gt;Stephen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1"&gt;Roy Frostig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training strategy for a lightweight countermeasure model for automatic speaker verification. (arXiv:2203.17031v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.17031</id>
        <link href="http://arxiv.org/abs/2203.17031"/>
        <updated>2022-04-02T00:47:20.076Z</updated>
        <summary type="html"><![CDATA[The countermeasure (CM) model is developed to protect Automatic Speaker
Verification (ASV) systems from spoof attacks and prevent resulting personal
information leakage. Based on practicality and security considerations, the CM
model is usually deployed on edge devices, which have more limited computing
resources and storage space than cloud- based systems. This work proposes
training strategies for a lightweight CM model for ASV, using generalized end-
to-end (GE2E) pre-training and adversarial fine-tuning to improve performance,
and applying knowledge distillation (KD) to reduce the size of the CM model. In
the evalua- tion phase of the ASVspoof 2021 Logical Access task, the
lightweight ResNetSE model reaches min t-DCF 0.2695 and EER 3.54%. Compared to
the teacher model, the lightweight student model only uses 22.5% of parameters
and 21.1% of multiply and accumulate operands of the teacher model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yen-Lun Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanjun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chung-Che Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1"&gt;Jyh-Shing Roger Jang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CatIss: An Intelligent Tool for Categorizing Issues Reports using Transformers. (arXiv:2203.17196v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2203.17196</id>
        <link href="http://arxiv.org/abs/2203.17196"/>
        <updated>2022-04-02T00:47:20.076Z</updated>
        <summary type="html"><![CDATA[Users use Issue Tracking Systems to keep track and manage issue reports in
their repositories. An issue is a rich source of software information that
contains different reports including a problem, a request for new features, or
merely a question about the software product. As the number of these issues
increases, it becomes harder to manage them manually. Thus, automatic
approaches are proposed to help facilitate the management of issue reports.

This paper describes CatIss, an automatic CATegorizer of ISSue reports which
is built upon the Transformer-based pre-trained RoBERTa model. CatIss
classifies issue reports into three main categories of Bug reports,
Enhancement/feature requests, and Questions. First, the datasets provided for
the NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained
RoBERTa model is fine-tuned on the preprocessed dataset. Evaluating CatIss on
about 80 thousand issue reports from GitHub, indicates that it performs very
well surpassing the competition baseline, TicketTagger, and achieving 87.2%
F1-score (micro average). Additionally, as CatIss is trained on a wide set of
repositories, it is a generic prediction model, hence applicable for any unseen
software project or projects with little historical data. Scripts for cleaning
the datasets, training CatIss, and evaluating the model are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1"&gt;Maliheh Izadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DINE: Domain Adaptation from Single and Multiple Black-box Predictors. (arXiv:2104.01539v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01539</id>
        <link href="http://arxiv.org/abs/2104.01539"/>
        <updated>2022-04-02T00:47:20.076Z</updated>
        <summary type="html"><![CDATA[To ease the burden of labeling, unsupervised domain adaptation (UDA) aims to
transfer knowledge in previous and related labeled datasets (sources) to a new
unlabeled dataset (target). Despite impressive progress, prior methods always
need to access the raw source data and develop data-dependent alignment
approaches to recognize the target samples in a transductive learning manner,
which may raise privacy concerns from source individuals. Several recent
studies resort to an alternative solution by exploiting the well-trained
white-box model from the source domain, yet, it may still leak the raw data
through generative adversarial learning. This paper studies a practical and
interesting setting for UDA, where only black-box source models (i.e., only
network predictions are available) are provided during adaptation in the target
domain. To solve this problem, we propose a new two-step knowledge adaptation
framework called DIstill and fine-tuNE (DINE). Taking into consideration the
target data structure, DINE first distills the knowledge from the source
predictor to a customized target model, then fine-tunes the distilled model to
further fit the target domain. Besides, neural networks are not required to be
identical across domains in DINE, even allowing effective adaptation on a
low-resource device. Empirical results on three UDA scenarios (i.e.,
single-source, multi-source, and partial-set) confirm that DINE achieves highly
competitive performance compared to state-of-the-art data-dependent approaches.
Code is available at \url{https://github.com/tim-learn/DINE/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ran He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Q-learning for solving elliptic PDEs. (arXiv:2203.17128v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2203.17128</id>
        <link href="http://arxiv.org/abs/2203.17128"/>
        <updated>2022-04-02T00:47:20.075Z</updated>
        <summary type="html"><![CDATA[Solving high-dimensional partial differential equations (PDEs) is a major
challenge in scientific computing. We develop a new numerical method for
solving elliptic-type PDEs by adapting the Q-learning algorithm in
reinforcement learning. Our "Q-PDE" algorithm is mesh-free and therefore has
the potential to overcome the curse of dimensionality. Using a neural tangent
kernel (NTK) approach, we prove that the neural network approximator for the
PDE solution, trained with the Q-PDE algorithm, converges to the trajectory of
an infinite-dimensional ordinary differential equation (ODE) as the number of
hidden units $\rightarrow \infty$. For monotone PDE (i.e. those given by
monotone operators, which may be nonlinear), despite the lack of a spectral gap
in the NTK, we then prove that the limit neural network, which satisfies the
infinite-dimensional ODE, converges in $L^2$ to the PDE solution as the
training time $\rightarrow \infty$. More generally, we can prove that any fixed
point of the wide-network limit for the Q-PDE algorithm is a solution of the
PDE (not necessarily under the monotone condition). The numerical performance
of the Q-PDE algorithm is studied for several elliptic PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Samuel N. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Deqing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sirignano_J/0/1/0/all/0/1"&gt;Justin Sirignano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ternary and Binary Quantization for Improved Classification. (arXiv:2203.16798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16798</id>
        <link href="http://arxiv.org/abs/2203.16798"/>
        <updated>2022-04-02T00:47:20.074Z</updated>
        <summary type="html"><![CDATA[Dimension reduction and data quantization are two important methods for
reducing data complexity. In the paper, we study the methodology of first
reducing data dimension by random projection and then quantizing the
projections to ternary or binary codes, which has been widely applied in
classification. Usually, the quantization will seriously degrade the accuracy
of classification due to high quantization errors. Interestingly, however, we
observe that the quantization could provide comparable and often superior
accuracy, as the data to be quantized are sparse features generated with common
filters. Furthermore, this quantization property could be maintained in the
random projections of sparse features, if both the features and random
projection matrices are sufficiently sparse. By conducting extensive
experiments, we validate and analyze this intriguing property.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Weizhi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingrui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1"&gt;Kai Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weiyu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performative Power. (arXiv:2203.17232v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17232</id>
        <link href="http://arxiv.org/abs/2203.17232"/>
        <updated>2022-04-02T00:47:20.074Z</updated>
        <summary type="html"><![CDATA[We introduce the notion of performative power, which measures the ability of
a firm operating an algorithmic system, such as a digital content
recommendation platform, to steer a population. We relate performative power to
the economic theory of market power. Traditional economic concepts are well
known to struggle with identifying anti-competitive patterns in digital
platforms--a core challenge is the difficulty of defining the market, its
participants, products, and prices. Performative power sidesteps the problem of
market definition by focusing on a directly observable statistical measure
instead. High performative power enables a platform to profit from steering
participant behavior, whereas low performative power ensures that learning from
historical data is close to optimal.

Our first general result shows that under low performative power, a firm
cannot do better than standard supervised learning on observed data. We draw an
analogy with a firm being a price-taker, an economic condition that arises
under perfect competition in classical market models. We then contrast this
with a market where performative power is concentrated and show that the
equilibrium state can differ significantly. We go on to study performative
power in a concrete setting of strategic classification where participants can
switch between competing firms. We show that monopolies maximize performative
power and disutility for the participant, while competition and outside options
decrease performative power. We end on a discussion of connections to measures
of market power in economics and of the relationship with ongoing antitrust
debates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1"&gt;Moritz Hardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1"&gt;Meena Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1"&gt;Celestine Mendler-D&amp;#xfc;nner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Factored Adaptation for Non-Stationary Reinforcement Learning. (arXiv:2203.16582v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16582</id>
        <link href="http://arxiv.org/abs/2203.16582"/>
        <updated>2022-04-02T00:47:20.073Z</updated>
        <summary type="html"><![CDATA[Dealing with non-stationarity in environments (i.e., transition dynamics) and
objectives (i.e., reward functions) is a challenging problem that is crucial in
real-world applications of reinforcement learning (RL). Most existing
approaches only focus on families of stationary MDPs, in which the
non-stationarity is episodic, i.e., the change is only possible across
episodes. The few works that do consider non-stationarity without a specific
boundary, i.e., also allow for changes within an episode, model the changes
monolithically in a single shared embedding vector. In this paper, we propose
Factored Adaptation for Non-Stationary RL (FANS-RL), a factored adaption
approach that explicitly learns the individual latent change factors affecting
the transition dynamics and reward functions. FANS-RL learns jointly the
structure of a factored MDP and a factored representation of the time-varying
change factors, as well as the specific state components that they affect, via
a factored non-stationary variational autoencoder. Through this general
framework, we can consider general non-stationary scenarios with different
changing function types and changing frequency. Experimental results
demonstrate that FANS-RL outperforms existing approaches in terms of rewards,
compactness of the latent state representation and robustness to varying
degrees of non-stationarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Biwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magliacane_S/0/1/0/all/0/1"&gt;Sara Magliacane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17269</id>
        <link href="http://arxiv.org/abs/2203.17269"/>
        <updated>2022-04-02T00:47:20.070Z</updated>
        <summary type="html"><![CDATA[Continual learning describes a setting where machine learning models learn
novel concepts from continuously shifting training data, while simultaneously
avoiding degradation of knowledge on previously seen classes (a phenomenon
known as the catastrophic forgetting problem) which may disappear from the
training data for extended periods of time. Current approaches for continual
learning of a single expanding task (aka class-incremental continual learning)
require extensive rehearsal of previously seen data to avoid this degradation
of knowledge. Unfortunately, rehearsal comes at a sharp cost to memory and
computation, and it may also violate data-privacy. Instead, we explore
combining knowledge distillation and parameter regularization in new ways to
achieve strong continual learning performance without rehearsal. Specifically,
we take a deep dive into common continual learning techniques: prediction
distillation, feature distillation, L2 parameter regularization, and EWC
parameter regularization. We first disprove the common assumption that
parameter regularization techniques fail for rehearsal-free continual learning
of a single, expanding task. Next, we explore how to leverage knowledge from a
pre-trained model in rehearsal-free continual learning and find that vanilla L2
parameter regularization outperforms EWC parameter regularization and feature
distillation. We then highlight the impact of the rehearsal-free continual
learning settings with a classifier expansion benchmark, showing that a
strategy based on our findings combined with a positive/negative label
balancing heuristic can close the performance gap between the upper bound and
the existing strategies by up to roughly 50%. Finally, we show that a simple
method consisting of pre-training, L2 regularization, and prediction
distillation can even outperform rehearsal-based methods on the common
CIFAR-100 benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1"&gt;James Seale Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Junjiao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1"&gt;Zsolt Kira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobIn: A Robust Interpretable Deep Network for Schizophrenia Diagnosis. (arXiv:2203.17085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17085</id>
        <link href="http://arxiv.org/abs/2203.17085"/>
        <updated>2022-04-02T00:47:20.069Z</updated>
        <summary type="html"><![CDATA[Schizophrenia is a severe mental health condition that requires a long and
complicated diagnostic process. However, early diagnosis is vital to control
symptoms. Deep learning has recently become a popular way to analyse and
interpret medical data. Past attempts to use deep learning for schizophrenia
diagnosis from brain-imaging data have shown promise but suffer from a large
training-application gap - it is difficult to apply lab research to the real
world. We propose to reduce this training-application gap by focusing on
readily accessible data. We collect a data set of psychiatric observations of
patients based on DSM-5 criteria. Because similar data is already recorded in
all mental health clinics that diagnose schizophrenia using DSM-5, our method
could be easily integrated into current processes as a tool to assist
clinicians, whilst abiding by formal diagnostic criteria. To facilitate
real-world usage of our system, we show that it is interpretable and robust.
Understanding how a machine learning tool reaches its diagnosis is essential to
allow clinicians to trust that diagnosis. To interpret the framework, we fuse
two complementary attention mechanisms, 'squeeze and excitation' and
'self-attention', to determine global attribute importance and attribute
interactivity, respectively. The model uses these importance scores to make
decisions. This allows clinicians to understand how a diagnosis was reached,
improving trust in the model. Because machine learning models often struggle to
generalise to data from different sources, we perform experiments with
augmented test data to evaluate the model's applicability to the real world. We
find that our model is more robust to perturbations, and should therefore
perform better in a clinical setting. It achieves 98% accuracy with 10-fold
cross-validation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1"&gt;Daniel Organisciak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nwoye_E/0/1/0/all/0/1"&gt;Ephraim Nwoye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1"&gt;Wai Lok Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Learning of Graph Representations using Radar Point Cloud for Long-Range Gesture Recognition. (arXiv:2203.17066v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2203.17066</id>
        <link href="http://arxiv.org/abs/2203.17066"/>
        <updated>2022-04-02T00:47:20.067Z</updated>
        <summary type="html"><![CDATA[Gesture recognition is one of the most intuitive ways of interaction and has
gathered particular attention for human computer interaction. Radar sensors
possess multiple intrinsic properties, such as their ability to work in low
illumination, harsh weather conditions, and being low-cost and compact, making
them highly preferable for a gesture recognition solution. However, most
literature work focuses on solutions with a limited range that is lower than a
meter. We propose a novel architecture for a long-range (1m - 2m) gesture
recognition solution that leverages a point cloud-based cross-learning approach
from camera point cloud to 60-GHz FMCW radar point cloud, which allows learning
better representations while suppressing noise. We use a variant of Dynamic
Graph CNN (DGCNN) for the cross-learning, enabling us to model relationships
between the points at a local and global level and to model the temporal
dynamics a Bi-LSTM network is employed. In the experimental results section, we
demonstrate our model's overall accuracy of 98.4% for five gestures and its
generalization capability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hazra_S/0/1/0/all/0/1"&gt;Souvik Hazra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1"&gt;Hao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiprit_G/0/1/0/all/0/1"&gt;Gamze Naz Kiprit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stephan_M/0/1/0/all/0/1"&gt;Michael Stephan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Servadei_L/0/1/0/all/0/1"&gt;Lorenzo Servadei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wille_R/0/1/0/all/0/1"&gt;Robert Wille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weigel_R/0/1/0/all/0/1"&gt;Robert Weigel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Santra_A/0/1/0/all/0/1"&gt;Avik Santra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.16965</id>
        <link href="http://arxiv.org/abs/2203.16965"/>
        <updated>2022-04-02T00:47:20.064Z</updated>
        <summary type="html"><![CDATA[While self-supervised speech representation learning (SSL) models serve a
variety of downstream tasks, these models have been observed to overfit to the
domain from which the unlabelled data originates. To alleviate this issue, we
propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant
weights from models pre-trained on large amounts of out-of-domain (OOD) data.
Intuitively, this helps to make space for the target-domain ASR finetuning. The
redundant weights can be identified through various pruning strategies which
have been discussed in detail as a part of this work. Specifically, we
investigate the effect of the recently discovered Task-Agnostic and Task-Aware
pruning on PADA and propose a new pruning paradigm based on the latter, which
we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial
pruning mask from a well fine-tuned OOD model, which makes it starkly different
from the rest of the pruning strategies discussed in the paper. Our proposed
CD-TAW methodology achieves up to 20.6% relative WER improvement over our
baseline when fine-tuned on a 2-hour subset of Switchboard data without
language model (LM) decoding. Furthermore, we conduct a detailed analysis to
highlight the key design choices of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_L/0/1/0/all/0/1"&gt;Lodagala V S V Durga Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Sreyan Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1"&gt;S. Umesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual information estimation for graph convolutional neural networks. (arXiv:2203.16887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16887</id>
        <link href="http://arxiv.org/abs/2203.16887"/>
        <updated>2022-04-02T00:47:20.019Z</updated>
        <summary type="html"><![CDATA[Measuring model performance is a key issue for deep learning practitioners.
However, we often lack the ability to explain why a specific architecture
attains superior predictive accuracy for a given data set. Often, validation
accuracy is used as a performance heuristic quantifying how well a network
generalizes to unseen data, but it does not capture anything about the
information flow in the model. Mutual information can be used as a measure of
the quality of internal representations in deep learning models, and the
information plane may provide insights into whether the model exploits the
available information in the data. The information plane has previously been
explored for fully connected neural networks and convolutional architectures.
We present an architecture-agnostic method for tracking a network's internal
representations during training, which are then used to create the mutual
information plane. The method is exemplified for graph-based neural networks
fitted on citation data. We compare how the inductive bias introduced in
graph-based architectures changes the mutual information plane relative to a
fully connected neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Landverk_M/0/1/0/all/0/1"&gt;Marius C. Landverk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riemer_Sorensen_S/0/1/0/all/0/1"&gt;Signe Riemer-S&amp;#xf8;rensen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doubly-Robust Estimation for Unbiased Learning-to-Rank from Position-Biased Click Feedback. (arXiv:2203.17118v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17118</id>
        <link href="http://arxiv.org/abs/2203.17118"/>
        <updated>2022-04-02T00:47:20.019Z</updated>
        <summary type="html"><![CDATA[Clicks on rankings suffer from position bias: generally items on lower ranks
are less likely to be examined - and thus clicked - by users, in spite of their
actual preferences between items. The prevalent approach to unbiased
click-based Learning-to-Rank (LTR) is based on counterfactual
Inverse-Propensity-Scoring (IPS) estimation. Unique about LTR is the fact that
standard Doubly-Robust (DR) estimation - which combines IPS with regression
predictions - is inapplicable since the treatment variable - indicating whether
a user examined an item - cannot be observed in the data. In this paper, we
introduce a novel DR estimator that uses the expectation of treatment per rank
instead. Our novel DR estimator has more robust unbiasedness conditions than
the existing IPS approach, and in addition, provides enormous decreases in
variance: our experimental results indicate it requires several orders of
magnitude fewer datapoints to converge at optimal performance. For the unbiased
LTR field, our DR estimator contributes both increases in state-of-the-art
performance and the most robust theoretical guarantees of all known LTR
estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oosterhuis_H/0/1/0/all/0/1"&gt;Harrie Oosterhuis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16487</id>
        <link href="http://arxiv.org/abs/2203.16487"/>
        <updated>2022-04-02T00:47:20.019Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel
approach to accelerating autoregressive translation with no quality loss,
through the collaboration of autoregressive and non-autoregressive translation
(NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes
a number of tokens in parallel as a draft through NAT and then verifies them in
the autoregressive manner, where only the tokens that pass the verification are
kept as decoded tokens. GAD can achieve the same performance as autoregressive
translation but much more efficiently because both NAT drafting and
autoregressive verification are fast due to parallel computing. We conduct
experiments in the WMT14 English-German translation task and confirm that the
vanilla GAD yields exactly the same results as greedy decoding with an around
3x speedup, and that its variant (GAD++) with an advanced verification strategy
not only outperforms the greedy translation and even achieves the comparable
translation quality with the beam search result, but also further improves the
decoding speed, resulting in an around 5x speedup over autoregressive
translation. Our models and codes are available at
https://github.com/hemingkx/Generalized-Aggressive-Decoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1"&gt;Heming Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1"&gt;Tao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$. (arXiv:2203.17189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17189</id>
        <link href="http://arxiv.org/abs/2203.17189"/>
        <updated>2022-04-02T00:47:20.018Z</updated>
        <summary type="html"><![CDATA[Recent neural network-based language models have benefited greatly from
scaling up the size of training datasets and the number of parameters in the
models themselves. Scaling can be complicated due to various factors including
the need to distribute computation on supercomputer clusters (e.g., TPUs),
prevent bottlenecks when infeeding data, and ensure reproducible results. In
this work, we present two software libraries that ease these issues:
$\texttt{t5x}$ simplifies the process of building and training large language
models at scale while maintaining ease of use, and $\texttt{seqio}$ provides a
task-based API for simple creation of fast and reproducible training data and
evaluation pipelines. These open-source libraries have been used to train
models with hundreds of billions of parameters on datasets with multiple
terabytes of training data.

Along with the libraries, we release configurations and instructions for
T5-like encoder-decoder models as well as GPT-like decoder-only architectures.

$\texttt{t5x}$ and $\texttt{seqio}$ are open source and available at
https://github.com/google-research/t5x and https://github.com/google/seqio,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1"&gt;Adam Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyung Won Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levskaya_A/0/1/0/all/0/1"&gt;Anselm Levskaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1"&gt;Gaurav Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1"&gt;James Bradbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andor_D/0/1/0/all/0/1"&gt;Daniel Andor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1"&gt;Sharan Narang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1"&gt;Brian Lester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffney_C/0/1/0/all/0/1"&gt;Colin Gaffney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohiuddin_A/0/1/0/all/0/1"&gt;Afroz Mohiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1"&gt;Curtis Hawthorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1"&gt;Aitor Lewkowycz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salcianu_A/0/1/0/all/0/1"&gt;Alex Salcianu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zee_M/0/1/0/all/0/1"&gt;Marc van Zee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1"&gt;Jacob Austin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1"&gt;Sebastian Goodman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soares_L/0/1/0/all/0/1"&gt;Livio Baldini Soares&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haitang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvyashchenko_S/0/1/0/all/0/1"&gt;Sasha Tsvyashchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1"&gt;Aakanksha Chowdhery&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1"&gt;Jasmijn Bastings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulian_J/0/1/0/all/0/1"&gt;Jannis Bulian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1"&gt;Xavier Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1"&gt;Jianmo Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Andrew Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenealy_K/0/1/0/all/0/1"&gt;Kathleen Kenealy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1"&gt;Jonathan H. Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Stephan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1"&gt;Dan Garrette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1"&gt;James Lee-Thorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1"&gt;Colin Raffel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1"&gt;Noam Shazeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritter_M/0/1/0/all/0/1"&gt;Marvin Ritter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1"&gt;Maarten Bosma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1"&gt;Alexandre Passos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maitin_Shepard_J/0/1/0/all/0/1"&gt;Jeremy Maitin-Shepard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1"&gt;Noah Fiedel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1"&gt;Mark Omernick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saeta_B/0/1/0/all/0/1"&gt;Brennan Saeta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sepassi_R/0/1/0/all/0/1"&gt;Ryan Sepassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiridonov_A/0/1/0/all/0/1"&gt;Alexander Spiridonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newlan_J/0/1/0/all/0/1"&gt;Joshua Newlan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesmundo_A/0/1/0/all/0/1"&gt;Andrea Gesmundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data. (arXiv:2203.17113v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.17113</id>
        <link href="http://arxiv.org/abs/2203.17113"/>
        <updated>2022-04-02T00:47:20.004Z</updated>
        <summary type="html"><![CDATA[This paper studies a novel pre-training technique with unpaired speech data,
Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within
a multi-task learning framework, we introduce two pre-training tasks for the
encoder-decoder network using acoustic units, i.e., pseudo codes, derived from
an offline clustering model. One is to predict the pseudo codes via masked
language modeling in encoder output, like HuBERT model, while the other lets
the decoder learn to reconstruct pseudo codes autoregressively instead of
generating textual scripts. In this way, the decoder learns to reconstruct
original speech information with codes before learning to generate correct
text. Comprehensive experiments on the LibriSpeech corpus show that the
proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over
the method without decoder pre-training, and also outperforms significantly the
state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1"&gt;Junyi Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Long Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shujie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haizhou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1"&gt;Tom Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1"&gt;Lirong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yao Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors. (arXiv:2203.17138v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2203.17138</id>
        <link href="http://arxiv.org/abs/2203.17138"/>
        <updated>2022-04-02T00:47:20.004Z</updated>
        <summary type="html"><![CDATA[We investigate the use of prior knowledge of human and animal movement to
learn reusable locomotion skills for real legged robots. Our approach builds
upon previous work on imitating human or dog Motion Capture (MoCap) data to
learn a movement skill module. Once learned, this skill module can be reused
for complex downstream tasks. Importantly, due to the prior imposed by the
MoCap data, our approach does not require extensive reward engineering to
produce sensible and natural looking behavior at the time of reuse. This makes
it easy to create well-regularized, task-oriented controllers that are suitable
for deployment on real robots. We demonstrate how our skill module can be used
for imitation, and train controllable walking and ball dribbling policies for
both the ANYmal quadruped and OP3 humanoid. These policies are then deployed on
hardware via zero-shot simulation-to-reality transfer. Accompanying videos are
available at https://bit.ly/robot-npmp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohez_S/0/1/0/all/0/1"&gt;Steven Bohez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tunyasuvunakool_S/0/1/0/all/0/1"&gt;Saran Tunyasuvunakool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brakel_P/0/1/0/all/0/1"&gt;Philemon Brakel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghi_F/0/1/0/all/0/1"&gt;Fereshteh Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1"&gt;Leonard Hasenclever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tassa_Y/0/1/0/all/0/1"&gt;Yuval Tassa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1"&gt;Emilio Parisotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Humplik_J/0/1/0/all/0/1"&gt;Jan Humplik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haarnoja_T/0/1/0/all/0/1"&gt;Tuomas Haarnoja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hafner_R/0/1/0/all/0/1"&gt;Roland Hafner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1"&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neunert_M/0/1/0/all/0/1"&gt;Michael Neunert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_B/0/1/0/all/0/1"&gt;Ben Moran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siegel_N/0/1/0/all/0/1"&gt;Noah Siegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huber_A/0/1/0/all/0/1"&gt;Andrea Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romano_F/0/1/0/all/0/1"&gt;Francesco Romano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batchelor_N/0/1/0/all/0/1"&gt;Nathan Batchelor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casarini_F/0/1/0/all/0/1"&gt;Federico Casarini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merel_J/0/1/0/all/0/1"&gt;Josh Merel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1"&gt;Raia Hadsell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-Aided Meta-Learning for Bayesian Binary Neural Networks via Born Machines. (arXiv:2203.17089v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2203.17089</id>
        <link href="http://arxiv.org/abs/2203.17089"/>
        <updated>2022-04-02T00:47:20.003Z</updated>
        <summary type="html"><![CDATA[Near-term noisy intermediate-scale quantum circuits can efficiently implement
implicit probabilistic models in discrete spaces, supporting distributions that
are practically infeasible to sample from using classical means. One of the
possible applications of such models, also known as Born machines, is
probabilistic inference, which is at the core of Bayesian methods. This paper
studies the use of Born machines for the problem of training binary Bayesian
neural networks. In the proposed approach, a Born machine is used to model the
variational distribution of the binary weights of the neural network, and data
from multiple tasks is used to reduce training data requirements on new tasks.
The method combines gradient-based meta-learning and variational inference via
Born machines, and is shown in a prototypical regression problem to outperform
conventional joint learning strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Nikoloska_I/0/1/0/all/0/1"&gt;Ivana Nikoloska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traffic4cast at NeurIPS 2021 - Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes. (arXiv:2203.17070v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17070</id>
        <link href="http://arxiv.org/abs/2203.17070"/>
        <updated>2022-04-02T00:47:20.002Z</updated>
        <summary type="html"><![CDATA[The IARAI Traffic4cast competitions at NeurIPS 2019 and 2020 showed that
neural networks can successfully predict future traffic conditions 1 hour into
the future on simply aggregated GPS probe data in time and space bins. We thus
reinterpreted the challenge of forecasting traffic conditions as a movie
completion task. U-Nets proved to be the winning architecture, demonstrating an
ability to extract relevant features in this complex real-world geo-spatial
process. Building on the previous competitions, Traffic4cast 2021 now focuses
on the question of model robustness and generalizability across time and space.
Moving from one city to an entirely different city, or moving from pre-COVID
times to times after COVID hit the world thus introduces a clear domain shift.
We thus, for the first time, release data featuring such domain shifts. The
competition now covers ten cities over 2 years, providing data compiled from
over 10^12 GPS probe data. Winning solutions captured traffic dynamics
sufficiently well to even cope with these complex domain shifts. Surprisingly,
this seemed to require only the previous 1h traffic dynamic history and static
road graph as input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eichenberger_C/0/1/0/all/0/1"&gt;Christian Eichenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neun_M/0/1/0/all/0/1"&gt;Moritz Neun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_H/0/1/0/all/0/1"&gt;Henry Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herruzo_P/0/1/0/all/0/1"&gt;Pedro Herruzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spanring_M/0/1/0/all/0/1"&gt;Markus Spanring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yichao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Sungbin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konyakhin_V/0/1/0/all/0/1"&gt;Vsevolod Konyakhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukashina_N/0/1/0/all/0/1"&gt;Nina Lukashina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shpilman_A/0/1/0/all/0/1"&gt;Aleksei Shpilman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiedemann_N/0/1/0/all/0/1"&gt;Nina Wiedemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raubal_M/0/1/0/all/0/1"&gt;Martin Raubal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_H/0/1/0/all/0/1"&gt;Hai L. Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohajerpoor_R/0/1/0/all/0/1"&gt;Reza Mohajerpoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Chen Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1"&gt;Inhi Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermes_L/0/1/0/all/0/1"&gt;Luca Hermes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1"&gt;Andrew Melnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velioglu_R/0/1/0/all/0/1"&gt;Riza Velioglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vieth_M/0/1/0/all/0/1"&gt;Markus Vieth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_M/0/1/0/all/0/1"&gt;Malte Schilling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojesomo_A/0/1/0/all/0/1"&gt;Alabi Bojesomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzouqi_H/0/1/0/all/0/1"&gt;Hasan Al Marzouqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liatsis_P/0/1/0/all/0/1"&gt;Panos Liatsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santokhi_J/0/1/0/all/0/1"&gt;Jay Santokhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hillier_D/0/1/0/all/0/1"&gt;Dylan Hillier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yiming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarwar_J/0/1/0/all/0/1"&gt;Joned Sarwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_A/0/1/0/all/0/1"&gt;Anna Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hewage_E/0/1/0/all/0/1"&gt;Emil Hewage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonietz_D/0/1/0/all/0/1"&gt;David Jonietz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruca_A/0/1/0/all/0/1"&gt;Aleksandra Gruca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Michael Kopp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreil_D/0/1/0/all/0/1"&gt;David Kreil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1"&gt;Sepp Hochreiter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy. (arXiv:2203.17001v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.17001</id>
        <link href="http://arxiv.org/abs/2203.17001"/>
        <updated>2022-04-02T00:47:20.000Z</updated>
        <summary type="html"><![CDATA[Deep learning based singing voice synthesis (SVS) systems have been
demonstrated to flexibly generate singing with better qualities, compared to
conventional statistical parametric based methods. However, neural systems are
generally data-hungry and have difficulty to reach reasonable singing quality
with limited public available training data. In this work, we explore different
data augmentation methods to boost the training of SVS systems, including
several strategies customized to SVS based on pitch augmentation and mix-up
augmentation. To further stabilize the training, we introduce the
cycle-consistent training strategy. Extensive experiments on two public singing
databases demonstrate that our proposed augmentation methods and the
stabilizing training strategy can significantly improve the performance on both
objective and subjective evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shuai Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiatong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qian_T/0/1/0/all/0/1"&gt;Tao Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WavThruVec: Latent speech representation as intermediate features for neural speech synthesis. (arXiv:2203.16930v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.16930</id>
        <link href="http://arxiv.org/abs/2203.16930"/>
        <updated>2022-04-02T00:47:19.999Z</updated>
        <summary type="html"><![CDATA[Recent advances in neural text-to-speech research have been dominated by
two-stage pipelines utilizing low-level intermediate speech representation such
as mel-spectrograms. However, such predetermined features are fundamentally
limited, because they do not allow to exploit the full potential of a
data-driven approach through learning hidden representations. For this reason,
several end-to-end methods have been proposed. However, such models are harder
to train and require a large number of high-quality recordings with
transcriptions. Here, we propose WavThruVec - a two-stage architecture that
resolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as
intermediate speech representation. Since these hidden activations provide
high-level linguistic features, they are more robust to noise. That allows us
to utilize annotated speech datasets of a lower quality to train the
first-stage module. At the same time, the second-stage component can be trained
on large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are
time-aligned and speaker-independent. This results in an increased
generalization capability to out-of-vocabulary words, as well as to a better
generalization to unseen speakers. We show that the proposed model not only
matches the quality of state-of-the-art neural models, but also presents useful
properties enabling tasks like voice conversion or zero-shot synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siuzdak_H/0/1/0/all/0/1"&gt;Hubert Siuzdak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dura_P/0/1/0/all/0/1"&gt;Piotr Dura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1"&gt;Pol van Rijn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1"&gt;Nori Jacoby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo. (arXiv:2203.16682v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16682</id>
        <link href="http://arxiv.org/abs/2203.16682"/>
        <updated>2022-04-02T00:47:19.996Z</updated>
        <summary type="html"><![CDATA[We present a debiased dataset for the Person-centric Visual Grounding (PCVG)
task first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an
image and a caption, PCVG requires pairing up a person's name mentioned in a
caption with a bounding box that points to the person in the image. We find
that the original Who's Waldo dataset compiled for this task contains a large
number of biased samples that are solvable simply by heuristic methods; for
instance, in many cases the first name in the sentence corresponds to the
largest bounding box, or the sequence of names in the sentence corresponds to
an exact left-to-right order in the image. Naturally, models trained on these
biased data lead to over-estimation of performance on the benchmark. To enforce
models being correct for the correct reasons, we design automated tools to
filter and debias the original dataset by ruling out all examples of
insufficient context, such as those with no verb or with a long chain of
conjunct names in their captions. Our experiments show that our new sub-sampled
dataset contains less bias with much lowered heuristic performances and widened
gaps between heuristic and supervised methods. We also demonstrate the same
benchmark model trained on our debiased training set outperforms that trained
on the original biased (and larger) training set on our debiased test set. We
argue our debiased dataset offers the PCVG task a more practical baseline for
reliable benchmarking and future improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yiran Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1"&gt;Pratyay Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1"&gt;Tejas Gokhale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yezhou Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1"&gt;Chitta Baral&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices. (arXiv:2203.16788v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.16788</id>
        <link href="http://arxiv.org/abs/2203.16788"/>
        <updated>2022-04-02T00:47:19.995Z</updated>
        <summary type="html"><![CDATA[Environmental, Social, and Governance (ESG) are non-financial factors that
are garnering attention from investors as they increasingly look to apply these
as part of their analysis to identify material risks and growth opportunities.
Some of this attention is also driven by clients who, now more aware than ever,
are demanding for their money to be managed and invested responsibly. As the
interest in ESG grows, so does the need for investors to have access to
consumable ESG information. Since most of it is in text form in reports,
disclosures, press releases, and 10-Q filings, we see a need for sophisticated
NLP techniques for classification tasks for ESG text. We hypothesize that an
ESG domain-specific pre-trained model will help with such and study building of
the same in this paper. We explored doing this by fine-tuning BERTs pre-trained
weights using ESG specific text and then further fine-tuning the model for a
classification task. We were able to achieve accuracy better than the original
BERT and baseline models in environment-specific classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_S/0/1/0/all/0/1"&gt;Srishti Mehra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louka_R/0/1/0/all/0/1"&gt;Robert Louka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yixun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flat-topped Probability Density Functions for Mixture Models. (arXiv:2203.17027v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17027</id>
        <link href="http://arxiv.org/abs/2203.17027"/>
        <updated>2022-04-02T00:47:19.988Z</updated>
        <summary type="html"><![CDATA[This paper investigates probability density functions (PDFs) that are
continuous everywhere, nearly uniform around the mode of distribution, and
adaptable to a variety of distribution shapes ranging from bell-shaped to
rectangular. From the viewpoint of computational tractability, the PDF based on
the Fermi-Dirac or logistic function is advantageous in estimating its shape
parameters. The most appropriate PDF for $n$-variate distribution is of the
form:
$p\left(\mathbf{x}\right)\propto\left[\cosh\left(\left[\left(\mathbf{x}-\mathbf{m}\right)^{\mathsf{T}}\boldsymbol{\Sigma}^{-1}\left(\mathbf{x}-\mathbf{m}\right)\right]^{n/2}\right)+\cosh\left(r^{n}\right)\right]^{-1}$
where $\mathbf{x},\mathbf{m}\in\mathbb{R}^{n}$, $\boldsymbol{\Sigma}$ is an
$n\times n$ positive definite matrix, and $r>0$ is a shape parameter. The
flat-topped PDFs can be used as a component of mixture models in machine
learning to improve goodness of fit and make a model as simple as possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_O/0/1/0/all/0/1"&gt;Osamu Fujita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16952</id>
        <link href="http://arxiv.org/abs/2203.16952"/>
        <updated>2022-04-02T00:47:19.987Z</updated>
        <summary type="html"><![CDATA[Vision transformer (ViT) has been trending in image classification tasks due
to its promising performance when compared to convolutional neural networks
(CNNs). As a result, many researchers have tried to incorporate ViT models in
hyperspectral image (HSI) classification tasks, but without achieving
satisfactory performance. To this paper, we introduce a new multimodal fusion
transformer (MFT) network for HSI land-cover classification, which utilizes
other sources of multimodal data in addition to HSI. Instead of using
conventional feature fusion techniques, other multimodal data are used as an
external classification (CLS) token in the transformer encoder, which helps
achieving better generalization. ViT and other similar transformer models use a
randomly initialized external classification token {and fail to generalize
well}. However, the use of a feature embedding derived from other sources of
multimodal data, such as light detection and ranging (LiDAR), offers the
potential to improve those models by means of a CLS. The concept of
tokenization is used in our work to generate CLS and HSI patch tokens, helping
to learn key features in a reduced feature space. We also introduce a new
attention mechanism for improving the exchange of information between HSI
tokens and the CLS (e.g., LiDAR) token. Extensive experiments are carried out
on widely used and benchmark datasets i.e., the University of Houston, Trento,
University of Southern Mississippi Gulfpark (MUUFL), and Augsburg. In the
results section, we compare the proposed MFT model with other state-of-the-art
transformer models, classical CNN models, as well as conventional classifiers.
The superior performance achieved by the proposed model is due to the use of
multimodal information as external classification tokens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Swalpa Kumar Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deria_A/0/1/0/all/0/1"&gt;Ankur Deria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Danfeng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1"&gt;Behnood Rasti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1"&gt;Antonio Plaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HiFi-VC: High Quality ASR-Based Voice Conversion. (arXiv:2203.16937v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.16937</id>
        <link href="http://arxiv.org/abs/2203.16937"/>
        <updated>2022-04-02T00:47:19.985Z</updated>
        <summary type="html"><![CDATA[The goal of voice conversion (VC) is to convert input voice to match the
target speaker's voice while keeping text and prosody intact. VC is usually
used in entertainment and speaking-aid systems, as well as applied for speech
data generation and augmentation. The development of any-to-any VC systems,
which are capable of generating voices unseen during model training, is of
particular interest to both researchers and the industry. Despite recent
progress, any-to-any conversion quality is still inferior to natural speech.

In this work, we propose a new any-to-any voice conversion pipeline. Our
approach uses automated speech recognition (ASR) features, pitch tracking, and
a state-of-the-art waveform prediction model. According to multiple subjective
and objective evaluations, our method outperforms modern baselines in terms of
voice quality, similarity and consistency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kashkin_A/0/1/0/all/0/1"&gt;A. Kashkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpukhin_I/0/1/0/all/0/1"&gt;I. Karpukhin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shishkin_S/0/1/0/all/0/1"&gt;S. Shishkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher. (arXiv:2203.17008v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17008</id>
        <link href="http://arxiv.org/abs/2203.17008"/>
        <updated>2022-04-02T00:47:19.984Z</updated>
        <summary type="html"><![CDATA[Model quantization is considered as a promising method to greatly reduce the
resource requirements of deep neural networks. To deal with the performance
drop induced by quantization errors, a popular method is to use training data
to fine-tune quantized networks. In real-world environments, however, such a
method is frequently infeasible because training data is unavailable due to
security, privacy, or confidentiality concerns. Zero-shot quantization
addresses such problems, usually by taking information from the weights of a
full-precision teacher network to compensate the performance drop of the
quantized networks. In this paper, we first analyze the loss surface of
state-of-the-art zero-shot quantization techniques and provide several
findings. In contrast to usual knowledge distillation problems, zero-shot
quantization often suffers from 1) the difficulty of optimizing multiple loss
terms together, and 2) the poor generalization capability due to the use of
synthetic samples. Furthermore, we observe that many weights fail to cross the
rounding threshold during training the quantized networks even when it is
necessary to do so for better performance. Based on the observations, we
propose AIT, a simple yet powerful technique for zero-shot quantization, which
addresses the aforementioned two problems in the following way: AIT i) uses a
KL distance loss only without a cross-entropy loss, and ii) manipulates
gradients to guarantee that a certain portion of weights are properly updated
after crossing the rounding thresholds. Experiments show that AIT outperforms
the performance of many existing methods by a great margin, taking over the
overall state-of-the-art position in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1"&gt;Kanghyun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hye Yoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Deokki Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Joonsang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"&gt;Noseong Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngsok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasi-orthogonality and intrinsic dimensions as measures of learning and generalisation. (arXiv:2203.16687v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16687</id>
        <link href="http://arxiv.org/abs/2203.16687"/>
        <updated>2022-04-02T00:47:19.983Z</updated>
        <summary type="html"><![CDATA[Finding best architectures of learning machines, such as deep neural
networks, is a well-known technical and theoretical challenge. Recent work by
Mellor et al (2021) showed that there may exist correlations between the
accuracies of trained networks and the values of some easily computable
measures defined on randomly initialised networks which may enable to search
tens of thousands of neural architectures without training. Mellor et al used
the Hamming distance evaluated over all ReLU neurons as such a measure.
Motivated by these findings, in our work, we ask the question of the existence
of other and perhaps more principled measures which could be used as
determinants of success of a given neural architecture. In particular, we
examine, if the dimensionality and quasi-orthogonality of neural networks'
feature space could be correlated with the network's performance after
training. We showed, using the setup as in Mellor et al, that dimensionality
and quasi-orthogonality may jointly serve as network's performance
discriminants. In addition to offering new opportunities to accelerate neural
architecture search, our findings suggest important relationships between the
networks' final performance and properties of their randomly initialised
feature spaces: data dimension and quasi-orthogonality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qinghua Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1"&gt;Alexander N. Gorban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1"&gt;Evgeny M. Mirkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bac_J/0/1/0/all/0/1"&gt;Jonathan Bac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zinovyev_A/0/1/0/all/0/1"&gt;Andrei Zinovyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1"&gt;Ivan Y. Tyukin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The ideal data compression and automatic discovery of hidden law using neural network. (arXiv:2203.16941v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16941</id>
        <link href="http://arxiv.org/abs/2203.16941"/>
        <updated>2022-04-02T00:47:19.982Z</updated>
        <summary type="html"><![CDATA[Recently machine learning using neural networks has been developed, and many
new methods have been suggested. On the other hand, a system that has true
versatility has not been developed, and there remain many fields in which the
human brain has advantages over machine learning. We considered how the human
brain recognizes events and memorizes them and succeeded to reproduce the
system of the human brain on a machine learning model with a new autoencoder
neural network (NN). The previous autoencoders have the problem that they
cannot define well what is the features of the input data, and we need to
restrict the middle layer of the autoencoder artificially. We solve this
problem by defining a new loss function that reflects the information entropy,
and it enables the NN to compress the input data ideally and automatically
discover the hidden law behind the input data set. The loss function used in
our NN is based on the free-energy principle which is known as the unified
brain theory, and our study is the first concrete formularization of this
principle. The result of this study can be applied to any kind of data analysis
and also to cognitive science.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Katayose_T/0/1/0/all/0/1"&gt;Taisuke Katayose&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16773</id>
        <link href="http://arxiv.org/abs/2203.16773"/>
        <updated>2022-04-02T00:47:19.981Z</updated>
        <summary type="html"><![CDATA[Speech representations learned from Self-supervised learning (SSL) models
have been found beneficial for various speech processing tasks. However,
utilizing SSL representations usually requires fine-tuning the pre-trained
models or designing task-specific downstream models and loss functions, causing
much memory usage and human labor. On the other hand, prompting in Natural
Language Processing (NLP) is an efficient and widely used technique to leverage
pre-trained language models (LMs). Nevertheless, such a paradigm is little
studied in the speech community. We report in this paper the first exploration
of the prompt tuning paradigm for speech processing tasks based on Generative
Spoken Language Model (GSLM). Experiment results show that the prompt tuning
technique achieves competitive performance in speech classification tasks with
fewer trainable parameters than fine-tuning specialized downstream models. We
further study the technique in challenging sequence generation tasks. Prompt
tuning also demonstrates its potential, while the limitation and possible
research directions are discussed in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tseng_W/0/1/0/all/0/1"&gt;Wei-Cheng Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certified machine learning: A posteriori error estimation for physics-informed neural networks. (arXiv:2203.17055v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17055</id>
        <link href="http://arxiv.org/abs/2203.17055"/>
        <updated>2022-04-02T00:47:19.981Z</updated>
        <summary type="html"><![CDATA[Physics-informed neural networks (PINNs) are one popular approach to
introduce a priori knowledge about physical systems into the learning
framework. PINNs are known to be robust for smaller training sets, derive
better generalization problems, and are faster to train. In this paper, we show
that using PINNs in comparison with purely data-driven neural networks is not
only favorable for training performance but allows us to extract significant
information on the quality of the approximated solution. Assuming that the
underlying differential equation for the PINN training is an ordinary
differential equation, we derive a rigorous upper limit on the PINN prediction
error. This bound is applicable even for input data not included in the
training phase and without any prior knowledge about the true solution.
Therefore, our a posteriori error estimation is an essential step to certify
the PINN. We apply our error estimator exemplarily to two academic toy
problems, whereof one falls in the category of model-predictive control and
thereby shows the practical use of the derived results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hillebrecht_B/0/1/0/all/0/1"&gt;Birgit Hillebrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unger_B/0/1/0/all/0/1"&gt;Benjamin Unger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Explainable Metrics for Augmented SGD. (arXiv:2203.16723v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16723</id>
        <link href="http://arxiv.org/abs/2203.16723"/>
        <updated>2022-04-02T00:47:19.978Z</updated>
        <summary type="html"><![CDATA[Explaining the generalization characteristics of deep learning is an emerging
topic in advanced machine learning. There are several unanswered questions
about how learning under stochastic optimization really works and why certain
strategies are better than others. In this paper, we address the following
question: \textit{can we probe intermediate layers of a deep neural network to
identify and quantify the learning quality of each layer?} With this question
in mind, we propose new explainability metrics that measure the redundant
information in a network's layers using a low-rank factorization framework and
quantify a complexity measure that is highly correlated with the generalization
performance of a given optimizer, network, and dataset. We subsequently exploit
these metrics to augment the Stochastic Gradient Descent (SGD) optimizer by
adaptively adjusting the learning rate in each layer to improve in
generalization performance. Our augmented SGD -- dubbed RMSGD -- introduces
minimal computational overhead compared to SOTA methods and outperforms them by
exhibiting strong generalization characteristics across application,
architecture, and dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1"&gt;Mahdi S. Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1"&gt;Mathieu Tuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1"&gt;Konstantinos N. Plataniotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the risk of re-identification arising from an attack on anonymised data. (arXiv:2203.16921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16921</id>
        <link href="http://arxiv.org/abs/2203.16921"/>
        <updated>2022-04-02T00:47:19.951Z</updated>
        <summary type="html"><![CDATA[Objective: The use of routinely-acquired medical data for research purposes
requires the protection of patient confidentiality via data anonymisation. The
objective of this work is to calculate the risk of re-identification arising
from a malicious attack to an anonymised dataset, as described below. Methods:
We first present an analytical means of estimating the probability of
re-identification of a single patient in a k-anonymised dataset of Electronic
Health Record (EHR) data. Second, we generalize this solution to obtain the
probability of multiple patients being re-identified. We provide synthetic
validation via Monte Carlo simulations to illustrate the accuracy of the
estimates obtained. Results: The proposed analytical framework for risk
estimation provides re-identification probabilities that are in agreement with
those provided by simulation in a number of scenarios. Our work is limited by
conservative assumptions which inflate the re-identification probability.
Discussion: Our estimates show that the re-identification probability increases
with the proportion of the dataset maliciously obtained and that it has an
inverse relationship with the equivalence class size. Our recursive approach
extends the applicability domain to the general case of a multi-patient
re-identification attack in an arbitrary k-anonymisation scheme. Conclusion: We
prescribe a systematic way to parametrize the k-anonymisation process based on
a pre-determined re-identification probability. We observed that the benefits
of a reduced re-identification risk that come with increasing k-size may not be
worth the reduction in data granularity when one is considering benchmarking
the re-identification probability on the size of the portion of the dataset
maliciously obtained by the adversary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antoniou_A/0/1/0/all/0/1"&gt;Anna Antoniou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dossena_G/0/1/0/all/0/1"&gt;Giacomo Dossena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacMillan_J/0/1/0/all/0/1"&gt;Julia MacMillan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamblin_S/0/1/0/all/0/1"&gt;Steven Hamblin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrone_P/0/1/0/all/0/1"&gt;Paula Petrone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Diffusion for Molecule Generation in 3D. (arXiv:2203.17003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17003</id>
        <link href="http://arxiv.org/abs/2203.17003"/>
        <updated>2022-04-02T00:47:19.950Z</updated>
        <summary type="html"><![CDATA[This work introduces a diffusion model for molecule generation in 3D that is
equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model
(EDM) learns to denoise a diffusion process with an equivariant network that
jointly operates on both continuous (atom coordinates) and categorical features
(atom types). In addition, we provide a probabilistic analysis which admits
likelihood computation of molecules using our model. Experimentally, the
proposed method significantly outperforms previous 3D molecular generative
methods regarding the quality of generated samples and efficiency at training
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1"&gt;Emiel Hoogeboom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satorras_V/0/1/0/all/0/1"&gt;Victor Garcia Satorras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vignac_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Vignac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task Adaptive Parameter Sharing for Multi-Task Learning. (arXiv:2203.16708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16708</id>
        <link href="http://arxiv.org/abs/2203.16708"/>
        <updated>2022-04-02T00:47:19.937Z</updated>
        <summary type="html"><![CDATA[Adapting pre-trained models with broad capabilities has become standard
practice for learning a wide range of downstream tasks. The typical approach of
fine-tuning different models for each task is performant, but incurs a
substantial memory cost. To efficiently learn multiple downstream tasks we
introduce Task Adaptive Parameter Sharing (TAPS), a general method for tuning a
base model to a new task by adaptively modifying a small, task-specific subset
of layers. This enables multi-task learning while minimizing resources used and
competition between tasks. TAPS solves a joint optimization problem which
determines which layers to share with the base model and the value of the
task-specific weights. Further, a sparsity penalty on the number of active
layers encourages weight sharing with the base model. Compared to other
methods, TAPS retains high accuracy on downstream tasks while introducing few
task-specific parameters. Moreover, TAPS is agnostic to the model architecture
and requires only minor changes to the training scheme. We evaluate our method
on a suite of fine-tuning tasks and architectures (ResNet, DenseNet, ViT) and
show that it achieves state-of-the-art performance while being simple to
implement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1"&gt;Matthew Wallingford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1"&gt;Charless Fowlkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1"&gt;Rahul Bhotika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Estimation of Random Vectors with Bandit Feedback. (arXiv:2203.16810v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16810</id>
        <link href="http://arxiv.org/abs/2203.16810"/>
        <updated>2022-04-02T00:47:19.937Z</updated>
        <summary type="html"><![CDATA[We consider the problem of sequentially learning to estimate, in the mean
squared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by
observing only $m < K$ of its entries in each round. This reduces to learning
an optimal subset for estimating the entire vector. Towards this, we first
establish an exponential concentration bound for an estimate of the MSE for
each observable subset. We then frame the estimation problem with bandit
feedback in the best-subset identification setting. We propose a variant of the
successive elimination algorithm to cater to the adaptive estimation problem,
and we derive an upper bound on the sample complexity of this algorithm. In
addition, to understand the fundamental limit on the sample complexity of this
adaptive estimation bandit problem, we derive a minimax lower bound.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1"&gt;Dipayan Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+A%2E_P/0/1/0/all/0/1"&gt;Prashanth L.A.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1"&gt;Aditya Gopalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating High Fidelity Data from Low-density Regions using Diffusion Models. (arXiv:2203.17260v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17260</id>
        <link href="http://arxiv.org/abs/2203.17260"/>
        <updated>2022-04-02T00:47:19.934Z</updated>
        <summary type="html"><![CDATA[Our work focuses on addressing sample deficiency from low-density regions of
data manifold in common image datasets. We leverage diffusion process based
generative models to synthesize novel images from low-density regions. We
observe that uniform sampling from diffusion models predominantly samples from
high-density regions of the data manifold. Therefore, we modify the sampling
process to guide it towards low-density regions while simultaneously
maintaining the fidelity of synthetic data. We rigorously demonstrate that our
process successfully generates novel high fidelity samples from low-density
regions. We further examine generated samples and show that the model does not
memorize low-density data and indeed learns to generate novel samples from
low-density regions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1"&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazirbas_C/0/1/0/all/0/1"&gt;Caner Hazirbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gordo_A/0/1/0/all/0/1"&gt;Albert Gordo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozgenel_F/0/1/0/all/0/1"&gt;Firat Ozgenel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1"&gt;Cristian Canton Ferrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16776</id>
        <link href="http://arxiv.org/abs/2203.16776"/>
        <updated>2022-04-02T00:47:19.927Z</updated>
        <summary type="html"><![CDATA[Utilizing text-only data with an external language model (LM) in end-to-end
RNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class
of methods such as density ratio (DR) and ILM estimation (ILME) have been
developed, outperforming the classic shallow fusion (SF) method. The basic idea
behind these methods is that RNN-T posterior should first subtract the
implicitly learned ILM prior, in order to integrate the external LM. While
recent studies suggest that RNN-T only learns some low-order language model
information, the DR method uses a well-trained ILM. We hypothesize that this
setting is appropriate and may deteriorate the performance of the DR method,
and propose a low-order density ratio method (LODR) by training a low-order
weak ILM for DR. Extensive empirical experiments are conducted on both
in-domain and cross-domain scenarios on English LibriSpeech & Tedlium-2 and
Chinese WenetSpeech & AISHELL-1 datasets. It is shown that LODR consistently
outperforms SF in all tasks, while performing generally close to ILME and
better than DR in most tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huahuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+An_K/0/1/0/all/0/1"&gt;Keyu An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1"&gt;Zhijian Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1"&gt;Ke Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wan_G/0/1/0/all/0/1"&gt;Guanglu Wan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Driving-Oriented Metric for Lane Detection Models. (arXiv:2203.16851v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16851</id>
        <link href="http://arxiv.org/abs/2203.16851"/>
        <updated>2022-04-02T00:47:19.926Z</updated>
        <summary type="html"><![CDATA[After the 2017 TuSimple Lane Detection Challenge, its dataset and evaluation
based on accuracy and F1 score have become the de facto standard to measure the
performance of lane detection methods. While they have played a major role in
improving the performance of lane detection methods, the validity of this
evaluation method in downstream tasks has not been adequately researched. In
this study, we design 2 new driving-oriented metrics for lane detection:
End-to-End Lateral Deviation metric (E2E-LD) is directly formulated based on
the requirements of autonomous driving, a core downstream task of lane
detection; Per-frame Simulated Lateral Deviation metric (PSLD) is a lightweight
surrogate metric of E2E-LD. To evaluate the validity of the metrics, we conduct
a large-scale empirical study with 4 major types of lane detection approaches
on the TuSimple dataset and our newly constructed dataset Comma2k19-LD. Our
results show that the conventional metrics have strongly negative correlations
($\leq$-0.55) with E2E-LD, meaning that some recent improvements purely
targeting the conventional metrics may not have led to meaningful improvements
in autonomous driving, but rather may actually have made it worse by
overfitting to the conventional metrics. As autonomous driving is a
security/safety-critical system, the underestimation of robustness hinders the
sound development of practical lane detection models. We hope that our study
will help the community achieve more downstream task-aware evaluations for lane
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1"&gt;Takami Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qi Alfred Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[System Identification via Nuclear Norm Regularization. (arXiv:2203.16673v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.16673</id>
        <link href="http://arxiv.org/abs/2203.16673"/>
        <updated>2022-04-02T00:47:19.925Z</updated>
        <summary type="html"><![CDATA[This paper studies the problem of identifying low-order linear systems via
Hankel nuclear norm regularization. Hankel regularization encourages the
low-rankness of the Hankel matrix, which maps to the low-orderness of the
system. We provide novel statistical analysis for this regularization and
carefully contrast it with the unregularized ordinary least-squares (OLS)
estimator. Our analysis leads to new bounds on estimating the impulse response
and the Hankel matrix associated with the linear system. We first design an
input excitation and show that Hankel regularization enables one to recover the
system using optimal number of observations in the true system order and
achieve strong statistical estimation rates. Surprisingly, we demonstrate that
the input design indeed matters, by showing that intuitive choices such as
i.i.d. Gaussian input leads to provably sub-optimal sample complexity. To
better understand the benefits of regularization, we also revisit the OLS
estimator. Besides refining existing bounds, we experimentally identify when
regularized approach improves over OLS: (1) For low-order systems with slow
impulse-response decay, OLS method performs poorly in terms of sample
complexity, (2) Hankel matrix returned by regularization has a more clear
singular value gap that ease identification of the system order, (3) Hankel
regularization is less sensitive to hyperparameter choice. Finally, we
establish model selection guarantees through a joint train-validation procedure
where we tune the regularization parameter for near-optimal estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yue Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oymak_S/0/1/0/all/0/1"&gt;Samet Oymak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fazel_M/0/1/0/all/0/1"&gt;Maryam Fazel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting extreme events from data using deep machine learning: when and where. (arXiv:2203.17155v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17155</id>
        <link href="http://arxiv.org/abs/2203.17155"/>
        <updated>2022-04-02T00:47:19.924Z</updated>
        <summary type="html"><![CDATA[We develop a deep convolutional neural network (DCNN) based framework for
model-free prediction of the occurrence of extreme events both in time ("when")
and in space ("where") in nonlinear physical systems of spatial dimension two.
The measurements or data are a set of two-dimensional snapshots or images. For
a desired time horizon of prediction, a proper labeling scheme can be
designated to enable successful training of the DCNN and subsequent prediction
of extreme events in time. Given that an extreme event has been predicted to
occur within the time horizon, a space-based labeling scheme can be applied to
predict, within certain resolution, the location at which the event will occur.
We use synthetic data from the 2D complex Ginzburg-Landau equation and
empirical wind speed data of the North Atlantic ocean to demonstrate and
validate our machine-learning based prediction framework. The trade-offs among
the prediction horizon, spatial resolution, and accuracy are illustrated, and
the detrimental effect of spatially biased occurrence of extreme event on
prediction accuracy is discussed. The deep learning framework is viable for
predicting extreme events in the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junjie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi-Gang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grebogi_C/0/1/0/all/0/1"&gt;Celso Grebogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Ying-Cheng Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Autoregressors are Interpretable Classifiers. (arXiv:2203.17002v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17002</id>
        <link href="http://arxiv.org/abs/2203.17002"/>
        <updated>2022-04-02T00:47:19.923Z</updated>
        <summary type="html"><![CDATA[We explore the use of class-conditional autoregressive (CA) models to perform
image classification on MNIST-10. Autoregressive models assign probability to
an entire input by combining probabilities from each individual feature; hence
classification decisions made by a CA can be readily decomposed into
contributions from each each input feature. That is to say, CA are inherently
locally interpretable. Our experiments show that naively training a CA achieves
much worse accuracy compared to a standard classifier, however this is due to
over-fitting and not a lack of expressive power. Using knowledge distillation
from a standard classifier, a student CA can be trained to match the
performance of the teacher while still being interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elazar_N/0/1/0/all/0/1"&gt;Nathan Elazar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Scientific Articles with Machine Learning. (arXiv:2203.16569v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16569</id>
        <link href="http://arxiv.org/abs/2203.16569"/>
        <updated>2022-04-02T00:47:19.911Z</updated>
        <summary type="html"><![CDATA[In recent years, the field of machine learning has seen rapid growth, with
applications in a variety of domains, including image recognition, natural
language processing, and predictive modeling. In this paper, we explore the
application of machine learning to the generation of scientific articles. We
present a method for using machine learning to generate scientific articles
based on a data set of scientific papers. The method uses a machine-learning
algorithm to learn the structure of a scientific article and a set of training
data consisting of scientific papers. The machine-learning algorithm is used to
generate a scientific article based on the data set of scientific papers. We
evaluate the performance of the method by comparing the generated article to a
set of manually written articles. The results show that the machine-generated
article is of similar quality to the manually written articles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayache_E/0/1/0/all/0/1"&gt;Eliot H. Ayache&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omand_C/0/1/0/all/0/1"&gt;Conor M.B. Omand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially Adaptive Online Prediction of Piecewise Regular Functions. (arXiv:2203.16587v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2203.16587</id>
        <link href="http://arxiv.org/abs/2203.16587"/>
        <updated>2022-04-02T00:47:19.910Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating piecewise regular functions in an
online setting, i.e., the data arrive sequentially and at any round our task is
to predict the value of the true function at the next revealed point using the
available data from past predictions. We propose a suitably modified version of
a recently developed online learning algorithm called the sleeping experts
aggregation algorithm. We show that this estimator satisfies oracle risk bounds
simultaneously for all local regions of the domain. As concrete instantiations
of the expert aggregation algorithm proposed here, we study an online mean
aggregation and an online linear regression aggregation algorithm where experts
correspond to the set of dyadic subrectangles of the domain. The resulting
algorithms are near linear time computable in the sample size. We specifically
focus on the performance of these online algorithms in the context of
estimating piecewise polynomial and bounded variation function classes in the
fixed design setup. The simultaneous oracle risk bounds we obtain for these
estimators in this context provide new and improved (in certain aspects)
guarantees even in the batch setting and are not available for the state of the
art batch learning estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Sabyasachi Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Goswami_S/0/1/0/all/0/1"&gt;Subhajit Goswami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Meta-Reinforcement Learning with Curriculum-Based Task Sampling. (arXiv:2203.16801v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16801</id>
        <link href="http://arxiv.org/abs/2203.16801"/>
        <updated>2022-04-02T00:47:19.910Z</updated>
        <summary type="html"><![CDATA[Meta-reinforcement learning (meta-RL) acquires meta-policies that show good
performance for tasks in a wide task distribution. However, conventional
meta-RL, which learns meta-policies by randomly sampling tasks, has been
reported to show meta-overfitting for certain tasks, especially for easy tasks
where an agent can easily get high scores. To reduce effects of the
meta-overfitting, we considered meta-RL with curriculum-based task sampling.
Our method is Robust Meta Reinforcement Learning with Guided Task Sampling
(RMRL-GTS), which is an effective method that restricts task sampling based on
scores and epochs. We show that in order to achieve robust meta-RL, it is
necessary not only to intensively sample tasks with poor scores, but also to
restrict and expand the task regions of the tasks to be sampled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsumoto_M/0/1/0/all/0/1"&gt;Morio Matsumoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuba_H/0/1/0/all/0/1"&gt;Hiroya Matsuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kujirai_T/0/1/0/all/0/1"&gt;Toshihiro Kujirai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mask Atari for Deep Reinforcement Learning as POMDP Benchmarks. (arXiv:2203.16777v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16777</id>
        <link href="http://arxiv.org/abs/2203.16777"/>
        <updated>2022-04-02T00:47:19.908Z</updated>
        <summary type="html"><![CDATA[We present Mask Atari, a new benchmark to help solve partially observable
Markov decision process (POMDP) problems with Deep Reinforcement Learning
(DRL)-based approaches. To achieve a simulation environment for the POMDP
problems, Mask Atari is constructed based on Atari 2600 games with
controllable, moveable, and learnable masks as the observation area for the
target agent, especially with the active information gathering (AIG) setting in
POMDPs. Given that one does not yet exist, Mask Atari provides a challenging,
efficient benchmark for evaluating the methods that focus on the above problem.
Moreover, the mask operation is a trial for introducing the receptive field in
the human vision system into a simulation environment for an agent, which means
the evaluations are not biased from the sensing ability and purely focus on the
cognitive performance of the methods when compared with the human baseline. We
describe the challenges and features of our benchmark and evaluate several
baselines with Mask Atari.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yang Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Quan Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumura_T/0/1/0/all/0/1"&gt;Tadayuki Matsumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuji_T/0/1/0/all/0/1"&gt;Taiki Fuji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ito_K/0/1/0/all/0/1"&gt;Kiyoto Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mizuno_H/0/1/0/all/0/1"&gt;Hiroyuki Mizuno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Effect of Registration Hyperparameters with HyperMorph. (arXiv:2203.16680v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16680</id>
        <link href="http://arxiv.org/abs/2203.16680"/>
        <updated>2022-04-02T00:47:19.906Z</updated>
        <summary type="html"><![CDATA[We introduce HyperMorph, a framework that facilitates efficient
hyperparameter tuning in learning-based deformable image registration.
Classical registration algorithms perform an iterative pair-wise optimization
to compute a deformation field that aligns two images. Recent learning-based
approaches leverage large image datasets to learn a function that rapidly
estimates a deformation for a given image pair. In both strategies, the
accuracy of the resulting spatial correspondences is strongly influenced by the
choice of certain hyperparameter values. However, an effective hyperparameter
search consumes substantial time and human effort as it often involves training
multiple models for different fixed hyperparameter values and may lead to
suboptimal registration. We propose an amortized hyperparameter learning
strategy to alleviate this burden by learning the impact of hyperparameters on
deformation fields. We design a meta network, or hypernetwork, that predicts
the parameters of a registration network for input hyperparameters, thereby
comprising a single model that generates the optimal deformation field
corresponding to given hyperparameter values. This strategy enables fast,
high-resolution hyperparameter search at test-time, reducing the inefficiency
of traditional approaches while increasing flexibility. We also demonstrate
additional benefits of HyperMorph, including enhanced robustness to model
initialization and the ability to rapidly identify optimal hyperparameter
values specific to a dataset, image contrast, task, or even anatomical region,
all without the need to retrain models. We make our code publicly available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoopes_A/0/1/0/all/0/1"&gt;Andrew Hoopes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1"&gt;Malte Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greve_D/0/1/0/all/0/1"&gt;Douglas N. Greve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischl_B/0/1/0/all/0/1"&gt;Bruce Fischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1"&gt;John Guttag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1"&gt;Adrian V. Dalca&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAE-AST: Masked Autoencoding Audio Spectrogram Transformer. (arXiv:2203.16691v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16691</id>
        <link href="http://arxiv.org/abs/2203.16691"/>
        <updated>2022-04-02T00:47:19.904Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a simple yet powerful improvement over the recent
Self-Supervised Audio Spectrogram Transformer (SSAST) model for speech and
audio classification. Specifically, we leverage the insight that the SSAST uses
a very high masking ratio (75%) during pretraining, meaning that the vast
majority of self-attention compute is performed on mask tokens. We address this
by integrating the encoder-decoder architecture from Masked Autoencoders are
Scalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on
only unmasked input, and a shallow decoder operates on encoder outputs and mask
tokens. We find that MAE-like pretraining can provide a 3x speedup and 2x
memory usage reduction over the vanilla SSAST using current audio pretraining
strategies with ordinary model and input sizes. When fine-tuning on downstream
tasks, which only uses the encoder, we find that our approach outperforms the
SSAST on a variety of downstream tasks. We further conduct comprehensive
evaluations into different strategies of pretraining and explore differences in
MAE-style pretraining between the visual and audio domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Baade_A/0/1/0/all/0/1"&gt;Alan Baade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1"&gt;Puyuan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1"&gt;David Harwath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direction of Arrival Estimation of Sound Sources Using Icosahedral CNNs. (arXiv:2203.16940v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16940</id>
        <link href="http://arxiv.org/abs/2203.16940"/>
        <updated>2022-04-02T00:47:19.904Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new model for Direction of Arrival (DOA)
estimation of sound sources based on an Icosahedral Convolutional Neural
Network (CNN) applied over SRP-PHAT power maps computed from the signals
received by a microphone array. This icosahedral CNN is equivariant to the 60
rotational symmetries of the icosahedron, which represent a good approximation
of the continuous space of spherical rotations, and can be implemented using
standard 2D convolutional layers, having a lower computational cost than most
of the spherical CNNs. In addition, instead of using fully connected layers
after the icosahedral convolutions, we propose a new soft-argmax function that
can be seen as a differentiable version of the argmax function and allows us to
solve the DOA estimation as a regression problem interpreting the output of the
convolutional layers as a probability distribution. We prove that using models
that fit the equivariances of the problem allows us to outperform other
state-of-the-art models with a lower computational cost and more robustness,
obtaining root mean square localization errors lower than 10{\deg} even in
scenarios with a reverberation time $T_{60}$ of 1.5 s.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_Guerra_D/0/1/0/all/0/1"&gt;David Diaz-Guerra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miguel_A/0/1/0/all/0/1"&gt;Antonio Miguel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beltran_J/0/1/0/all/0/1"&gt;Jose R. Beltran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Federated Learning via Reconfigurable Intelligent Surface. (arXiv:2203.17028v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2203.17028</id>
        <link href="http://arxiv.org/abs/2203.17028"/>
        <updated>2022-04-02T00:47:19.901Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL), as a disruptive machine learning paradigm, enables
the collaborative training of a global model over decentralized local datasets
without sharing them. It spans a wide scope of applications from
Internet-of-Things (IoT) to biomedical engineering and drug discovery. To
support low-latency and high-privacy FL over wireless networks, in this paper,
we propose a reconfigurable intelligent surface (RIS) empowered over-the-air FL
system to alleviate the dilemma between learning accuracy and privacy. This is
achieved by simultaneously exploiting the channel propagation reconfigurability
with RIS for boosting the receive signal power, as well as waveform
superposition property with over-the-air computation (AirComp) for fast model
aggregation. By considering a practical scenario where high-dimensional local
model updates are transmitted across multiple communication blocks, we
characterize the convergence behaviors of the differentially private federated
optimization algorithm. We further formulate a system optimization problem to
optimize the learning accuracy while satisfying privacy and power constraints
via the joint design of transmit power, artificial noise, and phase shifts at
RIS, for which a two-step alternating minimization framework is developed.
Simulation results validate our systematic, theoretical, and algorithmic
achievements and demonstrate that RIS can achieve a better trade-off between
privacy and accuracy for over-the-air FL systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuhan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Youlong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuanming Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Message Passing Neural Networks for Hypergraphs. (arXiv:2203.16995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16995</id>
        <link href="http://arxiv.org/abs/2203.16995"/>
        <updated>2022-04-02T00:47:19.900Z</updated>
        <summary type="html"><![CDATA[Hypergraph representations are both more efficient and better suited to
describe data characterized by relations between two or more objects. In this
work, we present the first graph neural network based on message passing
capable of processing hypergraph-structured data. We show that the proposed
model defines a design space for neural network models for hypergraphs, thus
generalizing existing models for hypergraphs. We report experiments on a
benchmark dataset for node classification, highlighting the effectiveness of
the proposed model with respect to other state-of-the-art methods for graphs
and hypergraphs. We also discuss the benefits of using hypergraph
representations and, at the same time, highlight the limitation of using
equivalent graph representations when the underlying problem has relations
among more than two objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heydari_S/0/1/0/all/0/1"&gt;Sajjad Heydari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livi_L/0/1/0/all/0/1"&gt;Lorenzo Livi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bangla hate speech detection on social media using attention-based recurrent neural network. (arXiv:2203.16775v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.16775</id>
        <link href="http://arxiv.org/abs/2203.16775"/>
        <updated>2022-04-02T00:47:19.899Z</updated>
        <summary type="html"><![CDATA[Hate speech has spread more rapidly through the daily use of technology and,
most notably, by sharing your opinions or feelings on social media in a
negative aspect. Although numerous works have been carried out in detecting
hate speeches in English, German, and other languages, very few works have been
carried out in the context of the Bengali language. In contrast, millions of
people communicate on social media in Bengali. The few existing works that have
been carried out need improvements in both accuracy and interpretability. This
article proposed encoder decoder based machine learning model, a popular tool
in NLP, to classify user's Bengali comments on Facebook pages. A dataset of
7,425 Bengali comments, consisting of seven distinct categories of hate
speeches, was used to train and evaluate our model. For extracting and encoding
local features from the comments, 1D convolutional layers were used. Finally,
the attention mechanism, LSTM, and GRU based decoders have been used for
predicting hate speech categories. Among the three encoder decoder algorithms,
the attention-based decoder obtained the best accuracy (77%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Amit Kumar Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_A/0/1/0/all/0/1"&gt;Abdullah Al Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Anik Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1"&gt;Md. Nur Hossain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo. (arXiv:2203.17248v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17248</id>
        <link href="http://arxiv.org/abs/2203.17248"/>
        <updated>2022-04-02T00:47:19.898Z</updated>
        <summary type="html"><![CDATA[Contrastive learning (CL) is widely known to require many negative samples,
65536 in MoCo for instance, for which the performance of a dictionary-free
framework is often inferior because the negative sample size (NSS) is limited
by its mini-batch size (MBS). To decouple the NSS from the MBS, a dynamic
dictionary has been adopted in a large volume of CL frameworks, among which
arguably the most popular one is MoCo family. In essence, MoCo adopts a
momentum-based queue dictionary, for which we perform a fine-grained analysis
of its size and consistency. We point out that InfoNCE loss used in MoCo
implicitly attract anchors to their corresponding positive sample with various
strength of penalties and identify such inter-anchor hardness-awareness
property as a major reason for the necessity of a large dictionary. Our
findings motivate us to simplify MoCo v2 via the removal of its dictionary as
well as momentum. Based on an InfoNCE with the proposed dual temperature, our
simplified frameworks, SimMoCo and SimCo, outperform MoCo v2 by a visible
margin. Moreover, our work bridges the gap between CL and non-CL frameworks,
contributing to a more unified understanding of these two mainstream frameworks
in SSL. Code is available at: https://bit.ly/3LkQbaT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1"&gt;Trung X. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_A/0/1/0/all/0/1"&gt;Axi Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhinan Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1"&gt;Chang D. Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1"&gt;In So Kweon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Icing Detection Model of Wind Turbine Blades Based on SCADA data. (arXiv:2101.07914v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.07914</id>
        <link href="http://arxiv.org/abs/2101.07914"/>
        <updated>2022-04-02T00:47:19.898Z</updated>
        <summary type="html"><![CDATA[Diagnosis of ice accretion on wind turbine blades is all the time a hard nut
to crack in condition monitoring of wind farms. Existing methods focus on
mechanism analysis of icing process, deviation degree analysis of feature
engineering. However, there have not been deep researches of neural networks
applied in this field at present. Supervisory control and data acquisition
(SCADA) makes it possible to train networks through continuously providing not
only operation parameters and performance parameters of wind turbines but also
environmental parameters and operation modes. This paper explores the
possibility that using convolutional neural networks (CNNs), generative
adversarial networks (GANs) and domain adaption learning to establish
intelligent diagnosis frameworks under different training scenarios.
Specifically, PGANC and PGANT are proposed for sufficient and non-sufficient
target wind turbine labeled data, respectively. The basic idea is that we
consider a two-stage training with parallel GANs, which are aimed at capturing
intrinsic features for normal and icing samples, followed by classification CNN
or domain adaption module in various training cases. Model validation on three
wind turbine SCADA data shows that two-stage training can effectively improve
the model performance. Besides, if there is no sufficient labeled data for a
target turbine, which is an extremely common phenomenon in real industrial
practices, the addition of domain adaption learning makes the trained model
show better performance. Overall, our proposed intelligent diagnosis frameworks
can achieve more accurate detection on the same wind turbine and more
generalized capability on a new wind turbine, compared with other machine
learning models and conventional CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenqian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Junyang Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A data-driven approach for the closure of RANS models by the divergence of the Reynolds Stress Tensor. (arXiv:2203.16944v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2203.16944</id>
        <link href="http://arxiv.org/abs/2203.16944"/>
        <updated>2022-04-02T00:47:19.895Z</updated>
        <summary type="html"><![CDATA[In the present paper a new data-driven model to close and increase accuracy
of RANS equations is proposed. It is based on the direct approximation of the
divergence of the Reynolds Stress Tensor (RST) through a Neural Network (NN).
This choice is driven by the presence of the divergence of RST in the RANS
equations. Furthermore, once this data-driven approach is trained, there is no
need to run any turbulence model to close the equations. Finally, it is well
known that a good approximation of a function it is not necessarily a good
approximation of its derivative. The architecture and inputs choices of the
proposed network guarantee both Galilean and coordinates-frame rotation
invariances by looking to a vector basis expansion of the divergence of the
RST. Two well-known test cases are used to show advantages of the proposed
method compared to classic turbulence models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Berrone_S/0/1/0/all/0/1"&gt;Stefano Berrone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Oberto_D/0/1/0/all/0/1"&gt;Davide Oberto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Node-Feature Convolution for Representation Learning. (arXiv:1812.00086v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00086</id>
        <link href="http://arxiv.org/abs/1812.00086"/>
        <updated>2022-04-02T00:47:19.893Z</updated>
        <summary type="html"><![CDATA[Graph convolutional network (GCN) is an emerging neural network approach. It
learns new representation of a node by aggregating feature vectors of all
neighbors in the aggregation process without considering whether the neighbors
or features are useful or not. Recent methods have improved solutions by
sampling a fixed size set of neighbors, or assigning different weights to
different neighbors in the aggregation process, but features within a feature
vector are still treated equally in the aggregation process. In this paper, we
introduce a new convolution operation on regular size feature maps constructed
from features of a fixed node bandwidth via sampling to get the first-level
node representation, which is then passed to a standard GCN to learn the
second-level node representation. Experiments show that our method outperforms
competing methods in semi-supervised node classification tasks. Furthermore,
our method opens new doors for exploring new GCN architectures, particularly
deeper GCN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Heda Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1"&gt;Nikolaos Aletras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Set-based Estimation of Polynomial Systems with Application to SIR Epidemics. (arXiv:2111.04704v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.04704</id>
        <link href="http://arxiv.org/abs/2111.04704"/>
        <updated>2022-04-02T00:47:19.892Z</updated>
        <summary type="html"><![CDATA[This paper proposes a data-driven set-based estimation algorithm for a class
of nonlinear systems with polynomial nonlinearities. Using the system's
input-output data, the proposed method computes a set that guarantees the
inclusion of the system's state in real-time. Although the system is assumed to
be a polynomial type, the exact polynomial functions, and their coefficients
are assumed to be unknown. To this end, the estimator relies on offline and
online phases. The offline phase utilizes past input-output data to estimate a
set of possible coefficients of the polynomial system. Then, using this
estimated set of coefficients and the side information about the system, the
online phase provides a set estimate of the state. Finally, the proposed
methodology is evaluated through its application on SIR (Susceptible, Infected,
Recovered) epidemic model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alanwar_A/0/1/0/all/0/1"&gt;Amr Alanwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niazi_M/0/1/0/all/0/1"&gt;Muhammad Umar B. Niazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Johansson_K/0/1/0/all/0/1"&gt;Karl H. Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning Based Query Vertex Ordering Model for Subgraph Matching. (arXiv:2201.11251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.11251</id>
        <link href="http://arxiv.org/abs/2201.11251"/>
        <updated>2022-04-02T00:47:19.869Z</updated>
        <summary type="html"><![CDATA[Subgraph matching is a fundamental problem in various fields that use graph
structured data. Subgraph matching algorithms enumerate all isomorphic
embeddings of a query graph q in a data graph G. An important branch of
matching algorithms exploit the backtracking search approach which recursively
extends intermediate results following a matching order of query vertices. It
has been shown that the matching order plays a critical role in time efficiency
of these backtracking based subgraph matching algorithms. In recent years, many
advanced techniques for query vertex ordering (i.e., matching order generation)
have been proposed to reduce the unpromising intermediate results according to
the preset heuristic rules. In this paper, for the first time we apply the
Reinforcement Learning (RL) and Graph Neural Networks (GNNs) techniques to
generate the high-quality matching order for subgraph matching algorithms.
Instead of using the fixed heuristics to generate the matching order, our model
could capture and make full use of the graph information, and thus determine
the query vertex order with the adaptive learning-based rule that could
significantly reduces the number of redundant enumerations. With the help of
the reinforcement learning framework, our model is able to consider the
long-term benefits rather than only consider the local information at current
ordering step.Extensive experiments on six real-life data graphs demonstrate
that our proposed matching order generation technique could reduce up to two
orders of magnitude of query processing time compared to the state-of-the-art
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hanchen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Lu Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuemin Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretation of Black Box NLP Models: A Survey. (arXiv:2203.17081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17081</id>
        <link href="http://arxiv.org/abs/2203.17081"/>
        <updated>2022-04-02T00:47:19.836Z</updated>
        <summary type="html"><![CDATA[An increasing number of machine learning models have been deployed in domains
with high stakes such as finance and healthcare. Despite their superior
performances, many models are black boxes in nature which are hard to explain.
There are growing efforts for researchers to develop methods to interpret these
black-box models. Post hoc explanations based on perturbations, such as LIME,
are widely used approaches to interpret a machine learning model after it has
been built. This class of methods has been shown to exhibit large instability,
posing serious challenges to the effectiveness of the method itself and harming
user trust. In this paper, we propose S-LIME, which utilizes a hypothesis
testing framework based on central limit theorem for determining the number of
perturbation points needed to guarantee stability of the resulting explanation.
Experiments on both simulated and real world data sets are provided to
demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Shivani Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_N/0/1/0/all/0/1"&gt;Niladri Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Subir Kumar Saha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability and Generalization Capabilities of Message Passing Graph Neural Networks. (arXiv:2202.00645v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.00645</id>
        <link href="http://arxiv.org/abs/2202.00645"/>
        <updated>2022-04-02T00:47:19.836Z</updated>
        <summary type="html"><![CDATA[Message passing neural networks (MPNN) have seen a steep rise in popularity
since their introduction as generalizations of convolutional neural networks to
graph structured data, and are now considered state-of-the-art tools for
solving a large variety of graph-focused problems. We study the generalization
capabilities of MPNNs in graph classification. We assume that graphs of
different classes are sampled from different random graph models. Based on this
data distribution, we derive a non-asymptotic bound on the generalization gap
between the empirical and statistical loss, that decreases to zero as the
graphs become larger. This is proven by showing that a MPNN, applied on a
graph, approximates the MPNN applied on the geometric model that the graph
discretizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maskey_S/0/1/0/all/0/1"&gt;Sohir Maskey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1"&gt;Ron Levie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1"&gt;Gitta Kutyniok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications. (arXiv:2110.05781v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.05781</id>
        <link href="http://arxiv.org/abs/2110.05781"/>
        <updated>2022-04-02T00:47:19.834Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) allows transcribing the communications
between air traffic controllers (ATCOs) and aircraft pilots. The transcriptions
are used later to extract ATC named entities e.g., aircraft callsigns, command
types, or values. One common challenge is Speech Activity Detection (SAD) and
diarization system. If one of them fails then two or more single speaker
segments remain in the same recording, jeopardizing the overall system's
performance. We propose a system that combines the segmentation of a SAD module
with a BERT model that performs speaker change detection (SCD) and speaker role
detection (SRD) by chunking ASR transcripts i.e., diarization with a defined
number of speakers together with SRD. The proposed model is evaluated on
real-life ATC test sets. It reaches up to 0.90/0.95 F1-score on ATCO/pilot SRD,
which means a 27% relative improvement on diarization error rate (DER) compared
to standard acoustic-based diarization. Results are measured on ASR transcripts
of challenging ATC test sets with $\sim$13\% word error rate, and the
robustness of the system is even validated on noisy ASR transcripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1"&gt;Juan Zuluaga-Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1"&gt;Seyyed Saeed Sarfjoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1"&gt;Amrutha Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1"&gt;Iuliia Nigmatulina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1"&gt;Petr Motlicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ondrej_K/0/1/0/all/0/1"&gt;Karel Ondrej&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1"&gt;Oliver Ohneiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1"&gt;Hartmut Helmke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence Transduction with Graph-based Supervision. (arXiv:2111.01272v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.01272</id>
        <link href="http://arxiv.org/abs/2111.01272"/>
        <updated>2022-04-02T00:47:19.827Z</updated>
        <summary type="html"><![CDATA[The recurrent neural network transducer (RNN-T) objective plays a major role
in building today's best automatic speech recognition (ASR) systems for
production. Similarly to the connectionist temporal classification (CTC)
objective, the RNN-T loss uses specific rules that define how a set of
alignments is generated to form a lattice for the full-sum training. However,
it is yet largely unknown if these rules are optimal and do lead to the best
possible ASR results. In this work, we present a new transducer objective
function that generalizes the RNN-T loss to accept a graph representation of
the labels, thus providing a flexible and efficient framework to manipulate
training lattices, e.g., for studying different transition rules, implementing
different transducer losses, or restricting alignments. We demonstrate that
transducer-based ASR with CTC-like lattice achieves better results compared to
standard RNN-T, while also ensuring a strictly monotonic alignment, which will
allow better optimization of the decoding procedure. For example, the proposed
CTC-like transducer achieves an improvement of 4.8% on the test-other condition
of LibriSpeech relative to an equivalent RNN-T based system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moritz_N/0/1/0/all/0/1"&gt;Niko Moritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1"&gt;Takaaki Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.06484</id>
        <link href="http://arxiv.org/abs/2202.06484"/>
        <updated>2022-04-02T00:47:19.827Z</updated>
        <summary type="html"><![CDATA[In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present D2ADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate labeling efficiency, we design a
dynamic scheduling policy to adjust the labeling budgets between domain
exploration and model uncertainty over time. Extensive experiments show that
our method outperforms existing active learning and domain adaptation baselines
on two benchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than
5% target domain annotations, our method reaches comparable results with that
of full supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tsung-Han Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1"&gt;Yi-Syuan Liou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shao-Ji Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tung-I Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kuan-Chih Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian optimization with known experimental and design constraints for chemistry applications. (arXiv:2203.17241v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2203.17241</id>
        <link href="http://arxiv.org/abs/2203.17241"/>
        <updated>2022-04-02T00:47:19.825Z</updated>
        <summary type="html"><![CDATA[Optimization strategies driven by machine learning, such as Bayesian
optimization, are being explored across experimental sciences as an efficient
alternative to traditional design of experiment. When combined with automated
laboratory hardware and high-performance computing, these strategies enable
next-generation platforms for autonomous experimentation. However, the
practical application of these approaches is hampered by a lack of flexible
software and algorithms tailored to the unique requirements of chemical
research. One such aspect is the pervasive presence of constraints in the
experimental conditions when optimizing chemical processes or protocols, and in
the chemical space that is accessible when designing functional molecules or
materials. Although many of these constraints are known a priori, they can be
interdependent, non-linear, and result in non-compact optimization domains. In
this work, we extend our experiment planning algorithms Phoenics and Gryffin
such that they can handle arbitrary known constraints via an intuitive and
flexible interface. We benchmark these extended algorithms on continuous and
discrete test functions with a diverse set of constraints, demonstrating their
flexibility and robustness. In addition, we illustrate their practical utility
in two simulated chemical research scenarios: the optimization of the synthesis
of o-xylenyl Buckminsterfullerene adducts under constrained flow conditions,
and the design of redox active molecules for flow batteries under synthetic
accessibility constraints. The tools developed constitute a simple, yet
versatile strategy to enable model-based optimization with known experimental
constraints, contributing to its applicability as a core component of
autonomous platforms for scientific discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hickman_R/0/1/0/all/0/1"&gt;Riley J. Hickman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Aldeghi_M/0/1/0/all/0/1"&gt;Matteo Aldeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hase_F/0/1/0/all/0/1"&gt;Florian H&amp;#xe4;se&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1"&gt;Al&amp;#xe1;n Aspuru-Guzik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GoSafeOpt: Scalable Safe Exploration for Global Optimization of Dynamical Systems. (arXiv:2201.09562v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.09562</id>
        <link href="http://arxiv.org/abs/2201.09562"/>
        <updated>2022-04-02T00:47:19.824Z</updated>
        <summary type="html"><![CDATA[Learning optimal control policies directly on physical systems is challenging
since even a single failure can lead to costly hardware damage. Most existing
model-free learning methods that guarantee safety, i.e., no failures, during
exploration are limited to local optima. A notable exception is the GoSafe
algorithm, which, unfortunately, cannot handle high-dimensional systems and
hence cannot be applied to most real-world dynamical systems. This work
proposes GoSafeOpt as the first algorithm that can safely discover globally
optimal policies for high-dimensional systems while giving safety and
optimality guarantees. We demonstrate the superiority of GoSafeOpt over
competing model-free safe learning methods on a robot arm that would be
prohibitive for GoSafe.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukhija_B/0/1/0/all/0/1"&gt;Bhavya Sukhija&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchetta_M/0/1/0/all/0/1"&gt;Matteo Turchetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindner_D/0/1/0/all/0/1"&gt;David Lindner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1"&gt;Sebastian Trimpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumann_D/0/1/0/all/0/1"&gt;Dominik Baumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.15536</id>
        <link href="http://arxiv.org/abs/2203.15536"/>
        <updated>2022-04-02T00:47:19.824Z</updated>
        <summary type="html"><![CDATA[Our goal is to recover the 3D shape and pose of dogs from a single image.
This is a challenging task because dogs exhibit a wide range of shapes and
appearances, and are highly articulated. Recent work has proposed to directly
regress the SMAL animal model, with additional limb scale parameters, from
images. Our method, called BARC (Breed-Augmented Regression using
Classification), goes beyond prior work in several important ways. First, we
modify the SMAL shape space to be more appropriate for representing dog shape.
But, even with a better shape model, the problem of regressing dog shape from
an image is still challenging because we lack paired images with 3D ground
truth. To compensate for the lack of paired data, we formulate novel losses
that exploit information about dog breeds. In particular, we exploit the fact
that dogs of the same breed have similar body shapes. We formulate a novel
breed similarity loss consisting of two parts: One term encourages the shape of
dogs from the same breed to be more similar than dogs of different breeds. The
second one, a breed classification loss, helps to produce recognizable
breed-specific shapes. Through ablation studies, we find that our breed losses
significantly improve shape accuracy over a baseline without them. We also
compare BARC qualitatively to WLDO with a perceptual study and find that our
approach produces dogs that are significantly more realistic. This work shows
that a-priori information about genetic similarity can help to compensate for
the lack of 3D training data. This concept may be applicable to other animal
species or groups of species. Our code is publicly available for research
purposes at https://barc.is.tue.mpg.de/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rueegg_N/0/1/0/all/0/1"&gt;Nadine Rueegg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1"&gt;Silvia Zuffi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Single-Timescale Method for Stochastic Bilevel Optimization. (arXiv:2102.04671v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04671</id>
        <link href="http://arxiv.org/abs/2102.04671"/>
        <updated>2022-04-02T00:47:19.824Z</updated>
        <summary type="html"><![CDATA[Stochastic bilevel optimization generalizes the classic stochastic
optimization from the minimization of a single objective to the minimization of
an objective function that depends the solution of another optimization
problem. Recently, stochastic bilevel optimization is regaining popularity in
emerging machine learning applications such as hyper-parameter optimization and
model-agnostic meta learning. To solve this class of stochastic optimization
problems, existing methods require either double-loop or two-timescale updates,
which are sometimes less efficient. This paper develops a new optimization
method for a class of stochastic bilevel problems that we term Single-Timescale
stochAstic BiLevEl optimization (STABLE) method. STABLE runs in a single loop
fashion, and uses a single-timescale update with a fixed batch size. To achieve
an $\epsilon$-stationary point of the bilevel problem, STABLE requires ${\cal
O}(\epsilon^{-2})$ samples in total; and to achieve an $\epsilon$-optimal
solution in the strongly convex case, STABLE requires ${\cal O}(\epsilon^{-1})$
samples. To the best of our knowledge, this is the first bilevel optimization
algorithm achieving the same order of sample complexity as the stochastic
gradient descent method for the single-level stochastic optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_Q/0/1/0/all/0/1"&gt;Quan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.12970</id>
        <link href="http://arxiv.org/abs/2112.12970"/>
        <updated>2022-04-02T00:47:19.823Z</updated>
        <summary type="html"><![CDATA[Scene Graph Generation (SGG) remains a challenging visual understanding task
due to its compositional property. Most previous works adopt a bottom-up
two-stage or a point-based one-stage approach, which often suffers from high
time complexity or sub-optimal designs. In this work, we propose a novel SGG
method to address the aforementioned issues, formulating the task as a
bipartite graph construction problem. To solve the problem, we develop a
transformer-based end-to-end framework that first generates the entity and
predicate proposal set, followed by inferring directed edges to form the
relation triplets. In particular, we develop a new entity-aware predicate
representation based on a structural predicate generator that leverages the
compositional property of relationships. Moreover, we design a graph assembling
module to infer the connectivity of the bipartite scene graph based on our
entity-aware structure, enabling us to generate the scene graph in an
end-to-end manner. Extensive experimental results show that our design is able
to achieve the state-of-the-art or comparable performance on two challenging
benchmarks, surpassing most of the existing approaches and enjoying higher
efficiency in inference. We hope our model can serve as a strong baseline for
the Transformer-based scene graph generation. Code is available:
https://github.com/Scarecrow0/SGTR]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rongjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Songyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuming He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design. (arXiv:2112.11594v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.11594</id>
        <link href="http://arxiv.org/abs/2112.11594"/>
        <updated>2022-04-02T00:47:19.818Z</updated>
        <summary type="html"><![CDATA[Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art
graph learning model. However, it can be notoriously challenging to inference
GCNs over large graph datasets, limiting their application to large real-world
graphs and hindering the exploration of deeper and more sophisticated GCN
graphs. This is because real-world graphs can be extremely large and sparse.
Furthermore, the node degree of GCNs tends to follow the power-law distribution
and therefore have highly irregular adjacency matrices, resulting in
prohibitive inefficiencies in both data processing and movement and thus
substantially limiting the achievable GCN acceleration efficiency. To this end,
this paper proposes a GCN algorithm and accelerator Co-Design framework dubbed
GCoD which can largely alleviate the aforementioned GCN irregularity and boost
GCNs' inference efficiency. Specifically, on the algorithm level, GCoD
integrates a split and conquer GCN training strategy that polarizes the graphs
to be either denser or sparser in local neighborhoods without compromising the
model accuracy, resulting in graph adjacency matrices that (mostly) have merely
two levels of workload and enjoys largely enhanced regularity and thus ease of
acceleration. On the hardware level, we further develop a dedicated two-pronged
accelerator with a separated engine to process each of the aforementioned
denser and sparser workloads, further boosting the overall utilization and
acceleration efficiency. Extensive experiments and ablation studies validate
that our GCoD consistently reduces the number of off-chip accesses, leading to
speedups of 15286x, 294x, 7.8x, and 2.5x as compared to CPUs, GPUs, and
prior-art GCN accelerators including HyGCN and AWB-GCN, respectively, while
maintaining or even improving the task accuracy. Codes are available at
https://github.com/RICE-EIC/GCoD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1"&gt;Haoran You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1"&gt;Tong Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yingyan Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17261</id>
        <link href="http://arxiv.org/abs/2203.17261"/>
        <updated>2022-04-02T00:47:19.817Z</updated>
        <summary type="html"><![CDATA[Recent research explosion on Neural Radiance Field (NeRF) shows the
encouraging potential to represent complex scenes with neural networks. One
major drawback of NeRF is its prohibitive inference time: Rendering a single
pixel requires querying the NeRF network hundreds of times. To resolve it,
existing efforts mainly attempt to reduce the number of required sampled
points. However, the problem of iterative sampling still exists. On the other
hand, Neural Light Field (NeLF) presents a more straightforward representation
over NeRF in novel view synthesis -- the rendering of a pixel amounts to one
single forward pass without ray-marching. In this work, we present a deep
residual MLP network (88 layers) to effectively learn the light field. We show
the key to successfully learning such a deep NeLF network is to have sufficient
data, for which we transfer the knowledge from a pre-trained NeRF model via
data distillation. Extensive experiments on both synthetic and real-world
scenes show the merits of our method over other counterpart algorithms. On the
synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x
runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average
PSNR improvement) rendering quality than NeRF without any customized
implementation tricks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1"&gt;Kyle Olszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1"&gt;Menglei Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1"&gt;Sergey Tulyakov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flat-topped Probability Density Functions for Mixture Models. (arXiv:2203.17027v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17027</id>
        <link href="http://arxiv.org/abs/2203.17027"/>
        <updated>2022-04-02T00:47:19.817Z</updated>
        <summary type="html"><![CDATA[This paper investigates probability density functions (PDFs) that are
continuous everywhere, nearly uniform around the mode of distribution, and
adaptable to a variety of distribution shapes ranging from bell-shaped to
rectangular. From the viewpoint of computational tractability, the PDF based on
the Fermi-Dirac or logistic function is advantageous in estimating its shape
parameters. The most appropriate PDF for $n$-variate distribution is of the
form:
$p\left(\mathbf{x}\right)\propto\left[\cosh\left(\left[\left(\mathbf{x}-\mathbf{m}\right)^{\mathsf{T}}\boldsymbol{\Sigma}^{-1}\left(\mathbf{x}-\mathbf{m}\right)\right]^{n/2}\right)+\cosh\left(r^{n}\right)\right]^{-1}$
where $\mathbf{x},\mathbf{m}\in\mathbb{R}^{n}$, $\boldsymbol{\Sigma}$ is an
$n\times n$ positive definite matrix, and $r>0$ is a shape parameter. The
flat-topped PDFs can be used as a component of mixture models in machine
learning to improve goodness of fit and make a model as simple as possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_O/0/1/0/all/0/1"&gt;Osamu Fujita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Diffusion for Molecule Generation in 3D. (arXiv:2203.17003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17003</id>
        <link href="http://arxiv.org/abs/2203.17003"/>
        <updated>2022-04-02T00:47:19.814Z</updated>
        <summary type="html"><![CDATA[This work introduces a diffusion model for molecule generation in 3D that is
equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model
(EDM) learns to denoise a diffusion process with an equivariant network that
jointly operates on both continuous (atom coordinates) and categorical features
(atom types). In addition, we provide a probabilistic analysis which admits
likelihood computation of molecules using our model. Experimentally, the
proposed method significantly outperforms previous 3D molecular generative
methods regarding the quality of generated samples and efficiency at training
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1"&gt;Emiel Hoogeboom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satorras_V/0/1/0/all/0/1"&gt;Victor Garcia Satorras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vignac_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Vignac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based Reinforcement Learning: A Survey. (arXiv:2006.16712v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16712</id>
        <link href="http://arxiv.org/abs/2006.16712"/>
        <updated>2022-04-02T00:47:19.814Z</updated>
        <summary type="html"><![CDATA[Sequential decision making, commonly formalized as Markov Decision Process
(MDP) optimization, is a important challenge in artificial intelligence. Two
key approaches to this problem are reinforcement learning (RL) and planning.
This paper presents a survey of the integration of both fields, better known as
model-based reinforcement learning. Model-based RL has two main steps. First,
we systematically cover approaches to dynamics model learning, including
challenges like dealing with stochasticity, uncertainty, partial observability,
and temporal abstraction. Second, we present a systematic categorization of
planning-learning integration, including aspects like: where to start planning,
what budgets to allocate to planning and real data collection, how to plan, and
how to integrate planning in the learning and acting loop. After these two
sections, we also discuss implicit model-based RL as an end-to-end alternative
for model learning and planning, and we cover the potential benefits of
model-based RL. Along the way, the survey also draws connections to several
related RL fields, like hierarchical RL and transfer learning. Altogether, the
survey presents a broad conceptual overview of the combination of planning and
learning for MDP optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1"&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broekens_J/0/1/0/all/0/1"&gt;Joost Broekens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1"&gt;Catholijn M. Jonker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STICC: A multivariate spatial clustering method for repeated geographic pattern discovery with consideration of spatial contiguity. (arXiv:2203.09611v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.09611</id>
        <link href="http://arxiv.org/abs/2203.09611"/>
        <updated>2022-04-02T00:47:19.814Z</updated>
        <summary type="html"><![CDATA[Spatial clustering has been widely used for spatial data mining and knowledge
discovery. An ideal multivariate spatial clustering should consider both
spatial contiguity and aspatial attributes. Existing spatial clustering
approaches may face challenges for discovering repeated geographic patterns
with spatial contiguity maintained. In this paper, we propose a Spatial
Toeplitz Inverse Covariance-Based Clustering (STICC) method that considers both
attributes and spatial relationships of geographic objects for multivariate
spatial clustering. A subregion is created for each geographic object serving
as the basic unit when performing clustering. A Markov random field is then
constructed to characterize the attribute dependencies of subregions. Using a
spatial consistency strategy, nearby objects are encouraged to belong to the
same cluster. To test the performance of the proposed STICC algorithm, we apply
it in two use cases. The comparison results with several baseline methods show
that the STICC outperforms others significantly in terms of adjusted rand index
and macro-F1 score. Join count statistics is also calculated and shows that the
spatial contiguity is well preserved by STICC. Such a spatial clustering method
may benefit various applications in the fields of geography, remote sensing,
transportation, and urban planning, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yuhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kunlin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Song Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_I/0/1/0/all/0/1"&gt;Ignavier Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1"&gt;Jinmeng Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_T/0/1/0/all/0/1"&gt;Teng Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2203.16711</id>
        <link href="http://arxiv.org/abs/2203.16711"/>
        <updated>2022-04-02T00:47:19.813Z</updated>
        <summary type="html"><![CDATA[Parametrized quantum circuits can be used as quantum neural networks and have
the potential to outperform their classical counterparts when trained for
addressing learning problems. To date, much of the results on their performance
on practical problems are heuristic in nature. In particular, the convergence
rate for the training of quantum neural networks is not fully understood. Here,
we analyze the dynamics of gradient descent for the training error of a class
of variational quantum machine learning models. We define wide quantum neural
networks as parameterized quantum circuits in the limit of a large number of
qubits and variational parameters. We then find a simple analytic formula that
captures the average behavior of their loss function and discuss the
consequences of our findings. For example, for random quantum circuits, we
predict and characterize an exponential decay of the residual training error as
a function of the parameters of the system. We finally validate our analytic
results with numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Najafi_K/0/1/0/all/0/1"&gt;Khadijeh Najafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sharma_K/0/1/0/all/0/1"&gt;Kunal Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tacchino_F/0/1/0/all/0/1"&gt;Francesco Tacchino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Mezzacapo_A/0/1/0/all/0/1"&gt;Antonio Mezzacapo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Q-learning for solving elliptic PDEs. (arXiv:2203.17128v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2203.17128</id>
        <link href="http://arxiv.org/abs/2203.17128"/>
        <updated>2022-04-02T00:47:19.813Z</updated>
        <summary type="html"><![CDATA[Solving high-dimensional partial differential equations (PDEs) is a major
challenge in scientific computing. We develop a new numerical method for
solving elliptic-type PDEs by adapting the Q-learning algorithm in
reinforcement learning. Our "Q-PDE" algorithm is mesh-free and therefore has
the potential to overcome the curse of dimensionality. Using a neural tangent
kernel (NTK) approach, we prove that the neural network approximator for the
PDE solution, trained with the Q-PDE algorithm, converges to the trajectory of
an infinite-dimensional ordinary differential equation (ODE) as the number of
hidden units $\rightarrow \infty$. For monotone PDE (i.e. those given by
monotone operators, which may be nonlinear), despite the lack of a spectral gap
in the NTK, we then prove that the limit neural network, which satisfies the
infinite-dimensional ODE, converges in $L^2$ to the PDE solution as the
training time $\rightarrow \infty$. More generally, we can prove that any fixed
point of the wide-network limit for the Q-PDE algorithm is a solution of the
PDE (not necessarily under the monotone condition). The numerical performance
of the Q-PDE algorithm is studied for several elliptic PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Samuel N. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Deqing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sirignano_J/0/1/0/all/0/1"&gt;Justin Sirignano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Node-Feature Convolution for Representation Learning. (arXiv:1812.00086v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00086</id>
        <link href="http://arxiv.org/abs/1812.00086"/>
        <updated>2022-04-02T00:47:19.813Z</updated>
        <summary type="html"><![CDATA[Graph convolutional network (GCN) is an emerging neural network approach. It
learns new representation of a node by aggregating feature vectors of all
neighbors in the aggregation process without considering whether the neighbors
or features are useful or not. Recent methods have improved solutions by
sampling a fixed size set of neighbors, or assigning different weights to
different neighbors in the aggregation process, but features within a feature
vector are still treated equally in the aggregation process. In this paper, we
introduce a new convolution operation on regular size feature maps constructed
from features of a fixed node bandwidth via sampling to get the first-level
node representation, which is then passed to a standard GCN to learn the
second-level node representation. Experiments show that our method outperforms
competing methods in semi-supervised node classification tasks. Furthermore,
our method opens new doors for exploring new GCN architectures, particularly
deeper GCN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Heda Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1"&gt;Nikolaos Aletras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MBORE: Multi-objective Bayesian Optimisation by Density-Ratio Estimation. (arXiv:2203.16912v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16912</id>
        <link href="http://arxiv.org/abs/2203.16912"/>
        <updated>2022-04-02T00:47:19.811Z</updated>
        <summary type="html"><![CDATA[Optimisation problems often have multiple conflicting objectives that can be
computationally and/or financially expensive. Mono-surrogate Bayesian
optimisation (BO) is a popular model-based approach for optimising such
black-box functions. It combines objective values via scalarisation and builds
a Gaussian process (GP) surrogate of the scalarised values. The location which
maximises a cheap-to-query acquisition function is chosen as the next location
to expensively evaluate. While BO is an effective strategy, the use of GPs is
limiting. Their performance decreases as the problem input dimensionality
increases, and their computational complexity scales cubically with the amount
of data. To address these limitations, we extend previous work on BO by
density-ratio estimation (BORE) to the multi-objective setting. BORE links the
computation of the probability of improvement acquisition function to that of
probabilistic classification. This enables the use of state-of-the-art
classifiers in a BO-like framework. In this work we present MBORE:
multi-objective Bayesian optimisation by density-ratio estimation, and compare
it to BO across a range of synthetic and real-world benchmarks. We find that
MBORE performs as well as or better than BO on a wide variety of problems, and
that it outperforms BO on high-dimensional and real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ath_G/0/1/0/all/0/1"&gt;George De Ath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chugh_T/0/1/0/all/0/1"&gt;Tinkle Chugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahat_A/0/1/0/all/0/1"&gt;Alma A. M. Rahat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from many trajectories. (arXiv:2203.17193v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17193</id>
        <link href="http://arxiv.org/abs/2203.17193"/>
        <updated>2022-04-02T00:47:19.809Z</updated>
        <summary type="html"><![CDATA[We initiate a study of supervised learning from many independent sequences
("trajectories") of non-independent covariates, reflecting tasks in sequence
modeling, control, and reinforcement learning. Conceptually, our
multi-trajectory setup sits between two traditional settings in statistical
learning theory: learning from independent examples and learning from a single
auto-correlated sequence. Our conditions for efficient learning generalize the
former setting--trajectories must be non-degenerate in ways that extend
standard requirements for independent examples. They do not require that
trajectories be ergodic, long, nor strictly stable.

For linear least-squares regression, given $n$-dimensional examples produced
by $m$ trajectories, each of length $T$, we observe a notable change in
statistical efficiency as the number of trajectories increases from a few
(namely $m \lesssim n$) to many (namely $m \gtrsim n$). Specifically, we
establish that the worst-case error rate this problem is $\Theta(n / m T)$
whenever $m \gtrsim n$. Meanwhile, when $m \lesssim n$, we establish a (sharp)
lower bound of $\Omega(n^2 / m^2 T)$ on the worst-case error rate, realized by
a simple, marginally unstable linear dynamical system. A key upshot is that, in
domains where trajectories regularly reset, the error rate eventually behaves
as if all of the examples were independent altogether, drawn from their
marginals. As a corollary of our analysis, we also improve guarantees for the
linear system identification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1"&gt;Stephen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1"&gt;Roy Frostig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping. (arXiv:2203.16749v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16749</id>
        <link href="http://arxiv.org/abs/2203.16749"/>
        <updated>2022-04-02T00:47:19.804Z</updated>
        <summary type="html"><![CDATA[Neural vocoder using denoising diffusion probabilistic model (DDPM) has been
improved by adaptation of the diffusion noise distribution to given acoustic
features. In this study, we propose SpecGrad that adapts the diffusion noise so
that its time-varying spectral envelope becomes close to the conditioning
log-mel spectrogram. This adaptation by time-varying filtering improves the
sound quality especially in the high-frequency bands. It is processed in the
time-frequency domain to keep the computational cost almost the same as the
conventional DDPM-based neural vocoders. Experimental results showed that
SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based
neural vocoders in both analysis-synthesis and speech enhancement scenarios.
Audio demos are available at wavegrad.github.io/specgrad/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Koizumi_Y/0/1/0/all/0/1"&gt;Yuma Koizumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zen_H/0/1/0/all/0/1"&gt;Heiga Zen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yatabe_K/0/1/0/all/0/1"&gt;Kohei Yatabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nanxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bacchiani_M/0/1/0/all/0/1"&gt;Michiel Bacchiani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?. (arXiv:2110.04184v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.04184</id>
        <link href="http://arxiv.org/abs/2110.04184"/>
        <updated>2022-04-02T00:47:19.782Z</updated>
        <summary type="html"><![CDATA[Multi-agent reinforcement learning has made substantial empirical progresses
in solving games with a large number of players. However, theoretically, the
best known sample complexity for finding a Nash equilibrium in general-sum
games scales exponentially in the number of players due to the size of the
joint action space, and there is a matching exponential lower bound. This paper
investigates what learning goals admit better sample complexities in the
setting of $m$-player general-sum Markov games with $H$ steps, $S$ states, and
$A_i$ actions per player. First, we design algorithms for learning an
$\epsilon$-Coarse Correlated Equilibrium (CCE) in
$\widetilde{\mathcal{O}}(H^5S\max_{i\le m} A_i / \epsilon^2)$ episodes, and an
$\epsilon$-Correlated Equilibrium (CE) in
$\widetilde{\mathcal{O}}(H^6S\max_{i\le m} A_i^2 / \epsilon^2)$ episodes. This
is the first line of results for learning CCE and CE with sample complexities
polynomial in $\max_{i\le m} A_i$. Our algorithm for learning CE integrates an
adversarial bandit subroutine which minimizes a weighted swap regret, along
with several novel designs in the outer loop. Second, we consider the important
special case of Markov Potential Games, and design an algorithm that learns an
$\epsilon$-approximate Nash equilibrium within
$\widetilde{\mathcal{O}}(S\sum_{i\le m} A_i / \epsilon^3)$ episodes (when only
highlighting the dependence on $S$, $A_i$, and $\epsilon$), which only depends
linearly in $\sum_{i\le m} A_i$ and significantly improves over existing
efficient algorithm in the $\epsilon$ dependence. Overall, our results shed
light on what equilibria or structural assumptions on the game may enable
sample-efficient learning with many players.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Ziang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1"&gt;Song Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yu Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning. (arXiv:2203.16797v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16797</id>
        <link href="http://arxiv.org/abs/2203.16797"/>
        <updated>2022-04-02T00:47:19.777Z</updated>
        <summary type="html"><![CDATA[Physics-informed machine learning (PIML), referring to the combination of
prior knowledge of physics, which is the high level abstraction of natural
phenomenons and human behaviours in the long history, with data-driven machine
learning models, has emerged as an effective way to mitigate the shortage of
training data, to increase models' generalizability and to ensure the physical
plausibility of results. In this paper, we survey an abundant number of recent
works in PIML and summarize them from three aspects: (1) motivations of PIML,
(2) physics knowledge in PIML, (3) methods of physics knowledge integration in
PIML. We also discuss current challenges and corresponding research
opportunities in PIML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chuizheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sungyong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griesemer_S/0/1/0/all/0/1"&gt;Sam Griesemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05630</id>
        <link href="http://arxiv.org/abs/2006.05630"/>
        <updated>2022-04-02T00:47:19.775Z</updated>
        <summary type="html"><![CDATA[Policy learning using historical observational data is an important problem
that has found widespread applications. Examples include selecting offers,
prices, advertisements to send to customers, as well as selecting which
medication to prescribe to a patient. However, existing literature rests on the
crucial assumption that the future environment where the learned policy will be
deployed is the same as the past environment that has generated the data -- an
assumption that is often false or too coarse an approximation. In this paper,
we lift this assumption and aim to learn a distributionally robust policy with
incomplete observational data. We first present a policy evaluation procedure
that allows us to assess how well the policy does under the worst-case
environment shift. We then establish a central limit theorem type guarantee for
this proposed policy evaluation scheme. Leveraging this evaluation scheme, we
further propose a novel learning algorithm that is able to learn a policy that
is robust to adversarial perturbations and unknown covariate shifts with a
performance guarantee based on the theory of uniform convergence. Finally, we
empirically test the effectiveness of our proposed algorithm in synthetic
datasets and demonstrate that it provides the robustness that is missing using
standard policy learning algorithms. We conclude the paper by providing a
comprehensive application of our methods in the context of a real-world voting
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1"&gt;Nian Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A statistical framework for efficient out of distribution detection in deep neural networks. (arXiv:2102.12967v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12967</id>
        <link href="http://arxiv.org/abs/2102.12967"/>
        <updated>2022-04-02T00:47:19.775Z</updated>
        <summary type="html"><![CDATA[Background. Commonly, Deep Neural Networks (DNNs) generalize well on samples
drawn from a distribution similar to that of the training set. However, DNNs'
predictions are brittle and unreliable when the test samples are drawn from a
dissimilar distribution. This is a major concern for deployment in real-world
applications, where such behavior may come at a considerable cost, such as
industrial production lines, autonomous vehicles, or healthcare applications.
Contributions. We frame Out Of Distribution (OOD) detection in DNNs as a
statistical hypothesis testing problem. Tests generated within our proposed
framework combine evidence from the entire network. Unlike previous OOD
detection heuristics, this framework returns a $p$-value for each test sample.
It is guaranteed to maintain the Type I Error (T1E - incorrectly predicting OOD
for an actual in-distribution sample) for test data. Moreover, this allows to
combine several detectors while maintaining the T1E. Building on this
framework, we suggest a novel OOD procedure based on low-order statistics. Our
method achieves comparable or better results than state-of-the-art methods on
well-accepted OOD benchmarks, without retraining the network parameters or
assuming prior knowledge on the test distribution -- and at a fraction of the
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haroush_M/0/1/0/all/0/1"&gt;Matan Haroush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frostig_T/0/1/0/all/0/1"&gt;Tzviel Frostig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heller_R/0/1/0/all/0/1"&gt;Ruth Heller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1"&gt;Daniel Soudry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Feature Selection for Algorithmic Fairness. (arXiv:2006.06053v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06053</id>
        <link href="http://arxiv.org/abs/2006.06053"/>
        <updated>2022-04-02T00:47:19.773Z</updated>
        <summary type="html"><![CDATA[The use of machine learning (ML) in high-stakes societal decisions has
encouraged the consideration of fairness throughout the ML lifecycle. Although
data integration is one of the primary steps to generate high quality training
data, most of the fairness literature ignores this stage. In this work, we
consider fairness in the integration component of data management, aiming to
identify features that improve prediction without adding any bias to the
dataset. We work under the causal interventional fairness paradigm. Without
requiring the underlying structural causal model a priori, we propose an
approach to identify a sub-collection of features that ensure the fairness of
the dataset by performing conditional independence tests between different
subsets of features. We use group testing to improve the complexity of the
approach. We theoretically prove the correctness of the proposed algorithm to
identify features that ensure interventional fairness and show that sub-linear
conditional independence tests are sufficient to identify these variables. A
detailed empirical evaluation is performed on real-world datasets to
demonstrate the efficacy and efficiency of our technique.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1"&gt;Sainyam Galhotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1"&gt;Prasanna Sattigeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1"&gt;Kush R. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast, Accurate and Memory-Efficient Partial Permutation Synchronization. (arXiv:2203.16505v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.16505</id>
        <link href="http://arxiv.org/abs/2203.16505"/>
        <updated>2022-04-02T00:47:19.773Z</updated>
        <summary type="html"><![CDATA[Previous partial permutation synchronization (PPS) algorithms, which are
commonly used for multi-object matching, often involve computation-intensive
and memory-demanding matrix operations. These operations become intractable for
large scale structure-from-motion datasets. For pure permutation
synchronization, the recent Cycle-Edge Message Passing (CEMP) framework
suggests a memory-efficient and fast solution. Here we overcome the restriction
of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for
estimating the corruption levels of the observed partial permutations. It
allows us to subsequently implement a nonconvex weighted projected power method
without the need of spectral initialization. The resulting new PPS algorithm,
MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse
matrix operations, and thus enjoys lower time and space complexities in
comparison to previous PPS algorithms. We prove that under adversarial
corruption, though without additive noise and with certain assumptions,
CEMP-Partial is able to exactly classify corrupted and clean partial
permutations. We demonstrate the state-of-the-art accuracy, speed and memory
efficiency of our method on both synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaohan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yunpeng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerman_G/0/1/0/all/0/1"&gt;Gilad Lerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FBDNN: Filter Banks and Deep Neural Networks for Portable and Fast Brain-Computer Interfaces. (arXiv:2109.02165v4 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.02165</id>
        <link href="http://arxiv.org/abs/2109.02165"/>
        <updated>2022-04-02T00:47:19.760Z</updated>
        <summary type="html"><![CDATA[Objective: To propose novel SSVEP classification methodologies using deep
neural networks (DNNs) and improve performances in single-channel and
user-independent brain-computer interfaces (BCIs) with small data lengths.
Approach: We propose the utilization of filter banks (creating sub-band
components of the EEG signal) in conjunction with DNNs. In this context, we
created three different models: a recurrent neural network (FBRNN) analyzing
the time domain, a 2D convolutional neural network (FBCNN-2D) processing
complex spectrum features and a 3D convolutional neural network (FBCNN-3D)
analyzing complex spectrograms, which we introduce in this study as possible
input for SSVEP classification. We tested our neural networks on three open
datasets and conceived them so as not to require calibration from the final
user, simulating a user-independent BCI. Results: The DNNs with the filter
banks surpassed the accuracy of similar networks without this preprocessing
step by considerable margins, and they outperformed common SSVEP classification
methods (SVM and FBCCA) by even higher margins. Conclusion and significance:
Filter banks allow different types of deep neural networks to more efficiently
analyze the harmonic components of SSVEP. Complex spectrograms carry more
information than complex spectrum features and the magnitude spectrum, allowing
the FBCNN-3D to surpass the other CNNs. The performances obtained in the
challenging classification problems indicates a strong potential for the
construction of portable, economical, fast and low-latency BCIs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1"&gt;Pedro R. A. S. Bassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Attux_R/0/1/0/all/0/1"&gt;Romis Attux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning. (arXiv:2111.14213v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.14213</id>
        <link href="http://arxiv.org/abs/2111.14213"/>
        <updated>2022-04-02T00:47:19.748Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a promising strategy for performing
privacy-preserving, distributed learning with a network of clients (i.e., edge
devices). However, the data distribution among clients is often non-IID in
nature, making efficient optimization difficult. To alleviate this issue, many
FL algorithms focus on mitigating the effects of data heterogeneity across
clients by introducing a variety of proximal terms, some incurring considerable
compute and/or memory overheads, to restrain local updates with respect to the
global model. Instead, we consider rethinking solutions to data heterogeneity
in FL with a focus on local learning generality rather than proximal
restriction. To this end, we first present a systematic study informed by
second-order indicators to better understand algorithm effectiveness in FL.
Interestingly, we find that standard regularization methods are surprisingly
strong performers in mitigating data heterogeneity effects. Based on our
findings, we further propose a simple and effective method, FedAlign, to
overcome data heterogeneity and the pitfalls of previous methods. FedAlign
achieves competitive accuracy with state-of-the-art FL methods across a variety
of settings while minimizing computation and memory overhead. Code is available
at https://github.com/mmendiet/FedAlign]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1"&gt;Matias Mendieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Taojiannan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Minwoo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhengming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MBORE: Multi-objective Bayesian Optimisation by Density-Ratio Estimation. (arXiv:2203.16912v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16912</id>
        <link href="http://arxiv.org/abs/2203.16912"/>
        <updated>2022-04-02T00:47:19.737Z</updated>
        <summary type="html"><![CDATA[Optimisation problems often have multiple conflicting objectives that can be
computationally and/or financially expensive. Mono-surrogate Bayesian
optimisation (BO) is a popular model-based approach for optimising such
black-box functions. It combines objective values via scalarisation and builds
a Gaussian process (GP) surrogate of the scalarised values. The location which
maximises a cheap-to-query acquisition function is chosen as the next location
to expensively evaluate. While BO is an effective strategy, the use of GPs is
limiting. Their performance decreases as the problem input dimensionality
increases, and their computational complexity scales cubically with the amount
of data. To address these limitations, we extend previous work on BO by
density-ratio estimation (BORE) to the multi-objective setting. BORE links the
computation of the probability of improvement acquisition function to that of
probabilistic classification. This enables the use of state-of-the-art
classifiers in a BO-like framework. In this work we present MBORE:
multi-objective Bayesian optimisation by density-ratio estimation, and compare
it to BO across a range of synthetic and real-world benchmarks. We find that
MBORE performs as well as or better than BO on a wide variety of problems, and
that it outperforms BO on high-dimensional and real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ath_G/0/1/0/all/0/1"&gt;George De Ath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chugh_T/0/1/0/all/0/1"&gt;Tinkle Chugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahat_A/0/1/0/all/0/1"&gt;Alma A. M. Rahat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Temporal-oriented Broadcast ResNet for COVID-19 Detection. (arXiv:2203.17012v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.17012</id>
        <link href="http://arxiv.org/abs/2203.17012"/>
        <updated>2022-04-02T00:47:19.737Z</updated>
        <summary type="html"><![CDATA[Detecting COVID-19 from audio signals, such as breathing and coughing, can be
used as a fast and efficient pre-testing method to reduce the virus
transmission. Due to the promising results of deep learning networks in
modelling time sequences, and since applications to rapidly identify COVID
in-the-wild should require low computational effort, we present a
temporal-oriented broadcasting residual learning method that achieves efficient
computation and high accuracy with a small model size. Based on the
EfficientNet architecture, our novel network, named Temporal-oriented
ResNet~(TorNet), constitutes of a broadcasting learning block, i.e. the
Alternating Broadcast (AB) Block, which contains several Broadcast Residual
Blocks (BC ResBlocks) and a convolution layer. With the AB Block, the network
obtains useful audio-temporal features and higher level embeddings effectively
with much less computation than Recurrent Neural Networks~(RNNs), typically
used to model temporal information. TorNet achieves 72.2% Unweighted Average
Recall (UAR) on the INTERPSEECH 2021 Computational Paralinguistics Challenge
COVID-19 cough Sub-Challenge, by this showing competitive results with a higher
computational efficiency than other state-of-the-art alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1"&gt;Xin Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parada_Cabaleiro_E/0/1/0/all/0/1"&gt;Emilia Parada-Cabaleiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triantafyllopoulos_A/0/1/0/all/0/1"&gt;Andreas Triantafyllopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Meishu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zijiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn W. Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning for Traffic Routing under Unknown Preferences. (arXiv:2203.17150v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17150</id>
        <link href="http://arxiv.org/abs/2203.17150"/>
        <updated>2022-04-02T00:47:19.736Z</updated>
        <summary type="html"><![CDATA[In transportation networks, users typically choose routes in a decentralized
and self-interested manner to minimize their individual travel costs, which, in
practice, often results in inefficient overall outcomes for society. As a
result, there has been a growing interest in designing road tolling schemes to
cope with these efficiency losses and steer users toward a system-efficient
traffic pattern. However, the efficacy of road tolling schemes often relies on
having access to complete information on users' trip attributes, such as their
origin-destination (O-D) travel information and their values of time, which may
not be available in practice.

Motivated by this practical consideration, we propose an online learning
approach to set tolls in a traffic network to drive heterogeneous users with
different values of time toward a system-efficient traffic pattern. In
particular, we develop a simple yet effective algorithm that adjusts tolls at
each time period solely based on the observed aggregate flows on the roads of
the network without relying on any additional trip attributes of users, thereby
preserving user privacy. In the setting where the O-D pairs and values of time
of users are drawn i.i.d. at each period, we show that our approach obtains an
expected regret and road capacity violation of $O(\sqrt{T})$, where $T$ is the
number of periods over which tolls are updated. Our regret guarantee is
relative to an offline oracle that has complete information on users' trip
attributes. We further establish a $\Omega(\sqrt{T})$ lower bound on the regret
of any algorithm, which establishes that our algorithm is optimal up to
constants. Finally, we demonstrate the superior performance of our approach
relative to several benchmarks on a real-world transportation network, thereby
highlighting its practical applicability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalota_D/0/1/0/all/0/1"&gt;Devansh Jalota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1"&gt;Karthik Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizan_N/0/1/0/all/0/1"&gt;Navid Azizan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johari_R/0/1/0/all/0/1"&gt;Ramesh Johari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schema matching using Gaussian mixture models with Wasserstein distance. (arXiv:2111.14244v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.14244</id>
        <link href="http://arxiv.org/abs/2111.14244"/>
        <updated>2022-04-02T00:47:19.734Z</updated>
        <summary type="html"><![CDATA[Gaussian mixture models find their place as a powerful tool, mostly in the
clustering problem, but with proper preparation also in feature extraction,
pattern recognition, image segmentation and in general machine learning. When
faced with the problem of schema matching, different mixture models computed on
different pieces of data can maintain crucial information about the structure
of the dataset. In order to measure or compare results from mixture models, the
Wasserstein distance can be very useful, however it is not easy to calculate
for mixture distributions. In this paper we derive one of possible
approximations for the Wasserstein distance between Gaussian mixture models and
reduce it to linear problem. Furthermore, application examples concerning real
world data are shown.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Przyborowski_M/0/1/0/all/0/1"&gt;Mateusz Przyborowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pabis_M/0/1/0/all/0/1"&gt;Mateusz Pabi&amp;#x15b;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janusz_A/0/1/0/all/0/1"&gt;Andrzej Janusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1"&gt;Dominik &amp;#x15a;l&amp;#x119;zak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2022-04-02T00:47:19.732Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown an excellent ability to explicitly
learn the probability density function of data via a sequence of invertible
transformations. Yet, learning attentions in generative flows remains
understudied, while it has made breakthroughs in other domains. To fill the
gap, this paper introduces two types of invertible attention mechanisms, i.e.,
map-based and transformer-based attentions, for both unconditional and
conditional generative flows. The key idea is to exploit a masked scheme of
these two attentions to learn long-range data dependencies in the context of
generative flows. The masked scheme allows for invertible attention modules
with tractable Jacobian determinants, enabling its seamless integration at any
positions of the flow-based models. The proposed attention mechanisms lead to
more efficient generative flows, due to their capability of modeling the
long-term data dependencies. Evaluation on multiple image synthesis tasks shows
that the proposed attention flows result in efficient models and compare
favorably against the state-of-the-art unconditional and conditional generative
flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks. (arXiv:2203.17030v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.17030</id>
        <link href="http://arxiv.org/abs/2203.17030"/>
        <updated>2022-04-02T00:47:19.731Z</updated>
        <summary type="html"><![CDATA[New classes arise frequently in our ever-changing world, e.g., emerging
topics in social media and new types of products in e-commerce. A model should
recognize new classes and meanwhile maintain discriminability over old classes.
Under severe circumstances, only limited novel instances are available to
incrementally update the model. The task of recognizing few-shot new classes
without forgetting old classes is called few-shot class-incremental learning
(FSCIL). In this work, we propose a new paradigm for FSCIL based on
meta-learning by LearnIng Multi-phase Incremental Tasks (LIMIT), which
synthesizes fake FSCIL tasks from the base dataset. The data format of fake
tasks is consistent with the `real' incremental tasks, and we can build a
generalizable feature space for the unseen tasks through meta-learning.
Besides, LIMIT also constructs a calibration module based on transformer, which
calibrates the old class classifiers and new class prototypes into the same
scale and fills in the semantic gap. The calibration module also adaptively
contextualizes the instance-specific embedding with a set-to-set function.
LIMIT efficiently adapts to new classes and meanwhile resists forgetting over
old classes. Experiments on three benchmark datasets (CIFAR100, miniImageNet,
and CUB200) and large-scale dataset, i.e., ImageNet ILSVRC2012 validate that
LIMIT achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Da-Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Winners of the Reality TV Dating Show $\textit{The Bachelor}$ Using Machine Learning Algorithms. (arXiv:2203.16648v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16648</id>
        <link href="http://arxiv.org/abs/2203.16648"/>
        <updated>2022-04-02T00:47:19.728Z</updated>
        <summary type="html"><![CDATA[$\textit{The Bachelor}$ is a reality TV dating show in which a single
bachelor selects his wife from a pool of approximately 30 female contestants
over eight weeks of filming (American Broadcasting Company 2002). We collected
the following data on all 422 contestants that participated in seasons 11
through 25: their Age, Hometown, Career, Race, Week they got their first 1-on-1
date, whether they got the first impression rose, and what "place" they ended
up getting. We then trained three machine learning models to predict the ideal
characteristics of a successful contestant on $\textit{The Bachelor}$. The
three algorithms that we tested were: random forest classification, neural
networks, and linear regression. We found consistency across all three models,
although the neural network performed the best overall. Our models found that a
woman has the highest probability of progressing far on $\textit{The Bachelor}$
if she is: 26 years old, white, from the Northwest, works as an dancer,
received a 1-on-1 in week 6, and did not receive the First Impression Rose. Our
methodology is broadly applicable to all romantic reality television, and our
results will inform future $\textit{The Bachelor}$ production and contestant
strategies. While our models were relatively successful, we still encountered
high misclassification rates. This may be because: (1) Our training dataset had
fewer than 400 points or (2) Our models were too simple to parameterize the
complex romantic connections contestants forge over the course of a season.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1"&gt;Abigail J. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chesmore_G/0/1/0/all/0/1"&gt;Grace E. Chesmore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rocha_K/0/1/0/all/0/1"&gt;Kyle A. Rocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farah_A/0/1/0/all/0/1"&gt;Amanda Farah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayeed_M/0/1/0/all/0/1"&gt;Maryum Sayeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Myles_J/0/1/0/all/0/1"&gt;Justin Myles&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning. (arXiv:2203.16797v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16797</id>
        <link href="http://arxiv.org/abs/2203.16797"/>
        <updated>2022-04-02T00:47:19.728Z</updated>
        <summary type="html"><![CDATA[Physics-informed machine learning (PIML), referring to the combination of
prior knowledge of physics, which is the high level abstraction of natural
phenomenons and human behaviours in the long history, with data-driven machine
learning models, has emerged as an effective way to mitigate the shortage of
training data, to increase models' generalizability and to ensure the physical
plausibility of results. In this paper, we survey an abundant number of recent
works in PIML and summarize them from three aspects: (1) motivations of PIML,
(2) physics knowledge in PIML, (3) methods of physics knowledge integration in
PIML. We also discuss current challenges and corresponding research
opportunities in PIML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chuizheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Sungyong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griesemer_S/0/1/0/all/0/1"&gt;Sam Griesemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Derivation of Nesterov's Accelerated Gradient Algorithm from Optimal Control Theory. (arXiv:2203.17226v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2203.17226</id>
        <link href="http://arxiv.org/abs/2203.17226"/>
        <updated>2022-04-02T00:47:19.727Z</updated>
        <summary type="html"><![CDATA[Nesterov's accelerated gradient algorithm is derived from first principles.
The first principles are founded on the recently-developed optimal control
theory for optimization. This theory frames an optimization problem as an
optimal control problem whose trajectories generate various continuous-time
algorithms. The algorithmic trajectories satisfy the necessary conditions for
optimal control. The necessary conditions produce a controllable dynamical
system for accelerated optimization. Stabilizing this system via a quadratic
control Lyapunov function generates an ordinary differential equation. An Euler
discretization of the resulting differential equation produces Nesterov's
algorithm. In this context, this result solves the purported mystery
surrounding the algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ross_I/0/1/0/all/0/1"&gt;I. M. Ross&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load. (arXiv:2203.16637v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.16637</id>
        <link href="http://arxiv.org/abs/2203.16637"/>
        <updated>2022-04-02T00:47:19.726Z</updated>
        <summary type="html"><![CDATA[As a neurophysiological response to threat or adverse conditions, stress can
affect cognition, emotion and behaviour with potentially detrimental effects on
health in the case of sustained exposure. Since the affective content of speech
is inherently modulated by an individual's physical and mental state, a
substantial body of research has been devoted to the study of paralinguistic
correlates of stress-inducing task load. Historically, voice stress analysis
(VSA) has been conducted using conventional digital signal processing (DSP)
techniques. Despite the development of modern methods based on deep neural
networks (DNNs), accurately detecting stress in speech remains difficult due to
the wide variety of stressors and considerable variability in the individual
stress perception. To that end, we introduce a set of five datasets for task
load detection in speech. The voice recordings were collected as either
cognitive or physical stress was induced in the cohort of volunteers, with a
cumulative number of more than a hundred speakers. We used the datasets to
design and evaluate a novel self-supervised audio representation that leverages
the effectiveness of handcrafted features (DSP-based) and the complexity of
data-driven DNN representations. Notably, the proposed approach outperformed
both extensive handcrafted feature sets and novel DNN-based audio
representation learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elbanna_G/0/1/0/all/0/1"&gt;Gasser Elbanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biryukov_A/0/1/0/all/0/1"&gt;Alice Biryukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheidwasser_Clow_N/0/1/0/all/0/1"&gt;Neil Scheidwasser-Clow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orlandic_L/0/1/0/all/0/1"&gt;Lara Orlandic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mainar_P/0/1/0/all/0/1"&gt;Pablo Mainar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kegler_M/0/1/0/all/0/1"&gt;Mikolaj Kegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beckmann_P/0/1/0/all/0/1"&gt;Pierre Beckmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cernak_M/0/1/0/all/0/1"&gt;Milos Cernak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible and Efficient Contextual Bandits with Heterogeneous Treatment Effect Oracle. (arXiv:2203.16668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16668</id>
        <link href="http://arxiv.org/abs/2203.16668"/>
        <updated>2022-04-02T00:47:19.726Z</updated>
        <summary type="html"><![CDATA[Many popular contextual bandit algorithms estimate reward models to inform
decision making. However, true rewards can contain action-independent
redundancies that are not relevant for decision making and only increase the
statistical complexity of accurate estimation. It is sufficient and more
data-efficient to estimate the simplest function that explains the reward
differences between actions, that is, the heterogeneous treatment effect,
commonly understood to be more structured and simpler than the reward.
Motivated by this observation, building on recent work on oracle-based
algorithms, we design a statistically optimal and computationally efficient
algorithm using heterogeneous treatment effect estimation oracles. Our results
provide the first universal reduction of contextual bandits to a
general-purpose heterogeneous treatment effect estimation method. We show that
our approach is more robust to model misspecification than reward estimation
methods based on squared error regression oracles. Experimentally, we show the
benefits of heterogeneous treatment effect estimation in contextual bandits
over reward estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1"&gt;Aldo Gael Carranza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Sanath Kumar Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[System Identification via Nuclear Norm Regularization. (arXiv:2203.16673v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.16673</id>
        <link href="http://arxiv.org/abs/2203.16673"/>
        <updated>2022-04-02T00:47:19.726Z</updated>
        <summary type="html"><![CDATA[This paper studies the problem of identifying low-order linear systems via
Hankel nuclear norm regularization. Hankel regularization encourages the
low-rankness of the Hankel matrix, which maps to the low-orderness of the
system. We provide novel statistical analysis for this regularization and
carefully contrast it with the unregularized ordinary least-squares (OLS)
estimator. Our analysis leads to new bounds on estimating the impulse response
and the Hankel matrix associated with the linear system. We first design an
input excitation and show that Hankel regularization enables one to recover the
system using optimal number of observations in the true system order and
achieve strong statistical estimation rates. Surprisingly, we demonstrate that
the input design indeed matters, by showing that intuitive choices such as
i.i.d. Gaussian input leads to provably sub-optimal sample complexity. To
better understand the benefits of regularization, we also revisit the OLS
estimator. Besides refining existing bounds, we experimentally identify when
regularized approach improves over OLS: (1) For low-order systems with slow
impulse-response decay, OLS method performs poorly in terms of sample
complexity, (2) Hankel matrix returned by regularization has a more clear
singular value gap that ease identification of the system order, (3) Hankel
regularization is less sensitive to hyperparameter choice. Finally, we
establish model selection guarantees through a joint train-validation procedure
where we tune the regularization parameter for near-optimal estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yue Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oymak_S/0/1/0/all/0/1"&gt;Samet Oymak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fazel_M/0/1/0/all/0/1"&gt;Maryam Fazel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An unsupervised cluster-level based method for learning node representations of heterogeneous graphs in scientific papers. (arXiv:2203.16751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16751</id>
        <link href="http://arxiv.org/abs/2203.16751"/>
        <updated>2022-04-02T00:47:19.720Z</updated>
        <summary type="html"><![CDATA[Learning knowledge representation of scientific paper data is a problem to be
solved, and how to learn the representation of paper nodes in scientific paper
heterogeneous network is the core to solve this problem. This paper proposes an
unsupervised cluster-level scientific paper heterogeneous graph node
representation learning method (UCHL), aiming at obtaining the representation
of nodes (authors, institutions, papers, etc.) in the heterogeneous graph of
scientific papers. Based on the heterogeneous graph representation, this paper
performs link prediction on the entire heterogeneous graph and obtains the
relationship between the edges of the nodes, that is, the relationship between
papers and papers. Experiments results show that the proposed method achieves
excellent performance on multiple evaluation metrics on real scientific paper
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jie Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Meiyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhe Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Junping Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feifei_K/0/1/0/all/0/1"&gt;Kou Feifei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges in leveraging GANs for few-shot data augmentation. (arXiv:2203.16662v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.16662</id>
        <link href="http://arxiv.org/abs/2203.16662"/>
        <updated>2022-04-02T00:47:19.718Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the use of GAN-based few-shot data augmentation as
a method to improve few-shot classification performance. We perform an
exploration into how a GAN can be fine-tuned for such a task (one of which is
in a class-incremental manner), as well as a rigorous empirical investigation
into how well these models can perform to improve few-shot classification. We
identify issues related to the difficulty of training such generative models
under a purely supervised regime with very few examples, as well as issues
regarding the evaluation protocols of existing works. We also find that in this
regime, classification accuracy is highly sensitive to how the classes of the
dataset are randomly split. Therefore, we propose a semi-supervised fine-tuning
approach as a more pragmatic way forward to address these problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beckham_C/0/1/0/all/0/1"&gt;Christopher Beckham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laradji_I/0/1/0/all/0/1"&gt;Issam Laradji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rodriguez_P/0/1/0/all/0/1"&gt;Pau Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vazquez_D/0/1/0/all/0/1"&gt;David Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1"&gt;Derek Nowrouzezahrai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Architecture Search for Speech Emotion Recognition. (arXiv:2203.16928v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.16928</id>
        <link href="http://arxiv.org/abs/2203.16928"/>
        <updated>2022-04-02T00:47:19.716Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have brought significant advancements to speech emotion
recognition (SER). However, the architecture design in SER is mainly based on
expert knowledge and empirical (trial-and-error) evaluations, which is
time-consuming and resource intensive. In this paper, we propose to apply
neural architecture search (NAS) techniques to automatically configure the SER
models. To accelerate the candidate architecture optimization, we propose a
uniform path dropout strategy to encourage all candidate architecture
operations to be equally optimized. Experimental results of two different
neural structures on IEMOCAP show that NAS can improve SER performance (54.89\%
to 56.28\%) while maintaining model parameter sizes. The proposed dropout
strategy also shows superiority over the previous approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xixin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shoukang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xunying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acoustic-Net: A Novel Neural Network for Sound Localization and Quantification. (arXiv:2203.16988v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.16988</id>
        <link href="http://arxiv.org/abs/2203.16988"/>
        <updated>2022-04-02T00:47:19.716Z</updated>
        <summary type="html"><![CDATA[Acoustic source localization has been applied in different fields, such as
aeronautics and ocean science, generally using multiple microphones array data
to reconstruct the source location. However, the model-based beamforming
methods fail to achieve the high-resolution of conventional beamforming maps.
Deep neural networks are also appropriate to locate the sound source, but in
general, these methods with complex network structures are hard to be
recognized by hardware. In this paper, a novel neural network, termed the
Acoustic-Net, is proposed to locate and quantify the sound source simply using
the original signals. The experiments demonstrate that the proposed method
significantly improves the accuracy of sound source prediction and the
computing speed, which may generalize well to real data. The code and trained
models are available at https://github.com/JoaquinChou/Acoustic-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guanxing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xinghao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1"&gt;Xiaotong Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbas_S/0/1/0/all/0/1"&gt;Saqlain Abbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey of neural models for the automatic analysis of conversation: Towards a better integration of the social sciences. (arXiv:2203.16891v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.16891</id>
        <link href="http://arxiv.org/abs/2203.16891"/>
        <updated>2022-04-02T00:47:19.714Z</updated>
        <summary type="html"><![CDATA[Some exciting new approaches to neural architectures for the analysis of
conversation have been introduced over the past couple of years. These include
neural architectures for detecting emotion, dialogue acts, and sentiment
polarity. They take advantage of some of the key attributes of contemporary
machine learning, such as recurrent neural networks with attention mechanisms
and transformer-based approaches. However, while the architectures themselves
are extremely promising, the phenomena they have been applied to to date are
but a small part of what makes conversation engaging. In this paper we survey
these neural architectures and what they have been applied to. On the basis of
the social science literature, we then describe what we believe to be the most
fundamental and definitional feature of conversation, which is its
co-construction over time by two or more interlocutors. We discuss how neural
architectures of the sort surveyed could profitably be applied to these more
fundamental aspects of conversation, and what this buys us in terms of a better
analysis of conversation and even, in the longer term, a better way of
generating conversation for a conversational system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1"&gt;Chlo&amp;#xe9; Clavel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1"&gt;Matthieu Labeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassell_J/0/1/0/all/0/1"&gt;Justine Cassell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preventing Over-Smoothing for Hypergraph Neural Networks. (arXiv:2203.17159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.17159</id>
        <link href="http://arxiv.org/abs/2203.17159"/>
        <updated>2022-04-02T00:47:19.714Z</updated>
        <summary type="html"><![CDATA[In recent years, hypergraph learning has attracted great attention due to its
capacity in representing complex and high-order relationships. However, current
neural network approaches designed for hypergraphs are mostly shallow, thus
limiting their ability to extract information from high-order neighbors. In
this paper, we show both theoretically and empirically, that the performance of
hypergraph neural networks does not improve as the number of layers increases,
which is known as the over-smoothing problem. To tackle this issue, we develop
a new deep hypergraph convolutional network called Deep-HGCN, which can
maintain the heterogeneity of node representation in deep layers. Specifically,
we prove that a $k$-layer Deep-HGCN simulates a polynomial filter of order $k$
with arbitrary coefficients, which can relieve the problem of over-smoothing.
Experimental results on various datasets demonstrate the superior performance
of the proposed model comparing to the state-of-the-art hypergraph learning
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guanzi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiying Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from few examples with nonlinear feature maps. (arXiv:2203.16935v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16935</id>
        <link href="http://arxiv.org/abs/2203.16935"/>
        <updated>2022-04-02T00:47:19.694Z</updated>
        <summary type="html"><![CDATA[In this work we consider the problem of data classification in post-classical
settings were the number of training examples consists of mere few data points.
We explore the phenomenon and reveal key relationships between dimensionality
of AI model's feature space, non-degeneracy of data distributions, and the
model's generalisation capabilities. The main thrust of our present analysis is
on the influence of nonlinear feature transformations mapping original data
into higher- and possibly infinite-dimensional spaces on the resulting model's
generalisation capabilities. Subject to appropriate assumptions, we establish
new relationships between intrinsic dimensions of the transformed data and the
probabilities to learn successfully from few presentations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1"&gt;Ivan Y. Tyukin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_O/0/1/0/all/0/1"&gt;Oliver Sutton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1"&gt;Alexander N. Gorban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ransomware Detection using Process Memory. (arXiv:2203.16871v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2203.16871</id>
        <link href="http://arxiv.org/abs/2203.16871"/>
        <updated>2022-04-02T00:47:19.684Z</updated>
        <summary type="html"><![CDATA[Ransomware attacks have increased significantly in recent years, causing
great destruction and damage to critical systems and business operations.
Attackers are unfailingly finding innovative ways to bypass detection
mechanisms, whichencouraged the adoption of artificial intelligence. However,
most research summarizes the general features of AI and induces many false
positives, as the behavior of ransomware constantly differs to bypass
detection. Focusing on the key indicating features of ransomware becomes vital
as this guides the investigator to the inner workings and main function of
ransomware itself. By utilizing access privileges in process memory, the main
function of the ransomware can be detected more easily and accurately.
Furthermore, new signatures and fingerprints of ransomware families can be
identified to classify novel ransomware attacks correctly. The current research
used the process memory access privileges of the different memory regions of
the behavior of an executable to quickly determine its intent before serious
harm can occur. To achieve this aim, several well-known machine learning
algorithms were explored with an accuracy range of 81.38 to 96.28 percents. The
study thus confirms the feasibility of utilizing process memory as a detection
mechanism for ransomware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Avinash Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikuesan_R/0/1/0/all/0/1"&gt;Richard Adeyemi Ikuesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venter_H/0/1/0/all/0/1"&gt;Hein Venter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Optimal Control Method to Compute the Most Likely Transition Path for Stochastic Dynamical Systems with Jumps. (arXiv:2203.16874v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2203.16874</id>
        <link href="http://arxiv.org/abs/2203.16874"/>
        <updated>2022-04-02T00:47:19.675Z</updated>
        <summary type="html"><![CDATA[Many complex real world phenomena exhibit abrupt, intermittent or jumping
behaviors, which are more suitable to be described by stochastic differential
equations under non-Gaussian L\'evy noise. Among these complex phenomena, the
most likely transition paths between metastable states are important since
these rare events may have high impact in certain scenarios. Based on the large
deviation principle, the most likely transition path could be treated as the
minimizer of the rate function upon paths that connect two points. One of the
challenges to calculate the most likely transition path for stochastic
dynamical systems under non-Gaussian L\'evy noise is that the associated rate
function can not be explicitly expressed by paths. For this reason, we
formulate an optimal control problem to obtain the optimal state as the most
likely transition path. We then develop a neural network method to solve this
issue. Several experiments are investigated for both Gaussian and non-Gaussian
cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoli Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gao_T/0/1/0/all/0/1"&gt;Ting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinqiao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2203.16711</id>
        <link href="http://arxiv.org/abs/2203.16711"/>
        <updated>2022-04-02T00:47:19.645Z</updated>
        <summary type="html"><![CDATA[Parametrized quantum circuits can be used as quantum neural networks and have
the potential to outperform their classical counterparts when trained for
addressing learning problems. To date, much of the results on their performance
on practical problems are heuristic in nature. In particular, the convergence
rate for the training of quantum neural networks is not fully understood. Here,
we analyze the dynamics of gradient descent for the training error of a class
of variational quantum machine learning models. We define wide quantum neural
networks as parameterized quantum circuits in the limit of a large number of
qubits and variational parameters. We then find a simple analytic formula that
captures the average behavior of their loss function and discuss the
consequences of our findings. For example, for random quantum circuits, we
predict and characterize an exponential decay of the residual training error as
a function of the parameters of the system. We finally validate our analytic
results with numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Najafi_K/0/1/0/all/0/1"&gt;Khadijeh Najafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sharma_K/0/1/0/all/0/1"&gt;Kunal Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tacchino_F/0/1/0/all/0/1"&gt;Francesco Tacchino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Mezzacapo_A/0/1/0/all/0/1"&gt;Antonio Mezzacapo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech. (arXiv:2203.16852v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16852</id>
        <link href="http://arxiv.org/abs/2203.16852"/>
        <updated>2022-04-02T00:47:19.632Z</updated>
        <summary type="html"><![CDATA[In neural text-to-speech (TTS), two-stage system or a cascade of separately
learned models have shown synthesis quality close to human speech. For example,
FastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN
generates a raw waveform from a mel-spectogram where they are called an
acoustic feature generator and a neural vocoder respectively. However, their
training pipeline is somewhat cumbersome in that it requires a fine-tuning and
an accurate speech-text alignment for optimal performance. In this work, we
present end-to-end text-to-speech (E2E-TTS) model which has a simplified
training pipeline and outperforms a cascade of separately learned models.
Specifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN
with an alignment module. Since there is no acoustic feature mismatch between
training and inference, it does not requires fine-tuning. Furthermore, we
remove dependency on an external speech-text alignment tool by adopting an
alignment learning objective in our joint training framework. Experiments on
LJSpeech corpus shows that the proposed model outperforms publicly available,
state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS)
and some objective evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lim_D/0/1/0/all/0/1"&gt;Dan Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1"&gt;Sunghee Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_E/0/1/0/all/0/1"&gt;Eesung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven Prediction of Relevant Scenarios for Robust Optimization. (arXiv:2203.16642v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2203.16642</id>
        <link href="http://arxiv.org/abs/2203.16642"/>
        <updated>2022-04-02T00:47:19.618Z</updated>
        <summary type="html"><![CDATA[In this work we study robust one- and two-stage problems with discrete
uncertainty sets which are known to be hard to solve even if the underlying
deterministic problem is easy. Popular solution methods iteratively generate
scenario constraints and possibly second-stage variables. This way, by solving
a sequence of smaller problems, it is often possible to avoid the complexity of
considering all scenarios simultaneously. A key ingredient for the performance
of the iterative methods is a good selection of start scenarios. In this paper
we propose a data-driven heuristic to seed the iterative solution method with a
set of starting scenarios that provide a strong lower bound early in the
process, and result in considerably smaller overall solution times compared to
other benchmark methods. Our heuristic learns the relevance of a scenario by
extracting information from training data based on a combined similarity
measure between robust problem instances and single scenarios. Our experiments
show that predicting even a small number of good start scenarios by our method
can considerably reduce the computation time of the iterative methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Goerigk_M/0/1/0/all/0/1"&gt;Marc Goerigk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kurtz_J/0/1/0/all/0/1"&gt;Jannis Kurtz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially Adaptive Online Prediction of Piecewise Regular Functions. (arXiv:2203.16587v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2203.16587</id>
        <link href="http://arxiv.org/abs/2203.16587"/>
        <updated>2022-04-02T00:47:19.544Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating piecewise regular functions in an
online setting, i.e., the data arrive sequentially and at any round our task is
to predict the value of the true function at the next revealed point using the
available data from past predictions. We propose a suitably modified version of
a recently developed online learning algorithm called the sleeping experts
aggregation algorithm. We show that this estimator satisfies oracle risk bounds
simultaneously for all local regions of the domain. As concrete instantiations
of the expert aggregation algorithm proposed here, we study an online mean
aggregation and an online linear regression aggregation algorithm where experts
correspond to the set of dyadic subrectangles of the domain. The resulting
algorithms are near linear time computable in the sample size. We specifically
focus on the performance of these online algorithms in the context of
estimating piecewise polynomial and bounded variation function classes in the
fixed design setup. The simultaneous oracle risk bounds we obtain for these
estimators in this context provide new and improved (in certain aspects)
guarantees even in the batch setting and are not available for the state of the
art batch learning estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Sabyasachi Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Goswami_S/0/1/0/all/0/1"&gt;Subhajit Goswami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Differential Relational Privacy and its use in Question Answering. (arXiv:2203.16701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16701</id>
        <link href="http://arxiv.org/abs/2203.16701"/>
        <updated>2022-04-02T00:47:19.531Z</updated>
        <summary type="html"><![CDATA[Memorization of the relation between entities in a dataset can lead to
privacy issues when using a trained model for question answering. We introduce
Relational Memorization (RM) to understand, quantify and control this
phenomenon. While bounding general memorization can have detrimental effects on
the performance of a trained model, bounding RM does not prevent effective
learning. The difference is most pronounced when the data distribution is
long-tailed, with many queries having only few training examples: Impeding
general memorization prevents effective learning, while impeding only
relational memorization still allows learning general properties of the
underlying concepts. We formalize the notion of Relational Privacy (RP) and,
inspired by Differential Privacy (DP), we provide a possible definition of
Differential Relational Privacy (DrP). These notions can be used to describe
and compute bounds on the amount of RM in a trained model. We illustrate
Relational Privacy concepts in experiments with large-scale models for Question
Answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bombari_S/0/1/0/all/0/1"&gt;Simone Bombari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yusheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kunwar Yashraj Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1"&gt;Srikar Appalaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1"&gt;Vijay Mahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wind Farm Layout Optimisation using Set Based Multi-objective Bayesian Optimisation. (arXiv:2203.17065v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.17065</id>
        <link href="http://arxiv.org/abs/2203.17065"/>
        <updated>2022-04-02T00:47:19.529Z</updated>
        <summary type="html"><![CDATA[Wind energy is one of the cleanest renewable electricity sources and can help
in addressing the challenge of climate change. One of the drawbacks of
wind-generated energy is the large space necessary to install a wind farm; this
arises from the fact that placing wind turbines in a limited area would hinder
their productivity and therefore not be economically convenient. This naturally
leads to an optimisation problem, which has three specific challenges: (1)
multiple conflicting objectives (2) computationally expensive simulation models
and (3) optimisation over design sets instead of design vectors. The first and
second challenges can be addressed by using surrogate-assisted e.g.\ Bayesian
multi-objective optimisation. However, the traditional Bayesian optimisation
cannot be applied as the optimisation function in the problem relies on design
sets instead of design vectors. This paper extends the applicability of
Bayesian multi-objective optimisation to set based optimisation for solving the
wind farm layout problem. We use a set-based kernel in Gaussian process to
quantify the correlation between wind farms (with a different number of
turbines). The results on the given data set of wind energy and direction
clearly show the potential of using set-based Bayesian multi-objective
optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chugh_T/0/1/0/all/0/1"&gt;Tinkle Chugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ymeraj_E/0/1/0/all/0/1"&gt;Endi Ymeraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization. (arXiv:2107.12438v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12438</id>
        <link href="http://arxiv.org/abs/2107.12438"/>
        <updated>2022-04-02T00:47:19.529Z</updated>
        <summary type="html"><![CDATA[Motivated by the poor performance of cross-validation in settings where data
are scarce, we propose a novel estimator of the out-of-sample performance of a
policy in data-driven optimization.Our approach exploits the optimization
problem's sensitivity analysis to estimate the gradient of the optimal
objective value with respect to the amount of noise in the data and uses the
estimated gradient to debias the policy's in-sample performance. Unlike
cross-validation techniques, our approach avoids sacrificing data for a test
set, utilizes all data when training and, hence, is well-suited to settings
where data are scarce. We prove bounds on the bias and variance of our
estimator for optimization problems with uncertain linear objectives but known,
potentially non-convex, feasible regions. For more specialized optimization
problems where the feasible region is "weakly-coupled" in a certain sense, we
prove stronger results. Specifically, we provide explicit high-probability
bounds on the error of our estimator that hold uniformly over a policy class
and depends on the problem's dimension and policy class's complexity. Our
bounds show that under mild conditions, the error of our estimator vanishes as
the dimension of the optimization problem grows, even if the amount of
available data remains small and constant. Said differently, we prove our
estimator performs well in the small-data, large-scale regime. Finally, we
numerically compare our proposed method to state-of-the-art approaches through
a case-study on dispatching emergency medical response services using real
data. Our method provides more accurate estimates of out-of-sample performance
and learns better-performing policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vishal Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_M/0/1/0/all/0/1"&gt;Michael Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rusmevichientong_P/0/1/0/all/0/1"&gt;Paat Rusmevichientong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recovering models of open quantum systems from data via polynomial optimization: Towards globally convergent quantum system identification. (arXiv:2203.17164v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2203.17164</id>
        <link href="http://arxiv.org/abs/2203.17164"/>
        <updated>2022-04-02T00:47:19.521Z</updated>
        <summary type="html"><![CDATA[Current quantum devices suffer imperfections as a result of fabrication, as
well as noise and dissipation as a result of coupling to their immediate
environments. Because of this, it is often difficult to obtain accurate models
of their dynamics from first principles. An alternative is to extract such
models from time-series measurements of their behavior. Here, we formulate this
system-identification problem as a polynomial optimization problem. Recent
advances in optimization have provided globally convergent solvers for this
class of problems, which using our formulation prove estimates of the Kraus map
or the Lindblad equation. We include an overview of the state-of-the-art
algorithms, bounds, and convergence rates, and illustrate the use of this
approach to modeling open quantum systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bondar_D/0/1/0/all/0/1"&gt;Denys I. Bondar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Popovych_Z/0/1/0/all/0/1"&gt;Zakhar Popovych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jacobs_K/0/1/0/all/0/1"&gt;Kurt Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Korpas_G/0/1/0/all/0/1"&gt;Georgios Korpas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Marecek_J/0/1/0/all/0/1"&gt;Jakub Marecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping. (arXiv:2203.16749v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.16749</id>
        <link href="http://arxiv.org/abs/2203.16749"/>
        <updated>2022-04-02T00:47:19.470Z</updated>
        <summary type="html"><![CDATA[Neural vocoder using denoising diffusion probabilistic model (DDPM) has been
improved by adaptation of the diffusion noise distribution to given acoustic
features. In this study, we propose SpecGrad that adapts the diffusion noise so
that its time-varying spectral envelope becomes close to the conditioning
log-mel spectrogram. This adaptation by time-varying filtering improves the
sound quality especially in the high-frequency bands. It is processed in the
time-frequency domain to keep the computational cost almost the same as the
conventional DDPM-based neural vocoders. Experimental results showed that
SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based
neural vocoders in both analysis-synthesis and speech enhancement scenarios.
Audio demos are available at wavegrad.github.io/specgrad/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Koizumi_Y/0/1/0/all/0/1"&gt;Yuma Koizumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zen_H/0/1/0/all/0/1"&gt;Heiga Zen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yatabe_K/0/1/0/all/0/1"&gt;Kohei Yatabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nanxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bacchiani_M/0/1/0/all/0/1"&gt;Michiel Bacchiani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05630</id>
        <link href="http://arxiv.org/abs/2006.05630"/>
        <updated>2022-04-02T00:47:19.450Z</updated>
        <summary type="html"><![CDATA[Policy learning using historical observational data is an important problem
that has found widespread applications. Examples include selecting offers,
prices, advertisements to send to customers, as well as selecting which
medication to prescribe to a patient. However, existing literature rests on the
crucial assumption that the future environment where the learned policy will be
deployed is the same as the past environment that has generated the data -- an
assumption that is often false or too coarse an approximation. In this paper,
we lift this assumption and aim to learn a distributionally robust policy with
incomplete observational data. We first present a policy evaluation procedure
that allows us to assess how well the policy does under the worst-case
environment shift. We then establish a central limit theorem type guarantee for
this proposed policy evaluation scheme. Leveraging this evaluation scheme, we
further propose a novel learning algorithm that is able to learn a policy that
is robust to adversarial perturbations and unknown covariate shifts with a
performance guarantee based on the theory of uniform convergence. Finally, we
empirically test the effectiveness of our proposed algorithm in synthetic
datasets and demonstrate that it provides the robustness that is missing using
standard policy learning algorithms. We conclude the paper by providing a
comprehensive application of our methods in the context of a real-world voting
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1"&gt;Nian Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An energy-based deep splitting method for the nonlinear filtering problem. (arXiv:2203.17153v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2203.17153</id>
        <link href="http://arxiv.org/abs/2203.17153"/>
        <updated>2022-04-02T00:47:19.332Z</updated>
        <summary type="html"><![CDATA[The main goal of this paper is to approximately solve the nonlinear filtering
problem through deep learning. This is achieved by solving the Zakai equation
by a deep splitting method, previously developed for approximate solution of
(stochastic) partial differential equations. This is combined with an
energy-based model for the approximation of functions by a deep neural network.
This results in a computationally fast filter that takes observations as input
and that does not require re-training when new observations are received. The
method is tested on three examples, one linear Gaussian and two nonlinear. The
method shows promising performance when benchmarked against the Kalman filter
and the bootstrap particle filter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Baagmark_K/0/1/0/all/0/1"&gt;Kasper B&amp;#xe5;gmark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Andersson_A/0/1/0/all/0/1"&gt;Adam Andersson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Larsson_S/0/1/0/all/0/1"&gt;Stig Larsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommender Systems meet Mechanism Design. (arXiv:2110.12558v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.12558</id>
        <link href="http://arxiv.org/abs/2110.12558"/>
        <updated>2022-04-02T00:47:19.332Z</updated>
        <summary type="html"><![CDATA[Machine learning has developed a variety of tools for learning and
representing high-dimensional distributions with structure. Recent years have
also seen big advances in designing multi-item mechanisms. Akin to overfitting,
however, these mechanisms can be extremely sensitive to the Bayesian prior that
they target, which becomes problematic when that prior is only approximately
known. At the same time, even if access to the exact Bayesian prior is given,
it is known that optimal or even approximately optimal multi-item mechanisms
run into sample, computational, representation and communication intractability
barriers.

We consider a natural class of multi-item mechanism design problems with very
large numbers of items, but where the bidders' value distributions can be
well-approximated by a topic model akin to those used in recommendation systems
with very large numbers of possible recommendations. We propose a mechanism
design framework for this setting, building on a recent robustification
framework by Brustle et al., which disentangles the statistical challenge of
estimating a multi-dimensional prior from the task of designing a good
mechanism for it, and robustifies the performance of the latter against the
estimation error of the former. We provide an extension of this framework
appropriate for our setting, which allows us to exploit the expressive power of
topic models to reduce the effective dimensionality of the mechanism design
problem and remove the dependence of its computational, communication and
representation complexity on the number of items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1"&gt;Constantinos Daskalakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unifying Framework for Reinforcement Learning and Planning. (arXiv:2006.15009v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15009</id>
        <link href="http://arxiv.org/abs/2006.15009"/>
        <updated>2022-04-02T00:47:19.034Z</updated>
        <summary type="html"><![CDATA[Sequential decision making, commonly formalized as optimization of a Markov
Decision Process, is a key challenge in artificial intelligence. Two successful
approaches to MDP optimization are reinforcement learning and planning, which
both largely have their own research communities. However, if both research
fields solve the same problem, then we might be able to disentangle the common
factors in their solution approaches. Therefore, this paper presents a unifying
algorithmic framework for reinforcement learning and planning (FRAP), which
identifies underlying dimensions on which MDP planning and learning algorithms
have to decide. At the end of the paper, we compare a variety of well-known
planning, model-free and model-based RL algorithms along these dimensions.
Altogether, the framework may help provide deeper insight in the algorithmic
design space of planning and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1"&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broekens_J/0/1/0/all/0/1"&gt;Joost Broekens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1"&gt;Aske Plaat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1"&gt;Catholijn M. Jonker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrating constitutive models with full-field data via physics informed neural networks. (arXiv:2203.16577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16577</id>
        <link href="http://arxiv.org/abs/2203.16577"/>
        <updated>2022-04-02T00:47:18.841Z</updated>
        <summary type="html"><![CDATA[The calibration of solid constitutive models with full-field experimental
data is a long-standing challenge, especially in materials which undergo large
deformation. In this paper, we propose a physics-informed deep-learning
framework for the discovery of constitutive model parameterizations given
full-field displacement data and global force-displacement data. Contrary to
the majority of recent literature in this field, we work with the weak form of
the governing equations rather than the strong form to impose physical
constraints upon the neural network predictions. The approach presented in this
paper is computationally efficient, suitable for irregular geometric domains,
and readily ingests displacement data without the need for interpolation onto a
computational grid. A selection of canonical hyperelastic materials models
suitable for different material classes is considered including the
Neo-Hookean, Gent, and Blatz-Ko constitutive models as exemplars for general
hyperelastic behavior, polymer behavior with lock-up, and compressible foam
behavior respectively. We demonstrate that physics informed machine learning is
an enabling technology and may shift the paradigm of how full-field
experimental data is utilized to calibrate constitutive models under finite
deformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamel_C/0/1/0/all/0/1"&gt;Craig M. Hamel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_K/0/1/0/all/0/1"&gt;Kevin N. Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kramer_S/0/1/0/all/0/1"&gt;Sharlotte L.B. Kramer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mind the gap: Challenges of deep learning approaches to Theory of Mind. (arXiv:2203.16540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16540</id>
        <link href="http://arxiv.org/abs/2203.16540"/>
        <updated>2022-04-02T00:47:18.834Z</updated>
        <summary type="html"><![CDATA[Theory of Mind is an essential ability of humans to infer the mental states
of others. Here we provide a coherent summary of the potential, current
progress, and problems of deep learning approaches to Theory of Mind. We
highlight that many current findings can be explained through shortcuts. These
shortcuts arise because the tasks used to investigate Theory of Mind in deep
learning systems have been too narrow. Thus, we encourage researchers to
investigate Theory of Mind in complex open-ended environments. Furthermore, to
inspire future deep learning systems we provide a concise overview of prior
work done in humans. We further argue that when studying Theory of Mind with
deep learning, the research's main focus and contribution ought to be opening
up the network's representations. We recommend researchers use tools from the
field of interpretability of AI to study the relationship between different
network components and aspects of Theory of Mind.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aru_J/0/1/0/all/0/1"&gt;Jaan Aru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labash_A/0/1/0/all/0/1"&gt;Aqeel Labash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corcoll_O/0/1/0/all/0/1"&gt;Oriol Corcoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_R/0/1/0/all/0/1"&gt;Raul Vicente&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fast and Convergent Proximal Algorithm for Regularized Nonconvex and Nonsmooth Bi-level Optimization. (arXiv:2203.16615v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16615</id>
        <link href="http://arxiv.org/abs/2203.16615"/>
        <updated>2022-04-02T00:47:18.783Z</updated>
        <summary type="html"><![CDATA[Many important machine learning applications involve regularized nonconvex
bi-level optimization. However, the existing gradient-based bi-level
optimization algorithms cannot handle nonconvex or nonsmooth regularizers, and
they suffer from a high computation complexity in nonconvex bi-level
optimization. In this work, we study a proximal gradient-type algorithm that
adopts the approximate implicit differentiation (AID) scheme for nonconvex
bi-level optimization with possibly nonconvex and nonsmooth regularizers. In
particular, the algorithm applies the Nesterov's momentum to accelerate the
computation of the implicit gradient involved in AID. We provide a
comprehensive analysis of the global convergence properties of this algorithm
through identifying its intrinsic potential function. In particular, we
formally establish the convergence of the model parameters to a critical point
of the bi-level problem, and obtain an improved computation complexity
$\mathcal{O}(\kappa^{3.5}\epsilon^{-2})$ over the state-of-the-art result.
Moreover, we analyze the asymptotic convergence rates of this algorithm under a
class of local nonconvex geometries characterized by a {\L}ojasiewicz-type
gradient inequality. Experiment on hyper-parameter optimization demonstrates
the effectiveness of our algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Ziyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yi Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Differential Relational Privacy and its use in Question Answering. (arXiv:2203.16701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16701</id>
        <link href="http://arxiv.org/abs/2203.16701"/>
        <updated>2022-04-02T00:47:18.726Z</updated>
        <summary type="html"><![CDATA[Memorization of the relation between entities in a dataset can lead to
privacy issues when using a trained model for question answering. We introduce
Relational Memorization (RM) to understand, quantify and control this
phenomenon. While bounding general memorization can have detrimental effects on
the performance of a trained model, bounding RM does not prevent effective
learning. The difference is most pronounced when the data distribution is
long-tailed, with many queries having only few training examples: Impeding
general memorization prevents effective learning, while impeding only
relational memorization still allows learning general properties of the
underlying concepts. We formalize the notion of Relational Privacy (RP) and,
inspired by Differential Privacy (DP), we provide a possible definition of
Differential Relational Privacy (DrP). These notions can be used to describe
and compute bounds on the amount of RM in a trained model. We illustrate
Relational Privacy concepts in experiments with large-scale models for Question
Answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bombari_S/0/1/0/all/0/1"&gt;Simone Bombari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yusheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kunwar Yashraj Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1"&gt;Srikar Appalaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1"&gt;Vijay Mahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flexible and Efficient Contextual Bandits with Heterogeneous Treatment Effect Oracle. (arXiv:2203.16668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16668</id>
        <link href="http://arxiv.org/abs/2203.16668"/>
        <updated>2022-04-02T00:47:18.378Z</updated>
        <summary type="html"><![CDATA[Many popular contextual bandit algorithms estimate reward models to inform
decision making. However, true rewards can contain action-independent
redundancies that are not relevant for decision making and only increase the
statistical complexity of accurate estimation. It is sufficient and more
data-efficient to estimate the simplest function that explains the reward
differences between actions, that is, the heterogeneous treatment effect,
commonly understood to be more structured and simpler than the reward.
Motivated by this observation, building on recent work on oracle-based
algorithms, we design a statistically optimal and computationally efficient
algorithm using heterogeneous treatment effect estimation oracles. Our results
provide the first universal reduction of contextual bandits to a
general-purpose heterogeneous treatment effect estimation method. We show that
our approach is more robust to model misspecification than reward estimation
methods based on squared error regression oracles. Experimentally, we show the
benefits of heterogeneous treatment effect estimation in contextual bandits
over reward estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1"&gt;Aldo Gael Carranza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1"&gt;Sanath Kumar Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo Tree Search based Hybrid Optimization of Variational Quantum Circuits. (arXiv:2203.16707v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2203.16707</id>
        <link href="http://arxiv.org/abs/2203.16707"/>
        <updated>2022-04-02T00:47:18.372Z</updated>
        <summary type="html"><![CDATA[Variational quantum algorithms stand at the forefront of simulations on
near-term and future fault-tolerant quantum devices. While most variational
quantum algorithms involve only continuous optimization variables, the
representational power of the variational ansatz can sometimes be significantly
enhanced by adding certain discrete optimization variables, as is exemplified
by the generalized quantum approximate optimization algorithm (QAOA). However,
the hybrid discrete-continuous optimization problem in the generalized QAOA
poses a challenge to the optimization. We propose a new algorithm called
MCTS-QAOA, which combines a Monte Carlo tree search method with an improved
natural policy gradient solver to optimize the discrete and continuous
variables in the quantum circuit, respectively. We find that MCTS-QAOA has
excellent noise-resilience properties and outperforms prior algorithms in
challenging instances of the generalized QAOA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jiahao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoya Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bukov_M/0/1/0/all/0/1"&gt;Marin Bukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ying_L/0/1/0/all/0/1"&gt;Lexing Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent improvements of ASR models in the face of adversarial attacks. (arXiv:2203.16536v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2203.16536</id>
        <link href="http://arxiv.org/abs/2203.16536"/>
        <updated>2022-04-02T00:47:18.330Z</updated>
        <summary type="html"><![CDATA[Like many other tasks involving neural networks, Speech Recognition models
are vulnerable to adversarial attacks. However recent research has pointed out
differences between attacks and defenses on ASR models compared to image
models. Improving the robustness of ASR models requires a paradigm shift from
evaluating attacks on one or a few models to a systemic approach in evaluation.
We lay the ground for such research by evaluating on various architectures a
representative set of adversarial attacks: targeted and untargeted,
optimization and speech processing-based, white-box, black-box and targeted
attacks. Our results show that the relative strengths of different attack
algorithms vary considerably when changing the model architecture, and that the
results of some attacks are not to be blindly trusted. They also indicate that
training choices such as self-supervised pretraining can significantly impact
robustness by enabling transferable perturbations. We release our source code
as a package that should help future research in evaluating their attacks and
defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olivier_R/0/1/0/all/0/1"&gt;Raphael Olivier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel framework for Dynamic Domain Decomposition of Data Assimilation problems a case study on Kalman Filter algorithm. (arXiv:2203.16535v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16535</id>
        <link href="http://arxiv.org/abs/2203.16535"/>
        <updated>2022-04-02T00:47:18.324Z</updated>
        <summary type="html"><![CDATA[We focus on Partial Differential Equation (PDE) based Data Assimilatio
problems (DA) solved by means of variational approaches and Kalman filter
algorithm. Recently, we presented a Domain Decomposition framework (we call it
DD-DA, for short) performing a decomposition of the whole physical domain along
space and time directions, and joining the idea of Schwarz' methods and
parallel in time approaches. For effective parallelization of DD-DA algorithms,
the computational load assigned to subdomains must be equally distributed.
Usually computational cost is proportional to the amount of data entities
assigned to partitions. Good quality partitioning also requires the volume of
communication during calculation to be kept at its minimum. In order to deal
with DD-DA problems where the observations are nonuniformly distributed and
general sparse, in the present work we employ a parallel load balancing
algorithm based on adaptive and dynamic defining of boundaries of DD -- which
is aimed to balance workload according to data location. We call it DyDD. As
the numerical model underlying DA problems arising from the so-called
discretize-then-optimize approach is the constrained least square model (CLS),
we will use CLS as a reference state estimation problem and we validate DyDD on
different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cacciapuoti_R/0/1/0/all/0/1"&gt;Rosalba Cacciapuoti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmore_L/0/1/0/all/0/1"&gt;Luisa D&amp;#x27;Amore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning for Computationally Efficient Distribution of Binary Evolution Simulations. (arXiv:2203.16683v1 [astro-ph.SR])]]></title>
        <id>http://arxiv.org/abs/2203.16683</id>
        <link href="http://arxiv.org/abs/2203.16683"/>
        <updated>2022-04-02T00:47:18.317Z</updated>
        <summary type="html"><![CDATA[Binary stars undergo a variety of interactions and evolutionary phases,
critical for predicting and explaining observed properties. Binary population
synthesis with full stellar-structure and evolution simulations are
computationally expensive requiring a large number of mass-transfer sequences.
The recently developed binary population synthesis code POSYDON incorporates
grids of MESA binary star simulations which are then interpolated to model
large-scale populations of massive binaries. The traditional method of
computing a high-density rectilinear grid of simulations is not scalable for
higher-dimension grids, accounting for a range of metallicities, rotation, and
eccentricity. We present a new active learning algorithm, psy-cris, which uses
machine learning in the data-gathering process to adaptively and iteratively
select targeted simulations to run, resulting in a custom, high-performance
training set. We test psy-cris on a toy problem and find the resulting training
sets require fewer simulations for accurate classification and regression than
either regular or randomly sampled grids. We further apply psy-cris to the
target problem of building a dynamic grid of MESA simulations, and we
demonstrate that, even without fine tuning, a simulation set of only $\sim 1/4$
the size of a rectilinear grid is sufficient to achieve the same classification
accuracy. We anticipate further gains when algorithmic parameters are optimized
for the targeted application. We find that optimizing for classification only
may lead to performance losses in regression, and vice versa. Lowering the
computational cost of producing grids will enable future versions of POSYDON to
cover more input parameters while preserving interpolation accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rocha_K/0/1/0/all/0/1"&gt;Kyle Akira Rocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Andrews_J/0/1/0/all/0/1"&gt;Jeff J. Andrews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Berry_C/0/1/0/all/0/1"&gt;Christopher P. L. Berry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Doctor_Z/0/1/0/all/0/1"&gt;Zoheyr Doctor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Marchant_P/0/1/0/all/0/1"&gt;Pablo Marchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kalogera_V/0/1/0/all/0/1"&gt;Vicky Kalogera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Coughlin_S/0/1/0/all/0/1"&gt;Scott Coughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bavera_S/0/1/0/all/0/1"&gt;Simone S. Bavera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Dotter_A/0/1/0/all/0/1"&gt;Aaron Dotter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Fragos_T/0/1/0/all/0/1"&gt;Tassos Fragos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kovlakas_K/0/1/0/all/0/1"&gt;Konstantinos Kovlakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Misra_D/0/1/0/all/0/1"&gt;Devina Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Xing_Z/0/1/0/all/0/1"&gt;Zepei Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Zapartas_E/0/1/0/all/0/1"&gt;Emmanouil Zapartas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-constrained Unsupervised Learning of Partial Differential Equations using Meshes. (arXiv:2203.16628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16628</id>
        <link href="http://arxiv.org/abs/2203.16628"/>
        <updated>2022-04-02T00:47:18.309Z</updated>
        <summary type="html"><![CDATA[Enhancing neural networks with knowledge of physical equations has become an
efficient way of solving various physics problems, from fluid flow to
electromagnetism. Graph neural networks show promise in accurately representing
irregularly meshed objects and learning their dynamics, but have so far
required supervision through large datasets. In this work, we represent meshes
naturally as graphs, process these using Graph Networks, and formulate our
physics-based loss to provide an unsupervised learning framework for partial
differential equations (PDE). We quantitatively compare our results to a
classical numerical PDE solver, and show that our computationally efficient
approach can be used as an interactive PDE solver that is adjusting boundary
conditions in real-time and remains sufficiently close to the baseline
solution. Our inherently differentiable framework will enable the application
of PDE solvers in interactive settings, such as model-based control of
soft-body deformations, or in gradient-based optimization methods that require
a fully differentiable pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michelis_M/0/1/0/all/0/1"&gt;Mike Y. Michelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katzschmann_R/0/1/0/all/0/1"&gt;Robert K. Katzschmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.16622</id>
        <link href="http://arxiv.org/abs/2203.16622"/>
        <updated>2022-04-02T00:47:18.291Z</updated>
        <summary type="html"><![CDATA[We evaluate the performance of federated learning (FL) in developing deep
learning models for analysis of digitized tissue sections. A classification
application was considered as the example use case, on quantifiying the
distribution of tumor infiltrating lymphocytes within whole slide images
(WSIs). A deep learning classification model was trained using 50*50 square
micron patches extracted from the WSIs. We simulated a FL environment in which
a dataset, generated from WSIs of cancer from numerous anatomical sites
available by The Cancer Genome Atlas repository, is partitioned in 8 different
nodes. Our results show that the model trained with the federated training
approach achieves similar performance, both quantitatively and qualitatively,
to that of a model trained with all the training data pooled at a centralized
location. Our study shows that FL has tremendous potential for enabling
development of more robust and accurate models for histopathology image
analysis without having to collect large and diverse training data at a single
location.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1"&gt;Ujjwal Baid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pati_S/0/1/0/all/0/1"&gt;Sarthak Pati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurc_T/0/1/0/all/0/1"&gt;Tahsin M. Kurc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rajarsi Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bremer_E/0/1/0/all/0/1"&gt;Erich Bremer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abousamra_S/0/1/0/all/0/1"&gt;Shahira Abousamra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thakur_S/0/1/0/all/0/1"&gt;Siddhesh P. Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saltz_J/0/1/0/all/0/1"&gt;Joel H. Saltz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1"&gt;Spyridon Bakas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Localness Transformer for Smart Sensor-Based Energy Disaggregation. (arXiv:2203.16537v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16537</id>
        <link href="http://arxiv.org/abs/2203.16537"/>
        <updated>2022-04-02T00:47:18.284Z</updated>
        <summary type="html"><![CDATA[Modern smart sensor-based energy management systems leverage non-intrusive
load monitoring (NILM) to predict and optimize appliance load distribution in
real-time. NILM, or energy disaggregation, refers to the decomposition of
electricity usage conditioned on the aggregated power signals (i.e., smart
sensor on the main channel). Based on real-time appliance power prediction
using sensory technology, energy disaggregation has great potential to increase
electricity efficiency and reduce energy expenditure. With the introduction of
transformer models, NILM has achieved significant improvements in predicting
device power readings. Nevertheless, transformers are less efficient due to
O(l^2) complexity w.r.t. sequence length l. Moreover, transformers can fail to
capture local signal patterns in sequence-to-point settings due to the lack of
inductive bias in local context. In this work, we propose an efficient
localness transformer for non-intrusive load monitoring (ELTransformer).
Specifically, we leverage normalization functions and switch the order of
matrix multiplication to approximate self-attention and reduce computational
complexity. Additionally, we introduce localness modeling with sparse local
attention heads and relative position encodings to enhance the model capacity
in extracting short-term local patterns. To the best of our knowledge,
ELTransformer is the first NILM model that addresses computational complexity
and localness modeling in NILM. With extensive experiments and quantitative
analyses, we demonstrate the efficiency and effectiveness of the the proposed
ELTransformer with considerable improvements compared to state-of-the-art
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zhenrui Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Huimin Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kou_Z/0/1/0/all/0/1"&gt;Ziyi Kou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1"&gt;Lanyu Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation of Speaker Representations Using Heterogeneous Training Batch Assembly. (arXiv:2203.16646v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.16646</id>
        <link href="http://arxiv.org/abs/2203.16646"/>
        <updated>2022-04-02T00:47:18.278Z</updated>
        <summary type="html"><![CDATA[In traditional speaker diarization systems, a well-trained speaker model is a
key component to extract representations from consecutive and partially
overlapping segments in a long speech session. To be more consistent with the
back-end segmentation and clustering, we propose a new CNN-based speaker
modeling scheme, which takes into account the heterogeneity of the speakers in
each training segment and batch. We randomly and synthetically augment the
training data into a set of segments, each of which contains more than one
speaker and some overlapping parts. A soft label is imposed on each segment
based on its speaker occupation ratio, and the standard cross entropy loss is
implemented in model training. In this way, the speaker model should have the
ability to generate a geometrically meaningful embedding for each multi-speaker
segment. Experimental results show that our system is superior to the baseline
system using x-vectors in two speaker diarization tasks. In the CALLHOME task
trained on the NIST SRE and Switchboard datasets, our system achieves a
relative reduction of 12.93% in DER. In Track 2 of CHiME-6, our system provides
13.24%, 12.60%, and 5.65% relative reductions in DER, JER, and WER,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yu-Huai Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-Shin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Pin-Tuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Refinement for Coreference Resolution. (arXiv:2203.16574v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.16574</id>
        <link href="http://arxiv.org/abs/2203.16574"/>
        <updated>2022-04-02T00:47:18.269Z</updated>
        <summary type="html"><![CDATA[The state-of-the-art models for coreference resolution are based on
independent mention pair-wise decisions. We propose a modelling approach that
learns coreference at the document-level and takes global decisions. For this
purpose, we model coreference links in a graph structure where the nodes are
tokens in the text, and the edges represent the relationship between them. Our
model predicts the graph in a non-autoregressive manner, then iteratively
refines it based on previous predictions, allowing global dependencies between
decisions. The experimental results show improvements over various baselines,
reinforcing the hypothesis that document-level information improves conference
resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miculicich_L/0/1/0/all/0/1"&gt;Lesly Miculicich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1"&gt;James Henderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges in leveraging GANs for few-shot data augmentation. (arXiv:2203.16662v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.16662</id>
        <link href="http://arxiv.org/abs/2203.16662"/>
        <updated>2022-04-02T00:47:18.263Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the use of GAN-based few-shot data augmentation as
a method to improve few-shot classification performance. We perform an
exploration into how a GAN can be fine-tuned for such a task (one of which is
in a class-incremental manner), as well as a rigorous empirical investigation
into how well these models can perform to improve few-shot classification. We
identify issues related to the difficulty of training such generative models
under a purely supervised regime with very few examples, as well as issues
regarding the evaluation protocols of existing works. We also find that in this
regime, classification accuracy is highly sensitive to how the classes of the
dataset are randomly split. Therefore, we propose a semi-supervised fine-tuning
approach as a more pragmatic way forward to address these problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beckham_C/0/1/0/all/0/1"&gt;Christopher Beckham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laradji_I/0/1/0/all/0/1"&gt;Issam Laradji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rodriguez_P/0/1/0/all/0/1"&gt;Pau Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vazquez_D/0/1/0/all/0/1"&gt;David Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1"&gt;Derek Nowrouzezahrai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Few-shot Class-incremental Learning. (arXiv:2203.16588v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16588</id>
        <link href="http://arxiv.org/abs/2203.16588"/>
        <updated>2022-04-02T00:47:18.256Z</updated>
        <summary type="html"><![CDATA[Continually learning new classes from fresh data without forgetting previous
knowledge of old classes is a very challenging research problem. Moreover, it
is imperative that such learning must respect certain memory and computational
constraints such as (i) training samples are limited to only a few per class,
(ii) the computational cost of learning a novel class remains constant, and
(iii) the memory footprint of the model grows at most linearly with the number
of classes observed. To meet the above constraints, we propose C-FSCIL, which
is architecturally composed of a frozen meta-learned feature extractor, a
trainable fixed-size fully connected layer, and a rewritable dynamically
growing memory that stores as many vectors as the number of encountered
classes. C-FSCIL provides three update modes that offer a trade-off between
accuracy and compute-memory cost of learning novel classes. C-FSCIL exploits
hyperdimensional embedding that allows to continually express many more classes
than the fixed dimensions in the vector space, with minimal interference. The
quality of class vector representations is further improved by aligning them
quasi-orthogonally to each other by means of novel loss functions. Experiments
on the CIFAR100, miniImageNet, and Omniglot datasets show that C-FSCIL
outperforms the baselines with remarkable accuracy and compression. It also
scales up to the largest problem size ever tried in this few-shot setting by
learning 423 novel classes on top of 1200 base classes with less than 1.6%
accuracy drop. Our code is available at
https://github.com/IBM/constrained-FSCIL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hersche_M/0/1/0/all/0/1"&gt;Michael Hersche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karunaratne_G/0/1/0/all/0/1"&gt;Geethan Karunaratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cherubini_G/0/1/0/all/0/1"&gt;Giovanni Cherubini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1"&gt;Abu Sebastian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1"&gt;Abbas Rahimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identification of diffracted vortex beams at different propagation distances using deep learning. (arXiv:2203.16539v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16539</id>
        <link href="http://arxiv.org/abs/2203.16539"/>
        <updated>2022-04-02T00:47:18.225Z</updated>
        <summary type="html"><![CDATA[Orbital angular momentum of light is regarded as a valuable resource in
quantum technology, especially in quantum communication and quantum sensing and
ranging. However, the OAM state of light is susceptible to undesirable
experimental conditions such as propagation distance and phase distortions,
which hinders the potential for the realistic implementation of relevant
technologies. In this article, we exploit an enhanced deep learning neural
network to identify different OAM modes of light at multiple propagation
distances with phase distortions. Specifically, our trained deep learning
neural network can efficiently identify the vortex beam's topological charge
and propagation distance with 97% accuracy. Our technique has important
implications for OAM based communication and sensing protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1"&gt;Heng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zi-Xiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Chunling Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Wu-Hao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenglong You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rui-Bo Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.16639</id>
        <link href="http://arxiv.org/abs/2203.16639"/>
        <updated>2022-04-02T00:47:18.216Z</updated>
        <summary type="html"><![CDATA[We present a meta-learning framework for learning new visual concepts
quickly, from just one or a few examples, guided by multiple naturally
occurring data streams: simultaneously looking at images, reading sentences
that describe the objects in the scene, and interpreting supplemental sentences
that relate the novel concept with other concepts. The learned concepts support
downstream applications, such as answering questions by reasoning about unseen
images. Our model, namely FALCON, represents individual visual concepts, such
as colors and shapes, as axis-aligned boxes in a high-dimensional space (the
"box embedding space"). Given an input image and its paired sentence, our model
first resolves the referential expression in the sentence and associates the
novel concept with particular objects in the scene. Next, our model interprets
supplemental sentences to relate the novel concept with other known concepts,
such as "X has property Y" or "X is a kind of Y". Finally, it infers an optimal
box embedding for the novel concept that jointly 1) maximizes the likelihood of
the observed instances in the image, and 2) satisfies the relationships between
the novel concepts and the known ones. We demonstrate the effectiveness of our
model on both synthetic and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mei_L/0/1/0/all/0/1"&gt;Lingjie Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer Language Models without Positional Encodings Still Learn Positional Information. (arXiv:2203.16634v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.16634</id>
        <link href="http://arxiv.org/abs/2203.16634"/>
        <updated>2022-04-02T00:47:18.191Z</updated>
        <summary type="html"><![CDATA[Transformers typically require some form of positional encoding, such as
positional embeddings, to process natural language sequences. Surprisingly, we
find that transformer language models without any explicit positional encoding
are still competitive with standard models, and that this phenomenon is robust
across different datasets, model sizes, and sequence lengths. Probing
experiments reveal that such models acquire an implicit notion of absolute
positions throughout the network, effectively compensating for the missing
information. We conjecture that causal attention enables the model to infer the
number of predecessors that each token can attend to, thereby approximating its
absolute position.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1"&gt;Adi Haviv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1"&gt;Ori Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1"&gt;Ofir Press&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1"&gt;Peter Izsak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1"&gt;Omer Levy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Approaches for Non-Intrusive Home Absence Detection Based on Appliance Electrical Use. (arXiv:2203.16538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.16538</id>
        <link href="http://arxiv.org/abs/2203.16538"/>
        <updated>2022-04-02T00:47:18.184Z</updated>
        <summary type="html"><![CDATA[Home absence detection is an emerging field on smart home installations.
Identifying whether or not the residents of the house are present, is important
in numerous scenarios. Possible scenarios include but are not limited to:
elderly people living alone, people suffering from dementia, home quarantine.
The majority of published papers focus on either pressure / door sensors or
cameras in order to detect outing events. Although the aforementioned
approaches provide solid results, they are intrusive and require modifications
for sensor placement. In our work, appliance electrical use is investigated as
a means for detecting the presence or absence of residents. The energy use is
the result of power disaggregation, a non intrusive / non invasive sensing
method. Since a dataset providing energy data and ground truth for home absence
is not available, artificial outing events were introduced on the UK-DALE
dataset, a well known dataset for Non Intrusive Load Monitoring (NILM). Several
machine learning algorithms were evaluated using the generated dataset.
Benchmark results have shown that home absence detection using appliance power
consumption is feasible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lentzas_A/0/1/0/all/0/1"&gt;Athanasios Lentzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vrakas_D/0/1/0/all/0/1"&gt;Dimitris Vrakas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any thought on: A universal parameter optimizer]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tu68ex/any_thought_on_a_universal_parameter_optimizer/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tu68ex/any_thought_on_a_universal_parameter_optimizer/"/>
        <updated>2022-04-02T00:45:21.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I have an thought on A universal parameter optimizer I wanted to share with you and to see if you know some related work. 
 Assume you have a simulation or access to an environment. There are certain parameters you can set to control the performance of a system which lives in this simulation/environment. Naturally, one wants to find the optimal parameters or optimal policy to set the parameter that can result the most reward, however that is defined. 
 For example, in the stock market, I may want to find the optimal market price to buy and sell, or the optimal policy. In a car driving game, I may want to determine the optimal policy to set speed and direction. 
 Do we know if there is formal way to study this type of problem? Thank you!
    submitted by    /u/DB8868  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeking respondents for a survey about AI text generation and reader interpretation of poetry]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tu4zyq/seeking_respondents_for_a_survey_about_ai_text/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tu4zyq/seeking_respondents_for_a_survey_about_ai_text/"/>
        <updated>2022-04-01T23:42:54.000Z</updated>
        <summary type="html"><![CDATA[Hello! I am doing an independent (non-academic) research study about AI text generation as relates to poetry and reader interpretation. The results of the study will be presented in a YouTube video. 
 I would really appreciate if some folks could take approximately 20-25 minutes to take this anonymous survey I put together. It involves reading some poems and answering questions about those poems. Thank you so much for the help! 
 https://form.jotform.com/220880249866062
    submitted by    /u/northern_frog  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Gentle Introduction to Decorators in Python]]></title>
        <id>https://machinelearningmastery.com/?p=13344</id>
        <link href="https://machinelearningmastery.com/a-gentle-introduction-to-decorators-in-python/"/>
        <updated>2022-04-01T23:00:40.000Z</updated>
        <summary type="html"><![CDATA[When working on code, whether we know it or not, we often come across the decorator design pattern. This is […]
The post A Gentle Introduction to Decorators in Python appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Zhe Ming Chng</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] How to add padding to an image in Pytorch?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tu41tl/p_how_to_add_padding_to_an_image_in_pytorch/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tu41tl/p_how_to_add_padding_to_an_image_in_pytorch/"/>
        <updated>2022-04-01T22:57:47.000Z</updated>
        <summary type="html"><![CDATA[Hi guys,
 I am trying to add padding to images in Pytorch - I need to standardize all the images in my dataset to be of the same size. I spent the whole day trying to find a good solution but nothing worked. I succeeded in resizing but that compromised my image quality, so that is why I want to proceed with padding. How to do this?
 Thanks in advance! :)
    submitted by    /u/whyhateverything  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] SSO (Single Sign-On) for CVAT, the labelling tool]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tu3qn2/p_sso_single_signon_for_cvat_the_labelling_tool/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tu3qn2/p_sso_single_signon_for_cvat_the_labelling_tool/"/>
        <updated>2022-04-01T22:43:11.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 For quite a long time, I have seen folks looking for a way to do SSO (Single Sign-On) for CVAT, a popular labeling tool. But unfortunately such capability is not readily available. It only supports local authentication and LDAP.
 So we decided to make a change proactively, and now we are in the process of enabling SSO for it. The initial result looks promising. Check it out to see what we have done: https://www.youtube.com/watch?v=R7hBBLG5Fdc
 Is this something that you would love to have?
 Any other machine learning tools that you want to have SSO capability as well?
 Any feedback are welcome.
    submitted by    /u/alexcgg1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Current algorithms consistently outperforming SAC and PPO]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tu3qbd/d_current_algorithms_consistently_outperforming/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tu3qbd/d_current_algorithms_consistently_outperforming/"/>
        <updated>2022-04-01T22:42:46.000Z</updated>
        <summary type="html"><![CDATA[Hi community. It has been 5 years now since these algorithms were released, and I don't feel like they have been quite replaced yet. In your opinion, do we currently have algorithms that make either of them obsolete in 2022?
    submitted by    /u/yannbouteiller  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Take Information Theory before the first course in machine learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tu2z97/d_take_information_theory_before_the_first_course/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tu2z97/d_take_information_theory_before_the_first_course/"/>
        <updated>2022-04-01T22:08:51.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I will study further Reinforcement Learning and Deep Learning in the future. I have completed probability theory, linear algebra, and multivariable calculus. I am taking Mathematical Statistics. Should I take Information Theory (IT) before ML? For me, I would definitely take IT, but I don't know whether to take it now or later.
    submitted by    /u/nwe2rw  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Token-Dropping Approach Used By ML Researchers From Google and NYU Reduces BERT Pretraining Time And Cost By 25%]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tu2gq4/the_tokendropping_approach_used_by_ml_researchers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tu2gq4/the_tokendropping_approach_used_by_ml_researchers/"/>
        <updated>2022-04-01T21:46:19.000Z</updated>
        <summary type="html"><![CDATA[The Pretraining of BERT-type large language models, which may scale up to billions of parameters, is essential to achieving best-in-class performance on various natural language processing (NLP) applications. However, the pretraining procedure is costly, and it has become a hurdle for the industrial deployment of big language models.
 In a research paper, researchers from Google, New York University, and the University of Maryland recommend a simple but effective “token dropping” method that drastically reduces the pretraining cost of transformer models like BERT while maintaining downstream fine-tuning performance.
 Token dropping is a technique for speeding up the pretraining of transformer models like BERT without sacrificing their performance on downstream tasks. Starting with an intermediate layer in the model, they eliminate uninteresting tokens to let the model focus on key tokens more effectively, given its limited computing resources. The model’s last layer then picks up the dropped tokens, producing full-length sequences. They use the built-in masked language modeling (MLM) loss and its dynamics to detect non-essential tokens with little computing complexity. According to their tests, this straightforward strategy decreases BERT’s pretraining cost by 25% while yielding somewhat higher overall fine-tuning performance on conventional downstream tasks.
 Continue Reading The Summary
 Paper: https://arxiv.org/pdf/2203.13240.pdf
 Github: https://github.com/tensorflow/models/tree/master/official/projects/token\_dropping
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top emerging artificial intelligence use cases]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tu0zk2/top_emerging_artificial_intelligence_use_cases/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tu0zk2/top_emerging_artificial_intelligence_use_cases/"/>
        <updated>2022-04-01T20:43:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Visionifyai  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Paper Explained - Improving Intrinsic Exploration with Language Abstractions (Full Video Analysis)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttyr0f/d_paper_explained_improving_intrinsic_exploration/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttyr0f/d_paper_explained_improving_intrinsic_exploration/"/>
        <updated>2022-04-01T19:06:41.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/NeGJAUSQEJI
 Exploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.
 ​
 OUTLINE:
 0:00 - Intro
 1:10 - Paper Overview: Language for exploration
 5:40 - The MiniGrid & MiniHack environments
 7:00 - Annotating states with language
 9:05 - Baseline algorithm: AMIGo
 12:20 - Adding language to AMIGo
 22:55 - Baseline algorithm: NovelD and Random Network Distillation
 29:45 - Adding language to NovelD
 31:50 - Aren't we just using extra data?
 34:55 - Investigating the experimental results
 40:45 - Final comments
 ​
 Paper: https://arxiv.org/abs/2202.08938
    submitted by    /u/ykilcher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating new molecules with graph grammar]]></title>
        <id>https://news.mit.edu/2022/generating-new-molecules-with-graph-grammar-0401</id>
        <link href="https://news.mit.edu/2022/generating-new-molecules-with-graph-grammar-0401"/>
        <updated>2022-04-01T18:30:00.000Z</updated>
        <summary type="html"><![CDATA[An efficient machine-learning method uses chemical knowledge to create a learnable grammar with production rules to build synthesizable monomers and polymers.]]></summary>
        <author>
            <name>Lauren Hinkel | MIT-IBM Watson AI Lab</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Are we running out of AI benchmarks?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttxn7q/n_are_we_running_out_of_ai_benchmarks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttxn7q/n_are_we_running_out_of_ai_benchmarks/"/>
        <updated>2022-04-01T18:19:42.000Z</updated>
        <summary type="html"><![CDATA[Benchmarks are an important way to measure progress in AI research – but artificial intelligence is constantly achieving new bests. Are we running out of AI benchmarks? 
 ...
 Researchers at the Medical University of Vienna and the University of Oxford now show in a meta-study of AI benchmarks that saturated or stagnant benchmarks are common. The researchers examined 1,688 benchmarks with 406 tasks in computer vision and natural language processing since 2013, and draw the following conclusions:
  
In some cases, there would be continuous growth, such as in the ImageNet benchmark.
 However, a majority of all benchmarks quickly reach technological stagnation or saturation.
 In some cases, a lack of research interest is also a cause of stagnation. The researchers cite the UCF101 action recognition benchmark as an example of saturation.
 However, the dynamics of performance improvement do not follow a clearly discernible pattern: in some cases, phases of stagnation are followed by unpredictable leaps. This is what happened in the PROTEINS benchmark.
  
...
 Moreover, of the 1,688 benchmarks, only 66 percent have more than three results at different points in time – so in practice, 33 percent of all AI benchmarks are not used and therefore useless. 
 ...
 In the future, new benchmarks should be developed by large, collaborative teams from many institutions, knowledge domains, and cultures to ensure high-quality benchmarks and avoid fragmentation of the benchmark landscape, the researchers conclude. 
 Source: https://mixed-news.com/en/are-we-running-out-of-ai-benchmarks/
    submitted by    /u/Sephirio  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whitepaper: Machine Learning Best Practices in Healthcare and Life Sciences]]></title>
        <id>9b427ae3ef63dc190c6b7ccb9890b546fe95fc05</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/whitepaper-machine-learning-best-practices-in-healthcare-and-life-sciences/"/>
        <updated>2022-04-01T18:16:40.000Z</updated>
        <summary type="html"><![CDATA[For customers looking to implement a GxP-compliant environment on AWS for artificial intelligence (AI) and machine learning (ML) systems, we have released a new whitepaper: Machine Learning Best Practices in Healthcare and Life Sciences. This whitepaper provides an overview of security and good ML compliance practices and guidance on building GxP-regulated AI/ML systems using AWS […]]]></summary>
        <author>
            <name>Susant Mallick</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta’s new speech AI can laugh, scream, yawn, and chit-chat]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ttxk1j/metas_new_speech_ai_can_laugh_scream_yawn_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ttxk1j/metas_new_speech_ai_can_laugh_scream_yawn_and/"/>
        <updated>2022-04-01T18:15:57.000Z</updated>
        <summary type="html"><![CDATA[Meta unveils new research on speech AI: Machine-generated voices can now cry, laugh, yawn or make more natural small talk. 
 ...
 Meta’s speech AI can now mimic emotional sounds such as laughing, yawning, or crying – which it says is important in communication to better convey the intention and context of a statement. 
 ...
 the new GSML model dGSML, which is optimized for dialogs, generates more natural-sounding audio dialogs using AI agents that can pause for thought or process overlaps in conversations. 
 ...
 dGSML was trained with about 2000 hours of unlabeled audio dialogues from the Fisher dataset, which contains about 16000 English-language telephone conversations. 
 Source and demos: https://mixed-news.com/en/meta-new-speech-ai-can-laugh-scream-yawn/
    submitted by    /u/Sephirio  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Methods for anomaly detection / clustering with high dimensional physics data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttwzjy/d_methods_for_anomaly_detection_clustering_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttwzjy/d_methods_for_anomaly_detection_clustering_with/"/>
        <updated>2022-04-01T17:52:48.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for models, workflows, algorithms in the pursuit of principled ways of conducting anomaly detection on high dimensional datasets from physical systems. I am already familiar with the application of autoencoders, isolation forests, etc. to trivial feature sets. 
 I have feature sets that abide physical equations and so there is also the capability of using differential equations or some prior generating process to also bound what is and isn't an "outlier". 
 Looking for papers/methods/texts that are in this vein.
    submitted by    /u/memproc  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rejecting GAN Off-Manifold Samples? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttw7w8/rejecting_gan_offmanifold_samples_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttw7w8/rejecting_gan_offmanifold_samples_d/"/>
        <updated>2022-04-01T17:21:08.000Z</updated>
        <summary type="html"><![CDATA[I am working on a project, where I do image editing in the latent space of an image. Are there any papers or suggestions on how to enforce that the samples lie on the manifold?
    submitted by    /u/avd4292  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oracle Releases MySQL HeatWave ML That Adds Powerful Machine Learning Capabilities to MySQL Applications]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ttvd98/oracle_releases_mysql_heatwave_ml_that_adds/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ttvd98/oracle_releases_mysql_heatwave_ml_that_adds/"/>
        <updated>2022-04-01T16:46:17.000Z</updated>
        <summary type="html"><![CDATA[Integrating machine learning capabilities to MySQL systems is prohibitively difficult and time-consuming. The process involves extracting data from the database and into another system to construct and deploy machine learning models. As data flows around, this strategy produces silos for applying machine learning to application data and causes latency. This results in data leakage, making the database more open to security attacks. Moreover, existing machine learning (ML) solutions lack the ability to explain why the model developers build delivers specific predictions.
 Recently, Oracle released MySQL HeatWave, the only MySQL cloud database service that supports in-database machine learning (ML). It automates the ML lifecycle and saves all trained models in the MySQL database, removing the need to migrate data or models to a machine learning tool or service. This decreases application complexity, saves costs, and increases data and model security. It produces a model with the best algorithm, features, and hyper-parameters for a specific data collection and application. 
 Continue Reading
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need Help with Project Idea]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ttujzz/need_help_with_project_idea/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ttujzz/need_help_with_project_idea/"/>
        <updated>2022-04-01T16:13:30.000Z</updated>
        <summary type="html"><![CDATA[Hey guys! So I am enrolled in a reinforcement learning course at my university, and I am really confused about a decent project idea. Primarily, I want to work on any game based environment apart from atari ones. Using unity seems promising but not sure if that is easy to pull off. Any suggestions to get me started? Thanks
    submitted by    /u/ishon_p  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing CVSS: A Massively Multilingual Speech-to-Speech Translation Corpus]]></title>
        <id>http://ai.googleblog.com/2022/04/introducing-cvss-massively-multilingual.html</id>
        <link href="http://ai.googleblog.com/2022/04/introducing-cvss-massively-multilingual.html"/>
        <updated>2022-04-01T16:06:00.007Z</updated>
        <summary type="html"><![CDATA[Posted by Ye Jia and Michelle Tadmor Ramanovich, Software Engineers, Google Research 
Automatic translation of speech from one language to speech in another language, called speech-to-speech translation (S2ST), is important for breaking down the communication barriers between people speaking different languages. Conventionally, automatic S2ST systems are built with a cascade of automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis sub-systems, so that the system overall is text-centric. Recently, work on S2ST that doesn’t rely on intermediate text representation is emerging, such as end-to-end direct S2ST (e.g., Translatotron) and cascade S2ST based on learned discrete representations of speech (e.g., Tjandra et al.). While early vers…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Completely removing the option taking illegal actions in custom gym environment]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ttu50r/d_completely_removing_the_option_taking_illegal/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ttu50r/d_completely_removing_the_option_taking_illegal/"/>
        <updated>2022-04-01T15:56:42.000Z</updated>
        <summary type="html"><![CDATA[Hi, 
 I have a two step custom gym env which is a graph network optimisation task (two steps due to high dimension action space). In the first step the agent chooses the first node and state reward passed to training. in the second step, the agent chooses the second node, and with a class attribute holding the history of the first selected node, now has a node pair which it is then able to remove or add and edge. The training loop now has a new graph state (edges changed between nodes) plus a vector of the first action selected. 
 The agent is able to learn well however, even with a negative reward provided to the agent if it takes illegal actions (choosing the same node in each step and thus creating a self connection), even when it learns to maximise reward, it still take illegal actions…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training drone via DRL to hover with just camera sensor]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tttw8h/training_drone_via_drl_to_hover_with_just_camera/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tttw8h/training_drone_via_drl_to_hover_with_just_camera/"/>
        <updated>2022-04-01T15:46:38.000Z</updated>
        <summary type="html"><![CDATA[Hi Everyone 
 This is my first time trying reinforcement learning and Unity and wanted some help. 
 I am trying to train a quadcopter to hover and keep a stationary ball in its camera frame using just visual inputs by a camera sensor. So the input to the network would be an 84*84 grayscale image and the output would be the four forces to the rotor.
 My reward function is 
 ​
 Reward Function
 where x and y is the position of the drone and a is the rotation of the drone with respect to the target. I have set A, d and c to 3 and lambda as 1/180. I have also added a condition where if the quadcopter drops below a certain height from the platform, it punishes it by a reward of -1 and resets the episode.
 The network I am using is the ppop network used by the coin-collector example in mlagents. 
 My training log is below: 
 Training log
 The cumulative reward and episode length just drops after a while and the value loss explodes. I think something maybe wrong with my rewards or network. 
 If anyone has any ideas what might be going wrong and tell me, that would be great. 
 Thanks
    submitted by    /u/voyager10  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do you know any port of StableBaselines 3 to C++?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tttecp/do_you_know_any_port_of_stablebaselines_3_to_c/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tttecp/do_you_know_any_port_of_stablebaselines_3_to_c/"/>
        <updated>2022-04-01T15:25:35.000Z</updated>
        <summary type="html"><![CDATA[Has anybody done it already?
    submitted by    /u/Live_Medium_3949  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Cross-lingual Wikipedia dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttt0l1/r_crosslingual_wikipedia_dataset/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttt0l1/r_crosslingual_wikipedia_dataset/"/>
        <updated>2022-04-01T15:09:33.000Z</updated>
        <summary type="html"><![CDATA[Hi! For a research project, I am trying to create a dataset that contains: the abstract of an article in EN; the abstract of the article in simple EN; the rest of the article in EN; the rest of the article in simple EN.
 When I worked in one language, I preprocessed the XML directly (the APIs seemed quite slow for processing the whole encyclopedia). However, I am struggling to find a way to join Wikis in different languages, as the dumps seem not to include a language-independent id.
 This seems to be a relatively "standard" task for creating cross-lingual datasets, so I hope someone has some tips, and I do not need to spend the next week reinventing the wheel :)
    submitted by    /u/ombelicoInfinito  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Activation functions for Neural Networks in Time Series]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttswq1/d_activation_functions_for_neural_networks_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttswq1/d_activation_functions_for_neural_networks_in/"/>
        <updated>2022-04-01T15:05:06.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I want to run a feedforward autoregressive network to forecast the potential sales of specific SKUs for the following months. The idea is to capture the non-linearity in the data. Does it make sense to use ReLU? or given that all my data points are positive values this function will return the same number (max(0,x)), therefore is not suitable for what I am trying to do?
 I have also checked other activation functions, but sigmoid for example is for classification, and hyperbolic tangent returns values that can be negative.
 ​
 Any help would be much appreciated
 Thanks!
    submitted by    /u/Old-Box-6684  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cloudy World AI Art]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tts48e/cloudy_world_ai_art/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tts48e/cloudy_world_ai_art/"/>
        <updated>2022-04-01T14:32:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Recent_Coffee_2551  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] I just watched AlphaGo - The Movie, are there any more well made documentaries about AI available?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tts3ac/d_i_just_watched_alphago_the_movie_are_there_any/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tts3ac/d_i_just_watched_alphago_the_movie_are_there_any/"/>
        <updated>2022-04-01T14:31:10.000Z</updated>
        <summary type="html"><![CDATA[AlphaGo - The Movie on Youtube
    submitted by    /u/Mighty__hammer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Cyclemoid -- a new activation function inspired by cyclical learning rates; SOTA on several benchmarks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttpg0e/p_cyclemoid_a_new_activation_function_inspired_by/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttpg0e/p_cyclemoid_a_new_activation_function_inspired_by/"/>
        <updated>2022-04-01T12:26:20.000Z</updated>
        <summary type="html"><![CDATA[Excited to share our latest research. The cyclemoid activation was inspired by the success of cyclical learning rate. Moreover, it has nice mathematical properties to stabilize gradients and maintain strong gradient signals in desired regions during training. 
 We designed it as a drop-in replacement for ReLU, and we would love to hear what you think.
 The code is up on GitHub, and the preprint should be up soon, too: https://github.com/rasbt/cyclemoid-pytorch
 PS: Currently, we only have a PyTorch implementation but would welcome it if someone could port it to TensorFlow/Keras (my Tf/Keras skills are just too rusty for it.)
    submitted by    /u/seraschka  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] I'm creating a tool to enrich your datasets with relevant external data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttp6fu/d_im_creating_a_tool_to_enrich_your_datasets_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttp6fu/d_im_creating_a_tool_to_enrich_your_datasets_with/"/>
        <updated>2022-04-01T12:11:40.000Z</updated>
        <summary type="html"><![CDATA[Hey all,
 I love doing market research and all kinds of exploratory analyses, but getting the data is a major pain point, as it is in many places (data dumps, apis, marketplaces, web data) and in all kinds of formats
 I'm trying a different approach, where instead of searching for data sources, and then integrating manually, you just upload your dataset. My service has a large index with datasets and api providers, and finds relevant ones for your dataset which you can add easily.
 ​
 example search via sdk
 Does this seem useful to you? Would love to hear your thoughts
    submitted by    /u/salmiakdrop  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Have there been successful applications of Deep RL to real problems other than board games/Atari?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttp4ub/d_have_there_been_successful_applications_of_deep/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttp4ub/d_have_there_been_successful_applications_of_deep/"/>
        <updated>2022-04-01T12:09:05.000Z</updated>
        <summary type="html"><![CDATA[Some successful applications I am able to gather, mostly from Deepmind:
 - comma.ai self-driving system.
 - Weather nowcasting
 - Tokamak fision reactor feeedback control
 - Hardware design: New gen. TPU
 - Datacenter cooling optimization
 - Adaptive locomotion for quadrupedal robots *
 - Portfolio optimization (financial instruments)
 There is a lot of work in games, particularly board games, but these do not really solve something "useful" for society. I have seen also lots of toy examples with libraries like gym and some robotics but in general these are rather proof-of-concept models or just models that do not work at all. One that actually does work is Solving Rubik’s Cube with a Robot Hand, not regarding the solution of the cube but its dexterous manipulation with a robotic hand. This is pretty cool, but again, the domain of the problem is too narrow to be considered actually a successful application to a real-world problem. So my question is, am I missing some examples? For example, is any company out there trying to apply deep RL to self-driving vehicles or to NLP, and have they had any success?
 * Boston dynamics solves this without ML, just good'ol control theory so this is a 50-50 win for RL.
 ​
 Edit: Thanks everyone for the responses, I have updated the list with more projects from the comments.
 Edit 2: Took Alphafold out because the current version (2.x) does not use RL.
    submitted by    /u/sid_276  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there a way to get PPO controlled agents to move a little more gracefully?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ttorpr/is_there_a_way_to_get_ppo_controlled_agents_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ttorpr/is_there_a_way_to_get_ppo_controlled_agents_to/"/>
        <updated>2022-04-01T11:49:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/user_00000000000001  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Chatbot is beneficial in the Retail Industry?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tto6sd/how_chatbot_is_beneficial_in_the_retail_industry/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tto6sd/how_chatbot_is_beneficial_in_the_retail_industry/"/>
        <updated>2022-04-01T11:15:18.000Z</updated>
        <summary type="html"><![CDATA[By offering your clients a blended virtual, tailored, and on-the-spot conversational level of conversation - you can engage them for longer and create an interactive method of selling to them. Chatbots will be armed with Conversational AI - and powered by an in-depth understanding of the client and their past behaviors - will be able to offer the service which is most likely to suit each client.
 Retailers can take advantage of the ability to engage potential customers through virtual attendants. Along with this opportunity is a growing range of new and exciting ways to utilize chatbots within retail spaces. A lack of customer engagement through mobile messaging costs retailers around $1 trillion annually, as customers are increasingly becoming reluctant to contact mainstream outlets for recommendations due to their negative experiences, or because they have already made up their mind on what they want.
 This can be potentially overcome by utilizing Chatbots; in-store systems powered by AI that can be utilized across multiple channels, including the outlet's website and branded social media profiles.
    submitted by    /u/botgo_io  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to make two policies in TRPO, PPO algorithms?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ttntyx/how_to_make_two_policies_in_trpo_ppo_algorithms/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ttntyx/how_to_make_two_policies_in_trpo_ppo_algorithms/"/>
        <updated>2022-04-01T10:53:47.000Z</updated>
        <summary type="html"><![CDATA[In both TRPO and PPO, we have r which is ration of new_policy/old_policy. Here we collect data from old_policy and improve new policy. I am confused on how to implement this. Correct me If I am wrong but I have two ways in mind.
 1) I collect prob while running simulation. When optimizing, I use same neural network to sample another action and its prob, this new and its probability becomes my new_policy and then I optimize for L_clip function. 
 2) I collect prob while running simulation. Before optimizing the ppo objective, I first run a simple policy gradient by using the original prob I collected. After updating the NN, I once more get new_prob which I use in L_clip function.
 Can someone please tell me which should I do and why?
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Request for Advice: Text-to-Image Synthesis]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttnj4i/d_request_for_advice_texttoimage_synthesis/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttnj4i/d_request_for_advice_texttoimage_synthesis/"/>
        <updated>2022-04-01T10:32:56.000Z</updated>
        <summary type="html"><![CDATA[Hello all, I hope that you are all doing good.
 The thing is that I want to study text-to-image synthesis; but I see that there are lots of work already done up until now. I am kind of confused about how I should start studying.
 About my experience, I am trying to learn as much as I can. I started by following Andrew Ng’s Machine Learning course on Coursera; and now I continue with Deep Learning Specialization (almost finished). Apart from these, I check NVIDIA blog, some YouTube channels, Slack communities and of course here on Reddit along with a few other channels. As I said, I am trying to follow what’s going on. 
 To tell the truth, I didn’t do much in terms of building models. I mean, not a real-life project or something like that. My experience is more based on projects for the courses that I attend/attended. By the way, I don’t know how it will affect things’ turning out; but I have recently begun a graduate program as well in artificial intelligence. Moreover, during my undergraduate program, I took some courses that might help (at least I guess so) including Calculus and Linear Algebra.
 I also want to mention that as for hardware there is an NVIDIA Jetson Nano Developer Kit available to me.
 I hope that what I am asking is clear and information I provided help you answering. If not, please ask me.
 Best regards.
    submitted by    /u/fgokmenoglu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Do you use data engineering pipelines for real life projects?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttncof/d_do_you_use_data_engineering_pipelines_for_real/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttncof/d_do_you_use_data_engineering_pipelines_for_real/"/>
        <updated>2022-04-01T10:20:31.000Z</updated>
        <summary type="html"><![CDATA[Usually I am working on huge and complex datasets with millions of rows (or images if it's CV) and most often I just feed them to pandas in a notebook, then transfer the code to a script and run it when it's needed. Then with the result I train my models. No external tools used for this.
 Do you have experience with data pipeline tools/frameworks and data validation tools/frameworks?
 For example I just found "Great Expectations" and "Kedro", "Flyte" and I was wondering at which point in time and project complexity should we choose one of these tools instead of the ancient cave man way?
 Any success/failure stories?
    submitted by    /u/gabegabe6  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic Curiosity Module Pytorch multithreading cpu unable to fix seeds]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ttn09z/intrinsic_curiosity_module_pytorch_multithreading/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ttn09z/intrinsic_curiosity_module_pytorch_multithreading/"/>
        <updated>2022-04-01T09:56:55.000Z</updated>
        <summary type="html"><![CDATA[Hello
 I am working on an extension of this implementation https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/ICM of the intrinsic curiosity module. It uses A3C(Actor -critic) as a policy and the ICM is a bolt on module.
 I need to fix the seeds for reproducibility but no matter what i have tried I cannot achieve it.
 The implementation uses multithreading on cpu and plays on the oepnai gym cartpole or atari environments.
 I believe that it has something to do with multithreading but im not sure.
 Could someone know what is the solution?
    submitted by    /u/Formal-Drawing-8421  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] The Joy of Finding Things Out (essay)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttm00m/d_the_joy_of_finding_things_out_essay/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttm00m/d_the_joy_of_finding_things_out_essay/"/>
        <updated>2022-04-01T08:43:49.000Z</updated>
        <summary type="html"><![CDATA[Currently I’m trying to figure out if I want to stay in academia or not. My decision hinges on what makes doing science enjoyable. My current understanding is in order for science to be enjoyable, there has to be an element of surprise. A tension that builds. And release of that tension. This is most obvious in theoretical works where the scientist makes a prediction, and later empirical data verifies that prediction. Excitement. Joy. Wonder. In experimental work, surprise can take the form of not knowing how the experiment will turn out. Once you get the result of the experiment, it'll disambiguate competing theories you had in your mind or elucidate a new theory. "Everything clicks" or at the very least you'll be put into a fever trying to integrate the new surprising data with previous …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is replay buffer can remove "done"?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ttkpf9/is_replay_buffer_can_remove_done/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ttkpf9/is_replay_buffer_can_remove_done/"/>
        <updated>2022-04-01T07:07:21.000Z</updated>
        <summary type="html"><![CDATA[Hi, These days, there are lots of implementation without next state and done for memory like drq-v2 official implementation. But, I have a question about is it okay to throw out "done" in replay buffer. In my point of view, there are some problems about done related signal. or did I read implementation code wrong?
    submitted by    /u/Spiritual_Fig3632  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Game AI Question (Retraining without losing characteristics)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ttkjwy/game_ai_question_retraining_without_losing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ttkjwy/game_ai_question_retraining_without_losing/"/>
        <updated>2022-04-01T06:57:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ICouldDoButWhyWouldI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] A fine grained classification dilemma]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttjoiy/d_a_fine_grained_classification_dilemma/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttjoiy/d_a_fine_grained_classification_dilemma/"/>
        <updated>2022-04-01T05:58:33.000Z</updated>
        <summary type="html"><![CDATA[Hi Folks,
 I'm in a bit of a dilemma here on a specific fine grained classification problem that we have. The task at hand is to classify the subspecies of plant seeds. Firstly, the seeds are super tiny, but we have camera setup to take zoomed images of the seeds. Secondly, even the experts can't tell the exact difference between 2 subspecies of seeds. They go by their experience and intuition. All the subspecies put together looks similar but the data points within the same subspecies are vastly different. The requirement being classifying the subspecies based on their morphological properties. Here is the catch, the requirement is 98%+ accuracy. I tried few preprocessing and models (transformers, resnets, inception and other sort) but can't hit the 98% accuracy mark. Even if I could, the model sometimes fail on external sets or on production.
 I would like some expert (ML side) take on this issue on how to approach this.
    submitted by    /u/happy_happy_feet  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Disco Diffusion v5] - "Bullet Time of Blood Animation"]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ttjncw/disco_diffusion_v5_bullet_time_of_blood_animation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ttjncw/disco_diffusion_v5_bullet_time_of_blood_animation/"/>
        <updated>2022-04-01T05:56:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/JoshGrambo  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do policy gradient methods also require some mechanism for exploration?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ttjeqq/do_policy_gradient_methods_also_require_some/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ttjeqq/do_policy_gradient_methods_also_require_some/"/>
        <updated>2022-04-01T05:41:10.000Z</updated>
        <summary type="html"><![CDATA[Algorithms like A2C, A3C, TRPO and PPO use a stochastic policy, i.e. the actions are sampled from a probability distribution so the exploration should be done inherently by these algorithms. Yet, when I am using PPO to train a bipedalwalker, it seems like some exploration mechanism is required because during the training process reward first go up and then after 1K episodes there is no progress. Please suggest me what can I do stop this from happening.
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] how to preprocess a 13k column dataset.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttibtz/d_how_to_preprocess_a_13k_column_dataset/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttibtz/d_how_to_preprocess_a_13k_column_dataset/"/>
        <updated>2022-04-01T04:34:09.000Z</updated>
        <summary type="html"><![CDATA[I have a single cell rna seq dataset containing 13k features. I would like to preprocess the dataset. What are the best methods to do that? Also, how to apply feature elimination/selection on this unsupervised data? Thanks
    submitted by    /u/Striking-Machine2763  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-generated pranks for your computer to play on you]]></title>
        <id>6243c01422e1bd003d3ef57f</id>
        <link href="https://www.aiweirdness.com/ai-generated-pranks-for-your-computer-to-play/"/>
        <updated>2022-04-01T04:19:29.000Z</updated>
        <summary type="html"><![CDATA[I've tried various methods of using AI to generate April Fools pranks for you to play on other people (although often they turned out to be pranks you play on yourself). But this is the first time I've tried to generate pranks for a computer to]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonus: Ada's pranks]]></title>
        <id>624522d522e1bd003d3ef656</id>
        <link href="https://www.aiweirdness.com/bonus-adas-pranks/"/>
        <updated>2022-04-01T04:19:08.000Z</updated>
        <summary type="html"><![CDATA[AI Weirdness: the strange side of machine learning]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Nix-TTS 🐤: An incredibly lightweight text-to-speech via non end-to-end distillation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttgsr4/r_nixtts_an_incredibly_lightweight_texttospeech/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttgsr4/r_nixtts_an_incredibly_lightweight_texttospeech/"/>
        <updated>2022-04-01T03:06:19.000Z</updated>
        <summary type="html"><![CDATA[Hi, Reddit!
 Excited to share with you guys, Nix-TTS 🐤! Our latest research in lightweight neural Text-to-Speech.
 We've seen how synthetic voices generated by recent neural TTS are getting more and more natural, but most of the time the models suffer from slow CPU inference and are not end-to-end, which requires an additional vocoder model.
 Nix-TTS 🐤 is an incredibly lightweight end-to-end TTS model achieved by applying non end-to-end knowledge distillation to a powerful yet large-sized generative TTS teacher model.
 Our proposed model is end-to-end (vocoder-free) with only 5.23M parameters or up to 82% reduction of the teacher model. We also employed a stochastic duration predictor to improve its expressiveness.
 It is capable to run 10x faster than real-time on Intel i7 CPU and 0.5 times faster than real-time on Raspberry Pi Model 3B. Making it suitable for deployment in resource-constrained settings. Here we attached the complexity and speedup detail from the paper.
 ​
 Nix-TTS speedup and complexity compared to other models.
 ​
 We released the paper (submitted to INTERSPEECH 2022) and the pre-trained models on the attached link below:
  
📄 Paper: https://arxiv.org/abs/2203.15643
 📦 Repository: https://github.com/rendchevi/nix-tts
 🤗 Interactive Demo: https://huggingface.co/spaces/rendchevi/nix-tts
  
A short video demo from the 🤗 HuggingFace Spaces:
 Nix-TTS Short Demo
 Let me know what you guys think in the thread! We're very excited to see the potential improvements & applications of this model or method and lightweight TTS in general. Feel free to reach me via DM as well if you'd like to discuss anything further.
    submitted by    /u/sourpeach_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If you could change one thing about building ML, what would it be?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ttf9ri/if_you_could_change_one_thing_about_building_ml/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ttf9ri/if_you_could_change_one_thing_about_building_ml/"/>
        <updated>2022-04-01T01:43:21.000Z</updated>
        <summary type="html"><![CDATA[I’d like to open this question up to people who are beginners, intermediates and experienced in the field of ML to get a wide variety of perspectives.
 If you could change/significantly improve one thing about building ML systems, what would it be? Some examples could be:
  
Reducing the computational overhead
 Reducing or eliminating the need for large datasets
 Simplifying the process of constructing models
  
However, it’s not limited to just those three.
 Curious to see where this goes!
    submitted by    /u/holamyeung  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Are there any good solutions for multimodal classification? Libraries, AutoML tool?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttd3bl/dare_there_any_good_solutions_for_multimodal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttd3bl/dare_there_any_good_solutions_for_multimodal/"/>
        <updated>2022-03-31T23:48:50.000Z</updated>
        <summary type="html"><![CDATA[Hi, Reddit! I'm a data scientist working in the dating app startup field. We have been trying to set up people with blind dates. We can get multimodal data(texts, images, and audio) and we want to do this: collect negative and positive pairs from each user's swiping history and do a binary classification (matching). Then we would get together as many users' data as possible and train a model.
 Though it sounds nice, this is hard as we could not find an existing developer tool or paper that supports combining such rich multimodal data. Any existing research out there to achieve this task? Moreover, is there already any library or AutoML tool that supports this? Checked Google AutoML, does not support this.
 Any help and advice would be much appreciated.
    submitted by    /u/meame2010  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] LAION-5B: public dataset of 5.85 billion image-text pairs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ttcsqz/p_laion5b_public_dataset_of_585_billion_imagetext/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ttcsqz/p_laion5b_public_dataset_of_585_billion_imagetext/"/>
        <updated>2022-03-31T23:33:06.000Z</updated>
        <summary type="html"><![CDATA[LAION-5B: A new era of open large-scale multi-modal datasets.
 Twitter thread.
 Related: [P] LAION-400M: open-source dataset of 400 million image-text pairs.
 I am not affiliated with this project.
    submitted by    /u/Wiskkey  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prepare data from Databricks for machine learning using Amazon SageMaker Data Wrangler]]></title>
        <id>8298a7a5db1b264ac6c37c3867417c3928769e17</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/prepare-data-from-databricks-for-machine-learning-using-amazon-sagemaker-data-wrangler/"/>
        <updated>2022-03-31T23:07:22.000Z</updated>
        <summary type="html"><![CDATA[Data science and data engineering teams spend a significant portion of their time in the data preparation phase of a machine learning (ML) lifecycle performing data selection, cleaning, and transformation steps. It’s a necessary and important step of any ML workflow in order to generate meaningful insights and predictions, because bad or low-quality data greatly […]]]></summary>
        <author>
            <name>Roop Bains</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Myth of Analytic Talent Shortage]]></title>
        <id>https://www.datasciencecentral.com/?p=57019</id>
        <link href="https://www.datasciencecentral.com/debunking-the-myth-of-analytic-talent/"/>
        <updated>2022-03-31T23:01:31.000Z</updated>
        <summary type="html"><![CDATA[I tested the job market in the last two weeks, both as an applicant, and as a hiring manager. I share my experience here. It is radically different from what you read in the news, or from what most people say. Data scientists and machine learning engineers looking for a new job are out there.… Read More »The Myth of Analytic Talent Shortage
The post The Myth of Analytic Talent Shortage appeared first on Data Science Central.]]></summary>
        <author>
            <name>Vincent Granville</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Adapting pixel attribution methods for models that output embeddings]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tt9lib/p_adapting_pixel_attribution_methods_for_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tt9lib/p_adapting_pixel_attribution_methods_for_models/"/>
        <updated>2022-03-31T20:59:12.000Z</updated>
        <summary type="html"><![CDATA[Hi r/MachineLearning,
 https://github.com/jacobgil/pytorch-grad-cam is a project that has a comprehensive collection of Pixel Attribution Methods for PyTorch (like the package name grad-cam that was the original algorithm implemented).
 Typically pixel attribution methods are adapted for classification: they let you understand what part of the image correspond to a certain classification category.
 However some deep learning models output embeddings instead of category scores.You can then match these embeddings against other embeddings and measure their similarity. For example: in face recognition models, or in self supervised networks.
 In this case to apply pixel attribution, we could create embeddings of concepts, and then for new query images we would be asking: "what parts of the image have feature representations that match the concept features?"
 Or in other words: "where in the query image do we see the concepts?"
 ​
 I wrote a tutorial that shows how to use the pytorch-grad-cam project, to adapt pixel attribution for the embedding case, and visualize where different concept feature representations match the image:
 https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/Pixel%20Attribution%20for%20embeddings.ipynb
 An example is the image below. The two left images are "concept" images of clouds, and a car.
 Then given a new query image, we can try to see where in the image do we feature representations that match these concepts.
 Given images of concepts and a query image, attribute what parts of the query image match the concepts
 I hope someone finds this useful !
    submitted by    /u/jacobgil  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradients with pytorch]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tt9anv/policy_gradients_with_pytorch/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tt9anv/policy_gradients_with_pytorch/"/>
        <updated>2022-03-31T20:45:17.000Z</updated>
        <summary type="html"><![CDATA[what will the shape of the tensor "probs" (marked) look like ? 
 will it look like this - 
 [ 0.32,0.40,0.28] ?
 ​
 or like this ?
 [ [ 0.32,0.40,0.28],
 [ 0.32,0.40,0.28],
 [ 0.32,0.40,0.28],
 [ 0.32,0.40,0.28] ] 
 https://preview.redd.it/3qiq3lzz6sq81.png?width=829&format=png&auto=webp&s=c15ece0e38b9dabf3f443466b2553e146845e92a
    submitted by    /u/Whole_Run_4485  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Featured video: L. Rafael Reif on the power of education]]></title>
        <id>https://news.mit.edu/2022/featured-video-mit-president-l-rafael-reif-power-of-education-0331</id>
        <link href="https://news.mit.edu/2022/featured-video-mit-president-l-rafael-reif-power-of-education-0331"/>
        <updated>2022-03-31T20:25:00.000Z</updated>
        <summary type="html"><![CDATA[At Monterrey Tec, MIT’s president discusses the impact of education in addressing global issues.]]></summary>
        <author>
            <name>Melanie Grados, MIT News Office</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does anyone know how to create animations like in the Google AI Blog?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tt8sel/d_does_anyone_know_how_to_create_animations_like/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tt8sel/d_does_anyone_know_how_to_create_animations_like/"/>
        <updated>2022-03-31T20:23:04.000Z</updated>
        <summary type="html"><![CDATA[I really would like to do some visualization of my ideas. I found the animation in the google ai blog:
 https://ai.googleblog.com/2022/02/4d-net-learning-multi-modal-alignment.html
 Anyone knows how to do this stuff, especially with the flowing lines? Any software suggestions?
    submitted by    /u/KonArtist01  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI podcast: DeepMind Mafia, DishBrain, PRIME, ZooKeeper AI, Instant NeRF]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tt8437/last_week_in_ai_podcast_deepmind_mafia_dishbrain/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tt8437/last_week_in_ai_podcast_deepmind_mafia_dishbrain/"/>
        <updated>2022-03-31T19:52:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Data centric fixes to a model that's fit to a spurious correlation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tt7q5h/d_data_centric_fixes_to_a_model_thats_fit_to_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tt7q5h/d_data_centric_fixes_to_a_model_thats_fit_to_a/"/>
        <updated>2022-03-31T19:34:38.000Z</updated>
        <summary type="html"><![CDATA[Say a computer vision model learns a pattern you don't want it to, you know that it's learnt it because of analysis through tools like occlusion sensitivity map.
 What data-centric techniques can you use to resolve it? Could some form of cropping augmentation do the trick?Classic example is ruler beside melanoma, while there may be a correlation between presence of ruler and presence of melanoma you don't want to the model to depend on that information because it may not exist 'in production'. Below is a quotation describing another similar problem.
 "In another paper a similar issue was found because doctors sometimes use purple markers to highlight potentially-malignant skin cancers for easier examination. Some argue that the purple marks are a real signal that should be incorporated in the model just as the visual appearance of the tumor itself is incorporated. However, if your goal is robust generalizability over time it is probably best to not have your AI incorporate the human applied purple marks as signal, as the standards for applying those marks may vary across teams and across time." https://menloml.com/2020/01/11/recognizing-a-ruler-instead-of-a-cancer/
 f you're working with that dataset, what tools are available to you to solve that problem?
    submitted by    /u/Georgehwp  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are the main roads to AGI?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tt7ns1/what_are_the_main_roads_to_agi/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tt7ns1/what_are_the_main_roads_to_agi/"/>
        <updated>2022-03-31T19:31:29.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if you can help me come up with a list fo all the specific proposals on how to achieve AGI. For example, one of them is: scaling is all you need. In more detail, scaling self-supervised pretrained deep network models (a.k.a. foundation models), data and compute can lead to AGI (scaling assumes "smart" one, i.e. as steep as possible/cost-efficient exponents in neural scaling laws). Do you know what other main roads there are to AGI?
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Newbie question]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tt7l1s/newbie_question/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tt7l1s/newbie_question/"/>
        <updated>2022-03-31T19:28:06.000Z</updated>
        <summary type="html"><![CDATA[Hello guys, I was wondering if anyone knew the easiest way to combine images together. Ideally I would have a bunch of images and it would take components of a couple (or just two) and put them together. I want to generate images of morphed anime figures, it doesn’t even need to look professional (or good lol). Just need some sort of website or software that I can easily achieve this. Any tips or ideas would be greatly appreciated!! Thank you!
    submitted by    /u/misakimeifanpage  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fighting AI's discrimination in mortgage lending | DualFair]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tt4uqf/fighting_ais_discrimination_in_mortgage_lending/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tt4uqf/fighting_ais_discrimination_in_mortgage_lending/"/>
        <updated>2022-03-31T17:24:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/qptbook  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jigsaw fixes bugs in machine-written software]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=830284</id>
        <link href="https://www.microsoft.com/en-us/research/blog/jigsaw-fixes-bugs-in-machine-written-software/"/>
        <updated>2022-03-31T17:00:00.000Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models such as GPT-3, Codex, and others can be tuned to generate code from natural language specifications of programmer intent. Such automated models have the potential to improve productivity for every programmer in the world. But since the models can struggle to understand program semantics, the quality of the resulting code can’t […]
The post Jigsaw fixes bugs in machine-written software appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Marketing Mix Modeling - How can we solve for negative Media Coefficients?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tt4abi/p_marketing_mix_modeling_how_can_we_solve_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tt4abi/p_marketing_mix_modeling_how_can_we_solve_for/"/>
        <updated>2022-03-31T16:59:06.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone
 I'm working on a Marketing Mix Modeling project for a client
 I'm using Python and sci-kit learn library to do regression analysis with Ridge and Linear Regression.
 I have pretty good results:
 R^2=0.87
 mape= 0.2
 But some of my media coefficients are negative
 And this doesn't make sense business-wise
 How can I model positive media coefficients without using Bayesian modeling?
    submitted by    /u/datagabriele  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] projUNN: efficient method for training deep networks with unitary matrices]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tt3qor/r_projunn_efficient_method_for_training_deep/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tt3qor/r_projunn_efficient_method_for_training_deep/"/>
        <updated>2022-03-31T16:34:13.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2203.05483
 TL;DR; from LeCun: Recurrent nets in which the weight matrix is unitary are interesting beasts: they are invertible, they don't suffer from vanishing/exploding gradient, and they perform computation akin to what happens in quantum computers. training them can be difficult or expensive. We propose a low-rank (low-cost) update method to update unitary weight matrices with gradient descent.
 Abstract: In learning with recurrent or very deep feed-forward networks, employing unitary matrices in each layer can be very effective at maintaining long-range stability. However, restricting network parameters to be unitary typically comes at the cost of expensive parameterizations or increased training runtime. We propose instead an efficient method based on rank-k updates -- or their rank-k approximation -- that maintains performance at a nearly optimal training runtime. We introduce two variants of this method, named Direct (projUNN-D) and Tangent (projUNN-T) projected Unitary Neural Networks, that can parameterize full N-dimensional unitary or orthogonal matrices with a training runtime scaling as O(kN^2). Our method either projects low-rank gradients onto the closest unitary matrix (projUNN-T) or transports unitary matrices in the direction of the low-rank gradient (projUNN-D). Even in the fastest setting (k=1), projUNN is able to train a model's unitary parameters to reach comparable performances against baseline implementations. By integrating our projUNN algorithm into both recurrent and convolutional neural networks, our models can closely match or exceed benchmarked results from state-of-the-art algorithms.
    submitted by    /u/lostmsu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers from U Texas and Apple Propose a Novel Transformer-Based Architecture for Global Multi-Object Tracking]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tt37xb/researchers_from_u_texas_and_apple_propose_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tt37xb/researchers_from_u_texas_and_apple_propose_a/"/>
        <updated>2022-03-31T16:10:30.000Z</updated>
        <summary type="html"><![CDATA[Multi-object tracking aims to locate and track all objects in a video feed. It’s a fundamental component in domains like mobile robots, where an autonomous system must navigate dynamic surroundings populated by other mobile agents. Thanks to breakthroughs in deep learning and object detection, tracking-by-detection has become the dominant tracking paradigm in recent years.
 Tracking-by-detection simplifies the process by reducing it to just two steps: detection and association. First, an object detector searches each video stream frame for probable items. The second phase is an association step, which connects detections over time. Local trackers are greedy when it comes to pairwise relationships. They keep track of each trajectory’s state based on its position and/or identity traits and correlate current-frame detections with it based on its last visible status.
 Continue Reading The Research Summary
 Paper: https://arxiv.org/pdf/2203.13250.pdf
 Github: https://github.com/xingyizhou/GTR
 https://i.redd.it/312ahhaxtqq81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI generated Personalized Implicit Neural Avatars (PINA)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tt2dxi/ai_generated_personalized_implicit_neural_avatars/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tt2dxi/ai_generated_personalized_implicit_neural_avatars/"/>
        <updated>2022-03-31T15:32:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/imapurplemango  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Reward Environments and Off Policy Algorithms]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tt291u/sparse_reward_environments_and_off_policy/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tt291u/sparse_reward_environments_and_off_policy/"/>
        <updated>2022-03-31T15:26:40.000Z</updated>
        <summary type="html"><![CDATA[In my experience, (continuous action space) off Policy algorithms are generally more sample efficient than on policy algorithms but don't perform as well on sparse rewards environments. Are there any papers that address this issue? Do you know of any algorithms that are both sample efficient and learn sparse reward Environments well?
    submitted by    /u/SirRantcelot  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Are text embeddings tabular data?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tt21k9/d_are_text_embeddings_tabular_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tt21k9/d_are_text_embeddings_tabular_data/"/>
        <updated>2022-03-31T15:16:49.000Z</updated>
        <summary type="html"><![CDATA[I keep hearing that NNs are not the best way to approach tabular data. But when it comes to document classification, in terms of using embeddings for a downstream classification task, would that be considered tabular data? You will end up with data that fits in a table that you wish to classify...it's high dimensional but you could reduce dimensions until you end up with just a smaller set of columns and the labels. 
 I guess I'm unclear about what defines tabular data in this context, and if it makes sense to use a different model (like XBGboost) for the classification task, vs having it as a final layer in the embedding network.
    submitted by    /u/bandalorian  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to deal with delayed, dense rewards]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tt19mu/how_to_deal_with_delayed_dense_rewards/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tt19mu/how_to_deal_with_delayed_dense_rewards/"/>
        <updated>2022-03-31T14:41:43.000Z</updated>
        <summary type="html"><![CDATA[I'm having a doubt that may be a little stupid, but I ask to be sure.
 Assume that in my environment rewards are delayed by a random number n of steps, i.e. the agent takes an action but receives the reward n steps after taking that action. At every step a reward is produced, therefore the reward r_t in transitions s_t, a_t, r_t, s_{t+1} collected by the agent is actually the reward corresponding to the transition at time t-n. 
 An example scenario: the RL agent control a transportation network, and a reward is generated only when a package reach its destination. Thus, the reward arrives with possibly several steps of delay with respect to when the relevant actions were taken. 
 Now, I know that delayed rewards are not generally an issue, e.g. all those settings in which there is only one reward +1 at the end, but I am wondering if this case is equivalent. What makes me wonder is that here, for a state s_t onwards to state s_{t+n}, there are n rewards in the middle that depend on states previous to s_t. 
 Does this make the problem non-markovian? How can one learn the value function V(s_t) if its estimation is always affected by unrelated rewards r_{t-n} ... r_{t-1}?
    submitted by    /u/fedetask  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sim2Real]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tt0jua/sim2real/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tt0jua/sim2real/"/>
        <updated>2022-03-31T14:07:10.000Z</updated>
        <summary type="html"><![CDATA[Hi! Does anyone know the “right” way to apply a policy to a robotic manipulator? Now I’m trying creating a real environment and simulate the policy on it but I can’t find anything on the web about this. Thanks!
    submitted by    /u/Big-Picture8323  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instant NeRF: Turn 2D Images into a 3D Models in Milliseconds]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tt02ro/instant_nerf_turn_2d_images_into_a_3d_models_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tt02ro/instant_nerf_turn_2d_images_into_a_3d_models_in/"/>
        <updated>2022-03-31T13:44:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An A-peel-ing GFN Thursday Sprouts 20+ New Games Coming to GeForce NOW in April]]></title>
        <id>https://blogs.nvidia.com/?p=56412</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/31/geforce-now-thursday-march-31/"/>
        <updated>2022-03-31T13:00:06.000Z</updated>
        <summary type="html"><![CDATA[In addition to GFN Thursday, it’s National Tater Day. Hooray! To honor the spud-tacular holiday, we’re closing out March with seven new games streaming this week. And a loaded 20+ titles are coming to the GeForce NOW library in April to play — even on a potato PC, thanks to GeForce NOW. Plus, the GeForce Read article >
The post An A-peel-ing GFN Thursday Sprouts 20+ New Games Coming to GeForce NOW in April appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just Tech: Centering Community-Driven Innovation at the Margins episode 2 with Dr. Tawanna Dillahunt, Zachary Rowe, and Joanna Velazquez]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=829618</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/just-tech-centering-community-driven-innovation-at-the-margins-episode-2-with-dr-tawanna-dillahunt-zachary-rowe-and-joanna-velazquez/"/>
        <updated>2022-03-31T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 134 | March 31, 2022 In “Just Tech: Centering Community-Driven Innovation at the Margins,” Senior Principal Researcher Mary L. Gray explores how technology and community intertwine and the role technology can play in supporting community-driven innovation and community-based organizations. Dr. Gray and her team are working to bring computer science, engineering, social science, and […]
The post Just Tech: Centering Community-Driven Innovation at the Margins episode 2 with Dr. Tawanna Dillahunt, Zachary Rowe, and Joanna Velazquez appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsyt3l/r_causal_inference_in_natural_language_processing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsyt3l/r_causal_inference_in_natural_language_processing/"/>
        <updated>2022-03-31T12:37:13.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2109.00725
 Abstract: A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the remaining challenges. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects, encompassing settings where text is used as an outcome, treatment, or as a means to address confounding. In addition, we explore potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the computational linguistics community.
    submitted by    /u/bikeskata  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] AIY Vision Kit]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsxu8x/p_aiy_vision_kit/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsxu8x/p_aiy_vision_kit/"/>
        <updated>2022-03-31T11:40:27.000Z</updated>
        <summary type="html"><![CDATA[Working on a group project for college, utilizing a AIY Vision kit and implementing some variation of machine learning with it. The initial idea was to use the camera to identify playing cards and sorting them into groups, like different suits, odd/even, face and non face, but after meeting with the professor in charge, was informed that this idea of identifying and sorting is not really marketable. I tried explaining that sorting in the different ways could be used in many different applications and businesses, but still was told it was not really marketable. I was wondering if maybe there was a way to tweak it a bit to make it so, or if maybe turning it into an API would help. Any ideas and/or opinions on the project would be very helpful. Thank you very much.
 I understand this may fall under beginner related questions and project, but was curious to know if my group and I are on a right track or not, as we haven’t had any sort of guidance up to date.
    submitted by    /u/EETQuestions  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What kind of teams do you work on?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsxj8q/d_what_kind_of_teams_do_you_work_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsxj8q/d_what_kind_of_teams_do_you_work_on/"/>
        <updated>2022-03-31T11:20:43.000Z</updated>
        <summary type="html"><![CDATA[I am looking to stand up a small ML group in my non big tech but pretty big organization. We have a few use cases, only see the list growing, and would like to have a dedicated group rather than having disparate teams each trying to roll their own algorithms for their own applications. I am starting to look at what the costs might be, and I can see some pushback and lowballing (eg onshore/offshore, skill set, % junior/senior), but I don’t have a lot of stories to start my thoughts with, let alone data. So, I’m interested to know what your teams are like to start thinking about it, and any other data or literature would also be appreciated!
    submitted by    /u/Stranger_Dude  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pass a seed arg in gyms reset method to play the same game - undocumented feature!]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tsw7rb/pass_a_seed_arg_in_gyms_reset_method_to_play_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tsw7rb/pass_a_seed_arg_in_gyms_reset_method_to_play_the/"/>
        <updated>2022-03-31T09:47:38.000Z</updated>
        <summary type="html"><![CDATA[I asked a while back how to save state of an episode when this was my original intention. 
 even a quick perusal of an environment's code reveals interesting information that's helpful to using gym as a whole
 https://www.github.com/openai/gym/tree/master/gym/envs
 I think it'd be interesting if papers supplied seed numbers for their test andntraining runs, where they're pulled from an array of ints contained in the agent.
    submitted by    /u/clockface99  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] China researches “brain-scale” AI]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsvfor/r_china_researches_brainscale_ai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsvfor/r_china_researches_brainscale_ai/"/>
        <updated>2022-03-31T08:47:59.000Z</updated>
        <summary type="html"><![CDATA[https://mixed-news.com/en/artificial-intelligence-china-researches-brain-scale-ai/
 In China, the state and companies are researching AI models with trillions of parameters. They want to prove that they can develop “brain-scale” AI.
 In the race to build ever-larger AI models, China is showing that cooperation between the state, universities and the private sector holds the potential for gigantic AI models. The researchers are talking about “brain-scale” AI: according to their definition, these are AI models with parameters beyond the 100-trillion mark.
 ...
 In a new paper, researchers from Tsinghua University, Alibaba Group, Zhejiang Lab and Beijing Academy of Artificial Intelligence present BaGuaLu, a framework that enables the training of large AI models using the Mixture-of-Experts (MoE) architecture.
 ...
 In an initial test, the researchers trained a 1.93 trillion model with their framework, outperforming Google’s Switch Transformer. They also demonstrate that their framework enables models with 14.5 trillion and a full 174 trillion parameters.
 ...
 BaGuaLu could soon be used to train the first models beyond 100 trillion parameters.
    submitted by    /u/Zirius_Sadfaces  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Coreset terrible perfomance on Datasets with a lot of redundancy]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsvc7m/d_coreset_terrible_perfomance_on_datasets_with_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsvc7m/d_coreset_terrible_perfomance_on_datasets_with_a/"/>
        <updated>2022-03-31T08:40:25.000Z</updated>
        <summary type="html"><![CDATA[Hello reddit hivemind,
 This might be a quite specific question but im not sure where else to go and ask. I'm currently an intern and charged with implementing and comparing different active learning algorithms to see which work best for our specific usecase. Since the coreset approach ( [1708.00489] Active Learning for Convolutional Neural Networks: A Core-Set Approach (arxiv.org) ) is now around for a long time, one of the best documented and shows promising results in a variety of papers I implemented it and ran some experiments with it. The results were a bit dissappointing. It got even outperformed by the random baseline .... To understand the bad performance I dug a bit deeper since I spend a significant amount of time implementing it. What I made out now as the issue is using the l_2 norm of penultimate layer as the metric. This leads to an oversampling of data samples with a certain softmax output due to the way the softmax function behaves. Has someone experienced the same issues? The only point where I could see coreset to be of some use is with a dataset that has a ton of redundant/similar images.
 Thank's a lot
    submitted by    /u/Fearless-Pumpkin-745  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dataset with labeled benign and malicious files]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tsv57j/dataset_with_labeled_benign_and_malicious_files/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tsv57j/dataset_with_labeled_benign_and_malicious_files/"/>
        <updated>2022-03-31T08:25:21.000Z</updated>
        <summary type="html"><![CDATA[Hi, Reddit,
 During the project implementation for my bachelor's thesis [1], a software (named dike, as the Greek goddess of justice) capable of analyzing malicious programs using artificial intelligence techniques, I was unable to locate an open source dataset with labeled malware samples in the public domain. As a result, I created DikeDataset, a dataset with labeled PE and OLE samples [2]. Because it was not the main focus of my thesis, the samples attributes are not evenly distributed (the benign-malicious and OLE-PE ratios are quite low), but the dataset aided greatly in the research process.
 This week, I was surprised to see that the public GitHub repository (which was used only for storage, without any promotion on communities like this) gained some organic reach (views, clones and stars). Furthermore, I was thrilled to learn that it was used in a research article published in 2021 [3]!
 As a result, I'd like to share this project with the community in the hopes that it will be useful to some members of the community.
 [1] dike
 [2] DikeDataset
 [3] Toward Identifying APT Malware through API System Calls
    submitted by    /u/iosifache  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Reinforcement learning in Finance project]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tsv55f/r_reinforcement_learning_in_finance_project/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tsv55f/r_reinforcement_learning_in_finance_project/"/>
        <updated>2022-03-31T08:25:13.000Z</updated>
        <summary type="html"><![CDATA[Hello, I hope that this message finds you in good health
 FinRL: Deep Reinforcement Learning for Quantitative Finance https://github.com/AI4Finance-Foundation/FinRL is a project from Columbia University. It offers environments for cryptocurrency, paper trading, stock trading, and forex trading. Also, it has support for three reinforcement learning libraries: Stable Baselines3, RLlib, and ElegantRL. This is from AI4Finance-foundation and it aims to provide a plug-play platform for RL in finance. Do check it out and help us to improve this project
 Some resources:
  
My contributions: https://medium.com/@athekunal/list/finrl-contributions-59de6997c5b1
 Resources to learn FinRL: https://github.com/AI4Finance-Foundation/FinRL#tutorials
 All tutorial notebooks: https://github.com/AI4Finance-Foundation/FinRL/tree/master/tutorials
 YouTube Channel: https://www.youtube.com/channel/UCrVri6k3KPBa3NhapVV4K5g
  
   submitted by    /u/A_the_kunal  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Reinforcement Learning in Finance research]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsv4sv/r_reinforcement_learning_in_finance_research/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsv4sv/r_reinforcement_learning_in_finance_research/"/>
        <updated>2022-03-31T08:24:27.000Z</updated>
        <summary type="html"><![CDATA[Hello, I hope that this message finds you in good health
 FinRL: Deep Reinforcement Learning for Quantitative Finance https://github.com/AI4Finance-Foundation/FinRL is a project from Columbia University. It offers environments for cryptocurrency, paper trading, stock trading, and forex trading. Also, it has support for three reinforcement learning libraries: Stable Baselines3, RLlib, and ElegantRL. This is from AI4Finance-foundation and it aims to provide a plug-play platform for RL in finance. Do check it out and help us to improve this project
 Some resources:
  
My contributions: https://medium.com/@athekunal/list/finrl-contributions-59de6997c5b1
 Resources to learn FinRL: https://github.com/AI4Finance-Foundation/FinRL#tutorials
 All tutorial notebooks: https://github.com/AI4Finance-Foundation/FinRL/tree/master/tutorials
 YouTube Channel: https://www.youtube.com/channel/UCrVri6k3KPBa3NhapVV4K5g
  
   submitted by    /u/A_the_kunal  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disney princesses according to AI. Is this done manually or through an AI app?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tsv417/disney_princesses_according_to_ai_is_this_done/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tsv417/disney_princesses_according_to_ai_is_this_done/"/>
        <updated>2022-03-31T08:22:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/cyberpunk1Q84  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Point Cloud Annotation Tool]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsv020/p_point_cloud_annotation_tool/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsv020/p_point_cloud_annotation_tool/"/>
        <updated>2022-03-31T08:14:01.000Z</updated>
        <summary type="html"><![CDATA[Hi Everyone,
 Some time ago, we had the idea to start building tools to facilitate 3D computer vision development. We started by looking at some of the 3D tools out there and realized that there wasn't anything that fit our needs or could be extended to do some of the things we wanted to do. 
 We started working on a tool to annotate point clouds with different label types (bounding box, rectangles, keypoints) to use as a base for our projects. We recently open sourced the tool, which you can find here: https://github.com/StrayRobots/3d-annotation-tool
 In the future we might add more tools, for example to paint point clouds or a polygon label type. 
 We would love to hear your feedback on the tool. Has anyone here built any 3D vision datasets? What kind of tools did you use?
    submitted by    /u/slash-dot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Is it just me or is machine learning difficult to learn?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsutvx/dis_it_just_me_or_is_machine_learning_difficult/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsutvx/dis_it_just_me_or_is_machine_learning_difficult/"/>
        <updated>2022-03-31T08:00:41.000Z</updated>
        <summary type="html"><![CDATA[Hello, 
 My Background:
 I work as a web dev for almost 2 years. Before that when I was studying in college, I thought ML is the only field which was in demand. I put my 100% into it but the professor was so bad that not only me but a lot of my peers found ML,DS to be very difficult. We were able to built project around but never tried to learn more. 
 I tired many udemy or coursea courses but never found it engaging. 
 Is it just me or did you also found ML difficult? Is my approach to learning it wrong? If anyone has any advice I'd really appreciate it
    submitted by    /u/Notalabel_4566  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to combine multimodal data of different sequence lengths in training?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsu81d/d_how_to_combine_multimodal_data_of_different/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsu81d/d_how_to_combine_multimodal_data_of_different/"/>
        <updated>2022-03-31T07:14:22.000Z</updated>
        <summary type="html"><![CDATA[I'm currently working on a project related to multimodal summarization using transformers.
 My input is a image and long text and output will be a summary of the text pertaining to the image.
 ​
 For extracting image features, I'm using pretrained resnet model. It gives me a [49 * 2048] vector for an image.
 For extracting paragraph features, I'm getting embeddings for each sentence, so the data_dimension will be [no_of_sentences * 512]
 ​
 I need to attend to both these set of features and generate output. I have gone through tutorials to understand the working of transformers but couldn't understand how to combine these into a single input so that the encoder can attend over both image and the paragraph together at the same time.
 ​
 Any pointers to tutorials will be very helpful.
    submitted by    /u/abisekrk  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] An open letter to DeepMinders]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tssy7n/n_an_open_letter_to_deepminders/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tssy7n/n_an_open_letter_to_deepminders/"/>
        <updated>2022-03-31T05:44:33.000Z</updated>
        <summary type="html"><![CDATA[Yesterday, an anonymous open letter and accompanying Financial Times (paywall) article were published accusing DeepMind of mishandling allegations of sexual abuse by a senior member of the research team. 
 The FT article (reported on here in Fortune) goes on to suggest that the mishandling may have been deliberate in order to exploit legal loopholes in the UK where victims have a limited amount of time to take a case to employment tribunal. 
 This comes shortly after Mustafa Suleyman, one of the cofounders was quietly shuffled out to Google (he has subsequently left and founded a new startup with another DeepMind alum) after he was found to have bullied and humiliated staff for years.
 Google itself also has a poor record when it comes to sexual harassment, bullying and retaliation at the highest levels resulting in payouts of hundreds of millions of dollars. 
 Given that DeepMind and Google have a pretty strong grip on the development of AI in terms of employing many of the key people across many of the various subfields, having access to unparalleled data and compute and pushing forward more and more into health (for example the DeepMind offshoot Isomorphic Labs which itself is headed by Demis and staffed by DeepMinders, and the various Google healthcare bets and projects), can we really trust them to be stewards of fair and responsible AI development? 
 Bad things happen in all large organizations. But DeepMind isn’t that big and in the past five years, DeepMind leadership have presided over a steady stream of sexual harassment, bullying and other scandals and handled them all extremely poorly and showed little signs that things have changed. This points to something rotten in the culture and leadership there and at it’s parent organization.
    submitted by    /u/ml-anon  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I only know one output of a neural net at a time. How to train, if i have two outputs?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tssfwb/i_only_know_one_output_of_a_neural_net_at_a_time/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tssfwb/i_only_know_one_output_of_a_neural_net_at_a_time/"/>
        <updated>2022-03-31T05:10:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/lullek4  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Use of Kesler Construction for Multi-class Prediction]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsrdhl/d_use_of_kesler_construction_for_multiclass/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsrdhl/d_use_of_kesler_construction_for_multiclass/"/>
        <updated>2022-03-31T04:05:43.000Z</updated>
        <summary type="html"><![CDATA[I noticed that in some literature they use the Kesler construction when discussing multi-class prediction: https://uclanlp.github.io/CS269-17/slides/CS269-03.pdf. Why do they do this versus represent all the w_i vectors in a K x N matrix, where N is the length of x and K is the number of classes, and then generate Wx = [w1^Tx, ..., wKx], which will essentially produce the same result but be more efficient because of the lack of zero multiplications which are in the Kesler construction?
    submitted by    /u/newperson77777777  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving the challenges of robotic pizza-making]]></title>
        <id>https://news.mit.edu/2022/robotic-deformable-object-0331</id>
        <link href="https://news.mit.edu/2022/robotic-deformable-object-0331"/>
        <updated>2022-03-31T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[A new technique could enable a robot to manipulate squishy objects like pizza dough or soft materials like clothing.]]></summary>
        <author>
            <name>Adam Zewe | MIT News Office</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A-List Celebrities Read my movie script]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tsqdg7/alist_celebrities_read_my_movie_script/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tsqdg7/alist_celebrities_read_my_movie_script/"/>
        <updated>2022-03-31T03:09:02.000Z</updated>
        <summary type="html"><![CDATA[Made this video tonight I had A.I. voices read my script for an upcoming movie.
 https://www.youtube.com/watch?v=RkK-iGAGcHA
    submitted by    /u/PapermoonPictures  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Guide to Getting Datasets for Machine Learning in Python]]></title>
        <id>https://machinelearningmastery.com/?p=13373</id>
        <link href="https://machinelearningmastery.com/a-guide-to-getting-datasets-for-machine-learning-in-python/"/>
        <updated>2022-03-31T01:00:09.000Z</updated>
        <summary type="html"><![CDATA[Compared to other programming exercises, a machine learning project is a blend of code and data. You need both to […]
The post A Guide to Getting Datasets for Machine Learning in Python appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Adrian Tam</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Making Robots Achieve Tasks Like Animals with Style Transfer + RL]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsmryt/r_making_robots_achieve_tasks_like_animals_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsmryt/r_making_robots_achieve_tasks_like_animals_with/"/>
        <updated>2022-03-30T23:55:18.000Z</updated>
        <summary type="html"><![CDATA[This work from Berkeley + Google Brain (Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions) describes using RL and GANs for transferring motion styles from animals successfully onto robots. 
 Super neat idea, and love to see ML being used more and more on real robots!
 Love the Cost of Transport analysis - naturalistic movement really is very efficient, and good luck getting RL to solve the hard exploration problem of good motion and task performance simultaneously tabula rasa!
 In particular I love this image. Down with hand specified reward functions! Let imitating nature reign supreme.
 What's next, GANs for moral style transfer?
    submitted by    /u/AristocraticOctopus  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will Smith's AI Persona was asked about his slapping performance on Oscar Stage]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tsmk24/will_smiths_ai_persona_was_asked_about_his/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tsmk24/will_smiths_ai_persona_was_asked_about_his/"/>
        <updated>2022-03-30T23:43:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kuasha7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers from MIT CSAIL Introduce ‘Privid’: an AI Tool, Build on Differential Privacy, to Guarantee Privacy in Video Footage from Surveillance Cameras]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tslzhy/researchers_from_mit_csail_introduce_privid_an_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tslzhy/researchers_from_mit_csail_introduce_privid_an_ai/"/>
        <updated>2022-03-30T23:14:51.000Z</updated>
        <summary type="html"><![CDATA[Surveillance cameras have an identity crisis exacerbated by a conflict between function and privacy. Machine learning techniques have automated video content analysis on a vast scale as these sophisticated small sensors have shown up seemingly everywhere. Still, with increased mass monitoring, there are currently no legally enforceable standards to curb privacy invasions.
 Security cameras have evolved into wiser and more capable tools than the grainy images of the past, which were frequently used as the “hero tool” in crime dramas. Video surveillance can now assist health regulators in determining the percentage of persons using masks, transportation departments in monitoring the density and flow of automobiles, cyclists and walkers, and businesses in gaining a better understanding of buying habits. But why has privacy remained a second-class citizen?
 Privid
 Currently, the footage is retrofitted with blurred faces or black boxes. This prevents analysts from asking some legitimate questions (for example, are people wearing masks? ). Dissatisfied with the present status quo, MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) developed a system with other institutions to better guarantee privacy in surveillance video footage. The system, dubbed “Privid,” allows analysts to input video data searches and then adds a tiny amount of noise (additional data) to the result to ensure that no one can be identified. The method is based on a formal notion of privacy known as “differential privacy,” which permits without having access to aggregate statistics about private data disclosing individually identifying information.
 Continue Reading
 Paper: https://arxiv.org/pdf/2106.12083.pdf
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This guy used AI to create a voice model of Sam O'Nella and made a video in his style.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tsknzb/this_guy_used_ai_to_create_a_voice_model_of_sam/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tsknzb/this_guy_used_ai_to_create_a_voice_model_of_sam/"/>
        <updated>2022-03-30T22:10:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/KirbyBWCH  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Low MPJPE and low PCK and AUC]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsitmb/d_low_mpjpe_and_low_pck_and_auc/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsitmb/d_low_mpjpe_and_low_pck_and_auc/"/>
        <updated>2022-03-30T20:47:35.000Z</updated>
        <summary type="html"><![CDATA[So in the pose estimation landscape (specifically 3D) there are 3 common evaluation metrics MPJPE (mean per joint precision error measured in mm), PCK (percent correct keypoints measured at a 150mm threshold) and then AUC. One oddity I have noticed when training a few models is that a model with a lower MPJPE then some methods does not alwayshave a higher PCK (higher is better for this metric) but the mean joint precsion error is substantially below the threshold used for PCK (I do recognize that this metric is an average). 
 ​
 Does anyone have any experience with this, seen the same behavior, or have any intuition why this would be occurring?
    submitted by    /u/AbjectDrink3276  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] A Conversational Paradigm for Program Synthesis]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsigde/r_a_conversational_paradigm_for_program_synthesis/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsigde/r_a_conversational_paradigm_for_program_synthesis/"/>
        <updated>2022-03-30T20:30:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Wiskkey  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Training Compute-Optimal Large Language Models. From the abstract: "We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant."]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsid21/r_training_computeoptimal_large_language_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsid21/r_training_computeoptimal_large_language_models/"/>
        <updated>2022-03-30T20:26:33.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Wiskkey  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Some challenges that must be taken care of while working with ML using medical data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsfppe/d_some_challenges_that_must_be_taken_care_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsfppe/d_some_challenges_that_must_be_taken_care_of/"/>
        <updated>2022-03-30T19:38:50.000Z</updated>
        <summary type="html"><![CDATA[More than a year ago, I wrote an article regarding some key obstacles that someone may face regarding working with AI in the medical field. A few days ago I submitted that article to "Towards Data Science", a 'Medium' based online publication. It got published yesterday. I am giving the link here. If anyone is interested in that topic, you can take a look. It mainly focuses on the part that - even if you have some previous experience in working with machine learning, there are some things you must know and be aware of before working with medical datasets. Link - https://towardsdatascience.com/some-key-challenges-in-building-an-ai-model-for-medical-diagnosis-63f7438f14a
    submitted by    /u/ishtiakmahmud  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is quantum ML pointless?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsffzi/d_is_quantum_ml_pointless/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsffzi/d_is_quantum_ml_pointless/"/>
        <updated>2022-03-30T19:36:14.000Z</updated>
        <summary type="html"><![CDATA[Today Google had a webinar on Tensorflow Quantum for big data, which I attended. I was surprised that it was almost all quantum computing theory, but there was a link in the talk resources to Tensorflow Quantum where I was told I could find a tutorial with demo code for a classifier system to compare it to my classical approaches -- I use logistic regression, support vector machine, and Tensorflow DNN classifiers; mostly SVM because it works almost as accurately on my job's data sets as DNNs but takes a tiny fraction of the time to train.
 So, I took a look at it: https://www.tensorflow.org/quantum/tutorials/mnist
 This was the first sign that quantum classification might not be a viable alternative:
  
An image size of 28x28 is much too large for current quantum computers.
 -- https://www.tensorflow.org/quantum/tutorials/mnist#12_downscale_the_images
  
You really have to see it to believe it, but this demo requires downscaling legible digits for handwriting recognition to 4-by-4 pixel completely indiscernible blobs! Resulting in, as you might expect, terrible accuracy. A classical model using the full resolution images achieves 99.9%+ accuracy in relatively almost no time to train.
 So I scrolled down to the "Comparison" section and saw this:
  
a classical model of similar power (~32 parameters) trains to a similar accuracy in a fraction of the time. One way or the other, the classical neural network easily outperforms the quantum neural network. For classical data, it is difficult to beat a classical neural network.
  
The remainder of the tutorial didn't offer any improvement. The "quantum convolutional" NN classifier wasn't any better in speed or accuracy.
  
So, am I correct in assuming that I am best off ignoring quantum computing for classification tasks for the foreseeable future?
 How long do you think it will be until quantum ML can compete on real-world problems?
    submitted by    /u/Competitive_Travel16  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why isn't epsilon reset regularly in epsilon greedy policies to aid exploration?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tsf5gf/why_isnt_epsilon_reset_regularly_in_epsilon/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tsf5gf/why_isnt_epsilon_reset_regularly_in_epsilon/"/>
        <updated>2022-03-30T19:31:09.000Z</updated>
        <summary type="html"><![CDATA[surely this improves exploration? e.g. I took a default dqn and after 200k frames mountain car env didn't get to the top but after modifying training to reset epsilon to eps max every 3k steps (with a 1000 frame decay rate from 1 to 0.1) helped increase the score during training and got -133.
 after testing a bit I found it helps training if you modify how many N steps you reset epsilon as you train, e.g. I start on 3k for 100k frames and then 10k for 100k frames and then no resetting.
 I can't be the first and I understand there are much more mathematically stronger exploration techniques but is this a poorman's exploration technique?
    submitted by    /u/clockface99  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] CodeGen (up to 16.1B code generating transformer trained on TPU-v4) is open-source]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tsd28t/r_codegen_up_to_161b_code_generating_transformer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tsd28t/r_codegen_up_to_161b_code_generating_transformer/"/>
        <updated>2022-03-30T18:31:35.000Z</updated>
        <summary type="html"><![CDATA[https://twitter.com/erik_nijkamp/status/1508956485379715072
 Paper: https://arxiv.org/abs/2203.13474
 Blog: https://blog.salesforceairesearch.com/codegen/
 Code: https://github.com/salesforce/CodeGen
    submitted by    /u/lucidraisin  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Classification With Vision Transformers in a Gradio Web App]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tscamj/image_classification_with_vision_transformers_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tscamj/image_classification_with_vision_transformers_in/"/>
        <updated>2022-03-30T17:57:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta AI Team Open-Sources Mephisto: A New Platform For Open And Collaborative Way Of Collecting Data To Train ML Models]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tsbf8i/meta_ai_team_opensources_mephisto_a_new_platform/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tsbf8i/meta_ai_team_opensources_mephisto_a_new_platform/"/>
        <updated>2022-03-30T17:17:57.000Z</updated>
        <summary type="html"><![CDATA[Training datasets are very important for experimenting with varied data to train new AI models. However, many commonly used public data sets contain labeling errors. This makes it challenging to train robust models, particularly for novel tasks. Many researchers use techniques such as employing a variety of data quality control procedures to overcome these shortcomings. However, there is no centralized repository consisting of examples of using these strategies.
 Meta AI researchers have recently released Mephisto. It is a new platform to collect, share, and iterate on the most promising approaches to collecting training datasets for AI models. Researchers can exchange unique collecting strategies with Mephisto in a reusable and iterable format. It also allows them to change out components and quickly locate the exact annotations required, minimizing the barrier to custom task creation.
 Continue Reading
 Github: https://github.com/facebookresearch/Mephisto
 Documentation: https://mephisto.ai/
 https://preview.redd.it/mae7igz11kq81.png?width=1920&format=png&auto=webp&s=764145c4350d73049ae49faafd43ac3806712a2d
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How is portable AM radio possible?]]></title>
        <id>https://www.johndcook.com/blog/?p=101269</id>
        <link href="https://www.johndcook.com/blog/2022/03/30/portable-am-radio/"/>
        <updated>2022-03-30T16:20:14.000Z</updated>
        <summary type="html"><![CDATA[The length of antenna you need to receive a radio signal is proportional to the signal’s wavelength, typically 1/2 or 1/4 of the wavelength. Cell phones operate at gigahertz frequencies, and so the antennas are small enough to hide inside the phone. But AM radio stations operate at much lower frequencies. For example, there’s a […]
How is portable AM radio possible? first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Computing Memristor To Unlock AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tsa1jr/quantum_computing_memristor_to_unlock_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tsa1jr/quantum_computing_memristor_to_unlock_ai/"/>
        <updated>2022-03-30T16:15:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/getrich_or_diemining  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Do you know application fields in the sector of machine learning where precise coordination might play a role?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts8omj/d_do_you_know_application_fields_in_the_sector_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts8omj/d_do_you_know_application_fields_in_the_sector_of/"/>
        <updated>2022-03-30T15:14:25.000Z</updated>
        <summary type="html"><![CDATA[Self-Synchronizing Oscillators are mainly a hardware-software combination that uses swinging oscillators for decentral synchronization of distributed units without a central steering element. It is a new approach to synchronize two or more entities with another. Instead of relying on a central clock which the other ones communicate with, this technology is mutually or naturally synchronized, so both entities know at any time what the other one is doing.
 My question would be, are there any possible application fields you could think of for this technology?
    submitted by    /u/mikeseboss  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Predicting and correcting error of a simple model with a lot of data against a much more complex model with less data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts81uq/d_predicting_and_correcting_error_of_a_simple/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts81uq/d_predicting_and_correcting_error_of_a_simple/"/>
        <updated>2022-03-30T14:45:24.000Z</updated>
        <summary type="html"><![CDATA[I tried to keep the title somewhat general in case my problem is interesting for others, but before continuing with the discussion I'll introduce some specifics to make it easier to talk about the specific problem.
 I'm a fluid dynamics engineer, in particular a CFD engineer (fluid simulations and such), working on a phenomenon known as cavitation on hydrofoils.
 The most common way to perform a full simulation for a cavitating hydrofoil requires approximately 8 hours to run on a 512 cores.
 I'm currently working on an approximate model that solves the same problem in less than a minute on a common laptop. Of course, as an approximate model it is less faithful than the full model, with the relative error increasing or decreasing as some of the foilparameters change.
 Namely, a foil is defi…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D],[P] Guidance required for solving a unique problem in application of DL in CFD]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts7rta/dp_guidance_required_for_solving_a_unique_problem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts7rta/dp_guidance_required_for_solving_a_unique_problem/"/>
        <updated>2022-03-30T14:32:31.000Z</updated>
        <summary type="html"><![CDATA[Hello All,
 I am currently working on the project wherein I have to apply DL to CFD simulations. The simulation data is basically a set of 2d points (x,y) and their corresponding velocity and pressure values. I have such data for a 100 timesteps. The goal is to predict the flow(i.e. velocity and pressure) for each grid point for the last time step. I was thinking of reshaping the data in the form of a square so that I can use a CNN, but using a CNN wouldn't take care of the time dependence between the data. Is there a hybrid approach I can use that can take care of both temporal and spatial dependencies?
 ​
 Really need some guidance. Even any unrelated advice would be much appreciated. Thank you in advance!
 ​
 Edit: Also needed some help with regards to making the dataset. I have 100 csv files, each file corresponding to one time step. Each file contains the pressure and velocities of around 900 points. How do I make a dataset out of this either in pytorch or tensorflow?
    submitted by    /u/Hour_Amphibian9738  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applications of continued fractions]]></title>
        <id>https://www.johndcook.com/blog/?p=101260</id>
        <link href="https://www.johndcook.com/blog/2022/03/30/continued-fraction-applications/"/>
        <updated>2022-03-30T13:32:24.000Z</updated>
        <summary type="html"><![CDATA[At first glance, continued fractions look more like a curiosity than like useful mathematics. And yet they come up surprisingly often in applications. For an irrational number x, the numbers you get by truncating the infinite continued fraction for x are the optimal rational approximations to x given the size of their denominators. For example, […]
Applications of continued fractions first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polestar’s Dennis Nobelius on the Sustainable Performance Brand’s Plans]]></title>
        <id>https://blogs.nvidia.com/?p=56405</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/30/polestar/"/>
        <updated>2022-03-30T13:00:49.000Z</updated>
        <summary type="html"><![CDATA[Four words: smart, sustainable, Super Bowl. Polestar’s commercial during the big game made it clear no-compromise electric vehicles are now mainstream. Polestar Chief Operating Officer Dennis Nobelius sees driving enjoyment and autonomous-driving capabilities complementing one another in sustainable vehicles that keep driving — and the driver — front and center. NVIDIA’s Katie Washabaugh spoke with Read article >
The post Polestar’s Dennis Nobelius on the Sustainable Performance Brand’s Plans appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Jurgen Ferchau</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How dstack works]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts4vbq/d_how_dstack_works/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts4vbq/d_how_dstack_works/"/>
        <updated>2022-03-30T12:02:15.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, I’m the creator of dstack, an open-core tool to train models and manage data. I’ve just published a post where I elaborated on the challenges AI researchers face today when training models, and how we at dstack aim to address them. In the post, you may find the details on the design decision we made for our tool. 
 Blog post: https://blog.dstack.ai/p/how-dstack-works
 Invite everyone to read it, and share their thoughts. Happy to discuss the future of the developer tooling for training models!
    submitted by    /u/cheptsov  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cooperative multi-agent with a global reward function]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ts4ct0/cooperative_multiagent_with_a_global_reward/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ts4ct0/cooperative_multiagent_with_a_global_reward/"/>
        <updated>2022-03-30T11:32:33.000Z</updated>
        <summary type="html"><![CDATA[In my environment, I have multiple agents that need to cooperate. The reward function is global, such that it depends on the overall state of the system, and not just the sum of each agent reward. Could you point me to some relevant literature in this field?
    submitted by    /u/fedetask  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NEW EXCITING STUDY: GRIEVERS AND CHATBOTS]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ts44n7/new_exciting_study_grievers_and_chatbots/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ts44n7/new_exciting_study_grievers_and_chatbots/"/>
        <updated>2022-03-30T11:18:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/annaksig  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Differentiable Conv Layer using FFT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts3wsb/r_differentiable_conv_layer_using_fft/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts3wsb/r_differentiable_conv_layer_using_fft/"/>
        <updated>2022-03-30T11:04:46.000Z</updated>
        <summary type="html"><![CDATA[This is convolutional layer for torch using fourier transform. I wouldn't be surprised if this already existed somewhere, but I could not find one with derivatives. 
 This is meant to be a drop in replacement for torch.Conv. It should be performant on kernel sizes above 20, depending on implementation. 
 One interesting thing, even if a person already had one of these, is the way the bias and bias gradient were calculated. It only cost O(out_channels), ignoring the data size entirely.
 github
    submitted by    /u/MKmisfit  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI that can expand the borders of video?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ts3pv4/ai_that_can_expand_the_borders_of_video/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ts3pv4/ai_that_can_expand_the_borders_of_video/"/>
        <updated>2022-03-30T10:53:01.000Z</updated>
        <summary type="html"><![CDATA[Hi do any of you know the name of a AI that can expand the borders of a video
 By making the AI guess what would be around the square of the video? 
 I remember I once saw something like this in a video on two minute papers youtube channel
 Where they used an example with a eagle flying in the sky
 But I haven't been able to find the video since
 And I have been looking for a long time 
 But having such AI will be very help full to video editing
    submitted by    /u/Pwichmann  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[11 Best Python Books for beginners to advanced to read in 2022 -]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ts3pud/11_best_python_books_for_beginners_to_advanced_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ts3pud/11_best_python_books_for_beginners_to_advanced_to/"/>
        <updated>2022-03-30T10:52:58.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Fully unsupervised multi-domain image-to-image translation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts3k7z/r_fully_unsupervised_multidomain_imagetoimage/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts3k7z/r_fully_unsupervised_multidomain_imagetoimage/"/>
        <updated>2022-03-30T10:42:37.000Z</updated>
        <summary type="html"><![CDATA[The code of "A Style-aware Discriminator for Controllable Image Translation" has been released! This is a novel multi-domain image-to-image translation method, which is fully unsupervised, and provide various applications, including style interpolation, content transplantation, and local image translation.
 Example of the prototype-guided synthesis
 Example of the reference-guided synthesis
 Paper: https://arxiv.org/abs/2203.15375
 Code: https://github.com/kunheek/style-aware-discriminator
    submitted by    /u/graysp4rrow  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Data Visualization books for Data Science to read in 2022]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ts3f5q/best_data_visualization_books_for_data_science_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ts3f5q/best_data_visualization_books_for_data_science_to/"/>
        <updated>2022-03-30T10:33:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any well known approaches to compare two sets of neural network weights ?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts3dqg/d_any_well_known_approaches_to_compare_two_sets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts3dqg/d_any_well_known_approaches_to_compare_two_sets/"/>
        <updated>2022-03-30T10:30:32.000Z</updated>
        <summary type="html"><![CDATA[Say given an MLP of 2 layers with non-linearity, are there established papers which explore if the sets of weights obtained after 2 trials of training end up with 'similar' weights.
 From an old stackexchange thread(2017) two possible methods outlined are. 1. Compare similarity on the predictions on validation inputs. 2. Instead of comparing pairwise similarity, simply concat them and use t-sne for dimensionality reduction. Based on a 2009 work.
 Link: https://cs.stackexchange.com/questions/74488/measuring-difference-between-two-sets-of-neural-network-weights
 Does anyone know of any recent work which tackles this problem ?
    submitted by    /u/PaganPasta  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining overfitting and why 100% accuracy is not a guarantee to clients]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ts1tyo/explaining_overfitting_and_why_100_accuracy_is/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ts1tyo/explaining_overfitting_and_why_100_accuracy_is/"/>
        <updated>2022-03-30T08:34:24.000Z</updated>
        <summary type="html"><![CDATA[What are your approaches to explaining these topics to business people?
    submitted by    /u/RubiksCodeNMZ  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What metric do you use for hyperparameter tuning?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ts1lcj/what_metric_do_you_use_for_hyperparameter_tuning/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ts1lcj/what_metric_do_you_use_for_hyperparameter_tuning/"/>
        <updated>2022-03-30T08:16:19.000Z</updated>
        <summary type="html"><![CDATA[Pretty much as the title says. I work in research with electroencephalography (EEG) classification (those electrodes they stick on peoples heads). EEG is notoriously noisy and prone to overfitting, I have generally used validation accuracy as an optimization metric for a Bayesian HP tuning approach, but I find this tends to result in fairly unreliable models, even using cross validation approaches. These models are really noisy and while they may reach pretty good accuracy, that is a single epoch where it got a decent spike. I was wondering if there are common resolutions for this that I have missed, or if anyone has had luck with a custom metric that takes into account not just the validation accuracy, but the consistency and the difference between validation and training accuracy to better account for overfitting. Thanks!
    submitted by    /u/Ozzod  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Stable Reference for candidate ops in NAS]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts19ao/d_stable_reference_for_candidate_ops_in_nas/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts19ao/d_stable_reference_for_candidate_ops_in_nas/"/>
        <updated>2022-03-30T07:49:58.000Z</updated>
        <summary type="html"><![CDATA[Good day, my fellow researchers and engineers.
 Recently I'm on research about neural architecture search. While reading through tons of papers about neural architecture search, I just got curious about 'how do we predefine primitive operations?' Because most of the paper goes like 'we have these ops in candidates, and we searched through candidates so elegantly...' I mean, how do we know we've predefined our candidates well? Is there any reference to 'good ops candidates'? I know certain cells and ops are often used in certain tasks, but still, I want to find a robust reference about 'The ops candidates for NAS'.
 It will cure my high blood pressure if you guys give your precious opinions about it.
    submitted by    /u/KindAd9527  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R]Introducing causal inference in the energy-efficient building design process]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts0wad/rintroducing_causal_inference_in_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts0wad/rintroducing_causal_inference_in_the/"/>
        <updated>2022-03-30T07:22:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Mammoth-Ad-5527  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Docs Now Auto-Generate Short Summaries Using Machine Learning]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ts0hg8/google_docs_now_autogenerate_short_summaries/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ts0hg8/google_docs_now_autogenerate_short_summaries/"/>
        <updated>2022-03-30T06:53:03.000Z</updated>
        <summary type="html"><![CDATA[Many of us find it difficult to keep up with the daily flood of documents in our inboxes. These could be reports, reviews, briefs, policies, etc. Nowadays, readers wish to have a concise summary including major elements of their document, helping them prioritize their work efficiently. However, writing a document summary from scratch manually is a time-consuming task.
 To aid document writers in writing content summaries, Google announced a new feature enabling Google Docs to generate ideas automatically when they are available. The team employs a machine learning (ML) model to understand document text and provide a one- to two-sentence natural language description of the material. On the other hand, the document writer retains complete control, choosing whether to accept the proposal as-is, make necessary adjustments to better capture the document summary, or ignore it entirely. This section, combined with the outline, can help readers understand and navigate the work at a high level. While anybody can contribute summaries, only Google Workspace business customers have access to auto-generated ideas.
 Continue Reading
 https://i.redd.it/pcrn8rqmxgq81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]: Why does almost no one study "Weakly Supervised Object Detection"(WSOD) since 2020?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts0fcs/d_why_does_almost_no_one_study_weakly_supervised/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts0fcs/d_why_does_almost_no_one_study_weakly_supervised/"/>
        <updated>2022-03-30T06:48:46.000Z</updated>
        <summary type="html"><![CDATA[I notice that there is almost no papers in this area since 2020. And the rank of WSOD hasn't been updated since 2020:https://paperswithcode.com/sota/weakly-supervised-object-detection-on-pascal-1
    submitted by    /u/voclee4  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Recursive error prediction]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts0dfs/d_recursive_error_prediction/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts0dfs/d_recursive_error_prediction/"/>
        <updated>2022-03-30T06:44:56.000Z</updated>
        <summary type="html"><![CDATA[I had an idea recently for a ML regression strategy and I'm just wondering if something like this already exists. It has similarities with both boosting and bagging, but I think it's ultimately different from both.
 The basic idea is that you start with a subset of input features and train a model on that subset. Any common model will do as long as it doesn't just spit out the exact target value when making a prediction on the training set (i.e., you couldn't use a 1-neighbor KNN). After fitting the model, you make predictions on the training set (with the same subset of features) and calculate the prediction errors.
 Then using another subset of features (I would think it should be mutually exclusive from the first subset but maybe it doesn't have to be), you train a separate model, but rather than training on the original Y training data, you use the error of the previous model as the target for the second model.
 You repeat this process until all features have been used or as many times as desired. To make a prediction, the parent model would simply sum the predictions of the child models.
 As an additional thought, you might use more regularization, larger leaf size for decision trees, etc. for each additional model. You could also use bagging to create multiple instances of the strategy with different feature subsets in order to create multiple "pathways" through the data.
 A few questions:
  
Is this sufficiently different from existing boosting/bagging techniques?
 If yes to #1, are there any existing packages (preferably in Python) that implement this kind of technique?
 Could this be used to reduce overfitting for higher dimensional datasets? If so, would additional steps need to be taken (e.g., like the iterative regularization scheme I mentioned)? My thought is that it's a kind of divide-and-conquer strategy where each subsequent model is asked to do a little less than the previous model.
  
Any thoughts are appreciated.
    submitted by    /u/JHogg11  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there an AI for psychological testing?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ts088h/is_there_an_ai_for_psychological_testing/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ts088h/is_there_an_ai_for_psychological_testing/"/>
        <updated>2022-03-30T06:34:34.000Z</updated>
        <summary type="html"><![CDATA[I want to test myself using a neural net in lieu of psychological testing, and was wondering if that's even publicly available.
    submitted by    /u/AdvancedRazzmatazz46  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] STaR: Bootstrapping Reasoning With Reasoning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ts0757/r_star_bootstrapping_reasoning_with_reasoning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ts0757/r_star_bootstrapping_reasoning_with_reasoning/"/>
        <updated>2022-03-30T06:32:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Pathways: Asynchronous Distributed Dataflow for Machine Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trzo02/r_pathways_asynchronous_distributed_dataflow_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trzo02/r_pathways_asynchronous_distributed_dataflow_for/"/>
        <updated>2022-03-30T05:57:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Increasing Importance of Master Data Management for Your Business: A Primer]]></title>
        <id>https://www.datasciencecentral.com/?p=57007</id>
        <link href="https://www.datasciencecentral.com/the-increasing-importance-of-master-data-management-for-your-business-a-primer/"/>
        <updated>2022-03-30T05:49:33.000Z</updated>
        <summary type="html"><![CDATA[By Rex Ahlstrom, CTO & EVP Growth & Innovation, Syniti   The modern enterprise is composed of a variety of systems, each of which holds data the company needs to conduct business: information about products, services, suppliers, customers, and more. This is the master data, and master data collected by these disparate systems is often stored… Read More »The Increasing Importance of Master Data Management for Your Business: A Primer
The post The Increasing Importance of Master Data Management for Your Business: A Primer appeared first on Data Science Central.]]></summary>
        <author>
            <name>Rex Ahlstrom</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] TorchMultimodal is a PyTorch library for training state-of-the-art multimodal multi-task models at scale.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tryeym/n_torchmultimodal_is_a_pytorch_library_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tryeym/n_torchmultimodal_is_a_pytorch_library_for/"/>
        <updated>2022-03-30T04:37:33.000Z</updated>
        <summary type="html"><![CDATA[It provides:
  
A repository of modular and composable building blocks (models, fusion layers, loss functions, datasets and utilities).
 
A repository of examples that show how to combine these building blocks with components and common infrastructure from across the PyTorch Ecosystem to replicate state-of-the-art models published in the literature. These examples should serve as baselines for ongoing research in the field, as well as a starting point for future work.
 
 https://github.com/facebookresearch/multimodal
    submitted by    /u/gurkitier  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI beats 8 world champions at bridge]]></title>
        <id>https://www.reddit.com/r/artificial/comments/try6jr/ai_beats_8_world_champions_at_bridge/</id>
        <link href="https://www.reddit.com/r/artificial/comments/try6jr/ai_beats_8_world_champions_at_bridge/"/>
        <updated>2022-03-30T04:23:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nick7566  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How ECash Will Change The Economy]]></title>
        <id>https://www.datasciencecentral.com/?p=57012</id>
        <link href="https://www.datasciencecentral.com/how-ecash-will-change-the-economy/"/>
        <updated>2022-03-30T03:19:56.000Z</updated>
        <summary type="html"><![CDATA[The Biden Administration made a recent announcement that it was setting up an exploratory committee for the creation of an e-Currency taskforce. In conjunction with this, a new bill, the Electronic Currency and Secure Hardware (ECASH) Act, was introduced by Rep. Stephen Lynch (MA-08), Chair of the House Committee on Financial Services’ Task Force on… Read More »How ECash Will Change The Economy
The post How ECash Will Change The Economy appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kurt Cagle</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] AI Simulators for Assisted Living (from Facebook, CMU, et al)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trvkih/r_ai_simulators_for_assisted_living_from_facebook/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trvkih/r_ai_simulators_for_assisted_living_from_facebook/"/>
        <updated>2022-03-30T01:56:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aidev2040  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Desiderata for Representation Learning: A Causal Perspective]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trvfjk/r_desiderata_for_representation_learning_a_causal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trvfjk/r_desiderata_for_representation_learning_a_causal/"/>
        <updated>2022-03-30T01:49:27.000Z</updated>
        <summary type="html"><![CDATA[Authors: Yixin Wang, Michael I. Jordan
 Abstract: Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets. 
 Paper: https://arxiv.org/abs/2109.03795
 Slides: https://yixinwang.github.io/papers/causal-rep-slides-public.pdf
    submitted by    /u/bikeskata  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is tau in the Dyna-Q+ algorithm?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/trv703/what_is_tau_in_the_dynaq_algorithm/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/trv703/what_is_tau_in_the_dynaq_algorithm/"/>
        <updated>2022-03-30T01:36:51.000Z</updated>
        <summary type="html"><![CDATA[https://imgur.com/UOdDUFH
 From the linked image I am wondering what tau is (the tau looks like a small r in the image unless you zoom in)? Is it a hard coded value like kappa (k)? If not how is the value for tau determined when Dyna Q+ runs?
    submitted by    /u/lifelifebalance  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Worthwhile to convert custom env to be dm_env compatible?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/truggg/worthwhile_to_convert_custom_env_to_be_dm_env/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/truggg/worthwhile_to_convert_custom_env_to_be_dm_env/"/>
        <updated>2022-03-30T00:55:48.000Z</updated>
        <summary type="html"><![CDATA[Can anyone speak to their experience using acme (https://github.com/deepmind/acme) and by extension dm_env (https://github.com/deepmind/dm_env)? I'm wondering if it would be worthwhile for me to invest the time into converting my custom environment (which loosely follows the standard RL setup) over to this format.
 I quite like how acme does a lot of heavy lifting in the background and lays out their thoughts on best practices, but perhaps I'm being shortsighted by all the bells and whistles
    submitted by    /u/whynotmehmm  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transitions and the Arc of Systems Theory]]></title>
        <id>https://www.datasciencecentral.com/?p=56990</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-digest-29-march-2022-transitions-and-the-arc-of-systems-theory/"/>
        <updated>2022-03-30T00:46:17.000Z</updated>
        <summary type="html"><![CDATA[DSC Weekly Digest 29 March 2022 Back in September, I made a prediction: Covid-19 would spike throughout the winter but fade by April as it transitioned from being a pandemic virus to an endemic one. As it turns out, I was mostly correct. Here in Washington State, we finally dropped the mask mandate that had… Read More »Transitions and the Arc of Systems Theory
The post Transitions and the Arc of Systems Theory appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kurt Cagle</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What would a "Production" RL stack look like in terms of tooling?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trstl4/d_what_would_a_production_rl_stack_look_like_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trstl4/d_what_would_a_production_rl_stack_look_like_in/"/>
        <updated>2022-03-29T23:28:58.000Z</updated>
        <summary type="html"><![CDATA[I was hoping I could get some insight into the tooling that you use (or would use) for some production RL work? I'm mostly doing it all on my home machine as a fun side project. 
 I've got the following existing infrastructure: 
 - An interface based loosely on the standard RL setup. I'm thinking about adapting it to fit Acme to let it do more heavy lifting since I quite like `Haiku`, `rlax` and the rest of what they do.
 - I've got some models across languages (Pytorch and Jax) and this has been causing me some headache trying to make sure everything is abstract enough. Should I just stick to one language and make sure all my friends just use that same language? 
 - I'm currently using comet-ml for my experiment tracking, and for the most part I like it. However, I'm now looking around to see what's out there and I'm a little overwhelmed by (1) how many tools there are and (2) how some of them seem to "overlap" so I don't really know how to compose them.
 - configs all stored in a python file in a separate repo that is kept synced between my other repos.
 - I currently store my agent experiences (off policy) in a database that I later query to rapidly fill up the replay buffer. The limitation is that this is for a single agent. What drew me to Acme is that it seems to allow multiple agents to all use the same buffer? 
 _____________
 tl;dr
 1) Has anyone used Acme? I'm thinking of moving my project to it, but it might end up being a lot of effort for very little reward
 2) How do you and your teams handle multiple languages? Do you just have abstract gym wrappers that convert data? 
 3) What tools do you use and how do you compose them together? I'm so so lost trying to navigate this space
 4) How do you keep your configs synced when they are used between repos?
    submitted by    /u/whynotmehmm  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I have data with connections and links but I don't know how to write a scrip for this. Help!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trrpbd/p_i_have_data_with_connections_and_links_but_i/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trrpbd/p_i_have_data_with_connections_and_links_but_i/"/>
        <updated>2022-03-29T22:33:06.000Z</updated>
        <summary type="html"><![CDATA[My dates are as follows:
 ​
 https://preview.redd.it/9htkypafgeq81.png?width=198&format=png&auto=webp&s=35e5475cf71364b8958b34e6100bd0ada2dc756d
 What I would like is to be able to map the following to a script:
 - Value 1440/1 in column FROM represented value 144019/1 in column TO.
 - Find value 144019/1 again in column FROM.
 - If found, take the value in column TO and find it again in column FROM. Not found, stop searching.
 ​
 Note: value 1440/1 does not have to be the initial value. In my data, 1440/1 can refer to another value again that is from column TO.
 ​
 I would like the following as output:
 - 1440/1, 144019/1, 144019/2;
 - 1440/1, 144018/1, 144018/2, 6038/1.
    submitted by    /u/Silver-Panda2518  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flower Team Releases Flower 0.18 With Cool New Updates For Federated Learning]]></title>
        <id>https://www.reddit.com/r/artificial/comments/trps2d/flower_team_releases_flower_018_with_cool_new/</id>
        <link href="https://www.reddit.com/r/artificial/comments/trps2d/flower_team_releases_flower_018_with_cool_new/"/>
        <updated>2022-03-29T22:12:42.000Z</updated>
        <summary type="html"><![CDATA[Flower is an end-to-end federated learning framework that allows for a smoother transition from simulation-based experimental research to system research on many real-world edge devices. Flower has individual strengths in both domains (i.e., simulation and real-world devices) and the capacity to switch back and forth between the two extremes as needed throughout exploration and development. Researchers present use cases that drive our viewpoint, design goals, the resultant framework architecture, and comparisons to other frameworks in this part.
 Federated Learning (FL) has shown to be a viable option for enabling edge devices to develop a shared prediction model cooperatively while maintaining their training data on the device, divorcing the capacity to execute machine learning from the requirement to store data in the cloud. However, FL is challenging to implement practically in size and system heterogeneity. Although there are several research frameworks for simulating FL algorithms, none of them facilitate the investigation of scalable FL workloads on heterogeneous edge devices.
 Flower 0.18 released
 Thanks to a longer gap than usual, the latest Flower release has more upgrades than any previous release. Also, thanks to the wonderful community for your continuing support and generosity.
 Continue Reading
 Paper: https://arxiv.org/pdf/2007.14390.pdf
 Github: https://github.com/adap/flower
 https://preview.redd.it/ywtttqlnceq81.png?width=1920&format=png&auto=webp&s=893fbe79c190aa66e293b296e35d4096eb178f97
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Quantization Aware Training Advice?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tro8go/d_quantization_aware_training_advice/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tro8go/d_quantization_aware_training_advice/"/>
        <updated>2022-03-29T21:57:47.000Z</updated>
        <summary type="html"><![CDATA[Hi, I'm trying to run QAT on a MobileNetV2 based model but having some issues hitting the same training losses in the QAT phase as I did in the not-QAT phase.
 As a test, I'd trained the network for 1 epoch, then trained the QAT phase for 5 epochs and managed to get the same loss (actually lower). However, after training the model (non-QAT part) for 150 epochs, the QAT phase is really struggling to get down to the same loss.
 In my first test it dropped then completely levelled off for 2-3 epochs then nosedived again for another epoch, I'm not seeing the same in this longer train though.
 I was wondering if anyone has any advice on things like, should the learning rate be reset at the start of the QAT phase or should it carry on from where the training left off? I'm using Adam as the optimiser in the first phase, is that still ok in the second phase. Any other things that I could try? I did read a paper on improving quantization loss in MobileNet by L2 weighting the separable conv weights and swapping out ReLU6 for ReLU but I wasn't really seeing the same benefit as the paper (https://arxiv.org/pdf/1803.08607.pdf) did in my tests, I was getting a worse initial network.
 Thanks for any insight that anyone can provide!
    submitted by    /u/ColdChancer  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Interactive Demo for Paper SketchEdit: Mask-Free Local Image Manipulation with Partial Sketches]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trno8g/p_interactive_demo_for_paper_sketchedit_maskfree/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trno8g/p_interactive_demo_for_paper_sketchedit_maskfree/"/>
        <updated>2022-03-29T21:45:39.000Z</updated>
        <summary type="html"><![CDATA[​
 https://reddit.com/link/trno8g/video/84uuyln8ceq81/player
 Hi everyone, here's an interactive demo I made for paper SketchEdit: Mask-Free Local Image Manipulation with Partial Sketches
 Demo: http://47.57.135.203:8001/
 Paper: https://arxiv.org/abs/2111.15078
 Project page: https://zengxianyu.github.io/sketchedit/
 Code: https://github.com/zengxianyu/sketchedit
    submitted by    /u/Educational_Ebb2502  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] DailyML quiz: A very high variance means the model likely has…]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trm12l/d_dailyml_quiz_a_very_high_variance_means_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trm12l/d_dailyml_quiz_a_very_high_variance_means_the/"/>
        <updated>2022-03-29T21:08:42.000Z</updated>
        <summary type="html"><![CDATA[Yesterday's answer: Pandas
 View Poll
    submitted by    /u/daichrony  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][N] CompilerGym Tutorial @ CGO]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trjiul/pn_compilergym_tutorial_cgo/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trjiul/pn_compilergym_tutorial_cgo/"/>
        <updated>2022-03-29T20:34:32.000Z</updated>
        <summary type="html"><![CDATA[This weekend we (Hugh, Mostafa, and Chris from Meta AI) will be running a tutorial on Autotuning and Reinforcement Learning for compilers using CompilerGym at CGO’22. Join us for a hands-on session that takes you from “zero to RL” in three hours! The tutorial stats 1:30pm ET on Saturday April 2nd. Full schedule:
 https://conf.researchr.org/program/cgo-2022/program-cgo-2022/?date=Sat%202%20Apr%202022
    submitted by    /u/melhoushi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI podcast: from neuroscience to deep learning]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tri77x/ai_podcast_from_neuroscience_to_deep_learning/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tri77x/ai_podcast_from_neuroscience_to_deep_learning/"/>
        <updated>2022-03-29T20:14:51.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/aidev2040  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R]Looking for papers with Date Inference from text]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trgzx4/rlooking_for_papers_with_date_inference_from_text/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trgzx4/rlooking_for_papers_with_date_inference_from_text/"/>
        <updated>2022-03-29T20:03:03.000Z</updated>
        <summary type="html"><![CDATA[Hey all,
 I'm going through research to figure out how the Date Inference protocols might be implemented. Given a text containing the phrase "5 days from now", I would need it to infer the date April 3rd. The 'now' part is a trivial problem, but the inference is something I'm struggling with. I could use regex but all the possible edge cases are tricky. 
 The inference would need to work on unstructured cases like "April 23rd", "Next Sunday", etc. It would need to work forwards and backward (5 days ago etc.)
 ​
 Any great papers/resources? I searched for date inference, but nothing similar to what I'm looking for
    submitted by    /u/ISeeThings404  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] AudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trglxa/research_audiotagging_done_right_2nd_comparison/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trglxa/research_audiotagging_done_right_2nd_comparison/"/>
        <updated>2022-03-29T19:59:22.000Z</updated>
        <summary type="html"><![CDATA[Paper: https://arxiv.org/abs/2203.13448
 Code: https://github.com/lijuncheng16/AudioTaggingDoneRight
 For anyone who's interested in AudioSet (2million youtube videos' sound). This is the SOTA comparison of models and training procedures.
    submitted by    /u/billyli_16  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive ML Strategy course with Foster Provost starting April 7]]></title>
        <id>https://machinelearningmastery.com/?p=13400</id>
        <link href="https://machinelearningmastery.com/interactive-ml-strategy-course-with-foster-provost-starting-april-7/"/>
        <updated>2022-03-29T19:41:10.000Z</updated>
        <summary type="html"><![CDATA[Sponsored Post Building successful machine learning products requires mastering ML Strategy, including problem formulation, evaluation, and tactics for dealing with […]
The post Interactive ML Strategy course with Foster Provost starting April 7 appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Mitch Bartlett</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] College course on ML with an object detection project and competition between teams within the class. Help me make the best model possible.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trejlz/p_college_course_on_ml_with_an_object_detection/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trejlz/p_college_course_on_ml_with_an_object_detection/"/>
        <updated>2022-03-29T18:54:42.000Z</updated>
        <summary type="html"><![CDATA[The objective is simple, a kit with some small hardware is given to us (nuts, bolts, washers, etc). Using our laptop cameras, we need to develop a model that is able to accurate classify what object is what when placed infront of the camera. There can be any number of objects in any orientation, displayed on any color surface. What is the best way to approach this problem, what is a good model structure (high level) and what can I do to be a step above the competition.
    submitted by    /u/Certified_User  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Carbon vs Silicon molecule]]></title>
        <id>https://www.reddit.com/r/artificial/comments/treb6c/carbon_vs_silicon_molecule/</id>
        <link href="https://www.reddit.com/r/artificial/comments/treb6c/carbon_vs_silicon_molecule/"/>
        <updated>2022-03-29T18:43:52.000Z</updated>
        <summary type="html"><![CDATA[Since we are about to create a silicon-based lifeform I was looking up the differences between the carbon and the silicon molecule.
 https://www.differencebetween.com/difference-between-silicon-and-vs-carbon/#:~:text=The%20key%20difference%20between%20silicon,in%20the%20outer%20energy%20level.
    submitted by    /u/asenz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI: Super fast 3D perception from Nvidia, Ukraine uses face recognition to identify dead Russian soldiers, US-China AI collaboration drops, and more!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/treaze/last_week_in_ai_super_fast_3d_perception_from/</id>
        <link href="https://www.reddit.com/r/artificial/comments/treaze/last_week_in_ai_super_fast_3d_perception_from/"/>
        <updated>2022-03-29T18:43:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] There is no time to read the textbook as a researcher]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/trbwwb/d_there_is_no_time_to_read_the_textbook_as_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/trbwwb/d_there_is_no_time_to_read_the_textbook_as_a/"/>
        <updated>2022-03-29T18:09:40.000Z</updated>
        <summary type="html"><![CDATA[I'm a researcher who is deeply interested in deep generative models. There are excellent textbooks I want to read if time allows, such as:
 Probabilistic ML: https://probml.github.io/pml-book/book1.html
 PRML: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf
 However, there are also many papers I have to read, new theories I have to learn, and works I need to finish. The problem is that reading textbooks could deepen the fundamental understanding of the field but rarely gives an immediate reward. Practically, reading a textbook from start to end can take > 1000 hours, in which one can read more than a hundred papers. Given the situation, I have studied basic stuff only when I need them for my research (you know, publish or perish).
 ​
 However, I think the time to read textbooks will decrease rather than increase, and only junior researchers will be able to afford to read them. It means that if I don't read them now, I won't be able to read later. Is there any general advice on this?
    submitted by    /u/SnooPandas3529  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Will a recommender system alone solve this issue?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/traoa1/p_will_a_recommender_system_alone_solve_this_issue/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/traoa1/p_will_a_recommender_system_alone_solve_this_issue/"/>
        <updated>2022-03-29T17:45:39.000Z</updated>
        <summary type="html"><![CDATA[I have a project featuring 5+ years of data detailing mechanic reports. Essentially, I want to use ML to build a model that can make suggested actions to fix an issue based on these mechanic reports. For example, if a user typed in that a car “makes a squeaky sound” then it suggests three courses of action that may fix the issue based on similar issues and solutions detailed in the mechanic reports. 
 Furthermore, when returning these suggestions, I want the user to see some sort of score indicating how likely it is to fix the issue (i.e. option A worked 97% of the time, option B 2% of the time, and option C 1% of the time). I also want the user to be able to try the options and give feedback on if they fixed the issue. 
 My brain immediately went to a recommender system, but I don’t have much experience with creating them. Can they do all of the above (recommend solutions, score solutions, and allow for user input to keep training the model) or do I need to somehow pair with another method/model? I’m just not sure where to start.
    submitted by    /u/ambiguousalmond  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalize cross-channel customer experiences with Amazon SageMaker, Amazon Personalize, and Twilio Segment]]></title>
        <id>baf199adf9d92a5962f4ec20ce81ff5fa0421b0d</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/personalize-cross-channel-customer-experiences-with-amazon-sagemaker-amazon-personalize-and-segment/"/>
        <updated>2022-03-29T17:41:54.000Z</updated>
        <summary type="html"><![CDATA[Today, customers interact with brands over an increasingly large digital and offline footprint, generating a wealth of interaction data known as behavioral data. As a result, marketers and customer experience teams must work with multiple overlapping tools to engage and target those customers across touchpoints. This increases complexity, creates multiple views of each customer, and […]]]></summary>
        <author>
            <name>Dwayne Browne</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Maps Utilizes Machine Learning To Block Nearly 100 Million Fraudulent Edits]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tra5qt/google_maps_utilizes_machine_learning_to_block/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tra5qt/google_maps_utilizes_machine_learning_to_block/"/>
        <updated>2022-03-29T17:23:05.000Z</updated>
        <summary type="html"><![CDATA[In their recent post on how Google keeps Maps information reliable, the company elaborates how they use machine learning and human operators to block nearly 100 million attempted fraudulent edits to Google Business Profiles. Machine learning, in simple terms, is a sort of artificial intelligence (AI) that lets software applications improve their accuracy at predicting events without having to be explicitly programmed to do so. Machine learning algorithms use past data as input to forecast new output values.
 The world changed with the introduction of vaccinations, revisions to mask regulations, and new COVID variations in 2021. Accordingly, their Maps community updated Google Maps with further information about their nearby areas. Their contributions have helped Google provide updated company information, such as a location’s hours of operation or its health and safety regulations, for 30% more firms in 2021 than 2020. 
 Quick Read
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latest ‘I AM AI’ Video Features Four-Legged Robots, Smart Cell Analysis, Tumor-Tracking Tech and More]]></title>
        <id>https://blogs.nvidia.com/?p=56394</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/29/i-am-ai-gtc/"/>
        <updated>2022-03-29T17:00:26.000Z</updated>
        <summary type="html"><![CDATA[“I am a visionary,” says an AI, kicking off the latest installment of NVIDIA’s I AM AI video series. Launched in 2017, I AM AI has become the iconic opening for GTC keynote addresses by NVIDIA founder and CEO Jensen Huang. Each video, with its AI-created narration and soundtrack, documents the newest advances in artificial Read article >
The post Latest ‘I AM AI’ Video Features Four-Legged Robots, Smart Cell Analysis, Tumor-Tracking Tech and More appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Angie Lee</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Understanding Dimensional Collapse in Contrastive Self-supervised Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tr93ls/r_understanding_dimensional_collapse_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tr93ls/r_understanding_dimensional_collapse_in/"/>
        <updated>2022-03-29T16:56:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/fasttosmile  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated, scalable, and cost-effective ML on AWS: Detecting invasive Australian tree ferns in Hawaiian forests]]></title>
        <id>e75f3ef671be2d0ce497a26f64eec80bfff4f07b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/automated-scalable-and-cost-effective-ml-on-aws-detecting-invasive-australian-tree-ferns-in-hawaiian-forests/"/>
        <updated>2022-03-29T16:53:41.000Z</updated>
        <summary type="html"><![CDATA[This is blog post is co-written by Theresa Cabrera Menard, an Applied Scientist/Geographic Information Systems Specialist at The Nature Conservancy (TNC) in Hawaii. In recent years, Amazon and AWS have developed a series of sustainability initiatives with the overall goal of helping preserve the natural environment. As part of these efforts, AWS Professional Services establishes […]]]></summary>
        <author>
            <name>Dan Iancu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Avoid vver fitting in iterative pruning [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tr8g5s/avoid_vver_fitting_in_iterative_pruning_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tr8g5s/avoid_vver_fitting_in_iterative_pruning_d/"/>
        <updated>2022-03-29T16:50:33.000Z</updated>
        <summary type="html"><![CDATA[Avoid over fitting in iterative pruning 
 For iterative pruning algorithms referring to research papers like:
 ​
  
Learning both Weights and Connections for Efficient Neural Networks
 Deep Compression
 Comparing Rewinding and Fine-Tuning in Neural Network Pruning
  
I have found that during these pruning rounds, the pruned sub-network starts to overfit excessively, with training accuracy approaching almost 100%. This can be attributed to the fact that the surviving trained parameters are not reinitialized to either the randomly initialized values or to a previous value from earlier in the training.
 Whereas, for "The Lottery Ticket Hypothesis" and it's family of related research papers such as:
  
The Lottery Ticket Hypothesis
 Stabilizing the Lottery Ticket Hypothesis
 One ticket to win them all
 Deconstructing Lottery Tickets
  
such overfitting is usually not observed due to the weight rewinding scheme.
 Since, the original & unpruned deep learning architecture is already trained with strategies such as: data augmentation, weight decay, learning rate schedule, etc., the resulting iterative pruning rounds result in overfitting.
 Can you suggest a way to avoid these overfitting during these iterative pruning rounds?
    submitted by    /u/grid_world  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatically generate model evaluation metrics using SageMaker Autopilot Model Quality Reports]]></title>
        <id>97aaf4a45fb3f98dbca55c0fa275d369e8dd1ccf</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/automatically-generate-model-evaluation-metrics-using-sagemaker-autopilot-model-quality-reports/"/>
        <updated>2022-03-29T16:47:55.000Z</updated>
        <summary type="html"><![CDATA[Amazon SageMaker Autopilot helps you complete an end-to-end machine learning (ML) workflow by automating the steps of feature engineering, training, tuning, and deploying an ML model for inference. You provide SageMaker Autopilot with a tabular data set and a target attribute to predict. Then, SageMaker Autopilot automatically explores your data, trains, tunes, ranks and finds […]]]></summary>
        <author>
            <name>Peter Chung</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyML Gearbox Fault Prediction on a $4 MCU]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tr6mpr/tinyml_gearbox_fault_prediction_on_a_4_mcu/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tr6mpr/tinyml_gearbox_fault_prediction_on_a_4_mcu/"/>
        <updated>2022-03-29T16:22:14.000Z</updated>
        <summary type="html"><![CDATA[Is it possible to make an AI-driven system that predicts gearbox failure on a simple $4 MCU? How to automatically build a compact model that does not require any additional compression? Can a non-data scientist implement such projects successfully?
 I will answer all these questions in my new project.
 In industry (e.g., wind power, automotive), gearboxes often operate under random speed variations. A condition monitoring system is expected to detect faults, broken tooth conditions and assess their severity using vibration signals collected under different speed profiles.
 Modern cars have hundreds of thousands of details and systems where it is necessary to predict breakdowns, control the state of temperature, pressure, etc.As such, in the automotive industry, it is critically important t…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signapse: Harnessing CNNs to Teach Sign Langauge [Project] [Discussion]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tr6ej7/signapse_harnessing_cnns_to_teach_sign_langauge/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tr6ej7/signapse_harnessing_cnns_to_teach_sign_langauge/"/>
        <updated>2022-03-29T16:17:52.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I'm a student at the University of Glasgow building a linux app that is trying to use CNNs to teach people the ASL (american sign language) alphabet. We just released the first version of our software which (although admittedly buggy) is worth sharing with interested communities. In brief, a MobileNetv2 model is trained on kaggle data for each sign in the ASL alphabet, this is executed within the OpenCV framework and run on camera frames of the user in real time. The user is challenged to make different signs and rewarded when the correct sign is made. We would love for interested people to try out our software and let us know about enhancement ideas or any bugs they may find.
 If you are interested in the project, please head over to our GitHub to have a look:
 https://github.com/albanjoseph/Signapse
 You can also follow us on Facebook:
 https://www.facebook.com/Signapse-125793226671815
 Twitter:
 https://twitter.com/GU_Signapse
 and YouTube:
 https://www.youtube.com/channel/UCh2uG2pYoSloEU0IFeqDQMA
 Cheers!
 Signapse Team
    submitted by    /u/rossythebossy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed step function]]></title>
        <id>https://www.johndcook.com/blog/?p=101207</id>
        <link href="https://www.johndcook.com/blog/2022/03/29/smoothed-step-function/"/>
        <updated>2022-03-29T16:07:30.000Z</updated>
        <summary type="html"><![CDATA[I mentioned smoothed step functions in the previous post. What would you do if you needed to concretely use a smoothed step function and not just know that one exists? We’ll look at smoothed versions of the signum function sgn(x) = x / |x| which equals -1 for negative x and +1 for positive x. […]
Smoothed step function first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teens Develop Handwriting-Recognition AI for Detecting Parkinson’s Disease]]></title>
        <id>https://blogs.nvidia.com/?p=56389</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/29/handwriting-recognition-ai-parkinsons-disease/"/>
        <updated>2022-03-29T16:00:56.000Z</updated>
        <summary type="html"><![CDATA[When Tanish Tyagi published his first research paper a year ago on deep learning to detect dementia, it started a family-driven pursuit. Great-grandparents in his family had suffered from Parkinson’s, a genetic disease that affects more than 10 million people worldwide. So the now 16-year-old turned to that next, together with his sister, Riya, 14. Read article >
The post Teens Develop Handwriting-Recognition AI for Detecting Parkinson’s Disease appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Scott Martin</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[neural networks project]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tr566v/neural_networks_project/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tr566v/neural_networks_project/"/>
        <updated>2022-03-29T15:54:02.000Z</updated>
        <summary type="html"><![CDATA[Hi, hope everyone is doing will
 ​
 ​
 i am an EE student, i am currently taking a class (my first) in machine learning, also i am enjoying it so much, now we have reached the point where we are studying neural networks.. back propgation, CNN, RNN, Autoencoders, Deep Learning etc..
 ​
 and my prof wants us to get working with some projects (using neural network)
 ​
 basically he wants us to get some already written code in Gethub (((written with Pytorch))) and understand it, understand the task, the motivation, the structure and the math, and modify it if needed, and implement it, and this will be the project, no one starts from scratch
 ​
 for example:
 image classification
 object detection
 ​
 also he said that the code that identify numbers is simple for a project, it will be an assignment.
 ​
 so any ideas?
 or links
 or any advice in general
 ​
 thanks guys and all the best
    submitted by    /u/Torvaldz_  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q&A: Alberto Rodriguez on teaching a robot to find your keys]]></title>
        <id>https://news.mit.edu/2022/qa-alberto-rodriguez-teaching-robot-find-your-keys-pile-clutter-0329</id>
        <link href="https://news.mit.edu/2022/qa-alberto-rodriguez-teaching-robot-find-your-keys-pile-clutter-0329"/>
        <updated>2022-03-29T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[Associate professor and principal investigator with the MIT Schwarzman College of Computing’s Science Hub discusses the future of robotics and the importance of industry-academia collaborations.]]></summary>
        <author>
            <name>Kim Martineau | MIT Schwarzman College of Computing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D]Hugging Face Model Comparator Space Builder]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tr0vaq/dhugging_face_model_comparator_space_builder/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tr0vaq/dhugging_face_model_comparator_space_builder/"/>
        <updated>2022-03-29T14:21:55.000Z</updated>
        <summary type="html"><![CDATA[You can now build a space comparing Hugging Face Models and Spaces or create clones of them with Model Comparator Space Builder 📷📷📷
 https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder
 ​
 https://preview.redd.it/t40n9amr0cq81.png?width=1813&format=png&auto=webp&s=f151ea9060f7c5b43f8dbcf3f91d1c308bdbb422
 ​
 https://preview.redd.it/fx1ztghs0cq81.png?width=1848&format=png&auto=webp&s=f1cfcb2b47831b10744b4d0e114fd21ed725b195
 Gradio: https://github.com/gradio-app/gradio
 Hugging Face: https://huggingface.co/
    submitted by    /u/Mundane-Apartment224  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] scikit-learn transformer that turns categorical variables into dense vector representations]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tr0glo/p_scikitlearn_transformer_that_turns_categorical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tr0glo/p_scikitlearn_transformer_that_turns_categorical/"/>
        <updated>2022-03-29T14:02:41.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone. Our DS & DA team open sourced a Python library that helps in dealing with categorical variables for machine learning algorithms.
 It leverages Tensorflow/Keras embedding layers and builds a neural network that learns a dense representation of each unique class. This is all packaged inside a regular scikit-learn transformer that can be used within pipelines and can have its hyperparameters optimized with regular sklearn methods.
 Just do pip install embedding-encoder[tf]. Check out the readme at Github or the blog post for examples.
 Github: https://github.com/cpa-analytics/embedding-encoder
 PyPI: https://pypi.org/project/embedding-encoder/
 Blog post: https://cpa-analytics.github.io/embedding-encoder-intro/
 This was inspired by the 3rd place solution in the Rossmann Store Sales Kaggle competition. Some implementations have surfaced over the years but we are not aware of working one that integrates well with existing libraries.
 This is just another preprocessing technique. It can be optimal for your task or not. As always, try multiple approaches and evaluate the results!
    submitted by    /u/rafa10pj  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Inventory Management System: An Ultimate Guide for 2022 and Beyond]]></title>
        <id>https://www.datasciencecentral.com/?p=56998</id>
        <link href="https://www.datasciencecentral.com/automated-inventory-management-system-an-ultimate-guide-for-2022-and-beyond/"/>
        <updated>2022-03-29T13:58:59.000Z</updated>
        <summary type="html"><![CDATA[Inventory management is an essential part of any eCommerce business. Especially if you are an eCommerce business owner juggling multiple sales channels, it can save you a lot of effort. However, manually managing your inventories is also a recipe for error. Also, let’s not forget the time you have to spend and the painful process… Read More »Automated Inventory Management System: An Ultimate Guide for 2022 and Beyond
The post Automated Inventory Management System: An Ultimate Guide for 2022 and Beyond appeared first on Data Science Central.]]></summary>
        <author>
            <name>James Wilson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] Dealing with variable length input data]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tr055q/research_dealing_with_variable_length_input_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tr055q/research_dealing_with_variable_length_input_data/"/>
        <updated>2022-03-29T13:47:21.000Z</updated>
        <summary type="html"><![CDATA[I am building a ML model to classify malware. My input data are windows binaries which get de-compiled into functions. From these functions I create embeddings, each is 150 float numbers. 
 https://preview.redd.it/4ejwz7ukubq81.png?width=581&format=png&auto=webp&s=cbde0b534a8424f2a17ea5f1c77fccd2860685f5
 Problem: Each binary has a variable amount of functions. Some may have 40 functions while others may have over 1000. Most will have between 50 and 200. The order of the functions is not important.
 Question: What is the best way to deal with these variable amount of input. Hashing trick? Or Deep sets? What would you recommend?
    submitted by    /u/laddi27  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Podcast with Jonathan Frankle of MosaicML]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqzno6/discussion_podcast_with_jonathan_frankle_of/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqzno6/discussion_podcast_with_jonathan_frankle_of/"/>
        <updated>2022-03-29T13:23:34.000Z</updated>
        <summary type="html"><![CDATA[Jonathan came on the Weaviate Podcast to discuss the story of MosaicML, their new open-source Python library for Efficient Deep Learning called Composer, Pareto Curves of Training Time X Accuracy, Model Surgey augmentations, Maximizing CPU and GPU throughput, and many more! I hope you find this useful, happy to continue discussions of what Jonathan presented!
 https://www.youtube.com/watch?v=ZiBkspwrICA
    submitted by    /u/HenryAILabs  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Using large language models for classification of natural-language input]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqzlrh/d_using_large_language_models_for_classification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqzlrh/d_using_large_language_models_for_classification/"/>
        <updated>2022-03-29T13:20:47.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone!
 I'd like to use a large language model like T0pp or GPT-NeoX-20B to take a natural-language input from a user and map it to one of ~2000 possible VS Code command palette commands. Essentially, this is a classification problem of the form "NLP input -> command".
 The idea is to let users give voice input in natural language and then have the model figure out what command they most likely want to activate.
 Given the number of possible commands I clearly can't rely on prompt design to solve this. It might be a good fit for a model with explicit retrieval augmentation like a memorizing transformer. But that's still a very active area of research without high-quality pre-trained models.
 Given that, I'm thinking that doing some kind of fine tuning to an existing model is the best bet. But it's unclear to me what the training data should look like... should I just generate a few examples of each command of the form input: "vscode command: 'open new file'", output: "explorer.newFile", and then fine-tune on those? Is there some way to ensure that the model understands that I *always* want it to return one of the commands provided in fine-tuning, instead of arbitrary text?
 Interested in others' experiences with similar tasks! 
 Background: I'm working on an open source VS Code extension called Clippy AI. Currently it only performs code modifications to the current file and is a thin wrapper around the OpenAI API. But I'd like to use it to automate other editor actions as well!
    submitted by    /u/corbt  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New program bolsters innovation in next-generation artificial intelligence hardware]]></title>
        <id>https://news.mit.edu/2022/new-program-bolsters-innovation-next-generation-artificial-intelligence-hardware-0329</id>
        <link href="https://news.mit.edu/2022/new-program-bolsters-innovation-next-generation-artificial-intelligence-hardware-0329"/>
        <updated>2022-03-29T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[MIT AI Hardware Program launches with five inaugural companies to advance AI technologies for the next decade.]]></summary>
        <author>
            <name>School of Engineering | MIT Schwarzman College of Computing</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence beats 8 world champions at a version of Bridge]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tqyyk3/artificial_intelligence_beats_8_world_champions/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tqyyk3/artificial_intelligence_beats_8_world_champions/"/>
        <updated>2022-03-29T12:46:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kevinwangg  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metaverse is considered the future of internet. It is used to designate a universe beyond physical world. Watch our video to know more about it.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqyohn/metaverse_is_considered_the_future_of_internet_it/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqyohn/metaverse_is_considered_the_future_of_internet_it/"/>
        <updated>2022-03-29T12:30:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Nitorblog  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Pathologies in Ultrasound Images Using OpenCV and Streamlit [P]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqynd5/visualizing_pathologies_in_ultrasound_images/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqynd5/visualizing_pathologies_in_ultrasound_images/"/>
        <updated>2022-03-29T12:28:13.000Z</updated>
        <summary type="html"><![CDATA[As part of our AI Challenge with a health-tech startup: https://omdena.com/blog/pathology-streamlit/
 https://preview.redd.it/zew2baykgbq81.png?width=640&format=png&auto=webp&s=e66180724c08f22db3d322d8b1fd6f56e8765a3c
    submitted by    /u/Lordobba  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building Decision Trees - Entropy, Information Gain & Gini Impurity]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqxzgx/building_decision_trees_entropy_information_gain/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqxzgx/building_decision_trees_entropy_information_gain/"/>
        <updated>2022-03-29T11:49:56.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TheNerdyDevYT  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partitions of unity, smooth ramps, and CW clicks]]></title>
        <id>https://www.johndcook.com/blog/?p=101189</id>
        <link href="https://www.johndcook.com/blog/2022/03/29/partition-of-unity/"/>
        <updated>2022-03-29T11:40:41.000Z</updated>
        <summary type="html"><![CDATA[Partitions of unity are a handy technical device. They’re seldom the focus of attention but rather are buried in the middle of proofs. The name sounds odd, but it’s descriptive. A partition of unity is a set of smooth functions into the interval [0, 1] that add up to 1 at every point. The functions […]
Partitions of unity, smooth ramps, and CW clicks first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] It is possible to build a time series model with this dataset.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqxr2u/d_it_is_possible_to_build_a_time_series_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqxr2u/d_it_is_possible_to_build_a_time_series_model/"/>
        <updated>2022-03-29T11:36:52.000Z</updated>
        <summary type="html"><![CDATA[So basically, I started my internship in business intelligence, and when my boss know that I have a background in machine learning and deep learning. so, he asked me to build a model that predicts a specific number for the next month. so, it time series problem, and the datasets that I have it is very small it starts from May 2019 so it is just 31 rows. And when I plotted the data, it had no clear trend. This pitcher for the graph looks like my dataset here! dataset (sorry I cannot share the dataset because of privacy). So, I started to take the difference in the data to remove the seasonality and log transformation and after that, I bullied the model using the Arima algorithm and LSTM, and prophet. And I applied a prediction interval for the predicted number to get periods and expect the number will be inside this interval. But unfortunately, the actual number (for this month) was out of the interval. So, I decided to look back in a database and I found a feature I think that may help and have a high correlation with the main feature now becoming a multivariate time series problem. so, I tried to use the VAR algorithm but unfortunately, the model also filed and the actual number for each feature was out of interval. This first time for me to build a time series model in the industry for a real dataset and I worked alone. So, there is an approach that can help me to build a better model that I do not follow in my step. Or I should go to my boss tell him cannot build a model for this dataset, especially the data is impacted by a coronavirus.
    submitted by    /u/xxsalehxx140  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Process Models for Data Science? (academic) [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqx0u7/process_models_for_data_science_academic_r/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqx0u7/process_models_for_data_science_academic_r/"/>
        <updated>2022-03-29T10:48:47.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone,
 I am a german student and currently writing my masters thesis. It is a rather simple ML task but for my thesis I need to describe the methodology and which process models are used and I am new to this. I found CRISP-DM and its successor ASUM-DM. However, I know that sounds stupid, I am not able to find information on these or useful pdfs. Like the general information and descriptions are accessable but I need an officisl source that I can cite. IBM itself has a link: ftp: //ftp.software.ibm.com/software/data/ sw-library/services/ASUM.pdf However, its not working for me as I have no ftp access. 
 So my question is, does anyone have a link where I can find relevant official information to these models and furthermore are there any other standards or process models to describe the approach of working with data that are rather used in the industry.
    submitted by    /u/terektus  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[China researches “brain-scale” AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqwqui/china_researches_brainscale_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqwqui/china_researches_brainscale_ai/"/>
        <updated>2022-03-29T10:29:46.000Z</updated>
        <summary type="html"><![CDATA[https://mixed-news.com/en/artificial-intelligence-china-researches-brain-scale-ai/
 From the article:
 In China, the state and companies are researching AI models with trillions of parameters. They want to prove that they can develop “brain-scale” AI.
 ...
 In a new paper, researchers from Tsinghua University, Alibaba Group, Zhejiang Lab and Beijing Academy of Artificial Intelligence present BaGuaLu, a framework that enables the training of large AI models using the Mixture-of-Experts (MoE) architecture.
 In an initial test, the researchers trained a 1.93 trillion model with their framework, outperforming Google’s Switch Transformer. They also demonstrate that their framework enables models with 14.5 trillion and a full 174 trillion parameters.
 ...
 The team expects that giant multimodal AI models could have far-reaching implications for numerous AI applications.
    submitted by    /u/Sephirio  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[9+ Best Deep Learning books for beginners to Expert 2022 [Updated] -]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqwp4r/9_best_deep_learning_books_for_beginners_to/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqwp4r/9_best_deep_learning_books_for_beginners_to/"/>
        <updated>2022-03-29T10:26:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAC and Position Control in Mujoco]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tqvy8c/sac_and_position_control_in_mujoco/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tqvy8c/sac_and_position_control_in_mujoco/"/>
        <updated>2022-03-29T09:31:33.000Z</updated>
        <summary type="html"><![CDATA[Hi! I'm currently using garage to simulate a robot EE controlled in position. Does anyone know how to make a relative action? I mean, I would like to have a small action space reinitialized at each step in order to have only small increases in position. Thanks!
    submitted by    /u/Big-Picture8323  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backpropogation in PPO and gym.ai]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tquve2/backpropogation_in_ppo_and_gymai/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tquve2/backpropogation_in_ppo_and_gymai/"/>
        <updated>2022-03-29T08:09:08.000Z</updated>
        <summary type="html"><![CDATA[I am currently build a PPO agent for OpenAI gym's Pong environment using Pytorch and I had a question:
 So typically the workflow is:
  
Use the state as the input for a CNN and output the action probabilities
 Sample the action distribution for an action and perform env.step(action) with it
 Obtain there rewards and calculate some reward / loss function such as log probs*rewards
 Backpropogate this loss/reward using reward/loss.backward() and optimizer.step()
  
Now, Pytorch's loss.backward() and optimizer.step() only calculates and updates gradients for pyTorch Variable objects where requires_grad=True.
 So how does Pytorch backpropogate through env.step() ? env.step() outputs numpy arrays (if you're using a parallel environment) or integers (not tensors)...
 Secondly, if I try to convert a Tensor output to a numpy array to input to env.step() - say an array of actions for parallel environments, it breaks my backpropogation right?
 Thirdly, does that mean that env.step() is a differentiable function?
 Thanks in advance!
    submitted by    /u/Ska82  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradients with Pytorch.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tqthpf/policy_gradients_with_pytorch/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tqthpf/policy_gradients_with_pytorch/"/>
        <updated>2022-03-29T06:27:54.000Z</updated>
        <summary type="html"><![CDATA[Can someone please explain what "pw" is on the third image(marked with arrow) ? 
 P.S: First two images are for context
 Thank you!
 ​
 https://preview.redd.it/v9ogmrxvn9q81.png?width=753&format=png&auto=webp&s=4ecfc49aabbf328ecccf468ee01aa3767a7a4c8a
 https://preview.redd.it/4mrnjsxvn9q81.png?width=753&format=png&auto=webp&s=42be60ebaed04468d1b10928ba729327a0057b0f
 https://preview.redd.it/o01n5sxvn9q81.png?width=753&format=png&auto=webp&s=112e876c252e075829c1693cf0f6a5d4751cf79a
    submitted by    /u/Whole_Run_4485  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JAX + Flower For Federated Learning Gives Machine Learning Researchers The Flexibility To Use The Deep Learning Framework For Their Projects]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqthl4/jax_flower_for_federated_learning_gives_machine/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqthl4/jax_flower_for_federated_learning_gives_machine/"/>
        <updated>2022-03-29T06:27:38.000Z</updated>
        <summary type="html"><![CDATA[Google researchers created JAX to conduct NumPy computations on GPUs and TPUs. DeepMind uses it to help and expedite its research, and it is increasingly gaining popularity. Differentiation with grad(), vectorization with map(), and JIT-compilation (just-in-time) with jit are some of the composable functions required for machine learning research in JAX (). As a result, adding a JAX-based workload to the Flower code samples is a must-have. The combination of JAX and Flower allows ML and FL researchers to employ the deep learning framework that their projects demand. The updated code example now serves as a template for migrating existing JAX projects to a federated environment.
 It’s pretty simple to put up a centralized machine learning architecture, and the JAX developer documentation has multiple examples. Because the ML model parameters are stored in the DeviceArray data format, setting up the federated workload requires some knowledge of JAX. To be compatible with the Flower NumPyClient, those arguments must be converted to NumPy ndarrays. The JAX meets Flower example below demonstrates how a Flower setup might work.
 Continue Reading
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqs0mk/pastiche_master_exemplarbased_highresolution/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqs0mk/pastiche_master_exemplarbased_highresolution/"/>
        <updated>2022-03-29T04:49:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discussions on Pattern Theory]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqq75c/discussions_on_pattern_theory/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqq75c/discussions_on_pattern_theory/"/>
        <updated>2022-03-29T03:00:53.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I have been interested in a field called pattern theory for some time now. Its a mathematical formalism to describe patterns in the world, as well as a framework for developing ai.
 As far as I can tell, pattern theory seems to be somewhat of a dead field. I'm not sure who is possibly still thinking about it other than a single digit number of academics (e.g. David Mumford). I find this a bit unfortunate, since while I'm still a bit naïve about the field, I'd like to find people I could talk to about it.
 Does anyone have any recommendations for where I could find people I could talk to about pattern theory/bounce ideas off of relating to pattern theory?
 Thanks in advance
    submitted by    /u/patterntheoryacc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the Difference Between Bounce Rate and Exit Rate?]]></title>
        <id>https://www.datasciencecentral.com/?p=56966</id>
        <link href="https://www.datasciencecentral.com/what-is-the-difference-between-bounce-rate-and-exit-rate/"/>
        <updated>2022-03-29T02:21:21.000Z</updated>
        <summary type="html"><![CDATA[Statistics gives business owners the freedom to evaluate how their websites are performing. The evaluation involves a couple of things: the bounce rate and the exit rate. But what is the difference between bounce rate and exit rate? This is a point of discussion that requires you to have an open mind to grasp the… Read More »What is the Difference Between Bounce Rate and Exit Rate?
The post What is the Difference Between Bounce Rate and Exit Rate? appeared first on Data Science Central.]]></summary>
        <author>
            <name>EdwardNick</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Guide to Obtaining Time Series Datasets in Python]]></title>
        <id>https://machinelearningmastery.com/?p=13355</id>
        <link href="https://machinelearningmastery.com/a-guide-to-obtaining-time-series-datasets-in-python/"/>
        <updated>2022-03-29T02:07:33.000Z</updated>
        <summary type="html"><![CDATA[Datasets from real-world scenarios are important for building and testing machine learning models. You may just want to have some […]
The post A Guide to Obtaining Time Series Datasets in Python appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Mehreen Saeed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Five Major Benefits That Microsoft Power BI Brings To Data Scientists]]></title>
        <id>https://www.datasciencecentral.com/?p=56972</id>
        <link href="https://www.datasciencecentral.com/five-major-benefits-that-microsoft-power-bi-brings-to-data-scientists/"/>
        <updated>2022-03-29T01:20:33.000Z</updated>
        <summary type="html"><![CDATA[There is no denying the importance of the internet and IT in the business scene. Businesses hailing from all sectors are dependent on the web, and they also make use of various types of software applications nowadays. However, with time, such technologies are also evolving. Businesses are coping with huge amounts of data, and to… Read More »Five Major Benefits That Microsoft Power BI Brings To Data Scientists
The post Five Major Benefits That Microsoft Power BI Brings To Data Scientists appeared first on Data Science Central.]]></summary>
        <author>
            <name>ImensoSoftware</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Object Inputs with Multiple Features]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqnhin/d_object_inputs_with_multiple_features/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqnhin/d_object_inputs_with_multiple_features/"/>
        <updated>2022-03-29T00:31:44.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I am looking at having a neural network take inputs of 8 objects that all have different features/attributes and then an input of the different features/attributes of the environment the objects are in. The output of this would be the rank of each object. The attached image demonstrates a diagram of the neural network. The objects actively interact/compete with each other. I thought about inputting a single object's features and the environment into a neural network with the output as a performance score of the object. However, the object does worse or better depending on what other objects it is competing against. The objects also get better or worse over time, so it may be good to backpropagate and analyze the objects also as a time series.
 Is it possible to input the object/object features as a matrix? I have not figured out a way to group this data. I was thinking maybe a convolution neural network may work. I am somewhat new to the machine learning world. Any recommendation or help would be great. Thank you
 https://preview.redd.it/ng7q9haxw7q81.jpg?width=6450&format=pjpg&auto=webp&s=9393006f00583a1a4a9af02044e726559176c403
    submitted by    /u/hypercar_junkie  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] time series clustering resources]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqn4k4/r_time_series_clustering_resources/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqn4k4/r_time_series_clustering_resources/"/>
        <updated>2022-03-29T00:12:36.000Z</updated>
        <summary type="html"><![CDATA[Hi, I am intending to write a paper (almost 25-30 pages) about time series clustering. I have done my online research, however, I ll be grateful if you can mention some other resources that might be of interest, either theoretical or applied. It can be blogs about machine learning you find interesting in this area, video series, lectures, lecture notes, whatever. Thank you very much.
    submitted by    /u/jiii95  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can AI create a safer online world?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqls56/can_ai_create_a_safer_online_world/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqls56/can_ai_create_a_safer_online_world/"/>
        <updated>2022-03-28T23:02:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ML_Firefighter  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Any resources on fine-tuning models? - Decision Transformer]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tql5b8/p_any_resources_on_finetuning_models_decision/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tql5b8/p_any_resources_on_finetuning_models_decision/"/>
        <updated>2022-03-28T22:31:22.000Z</updated>
        <summary type="html"><![CDATA[Hello, i'm trying to reproduce the Decision Transformer paper, however i feel seriously lost on how to do it. I find no documentation on fine-tuning models and have no idea how to use the datasets.
 Any help would be much appreciated, thanks.
    submitted by    /u/PM_ME_FREE_GAMES  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do people build physical perceptrons?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqjyi9/do_people_build_physical_perceptrons/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqjyi9/do_people_build_physical_perceptrons/"/>
        <updated>2022-03-28T21:33:09.000Z</updated>
        <summary type="html"><![CDATA[A friend and I are thinking about building a physical perceptron as a summer project. However, I cannot find any resources on the physical implementation of the perceptron since Rosenblatt's in the 40's. Does anyone do this? What are some good resources?
    submitted by    /u/HoldDoorHoldor  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Seems like people are finding my data/ML job aggregator helpful… do you have any feedback for me?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqjqjp/p_seems_like_people_are_finding_my_dataml_job/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqjqjp/p_seems_like_people_are_finding_my_dataml_job/"/>
        <updated>2022-03-28T21:22:50.000Z</updated>
        <summary type="html"><![CDATA[Hey everybody,
 around half a year ago, I created a simple job aggregator called datajoblist.com, which fetches/scrapes remote jobs in data science, data engineering and AI from multiple sources and presents them in a simple, unified interface. The jobs are collected both directly from interesting (to me) companies like Stripe or Shopify, as well as filtered from job boards such as weworkremotely.com.
 I have not touched the site since when I first built it half a year ago, but it seems that people are finding it helpful, as it is now getting rather stable lower few thousand unique visitors per month, and has facilitated thousands of “apply” click-throughs to company sites. A few dozen people even signed up for the mailing list. So, I was thinking about investing a little more time now and adding some improvements.
 Is there any information/functionality that you would like to see there?
 Shortly, I will be adding the possibility to post jobs for a small fee (till now, all jobs on the site have been aggregated from elsewhere), but would love to add some usability improvements that are reasonably simple to implement for me. (Perhaps salary ranges, where available?)
 Thanks for any feedback and have a great day!
    submitted by    /u/k_kristian  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Neural Networks are not the only universal approximators, so why are they so uniquely effective?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqjd3w/d_neural_networks_are_not_the_only_universal/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqjd3w/d_neural_networks_are_not_the_only_universal/"/>
        <updated>2022-03-28T21:04:54.000Z</updated>
        <summary type="html"><![CDATA[I often here the success of neural networks attributed to their status as universal approximators, but there are many algorithms that are universal approximators. For example, decision trees can also be universal approximators, but they don't seem to have nearly as much success. Why is this? What do neural networks have beyond just being universal approximators that makes them special?
 ​
 Is this a question that is currently well understood or is the answer to this question still an area of research?
    submitted by    /u/029187  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basic neural network robots learning by genetic algorithms - survival of the fittest! Crazy simulations - source code included]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tqiwz0/basic_neural_network_robots_learning_by_genetic/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tqiwz0/basic_neural_network_robots_learning_by_genetic/"/>
        <updated>2022-03-28T20:45:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/djrobsmith  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weekly China AI Newsletter: China Strengthens Ethics Reviews on AI, Life Science; Users Can Turn off Recommendation Algorithms; Chinese Self-Driving Startup Raises $400 Million]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqieul/weekly_china_ai_newsletter_china_strengthens/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqieul/weekly_china_ai_newsletter_china_strengthens/"/>
        <updated>2022-03-28T20:22:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/trcytony  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeRF Research Turns 2D Photos Into 3D Scenes]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqfux5/nerf_research_turns_2d_photos_into_3d_scenes/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqfux5/nerf_research_turns_2d_photos_into_3d_scenes/"/>
        <updated>2022-03-28T18:27:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/MarS_0ne  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence, Machine Learning and Society]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqex1u/artificial_intelligence_machine_learning_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqex1u/artificial_intelligence_machine_learning_and/"/>
        <updated>2022-03-28T17:44:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pmz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What’s Your Business Model Choice – Hammers or Casino?]]></title>
        <id>https://www.datasciencecentral.com/?p=56916</id>
        <link href="https://www.datasciencecentral.com/datastrategist-datamanagement/"/>
        <updated>2022-03-28T16:59:40.000Z</updated>
        <summary type="html"><![CDATA[We are in the middle of a business model revolution.  And we are active participants in that revolution.  We have been transitioning from a society where possession and application of physical commodities defined wealth and power, to a society where possession and application of knowledge define wealth and power.  Throughout the 20th century, oil had… Read More »What’s Your Business Model Choice – Hammers or Casino?
The post What’s Your Business Model Choice – Hammers or Casino? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why GNNs suffer from over-smoothing but CNNs don't?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqdpf8/d_why_gnns_suffer_from_oversmoothing_but_cnns_dont/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqdpf8/d_why_gnns_suffer_from_oversmoothing_but_cnns_dont/"/>
        <updated>2022-03-28T16:51:57.000Z</updated>
        <summary type="html"><![CDATA[Many articles online say GNNs suffer from over-smoothing because nodes aggregate their neighbors and many nodes share similar sets of neighbors. However, in CNN, each pixel also aggregates its neighbors. But CNN can still perform well on some pixel-level classification tasks such as segmentation.
    submitted by    /u/AirZealousideal1342  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[11 Best Python Books for Data Science beginners to advanced to read in 2022 -]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqdlou/11_best_python_books_for_data_science_beginners/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqdlou/11_best_python_books_for_data_science_beginners/"/>
        <updated>2022-03-28T16:47:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meet Jessica From LinkedIn, She Is Not A Human Being]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tqd5gy/meet_jessica_from_linkedin_she_is_not_a_human/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tqd5gy/meet_jessica_from_linkedin_she_is_not_a_human/"/>
        <updated>2022-03-28T16:27:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/satish_gaire  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Paper Review Video - Memory-assisted prompt editing to improve GPT-3 after deployment]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqd22m/d_paper_review_video_memoryassisted_prompt/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqd22m/d_paper_review_video_memoryassisted_prompt/"/>
        <updated>2022-03-28T16:23:08.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/gYxJEd3EUKs
 Large language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.
 ​
 OUTLINE:
 0:00 - Intro
 0:40 - Sponsor: Introduction to GNNs Course (link in description)
 1:30 - Paper Overview: Improve GPT-3 after deployment via user feedback
 5:30 - Proposed memory-based architecture
 13:00 - A detailed look at the components
 15:00 - Example tasks
 24:30 - My concerns with the example setup
 26:20 - Baselines used for comparison
 29:50 - Experimental Results
 34:20 - Conclusion & Comments
 ​
 Paper: https://arxiv.org/abs/2201.06009
 Code & Data: https://github.com/madaan/memprompt
    submitted by    /u/ykilcher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security tool guarantees privacy in surveillance footage]]></title>
        <id>https://news.mit.edu/2022/privid-security-tool-guarantees-privacy-surveillance-footage-0328</id>
        <link href="https://news.mit.edu/2022/privid-security-tool-guarantees-privacy-surveillance-footage-0328"/>
        <updated>2022-03-28T15:30:00.000Z</updated>
        <summary type="html"><![CDATA[“Privid” could help officials gather secure public health data or enable transportation departments to monitor the density and flow of pedestrians, without learning personal information about people.]]></summary>
        <author>
            <name>Rachel Gordon | MIT CSAIL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Diversity in Recommendation Systems with a Mostly Unexplored Item List]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqbpk5/d_diversity_in_recommendation_systems_with_a/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqbpk5/d_diversity_in_recommendation_systems_with_a/"/>
        <updated>2022-03-28T15:21:51.000Z</updated>
        <summary type="html"><![CDATA[Many recommendation systems start with a few very popular items that were heavily marketed. The rest of the item list is largely unexplored. How do recommender systems get around this bias and "test" out new items on users to develop richer training data?
 I could see how a multi-arm bandit might fix this problem but I'd love to hear other ideas and lessons learned.
    submitted by    /u/Shap177  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] I've released a Python package which lets you generate vector representations of images with a twist: neither PyTorch nor TensorFlow is used!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqbmwq/p_ive_released_a_python_package_which_lets_you/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqbmwq/p_ive_released_a_python_package_which_lets_you/"/>
        <updated>2022-03-28T15:18:29.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/minimaxir/imgbeddings
 Instead, this package uses an ONNX INT8-quantized version of CLIP's Vision layers, which in testing works just as well, with a significant performance boost.
 The demos also turned out very well, and try to a bit more fun than usual.
    submitted by    /u/minimaxir  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Decision Transformers in Transformers library and in Hugging Face Hub]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqb7py/p_decision_transformers_in_transformers_library/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqb7py/p_decision_transformers_in_transformers_library/"/>
        <updated>2022-03-28T14:59:36.000Z</updated>
        <summary type="html"><![CDATA[Hey there,
 We’re happy to announce that Edward Beeching from Hugging Face has integrated Decision Transformers an Offline Reinforcement Learning method, into the 🤗 transformers library and the Hugging Face Hub.
 In addition, we share nine pre-trained model checkpoints for continuous control tasks in the Gym environment.
 If you want to know more about Decision Transformers and how to start using it, we wrote a tutorial 👉 https://huggingface.co/blog/decision-transformers
 We would love to hear your feedback about it,
 Thanks,
    submitted by    /u/cranthir_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision Transformers in Transformers library and in Hugging Face Hub 🤗]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tqb5xs/decision_transformers_in_transformers_library_and/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tqb5xs/decision_transformers_in_transformers_library_and/"/>
        <updated>2022-03-28T14:57:14.000Z</updated>
        <summary type="html"><![CDATA[Hey there 👋🏻,
 We’re happy to announce that Edward Beeching from Hugging Face has integrated Decision Transformers an Offline Reinforcement Learning method, into the 🤗 transformers library and the Hugging Face Hub.
 In addition, we share nine pre-trained model checkpoints for continuous control tasks in the Gym environment.
 If you want to know more about Decision Transformers and how to start using it, we wrote a tutorial 👉 https://huggingface.co/blog/decision-transformers
 We would love to hear your feedback about it,
 In the coming weeks and months, we will be extending the reinforcement learning ecosystem by:
  
Being able to train your own Decision Transformers from scratch.
 Integrating RL-baselines3-zoo
 Uploading RL-trained-agents models into the Hub: a big collection of pre-trained Reinforcement Learning agents using stable-baselines3
 Integrating other Deep Reinforcement Learning libraries
 Implementing Convolutional Decision Transformers for Atari
  
And more to come 🥳, so 📢 The best way to keep in touch is to join our discord server to exchange with us and with the community.
 Thanks,
    submitted by    /u/cranthir_  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Catboost performance on Python vs C++]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqb4v0/d_catboost_performance_on_python_vs_c/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqb4v0/d_catboost_performance_on_python_vs_c/"/>
        <updated>2022-03-28T14:55:57.000Z</updated>
        <summary type="html"><![CDATA[Hi fellow nerds, was wondering if anyone has trained the same catboost model on the same dataset in Python and C++ to see which is quicker. Also posting in case someone knows why one language may be inherently quicker. 
 I assume that they are the same program with the same run time but I can’t be too sure of that. Thanks.
    submitted by    /u/econ1mods1are1cucks  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Release the Vision Transformer Cookbook with Tensorflow ! (Thanks to @lucidrains)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqb2mb/p_release_the_vision_transformer_cookbook_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqb2mb/p_release_the_vision_transformer_cookbook_with/"/>
        <updated>2022-03-28T14:53:08.000Z</updated>
        <summary type="html"><![CDATA[​
 Vision Transformer Cookbook
 ​
 Hello, I have released the Vision Transformer Cookbook with Tensorflow !
 Therefore, you can easy to use the 22 transformer architectures via just copy & paste.
 I hope this repository would help many people, including tensorflow users.
 Thank you.
 ​
 * code: vit-tensorflow
    submitted by    /u/taki0112  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["[Discussion] Create a Random Forest Regression to predict multiple values in future using past data"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tqak1i/discussion_create_a_random_forest_regression_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tqak1i/discussion_create_a_random_forest_regression_to/"/>
        <updated>2022-03-28T14:29:12.000Z</updated>
        <summary type="html"><![CDATA[I am using Random Forest Regression on a power vs time data of an experiment that is performed for a certain time duration. Using that data, I want to predict the trend of power in future using time as an input. The code that has been implemented is mentioned below.
 The data set consists of approximately 30 hours of power vs time values as mentioned below. Only active power and time_h columns are used in the algorithm.
 ​
 Data set used for modelling
 # Creating X and y X = np.array(series[['time_h']]).reshape(-1,1) y = np.array(series['active_power']) # Splitting dataset in training and testing X_train2,X_test2,y_train2,y_test2 = train_test_split(X,y,test_size = 0.15, random_state = 1) # Creating Random Forest model and fitting it on training data forest = RandomForestRegressor(n_estimat…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A mini-conversation with Kanye West's AI persona ended on a hilarious note]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tq9z0g/a_miniconversation_with_kanye_wests_ai_persona/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tq9z0g/a_miniconversation_with_kanye_wests_ai_persona/"/>
        <updated>2022-03-28T14:01:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kuasha7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for the next prime]]></title>
        <id>https://www.johndcook.com/blog/?p=101066</id>
        <link href="https://www.johndcook.com/blog/2022/03/28/next-prime/"/>
        <updated>2022-03-28T13:52:14.000Z</updated>
        <summary type="html"><![CDATA[Suppose you start with some large number x and want to find a prime number at least as big as x. First you test whether x is prime. Then you test whether x + 1 is prime. Then you test whether x + 2 is prime, and so on until you find a prime. Of […]
Looking for the next prime first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Datacenter relocation is now easier, faster, and more affordable]]></title>
        <id>https://www.datasciencecentral.com/?p=56969</id>
        <link href="https://www.datasciencecentral.com/datacenter-relocation-is-now-easier-faster-and-more-affordable/"/>
        <updated>2022-03-28T13:21:51.000Z</updated>
        <summary type="html"><![CDATA[It is common for growing organizations to reach a point where their existing data solution is no longer adequate for their needs. In most cases, it happens with companies that have used an on-premises infrastructure from the earliest days of business but now need to upgrade their network for continued growth. However, relocating equipment and… Read More »Datacenter relocation is now easier, faster, and more affordable
The post Datacenter relocation is now easier, faster, and more affordable appeared first on Data Science Central.]]></summary>
        <author>
            <name>Karen Anthony</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Evolution of Astronomical AI]]></title>
        <id>https://www.datasciencecentral.com/?p=56948</id>
        <link href="https://www.datasciencecentral.com/the-evolution-of-astronomical-ai/"/>
        <updated>2022-03-28T13:14:39.000Z</updated>
        <summary type="html"><![CDATA[Astronomy has seen an exponential rise in data collection over the last decade. This requires new methods for data analysis, including AI. With the launch of new surveys, big data methodology has become a necessity. A new class of extremely large telescopes has evolved to collect vast amounts of data; The volume of data collected… Read More »The Evolution of Astronomical AI
The post The Evolution of Astronomical AI appeared first on Data Science Central.]]></summary>
        <author>
            <name>Stephanie Glen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DataRobot’s plan to democratize machine learning with no-code AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tq8rwc/datarobots_plan_to_democratize_machine_learning/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tq8rwc/datarobots_plan_to_democratize_machine_learning/"/>
        <updated>2022-03-28T13:00:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bendee983  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] I am a sample in the dataset I have to analyze]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tq7cjn/discussion_i_am_a_sample_in_the_dataset_i_have_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tq7cjn/discussion_i_am_a_sample_in_the_dataset_i_have_to/"/>
        <updated>2022-03-28T11:42:10.000Z</updated>
        <summary type="html"><![CDATA[Basically the title. I work as a data engineer for a company I am also a customer of. From an Ethics in ML point of view: what do you think this implies on my responsibilities?
    submitted by    /u/Bani57  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discount factor Agents is available at different rates]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tq5zyo/discount_factor_agents_is_available_at_different/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tq5zyo/discount_factor_agents_is_available_at_different/"/>
        <updated>2022-03-28T10:10:00.000Z</updated>
        <summary type="html"><![CDATA[Assume that the time between two agent actions is not fixed, i.e. depending on the state-action, the agent can become unavailable for a time t = t(s, a). During the time the agent is unavailable, several rewards are produced by the environment, and they need to be given to the agent whenever it becomes available again. 
 One easy way to deal with this is to just store them and set the reward at the next available state as the sum of the accumulated rewards. But in the discounted reward framework with temporal difference (e.g. DQN) this does not discount rewards properly. How can I set the reward for the next state such that it contains all the accumulated rewards but it is correct in the DQN setting?
    submitted by    /u/fedetask  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Current State-of-the-art RL algorithms]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tq5p87/current_stateoftheart_rl_algorithms/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tq5p87/current_stateoftheart_rl_algorithms/"/>
        <updated>2022-03-28T09:48:53.000Z</updated>
        <summary type="html"><![CDATA[What are the current best algorithms in Reinforcement Learning? It seems everyone still uses TD3, SAC, PPO, Rainbow DQN, etc. However, these are mostly from 2018, which is old for RL standards. What happened afterwards? What is the current algorithm for these kinds of standard tasks? I'm especially interested in algorithms that can handle continuous action spaces. Thank you very much!
    submitted by    /u/Paraiso93  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Everything about Attention Family]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tq40t9/d_everything_about_attention_family/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tq40t9/d_everything_about_attention_family/"/>
        <updated>2022-03-28T07:40:25.000Z</updated>
        <summary type="html"><![CDATA[Hey, I have just published my latest medium article. 
 These days, in deep learning, it is usual to hear about transformers’ outstanding performance on the challenges where other algorithms can not meet our expectations when most of them are based on attention. This article gives you a detailed illustration of the code and mathematics of the four most-used types of attention in the Deep Learning era.
 https://rezayazdanfar.medium.com/everything-about-attention-family-644747903c60
    submitted by    /u/rezayazdanfar  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top 5 Python Time Series Libraries]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tq3dft/top_5_python_time_series_libraries/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tq3dft/top_5_python_time_series_libraries/"/>
        <updated>2022-03-28T06:54:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RubiksCodeNMZ  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top 5 Python Time Series Libraries]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tq3c3b/top_5_python_time_series_libraries/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tq3c3b/top_5_python_time_series_libraries/"/>
        <updated>2022-03-28T06:51:33.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RubiksCodeNMZ  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What to Do About the New AI Regulation?]]></title>
        <id>https://www.datasciencecentral.com/?p=56905</id>
        <link href="https://www.datasciencecentral.com/what-to-do-about-the-new-ai-regulation/"/>
        <updated>2022-03-28T05:05:44.000Z</updated>
        <summary type="html"><![CDATA[When such a sophisticated, risky, and complex technology like AI takes our lives by storm, a clearly defined set of rules on its usage is paramount. Previously, public concern was mostly focused on the inappropriate use of personal data. As AI becomes a key technology in many businesses and services, the attention is rightfully shifting… Read More »What to Do About the New AI Regulation?
The post What to Do About the New AI Regulation? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Andrey Koptelov</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Automation and AI Are Changing Internet Marketing]]></title>
        <id>https://www.datasciencecentral.com/?p=56710</id>
        <link href="https://www.datasciencecentral.com/how-automation-and-ai-are-changing-internet-marketing/"/>
        <updated>2022-03-28T05:03:28.000Z</updated>
        <summary type="html"><![CDATA[From chatbots and other remote helpers to producing content, improving client encounters, AI companies are now rolling out significant improvements to the advanced promoting scene. While it might be hard to foresee what’s to come, it’s not difficult to see that AI will proceed to advance and assume an undeniably focal point in computerized advertising.… Read More »How Automation and AI Are Changing Internet Marketing
The post How Automation and AI Are Changing Internet Marketing appeared first on Data Science Central.]]></summary>
        <author>
            <name>Rumzz Bajwa</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Python Became THE Language for Data Science]]></title>
        <id>https://www.datasciencecentral.com/?p=56797</id>
        <link href="https://www.datasciencecentral.com/how-python-became-the-language-for-data-science/"/>
        <updated>2022-03-28T04:58:08.000Z</updated>
        <summary type="html"><![CDATA[What is Data Science? Data science is a study that helps us to extract information from a set of structured or unstructured data. It makes use of the study of statistics, mathematics, scientific computation to analyze the data.  Demand for Python in Data Science: Before we deep dive into the topic let’s firstly discuss why… Read More »How Python Became THE Language for Data Science
The post How Python Became THE Language for Data Science appeared first on Data Science Central.]]></summary>
        <author>
            <name>Sonia Mathias</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three Critical Steps for Data-Driven Success]]></title>
        <id>https://www.datasciencecentral.com/?p=56924</id>
        <link href="https://www.datasciencecentral.com/three-critical-steps-for-data-driven-success/"/>
        <updated>2022-03-28T04:52:42.000Z</updated>
        <summary type="html"><![CDATA[In today’s landscape, businesses need to look for any competitive advantage they can to ensure their survival, growth and success. A key aspect of gaining a competitive advantage is using data-driven insights to empower decisions for marketing, consumer insights, consumer segmentation, and operations, such as merchandising and real estate.  Especially within large companies, it is… Read More »Three Critical Steps for Data-Driven Success
The post Three Critical Steps for Data-Driven Success appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kerry Pearce</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] New t-SNE guidelines, an experimental study, and automatic t-SNE hyperparameter selection]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tq1ia3/r_new_tsne_guidelines_an_experimental_study_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tq1ia3/r_new_tsne_guidelines_an_experimental_study_and/"/>
        <updated>2022-03-28T04:47:59.000Z</updated>
        <summary type="html"><![CDATA[t-SNE remains a popular embedding method for visualizing high-dimensional data. However, there is little consensus on how to select hyperparameters such as perplexity, learning rate, and exaggeration to best visualize arbitrary data sets.
 This work systematically explores t-SNE hyperparameters using almost 700 data sets. We replicate past studies, proving that some t-SNE guidelines generalize beyond their original context. But we find that some guidelines do not appear to generalize. We also show a proof of concept neural network system for featurizing data sets and automatically recommending good t-SNE hyperparameters.
 Paper: https://osf.io/6t5ax/
 Blog: https://twosixtech.com/new-guidance-for-using-t-sne/
    submitted by    /u/rpgove  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Governance Tool: What To Look For?]]></title>
        <id>https://www.datasciencecentral.com/?p=56894</id>
        <link href="https://www.datasciencecentral.com/data-governance-tool-what-to-look-for/"/>
        <updated>2022-03-28T04:38:53.000Z</updated>
        <summary type="html"><![CDATA[Data governance is the management of organizations’ data availability, usability, integrity, security, and privacy. According to Gartner, Data governance is the specification of decision rights and a framework for accountability to assure acceptable behavior in the value, generation, consumption, and control of data and analytics. Why Do Organizations Need It? It ensures that data is consistent,… Read More »Data Governance Tool: What To Look For?
The post Data Governance Tool: What To Look For? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Indhu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[noob question about Bellman's optimality principle]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tq1667/noob_question_about_bellmans_optimality_principle/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tq1667/noob_question_about_bellmans_optimality_principle/"/>
        <updated>2022-03-28T04:27:02.000Z</updated>
        <summary type="html"><![CDATA[i'm reading the Sutton-Barto and at page 63 it is written then v_star(s)=max_a q_star(a,s), my question is why we have this? where does it come from? i'm trying to start from the definition of v_star and q_star but I can't really find a way
    submitted by    /u/samas69420  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top MDM-Enabled Data Security Hacks You Should Know About]]></title>
        <id>https://www.datasciencecentral.com/?p=56908</id>
        <link href="https://www.datasciencecentral.com/top-mdm-enabled-data-security-hacks-you-should-know-about/"/>
        <updated>2022-03-28T04:24:35.000Z</updated>
        <summary type="html"><![CDATA[Introduction Security is the buzzword for the digital world today. Businesses have realized that thriving and surviving without a well-functioning security system in place is tough. Security breaches, malware, ransomware and similar incidents are real. The businesses that have suffered from malicious attacks very well know how grave these attacks can be.  The importance of… Read More »Top MDM-Enabled Data Security Hacks You Should Know About
The post Top MDM-Enabled Data Security Hacks You Should Know About appeared first on Data Science Central.]]></summary>
        <author>
            <name>EdwardNick</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Text to Mesh Without 3D Supervision Using Limit Subdivision (Clipmesh)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tq0e3k/r_text_to_mesh_without_3d_supervision_using_limit/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tq0e3k/r_text_to_mesh_without_3d_supervision_using_limit/"/>
        <updated>2022-03-28T03:39:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/InfamousPancakes  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] MLbot – Open-source tool to train ML models in your cloud, with a single command.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tq0dt3/p_mlbot_opensource_tool_to_train_ml_models_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tq0dt3/p_mlbot_opensource_tool_to_train_ml_models_in/"/>
        <updated>2022-03-28T03:39:23.000Z</updated>
        <summary type="html"><![CDATA[Hey ML Reddit!
 I just released the initial version of MLbot (https://github.com/thecooltechguy/mlbot): a new open-source tool that I’ve been working on for running distributed ML training jobs in your cloud, with a single command.
 How it works:
 In short, it allows you to run your training script in the cloud by simply swapping “python” for “mlbot run”.
 For example, if ``python train.py … can run your training script locally, then mlbot run --instance-type p3dn.24xlarge --num-nodes 2 train.py … should be able to run your code in the cloud across 2 GPU machines.
 Since this tool runs entirely inside your cloud environment, you don’t have to transfer your training data to a 3rd party, while having full observability into the underlying infrastructure.
 Why I built this:
 In a recent ML pr…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is the following NLP task called - explaining WHY someone feels a particular way in a product review? For example with the sentence - "I don't like the tone of the guitar because the strings are too old", the explanation for negative sentiment of guitar should be "strings are too old".]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tq097n/d_what_is_the_following_nlp_task_called/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tq097n/d_what_is_the_following_nlp_task_called/"/>
        <updated>2022-03-28T03:31:59.000Z</updated>
        <summary type="html"><![CDATA[Looking for ideas and pointers on how to solve this problem. Dependency parsing? Are there any open source ML models to solve this problem? Googling isn't helping.
 Made a mistake in the description - we want to explain the negative sentiment of the aspect "tone" (rather than guitar)
    submitted by    /u/ml_guy1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Difference between Research and Applied Research track in conferences?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tq00tu/d_difference_between_research_and_applied/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tq00tu/d_difference_between_research_and_applied/"/>
        <updated>2022-03-28T03:18:17.000Z</updated>
        <summary type="html"><![CDATA[Hello, I am in the process of finding a suitable conference for my paper, and I find that they usually have a research track and applied research track. One conference defines the applied research track as
  
The Applied Research Track aims at attracting submissions from both industry and academia that either solve or advance the understanding of issues related to deploying AI, Information Retrieval (IR), and big data technologies as part of actual applications.
  
My paper is roughly related to applying deep learning for time series anomaly detection. Should I go for the research track or the applied research track?
    submitted by    /u/mythrowaway0852  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to generate line drawings that convey geometry and semantics (CVPR 2022)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpz3wg/learning_to_generate_line_drawings_that_convey/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpz3wg/learning_to_generate_line_drawings_that_convey/"/>
        <updated>2022-03-28T02:25:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New AI Tool]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpz2t7/new_ai_tool/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpz2t7/new_ai_tool/"/>
        <updated>2022-03-28T02:23:42.000Z</updated>
        <summary type="html"><![CDATA[Now unlimited uses for anyone: https://botbox.dev/generator
    submitted by    /u/Recent_Coffee_2551  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The latest marketing tactic on LinkedIn: AI-generated faces : NPR]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpxtud/the_latest_marketing_tactic_on_linkedin/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpxtud/the_latest_marketing_tactic_on_linkedin/"/>
        <updated>2022-03-28T01:12:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Representative-Job23  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agile, Agile 2 and Agility, Part I]]></title>
        <id>https://www.datasciencecentral.com/?p=56911</id>
        <link href="https://www.datasciencecentral.com/agile-agile-2-and-agility-part-i/"/>
        <updated>2022-03-27T23:54:24.000Z</updated>
        <summary type="html"><![CDATA[If you are running a business today using Agile methods, it’s likely that you are not getting the productivity boost from it that you should, and your time to market for new features is probably not what it could be either. Is that the end of the world?  By and large, yes!  The problem is… Read More »Agile, Agile 2 and Agility, Part I
The post Agile, Agile 2 and Agility, Part I appeared first on Data Science Central.]]></summary>
        <author>
            <name>Howard M. Wiener</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Commercial Artificial Intelligence — The Future of BI]]></title>
        <id>https://www.datasciencecentral.com/?p=56926</id>
        <link href="https://www.datasciencecentral.com/commercial-artificial-intelligence-the-future-of-bi/"/>
        <updated>2022-03-27T23:29:31.000Z</updated>
        <summary type="html"><![CDATA[The dynamics of the global commercial artificial intelligence market continues to change over time, thanks to the persistent advancements in technology. This research report offers a detailed and insightful assessment of the global commercial artificial intelligence market, taking primary trends and the future prospects of this market in consideration. Various segments of this market, based on a… Read More »Commercial Artificial Intelligence — The Future of BI
The post Commercial Artificial Intelligence — The Future of BI appeared first on Data Science Central.]]></summary>
        <author>
            <name>Nikita Godse</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seed vault, but for code]]></title>
        <id>https://www.johndcook.com/blog/?p=101009</id>
        <link href="https://www.johndcook.com/blog/2022/03/27/seed-vault-but-for-code/"/>
        <updated>2022-03-27T23:19:41.000Z</updated>
        <summary type="html"><![CDATA[I had heard of the Svalbard Global Seed Vault, but I hadn’t heard of the nearby Arctic World Archive until today. The latter contains source code preserved on film, a format that should last at least 500 years.
Seed vault, but for code first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GitHub Co-Pilot Alternatives: Can They Match the Functionality of Co-Pilot?]]></title>
        <id>https://www.datasciencecentral.com/?p=56941</id>
        <link href="https://www.datasciencecentral.com/how-do-github-co-pilot-alternatives-work-and-can-they-match-the-functionality-of-co-pilot/"/>
        <updated>2022-03-27T23:00:41.000Z</updated>
        <summary type="html"><![CDATA[One of the best known examples of GPT-3 for developers is the Github co-pilot Trained on billions of lines of public code, GitHub Copilot is more than autocomplete of code. GitHub Copilot is powered by Codex, the new AI system created by OpenAI. GitHub Copilot understands significantly more context than most code assistants. GitHub Copilot… Read More »GitHub Co-Pilot Alternatives: Can They Match the Functionality of Co-Pilot?
The post GitHub Co-Pilot Alternatives: Can They Match the Functionality of Co-Pilot? appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A.I. that turns written documents into practice tests. For easy learning. Is this easy or challenging programming?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpuu9k/ai_that_turns_written_documents_into_practice/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpuu9k/ai_that_turns_written_documents_into_practice/"/>
        <updated>2022-03-27T22:26:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/143openyourmind  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Check out this research summary article based on the paper 'SS-SAM: Stochastic Scheduled Sharpness-Aware Minimization for Efficiently Training Deep Neural Networks' where Researchers From Tsinghua University Propose ‘Stochastic Scheduled SAM’ (SS-SAM) for reducing the computational overhead]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tptssd/check_out_this_research_summary_article_based_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tptssd/check_out_this_research_summary_article_based_on/"/>
        <updated>2022-03-27T21:34:10.000Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have excelled at solving complex real-world problems, however, training a good DNN has become more complex. It is challenging to ensure that the optimizers used will converge to reliable minima with acceptable model performance when only minimizing the conventional empirical loss.
 Tsinghua University’s research team proposes Stochastic Scheduled SAM (SS-SAM), a novel and effective DNN training strategy. In SS-SAM, the optimizer is set up by a predetermined scheduling function to run a random trial at each update step, which selects whether to perform the SGD or SAM optimization at random. The overall number of propagation pairs could be significantly decreased in this approach. The team’s approach provides equivalent or higher model training performance at a lower computational cost than baseline sharpness-aware minimization (SAM).
 Continue Reading
 Paper: https://arxiv.org/pdf/2203.09962.pdf
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Check out this research summary article based on the paper 'SS-SAM: Stochastic Scheduled Sharpness-Aware Minimization for Efficiently Training Deep Neural Networks' where Researchers From Tsinghua University Propose ‘Stochastic Scheduled SAM’ (SS-SAM) for reducing the computational overhead]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tptsj2/check_out_this_research_summary_article_based_on/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tptsj2/check_out_this_research_summary_article_based_on/"/>
        <updated>2022-03-27T21:33:49.000Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have excelled at solving complex real-world problems, however, training a good DNN has become more complex. It is challenging to ensure that the optimizers used will converge to reliable minima with acceptable model performance when only minimizing the conventional empirical loss.
 Tsinghua University’s research team proposes Stochastic Scheduled SAM (SS-SAM), a novel and effective DNN training strategy. In SS-SAM, the optimizer is set up by a predetermined scheduling function to run a random trial at each update step, which selects whether to perform the SGD or SAM optimization at random. The overall number of propagation pairs could be significantly decreased in this approach. The team’s approach provides equivalent or higher model training performance at a lower computational cost than baseline sharpness-aware minimization (SAM).
 Continue Reading
 Paper: https://arxiv.org/pdf/2203.09962.pdf
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are easy to use image editing AIs?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tptgix/what_are_easy_to_use_image_editing_ais/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tptgix/what_are_easy_to_use_image_editing_ais/"/>
        <updated>2022-03-27T21:17:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/xXLisa28Xx  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] We built a AI platform to advance stereotactic radiosurgery (SRS) for brain tumor patients]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tpt9xn/p_we_built_a_ai_platform_to_advance_stereotactic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tpt9xn/p_we_built_a_ai_platform_to_advance_stereotactic/"/>
        <updated>2022-03-27T21:08:23.000Z</updated>
        <summary type="html"><![CDATA[4 years ago, I posted here to introduce some work I did using AI for breast tumor detection and classification: 
 https://www.reddit.com/r/MachineLearning/comments/8rdpwy/pi_made_a_gpu_cluster_and_free_website_to_help/
 That post gain some traction on Reddit and I hope you would like the one I am gonna introduce here again. 
 In the recent years, I have been shifting my focus from cancer detection to the actual treatment. 
 One particular problem we really want to solve is to have more brain cancer patients to be accessible to stereotactic radiosurgery (SRS) which has a lot better treatment outcome and much better quality of life (QoL) for the patient than whole brain radiotherapy (WBRT) which is more common for patients with multiple brainiest (say more than 5 or 10). 
 The reason behind …]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Five Key Components of a Data Sharing Platform]]></title>
        <id>https://www.datasciencecentral.com/?p=56789</id>
        <link href="https://www.datasciencecentral.com/five-key-components-of-a-data-sharing-platform/"/>
        <updated>2022-03-27T20:40:07.000Z</updated>
        <summary type="html"><![CDATA[Increasingly, companies are focused on finding ways to connect to new and valuable sources of data in order to enhance their analytical capabilities, enrich their models, or deliver more insight to their business units.  Due to the increased demand for new data sources, companies are also looking at their internal data differently. Organizations that have… Read More »Five Key Components of a Data Sharing Platform
The post Five Key Components of a Data Sharing Platform appeared first on Data Science Central.]]></summary>
        <author>
            <name>Lewis Wynne-Jones</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is Colab Pro Worth the money?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tps7fl/d_is_colab_pro_worth_the_money/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tps7fl/d_is_colab_pro_worth_the_money/"/>
        <updated>2022-03-27T20:16:53.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I'm currently dealing with my bachelor degree's final project. But my pc seems to be slow and it gets really hot + I think it might be dying, I really need to send it to the technical service. :(
 Well, I'm not familiar with other cloud services like Azure or AWS but I used Google Colab a lot, and right at this moment I also use it. But it's constantly asking if I'm "there". It always wants an interaction otherwise it shutdowns the session and my time gets wasted, just gotta do everything from the start.
 So if I pay for the colab pro (unpluss) version, will my experience get better? Will I need to interact with colab every hour again? Or should I consider other alternatives?
    submitted by    /u/average_turanist  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Machine Learning - WAYR (What Are You Reading) - Week 134]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tpruqj/d_machine_learning_wayr_what_are_you_reading_week/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tpruqj/d_machine_learning_wayr_what_are_you_reading_week/"/>
        <updated>2022-03-27T20:00:05.000Z</updated>
        <summary type="html"><![CDATA[This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.
 Please try to provide some insight from your understanding and please don't post things which are present in wiki.
 Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.
 Previous weeks :
  
 1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80 81-90 91-100 101-110 111-120 121-130 131-140 
  
 Week 1 Week 11 Week 21 Week 31 Week 41 Week 51 Week 61 Week 71 Week 81 Week 91 Week 101 Week 111 Week 121 Week 131 
  Week 2 Week 12 Week 22 Week 32 Week 42 Week 52 Week 62 Week 72 Week 82 Week 92 Week 102 Week 112 Week 122 Week 132 
  Week 3 Week 13 Week 23 Week 33 Week 43 Week 53 Week 63 Week 73 Week 83 Week 93 Week 103 Week 113 Week 123 Week 133 
  Week 4 Week 14 Week 24 Week 34 Week 44 Week 54 Week 64 Week 74 Week 84 Week 94 Week 104 Week 114 Week 124  
  Week 5 Week 15 Week 25 Week 35 Week 45 Week 55 Week 65 Week 75 Week 85 Week 95 Week 105 Week 115 Week 125  
  Week 6 Week 16 Week 26 Week 36 Week 46 Week 56 Week 66 Week 76 Week 86 Week 96 Week 106 Week 116 Week 126  
  Week 7 Week 17 Week 27 Week 37 Week 47 Week 57 Week 67 Week 77 Week 87 Week 97 Week 107 Week 117 Week 127  
  Week 8 Week 18 Week 28 Week 38 Week 48 Week 58 Week 68 Week 78 Week 88 Week 98 Week 108 Week 118 Week 128  
  Week 9 Week 19 Week 29 Week 39 Week 49 Week 59 Week 69 Week 79 Week 89 Week 99 Week 109 Week 119 Week 129  
  Week 10 Week 20 Week 30 Week 40 Week 50 Week 60 Week 70 Week 80 Week 90 Week 100 Week 110 Week 120 Week 130  
 
 Most upvoted papers two weeks ago:
 /u/CatalyzeX_code_bot: Paper link
 /u/PaganPasta: https://arxiv.org/abs/2105.05233
 Besides that, there are no rules, have fun.
    submitted by    /u/ML_WAYR_bot  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smart Maintenance – How SaaS Frameworks Turn Insights Into Actions Quickly And Efficiently]]></title>
        <id>https://www.datasciencecentral.com/?p=56785</id>
        <link href="https://www.datasciencecentral.com/smart-maintenance-how-saas-frameworks-turn-insights-into-actions-quickly-and-efficiently/"/>
        <updated>2022-03-27T19:10:19.000Z</updated>
        <summary type="html"><![CDATA[Despite the technological breakthroughs in the advent of Industry 4.0, manufacturers seem to have taken a more gradual approach to adoption. In 2020, less than 30 percent of the industry considered themselves extensive users of advanced integrated tools and processes. The pandemic, however, brought out an unprecedented need to explore opportunities that make manufacturing systems… Read More »Smart Maintenance – How SaaS Frameworks Turn Insights Into Actions Quickly And Efficiently
The post Smart Maintenance – How SaaS Frameworks Turn Insights Into Actions Quickly And Efficiently appeared first on Data Science Central.]]></summary>
        <author>
            <name>Eric Whitley</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N][R] Combine Lidar and Cameras for 3D object detection - Waymo & Google Research]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tpqk9l/nr_combine_lidar_and_cameras_for_3d_object/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tpqk9l/nr_combine_lidar_and_cameras_for_3d_object/"/>
        <updated>2022-03-27T18:59:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares: Limgrave || Clip Guided Diffusion AI Art Video [4K 16 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpq5e1/artificial_nightmares_limgrave_clip_guided/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpq5e1/artificial_nightmares_limgrave_clip_guided/"/>
        <updated>2022-03-27T18:39:55.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Interesting prediction of ecosystem around giant DNN models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tppxo9/discussion_interesting_prediction_of_ecosystem/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tppxo9/discussion_interesting_prediction_of_ecosystem/"/>
        <updated>2022-03-27T18:29:24.000Z</updated>
        <summary type="html"><![CDATA[Saw this comment in a Q&A on big deep learning models. This seems to have side-effects both good and bad. The prediction, if true, forces accessibility (though expensive) but also creates silos. First time poster. What do you guys think?
 https://m12.vc/news/direct-line-with-saurabh-tiwary-whats-next-for-large-foundational-models
  
The economics are making it untenable for most people except the most well-funded organizations to invest in large language models. I will make the comparison to the semiconductor ecosystem. If you look at fabrication economics for semiconductor chips, they cost tens to hundreds of millions of dollars and have relatively short lifetimes. One needs very large volume usage to justify manufacturing a custom ASIC (Application Specific Integrated Circuits). Thus, we do not have that many companies fabricating chips. However, we have an entire software and systems eco-system which relies on these chips that have built massive industries around them. And, if you look at the biggest companies in the world (maybe, except Apple), they have very little to do with ASIC design and fabrication as part of their core business. I think a similar eco-system would pan out in the large-scale modeling space as well. We would have a few well-funded companies that would be training these extremely large and reusable models and other companies would build applications and services reusing and customizing these models.
  
   submitted by    /u/SufficientActive8895  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The first open-source project for financial reinforcement learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tppn35/the_first_opensource_project_for_financial/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tppn35/the_first_opensource_project_for_financial/"/>
        <updated>2022-03-27T18:14:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/zicxor  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using RL to play Jump King]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tppcom/using_rl_to_play_jump_king/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tppcom/using_rl_to_play_jump_king/"/>
        <updated>2022-03-27T18:00:56.000Z</updated>
        <summary type="html"><![CDATA[I am learning RL by having the algorithm play Jump King and am streaming it on twitch, having the chat play against it as well to see who can get the babe first. Check it out at: https://www.twitch.tv/unassignedseat
    submitted by    /u/UnassginedSeat  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Question][DRL] Are intermediate activations used during training?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tpoy7s/questiondrl_are_intermediate_activations_used/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tpoy7s/questiondrl_are_intermediate_activations_used/"/>
        <updated>2022-03-27T17:41:08.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 I have a question regarding optimizing a policy represented by a neural network. In Supervised Learning, the intermediate activations created during the forward pass are needed during backpropagation in order to compute weight gradients. This has led to a number of memory management techniques such as offloading and checkpointing being created.
 My question is whether the same is true in DRL. For policy-gradient methods for example, learning starts from an objective computed from the trajectory such as the discounted returns, but are the intermediate activations created during action inference needed when optimizing the policy (i.e. learning)? 
 Is there any academic source that covers this topic? 
 Thanks!
    submitted by    /u/PSylvan  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Research Turns 2D Photos Into 3D Scenes in the Blink of an AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpou15/nvidia_research_turns_2d_photos_into_3d_scenes_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpou15/nvidia_research_turns_2d_photos_into_3d_scenes_in/"/>
        <updated>2022-03-27T17:35:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/qptbook  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This endless live TV show run entirely by AI characters]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpoc3c/this_endless_live_tv_show_run_entirely_by_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpoc3c/this_endless_live_tv_show_run_entirely_by_ai/"/>
        <updated>2022-03-27T17:10:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_embassy_official  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[7 Best Natural Language Processing Courses (2022) | Best NLP Courses -]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpnsw0/7_best_natural_language_processing_courses_2022/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpnsw0/7_best_natural_language_processing_courses_2022/"/>
        <updated>2022-03-27T16:44:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[If you have expertise in this field, is it realistic whatsoever to create an A.I version of yourself to live on (let's say work on it for 30 years, starting from now)?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpndxo/if_you_have_expertise_in_this_field_is_it/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpndxo/if_you_have_expertise_in_this_field_is_it/"/>
        <updated>2022-03-27T16:24:22.000Z</updated>
        <summary type="html"><![CDATA[As in...get the voice down from improving an A.I recording.
 Then, give it some basic code or responses that you had at different ages.
 Also, getting a bunch of 3D images of yourself.
 Then coding it with some basic "values" and creating some sort of generic conditional statement for the 3 basic values that it has that match yours.
 Then, over time, actually diving into artificial intelligence and slowly updating and replacing those to continue improving it to match you (and your growth)?
 Hmmm, I wonder if there would be a way to preserve it. Everything changes (like sites -> VR and so on). So some sort of "survival" instinct (which seems impossible to code but would be fun to try undertaking).
    submitted by    /u/the_evil_intp  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Modern data augmentation techniques]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tpmowt/d_modern_data_augmentation_techniques/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tpmowt/d_modern_data_augmentation_techniques/"/>
        <updated>2022-03-27T15:50:49.000Z</updated>
        <summary type="html"><![CDATA[I've written a short blog post on modern data augmentation techniques. Please have a read and provide feedback. I've explained Cutout, Mixup, CutMix and Label smoothing with code and examples.
 https://pmgautam.com/augmentations/2022/03/27/Augmentations-visually-explained.html
    submitted by    /u/p1g1  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Future after Automation and AGI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpmjje/future_after_automation_and_agi/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpmjje/future_after_automation_and_agi/"/>
        <updated>2022-03-27T15:43:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/HumanSeeing  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Face filters on the web from just text descriptions]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpm86a/face_filters_on_the_web_from_just_text/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpm86a/face_filters_on_the_web_from_just_text/"/>
        <updated>2022-03-27T15:27:39.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pmz  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Simple Questions Thread]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tplo8r/d_simple_questions_thread/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tplo8r/d_simple_questions_thread/"/>
        <updated>2022-03-27T15:00:10.000Z</updated>
        <summary type="html"><![CDATA[Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!
 Thread will stay alive until next one so keep posting after the date in the title.
 Thanks to everyone for answering questions in the previous thread!
    submitted by    /u/AutoModerator  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RaveForce in 2022: The OpenAI Gym style toolkit for music generation experiments just got better]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tpj7eq/raveforce_in_2022_the_openai_gym_style_toolkit/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tpj7eq/raveforce_in_2022_the_openai_gym_style_toolkit/"/>
        <updated>2022-03-27T12:46:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/chaosprint  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Author pages on http://papers.labml.ai]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tpi5gx/p_author_pages_on_httppaperslabmlai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tpi5gx/p_author_pages_on_httppaperslabmlai/"/>
        <updated>2022-03-27T11:37:33.000Z</updated>
        <summary type="html"><![CDATA[We added author pages, which lists all the papers by the author and links to their social and academic web pages.
 E.g.: 
 https://papers.labml.ai/author/39815586a03711ecbb8c3d25c114d5ed
 https://papers.labml.ai/author/56b63a47a03711ecbb8c3d25c114d5ed
 Highlights, 
  
Links to Google and Arxiv searches.
 Sort papers based on the published date and the popularity in Twitter.
 Links to Twitter, Google Scholar, Github and Linkedin etc if available in our database.
  
We love to hear your feedback and suggestions. Thank you all, and I appreciate the support.
    submitted by    /u/hnipun  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is my problem suited for solving via reinforcement learning methods? What approach should I start with?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tpgi0k/is_my_problem_suited_for_solving_via/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tpgi0k/is_my_problem_suited_for_solving_via/"/>
        <updated>2022-03-27T09:34:22.000Z</updated>
        <summary type="html"><![CDATA[My goal is to determine the best course of actions to take given a certain state. 
 I'm working in some state space X. For every x in X, I can assign it a value. When I perform an action a given x, I map x to some new state x'. The state x' depends on my action up to some noise produced by the environment. 
 I think this is a reinforcement learning problem. What methods are suitable in this context?
    submitted by    /u/heylanguage  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toll-free number: What is it, and how can you get one for your business?]]></title>
        <id>https://www.datasciencecentral.com/?p=56930</id>
        <link href="https://www.datasciencecentral.com/toll-free-number-what-is-it-and-how-can-you-get-one-for-your-business/"/>
        <updated>2022-03-27T08:41:11.000Z</updated>
        <summary type="html"><![CDATA[What is the toll-free number? Businesses provide a cloud-based contact number to allow customers to contact them free of cost. In India, this number- the business toll-free number is available in the 1800 series in an easily recognizable format- 1800-ABC-DEFG. Customers do not have to incur any fee to contact the business, as the company… Read More »Toll-free number: What is it, and how can you get one for your business?
The post Toll-free number: What is it, and how can you get one for your business? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Aileen Scott</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top Strategies and Best Practices for Big Data Testing]]></title>
        <id>https://www.datasciencecentral.com/?p=56931</id>
        <link href="https://www.datasciencecentral.com/top-strategies-and-best-practices-for-big-data-testing/"/>
        <updated>2022-03-27T07:42:35.000Z</updated>
        <summary type="html"><![CDATA[With the exponential growth in the number of big data applications in the world, Testing in big data applications is related to database, infrastructure and performance testing, and functional testing. The advancement of technology is enabling the collection of a massive amount of data almost every second. And, big data has emerged as the buzzword… Read More »Top Strategies and Best Practices for Big Data Testing
The post Top Strategies and Best Practices for Big Data Testing appeared first on Data Science Central.]]></summary>
        <author>
            <name>Ryan Williamson</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[why is the variable being passed for iterations suddenly passing empty value?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tpcila/why_is_the_variable_being_passed_for_iterations/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tpcila/why_is_the_variable_being_passed_for_iterations/"/>
        <updated>2022-03-27T04:32:49.000Z</updated>
        <summary type="html"><![CDATA[As you can see in the figure, i'm passing the variable data_inputs to the feedforward_comp function over iterations. Up to epoch = 9, everything computes fine but after that, data_inputs suddently is being passed as empty?
 Can someone please tell me why this happens and how to fix it?
 https://preview.redd.it/2wds226ktup81.png?width=816&format=png&auto=webp&s=61c55db011708e2d2b373a0b4e4b02355ec50730
    submitted by    /u/lwhisper  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[👉 Impressed With AlphaFold? Checkout This Protein Structure Prediction Model (FastFold) That Reduces AlphaFold’s Training Time From 11 Days To 67 Hours]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tpcgi4/impressed_with_alphafold_checkout_this_protein/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tpcgi4/impressed_with_alphafold_checkout_this_protein/"/>
        <updated>2022-03-27T04:29:04.000Z</updated>
        <summary type="html"><![CDATA[DeepMind released AlphaFold 2 last year, which made headlines for its incredible accuracy in protein structure prediction. The success of AlphaFold demonstrated that deep neural networks might be used to solve challenging and critical structural biology problems.
 FastFold is a highly effective protein structure prediction model formulation for training and inference developed by a group of researchers from the National University of Singapore. Although AlphaFold 2 is a game-changer in protein structure prediction, training and inference remain time-consuming and costly. This is something that the study team is concerned about.
 Continue Reading This Article Here
 Paper: https://arxiv.org/pdf/2203.00854v1.pdf
 Github: https://github.com/hpcaitech/FastFold
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which laptop should I buy?]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tpc3bk/which_laptop_should_i_buy/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tpc3bk/which_laptop_should_i_buy/"/>
        <updated>2022-03-27T04:04:53.000Z</updated>
        <summary type="html"><![CDATA[Hey guys, I really need your help on this one. I go to university next year and I will be studying computer engineering which means there will be a lot of coding going into it. Now I have 4 options for laptop
 First : INSPIRON 15.6" INTEL CORE 17-1165G7 TOUCHSCREEN 2-IN-1 LAPTOP
 Second: GALAXY BOOK PRO 360 15" 2-IN-1 INTEL 17 LAPTOP
 Third: HP 15.6" Touchscreen 2-in-1 Laptop - Nightfall Black (AMD Ryzen 7 5700U/1TB SSD/16GB RAM/Windows 10)
 Fourth: HP Pavilion x360 15.6" Touchscreen 2-in-1 Laptop - Silver (Intel Core i7-1165G7/1TB SSD/16GB RAM/Win 11)
 Please and thank you guys :)
    submitted by    /u/Traditional-Cow47  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deck generation using reinforcement learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tpbg8h/deck_generation_using_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tpbg8h/deck_generation_using_reinforcement_learning/"/>
        <updated>2022-03-27T03:24:31.000Z</updated>
        <summary type="html"><![CDATA[A brief overview of the game: A deck of 50 objects are chosen from a set of ~1000 objects. The game is then played out deterministically, and rewards are dished out based on win/loss. 
 I would like to build a nn that can produce good decks , trained using self-play. However, I'm not too sure how to approach this problem. Relevant research or pointers would be very helpful. Thanks.
    submitted by    /u/nutpeabutter  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] GroupViT: Semantic Segmentation Emerges from Text Supervision + Hugging Face Gradio Web Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tpa89n/rp_groupvit_semantic_segmentation_emerges_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tpa89n/rp_groupvit_semantic_segmentation_emerges_from/"/>
        <updated>2022-03-27T02:10:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future. (arXiv:2106.04420v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04420</id>
        <link href="http://arxiv.org/abs/2106.04420"/>
        <updated>2022-03-27T00:51:42.565Z</updated>
        <summary type="html"><![CDATA[In real-time forecasting in public health, data collection is a non-trivial
and demanding task. Often after initially released, it undergoes several
revisions later (maybe due to human or technical constraints) - as a result, it
may take weeks until the data reaches to a stable value. This so-called
'backfill' phenomenon and its effect on model performance has been barely
studied in the prior literature. In this paper, we introduce the multi-variate
backfill problem using COVID-19 as the motivating example. We construct a
detailed dataset composed of relevant signals over the past year of the
pandemic. We then systematically characterize several patterns in backfill
dynamics and leverage our observations for formulating a novel problem and
neural framework Back2Future that aims to refines a given model's predictions
in real-time. Our extensive experiments demonstrate that our method refines the
performance of top models for COVID-19 forecasting, in contrast to non-trivial
baselines, yielding 18% improvement over baselines, enabling us obtain a new
SOTA performance. In addition, we show that our model improves model evaluation
too; hence policy-makers can better understand the true accuracy of forecasting
models in real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1"&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alexander Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1"&gt;B. Aditya Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I created a new AI art maker program (free)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tp7wpj/i_created_a_new_ai_art_maker_program_free/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tp7wpj/i_created_a_new_ai_art_maker_program_free/"/>
        <updated>2022-03-26T23:57:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Recent_Coffee_2551  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Does a Self-Driving Car See? (Waymo ‘s system explained)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tp7h6k/how_does_a_selfdriving_car_see_waymo_s_system/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tp7h6k/how_does_a_selfdriving_car_see_waymo_s_system/"/>
        <updated>2022-03-26T23:33:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 Best Movies Like 'After Yang' About Artificial Intelligence (A.I.)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tp687u/5_best_movies_like_after_yang_about_artificial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tp687u/5_best_movies_like_after_yang_about_artificial/"/>
        <updated>2022-03-26T22:31:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/NarutoNotBoruto  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Conditional GAN loss magnitudes]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tp5iyf/d_conditional_gan_loss_magnitudes/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tp5iyf/d_conditional_gan_loss_magnitudes/"/>
        <updated>2022-03-26T22:07:29.000Z</updated>
        <summary type="html"><![CDATA[Hey, I am wondering about the losses of conditional GANs, particularly their magnitude. Due to the amount of classes/identities, the classification loss will usually be significantly higher than discrimination loss (data identified as real or generated). When using traditional multitask learning where losses are simply summed up, how is the generator supposed to learn to generate realistic-appearing data when the loss that would encourage that is so low in comparison to the classification loss?
    submitted by    /u/Timboron  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some confusions about Actor-Critic, A2C]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tp0szc/some_confusions_about_actorcritic_a2c/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tp0szc/some_confusions_about_actorcritic_a2c/"/>
        <updated>2022-03-26T19:24:15.000Z</updated>
        <summary type="html"><![CDATA[In sutton's book, Actor-Critic firstly uses an approximated value function as a baseline, and uses the error to update the policy. In my opinion, the value function is used as a baseline since it assign high value to state with high expected return, and every action in this state have the same baseline. 
 Pseudocode in sutton's book
 But I also see a Q-version of AC algorithm, which use Q function as the baseline. In this algorithm, the Q function is used to update the policy, and the TD error is used to update the Q function. How could we get this?
 Q Actor-critic
 Another question is about AC and A2C. Is expected return (G) minus baseline the same as the advantage function in A2C? If so, is that AC with baseline the same as A2C?
    submitted by    /u/ZavierTi2021  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ai for enamels and paints]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tozvwt/ai_for_enamels_and_paints/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tozvwt/ai_for_enamels_and_paints/"/>
        <updated>2022-03-26T18:38:47.000Z</updated>
        <summary type="html"><![CDATA[Love how my keyboard konked out. 
 So I recently read about megasyn the AI being used to create 40k new weapons in 6 hours. 
 This got me wondering. Can AI be made to create different enamels and paints for things like pottery glazes, cloisonne enamels and paints for art work? 
 If so how would one go about doing this knowing literally nothing?
    submitted by    /u/Grendal87  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is deep rl possible on microcontrollers?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tozt3g/is_deep_rl_possible_on_microcontrollers/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tozt3g/is_deep_rl_possible_on_microcontrollers/"/>
        <updated>2022-03-26T18:34:49.000Z</updated>
        <summary type="html"><![CDATA[I am thinking of applying the deep rl on small scale robots. I have an Arduino Uno and some servos, so is it possible that deep rl can be applied using Arduino?
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Useful Tools and Programs list for AI/ML]]></title>
        <id>https://www.reddit.com/r/artificial/comments/toyu62/useful_tools_and_programs_list_for_aiml/</id>
        <link href="https://www.reddit.com/r/artificial/comments/toyu62/useful_tools_and_programs_list_for_aiml/"/>
        <updated>2022-03-26T17:47:30.000Z</updated>
        <summary type="html"><![CDATA[Found a useful list of Tools and Programs for AI/ML. Looks like it covers Machine Learning, Deep Learning, Computer Vision(CV), and Natural Language Processing (NLP). I thought I'd share it for anyone that's interested. https://github.com/mikeroyal/Machine-Learning-Guide
    submitted by    /u/Khaotic_Kernel  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Augmentation in GAN]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/toy50y/d_augmentation_in_gan/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/toy50y/d_augmentation_in_gan/"/>
        <updated>2022-03-26T17:13:10.000Z</updated>
        <summary type="html"><![CDATA[Hi, does anyone has experience in augmentation for GANs? Especially for Cycle-GAN like image to image translation. When I see image-to-image GANs, there is mostly no augmentation applied.
    submitted by    /u/SeucheAchat9115  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPT-3's knowledge was limited to the world until 2019. InstructGPT is apparently up-to-date.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/toxy88/gpt3s_knowledge_was_limited_to_the_world_until/</id>
        <link href="https://www.reddit.com/r/artificial/comments/toxy88/gpt3s_knowledge_was_limited_to_the_world_until/"/>
        <updated>2022-03-26T17:04:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/BeginningInfluence55  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crystal Forest AI Art]]></title>
        <id>https://www.reddit.com/r/artificial/comments/towlb9/crystal_forest_ai_art/</id>
        <link href="https://www.reddit.com/r/artificial/comments/towlb9/crystal_forest_ai_art/"/>
        <updated>2022-03-26T16:28:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Recent_Coffee_2551  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A possibly stupid question about deep q-learning]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tovpj0/a_possibly_stupid_question_about_deep_qlearning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tovpj0/a_possibly_stupid_question_about_deep_qlearning/"/>
        <updated>2022-03-26T16:16:26.000Z</updated>
        <summary type="html"><![CDATA[Hi Guys! I am just starting out in RL and I have a possibly stupid question about deep q-learning. Why do all of the code examples train the model on its own discounted prediction plus the reward, if they could just record all of the rewards in an episode and then calculate the total discounted rewards from the actual rewards the agent got in the episode? At least in my Implementations, the latter strategy seems to outperform the former, both in regard to the time it took the model to converge and the quality of the learned policy.
    submitted by    /u/KayJersch  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Convolutional Generative Network Tutorial in PyTorch]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tou59y/deep_convolutional_generative_network_tutorial_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tou59y/deep_convolutional_generative_network_tutorial_in/"/>
        <updated>2022-03-26T15:54:15.000Z</updated>
        <summary type="html"><![CDATA[I thought it will be quite interesting to see Deep Convolutional GAN’s capability in generating wildlife, so I wrote a tutorial on how to build a model based on the DCGAN architecture through PyTorch:
 https://taying-cheng.medium.com/create-new-animals-using-dcgan-with-pytorch-2ce47810ebd4
    submitted by    /u/Ok-Peanut-2681  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI News | Animal Language Translator AI | Heart Attack Prediction Algo | Nvidia H100 GPU & AI Supercomputer]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tosno9/ai_news_animal_language_translator_ai_heart/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tosno9/ai_news_animal_language_translator_ai_heart/"/>
        <updated>2022-03-26T15:18:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/getrich_or_diemining  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] How does apple FaceID work?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/toqvg1/discussion_how_does_apple_faceid_work/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/toqvg1/discussion_how_does_apple_faceid_work/"/>
        <updated>2022-03-26T14:24:56.000Z</updated>
        <summary type="html"><![CDATA[I am doing some face recognition and am wondering how does apple's FaceID work. They say it is a "true depth" camera. What does that mean? is that a lidar? some kind of a dot projector? Basically what I want to know is what kind of data does that device provide. Also, is there a commercially available device similar to that true depth camera available?
    submitted by    /u/user89320  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Keep training GAN?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ton9ir/p_keep_training_gan/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ton9ir/p_keep_training_gan/"/>
        <updated>2022-03-26T12:15:04.000Z</updated>
        <summary type="html"><![CDATA[Hi there,
 ​
 I'm trying to train a GAN to generate video game portraits (think Baldur's Gate, Divinity, that kind of stuff).
 My GAN is trained on 4096 portraits, 128x192 pixels in size.
 Batch size is 64, 64 features, 128 dim noise vector.
 ​
 After a few epochs of the expected low quality random stuff, my Generator starts generating images where you can kind of imagine silhouettes and faces. Here's after 250 epochs:
 ​
 https://preview.redd.it/afc4i2r6zpp81.png?width=568&format=png&auto=webp&s=f8e386a8da4976a20b8db525914e5944346b9240
 But after 500 the results are pretty much the same:
 ​
 https://preview.redd.it/k9ikn9k7zpp81.png?width=568&format=png&auto=webp&s=3ec106e5ef5d54c3a8162bb45941fb3f3f9af4a2
 Losses seem be mostly stable fairly quickly too (sorry, lost the graph after I accidentally shutdown my computer - I save the model every 25 epochs but not the losses graph):
 Epoch 255 Step 16320: Generator loss: 7.4651877045631405, critic loss: -9.97341676712036
 ...
 Epoch 310 Step 19840: Generator loss: 7.401800179481507, critic loss: -9.479909801483156
 ...
 Epoch 511 Step 32720: Generator loss: 11.518763446807862, critic loss: -8.105796337127687
 (The 7.5 -> 11.5 GLoss looks pretty much flat on the graph. Maybe the huge initial losses puts it out of perspective?)
 ​
 Is my GAN "stuck" or do I just need to keep training, and quality gains from that point on are going to be slower?
    submitted by    /u/-Anordil-  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Guidelines on how to add skip connections to DCGAN generator?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tomt4g/d_guidelines_on_how_to_add_skip_connections_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tomt4g/d_guidelines_on_how_to_add_skip_connections_to/"/>
        <updated>2022-03-26T11:45:56.000Z</updated>
        <summary type="html"><![CDATA[I'm experimenting with DCGan architecture and tried adding residual blocks to dcgan before each upscaling. The dcgan is trainiing via a WGAN-GP training procedure, and so far, I could nor really get any sensible result.
 Is there any guideline about how skip connections should be implemented in GANs? I'm using a very normal resnet type skip connection. The FID score keeps worsening since the start of training, and I've tweaked a lot of hyperparameters, but I still don't have see any improvements. Although I could not train that many epochs because I'm training this on colab.
 You can find the codes here to see the architecture: Generator and Critic.
 Another info: The dataset is roughly 100K book covers I've scraped from internet.
    submitted by    /u/feryet  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Intersection between Computer Engineering and Machine Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tolh63/d_intersection_between_computer_engineering_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tolh63/d_intersection_between_computer_engineering_and/"/>
        <updated>2022-03-26T10:10:58.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone. I am pursuing Masters in Computer Engineering. However, I did my Bachelor's in Computer Science and dabbled with ML a bit. I would like to continue with it during my Masters as well. I am wondering if there is an intersection between the disciplines. If the two fields share considerable overlap, would it be possible to pursue research in ML as a CE graduate?
 Thanks
    submitted by    /u/RealMatchesMalonee  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What exactly is the output of openai gym atari vram outputs?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tole7b/what_exactly_is_the_output_of_openai_gym_atari/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tole7b/what_exactly_is_the_output_of_openai_gym_atari/"/>
        <updated>2022-03-26T10:04:42.000Z</updated>
        <summary type="html"><![CDATA[the docs are light and I understand they're being revamped but I can't find a definition of the outputs for ale. I understand it depends on the specific environment exactly, e.g. for non-atari envs like lunar lander it gives positional data but for the atari games docs state nothing other than its the memory dump.
 do I treat it like an image without the processing being necessary. can I reshape it into a matrix the size of the raw image output, and throw it into a series of convolutional layers? or do I treat it as positional data like the location of all the objects?
    submitted by    /u/clockface99  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why does drug discovery with machine learning work? [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/toke65/why_does_drug_discovery_with_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/toke65/why_does_drug_discovery_with_machine_learning/"/>
        <updated>2022-03-26T08:47:12.000Z</updated>
        <summary type="html"><![CDATA[Machine learning models are statistical, not causal in nature. That means they don’t necessarily hold on intervention. This should make predicting properties of drugs particularly problematic because we are not drawing further testing samples from a distribution that is similar to the training distribution, but we’re completely changing the input as we wish. Why is machine learning/deep learning successful at predicting these properties when the wider research community is struggling to make deep learning models robust, never mind causal, in general.
    submitted by    /u/lemlo100  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Text-to-Image Generation Model with 3.9B Parameters is Publicly Available]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tojfs4/r_texttoimage_generation_model_with_39b/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tojfs4/r_texttoimage_generation_model_with_39b/"/>
        <updated>2022-03-26T07:34:13.000Z</updated>
        <summary type="html"><![CDATA[State-of-the-art autoregressive image generation model of Kakao Brain !
 The paper and codes of "Autoregressive Image Generation using Residual Quantization", which is accepted by CVPR'22, are released. Our study outperforms previous autoregressive image generation models, while increasing the sampling speed upto ~7x faster.
 In addition, we release RQ-Transformer with 3.9B parameters trained on 30M text-image pairs. To the best of our knowledge, it is the largest text-to-image model among public available models.
 ​
 Examples of Generated Images in the Paper
 ​
 Examples of Generated Images by RQ-Transformer with 3.9B parameters. The model is publicly available now !
 Enjoy !
 Paper: https://arxiv.org/abs/2203.01941
 Code: https://github.com/kakaobrain/rq-vae-transformer
    submitted by    /u/leedoyup  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why GPU Workstations are cheaper than rack-mount Servers?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/toitac/d_why_gpu_workstations_are_cheaper_than_rackmount/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/toitac/d_why_gpu_workstations_are_cheaper_than_rackmount/"/>
        <updated>2022-03-26T06:46:21.000Z</updated>
        <summary type="html"><![CDATA[I'm in the process of looking for a good GPU server for my university lab. We have a server room to install the new server. However, while looking into different vendors, I notice that workstations cost less than a rack-mount server even they have better specifications (I saw a gpu workstation with a 48-core 3.8 GHz processor and its cost still a few thousands less than a rack server with similar specifications and the same number and type of GPUs but with a lower grade processor with only 24-core 2.5 GHz). I was really surprised. Is there an advantage for buying a rack-mount GPU server over a GPU Workstation given it cost more for similar or lower specifications?
    submitted by    /u/majax21  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers Open-Source WiSE-FT Algorithm For Fine Tuning AI Models]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tohkd3/researchers_opensource_wiseft_algorithm_for_fine/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tohkd3/researchers_opensource_wiseft_algorithm_for_fine/"/>
        <updated>2022-03-26T05:15:39.000Z</updated>
        <summary type="html"><![CDATA[When making zero-shot inference, large pre-trained models like CLIP or ALIGN provide consistent accuracy across various data distributions (i.e., without fine-tuning on a specific dataset). While existing fine-tuning methods vastly improve accuracy on a given target distribution, they frequently compromise robustness to distribution shifts. This conflict can be resolved by presenting a simple and effective strategy for enhancing robustness while fine-tuning: assembling the zero-shot and fine-tuned models (WiSE-FT). 
 An approach for fine-tuning AI models that enhance robustness during distribution shift has been open-sourced by researchers from the University of Washington (UW), Google Brain, and Columbia University. According to tests, WISE-FT improves accuracy by up to 6% on specific computer vision (CV) benchmarks.
 Continue Reading The Research Summary Article Here
 Paper: https://arxiv.org/pdf/2109.01903.pdf
 Github: https://github.com/mlfoundations/wise-ft
 https://preview.redd.it/1ltzmg5iwnp81.png?width=2803&format=png&auto=webp&s=6c0727432072b67c1723838e3097ec901f34b1c0
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer + Hugging Face Gradio Web Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/togtat/rp_pastiche_master_exemplarbased_highresolution/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/togtat/rp_pastiche_master_exemplarbased_highresolution/"/>
        <updated>2022-03-26T04:25:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Medium Article on Deep Learning for Tabular Data - Your thoughts on which algorithms (DeepInsight, IGTD, SuperTML, TabNet, Tab-Transformer, AutoInt, FT-Transformer ) works most times?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/toadvm/d_medium_article_on_deep_learning_for_tabular/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/toadvm/d_medium_article_on_deep_learning_for_tabular/"/>
        <updated>2022-03-26T01:16:17.000Z</updated>
        <summary type="html"><![CDATA[I recently wrote a critical review on "Deep Learning for Tabular Data" which reviews whether we are ready to move from Tree-based models to Neural Network-based models for Tabular data. It covers many novel approaches such as DeepInsight, IGTD and SuperTML. It also includes some of the transformers based recent works such as TabNet, Tab-Transformer, AutoInt, FT-Transformer and regularisation models such MLP+. 
 I have most commonly found the lack of a defined benchmark which makes it hard for people to find the right algorithms for the task. I am creating this discussion so that people who are using some of these algorithms or have tested some of them in different scenarios can share their findings.
    submitted by    /u/Raghuvansh_Tahlan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Fixation: Dynamic Window Visual Transformer. (arXiv:2203.12856v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12856</id>
        <link href="http://arxiv.org/abs/2203.12856"/>
        <updated>2022-03-26T00:46:08.023Z</updated>
        <summary type="html"><![CDATA[Recently, a surge of interest in visual transformers is to reduce the
computational cost by limiting the calculation of self-attention to a local
window. Most current work uses a fixed single-scale window for modeling by
default, ignoring the impact of window size on model performance. However, this
may limit the modeling potential of these window-based models for multi-scale
information. In this paper, we propose a novel method, named Dynamic Window
Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT
goes beyond the model that employs a fixed single window setting. To the best
of our knowledge, we are the first to use dynamic multi-scale windows to
explore the upper limit of the effect of window settings on model performance.
In DW-ViT, multi-scale information is obtained by assigning windows of
different sizes to different head groups of window multi-head self-attention.
Then, the information is dynamically fused by assigning different weights to
the multi-scale window branches. We conducted a detailed performance evaluation
on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related
state-of-the-art (SoTA) methods, DW-ViT obtains the best performance.
Specifically, compared with the current SoTA Swin Transformers
\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements
on all three datasets with similar parameters and computational costs. In
addition, DW-ViT exhibits good scalability and can be easily inserted into any
window-based visual transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengzhen Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1"&gt;Qing Du Xiaodan Liang Xiaojun Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your Policy Regularizer is Secretly an Adversary. (arXiv:2203.12592v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.12592</id>
        <link href="http://arxiv.org/abs/2203.12592"/>
        <updated>2022-03-26T00:46:08.017Z</updated>
        <summary type="html"><![CDATA[Policy regularization methods such as maximum entropy regularization are
widely used in reinforcement learning to improve the robustness of a learned
policy. In this paper, we show how this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a
limited set by an imagined adversary. Using convex duality, we characterize
this robust set of adversarial reward perturbations under KL and
alpha-divergence regularization, which includes Shannon and Tsallis entropy
regularization as special cases. Importantly, generalization guarantees can be
given within this robust set. We provide detailed discussion of the worst-case
reward perturbations, and present intuitive empirical examples to illustrate
this robustness and its relationship with generalization. Finally, we discuss
how our analysis complements and extends previous results on adversarial reward
robustness and path consistency optimality conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brekelmans_R/0/1/0/all/0/1"&gt;Rob Brekelmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genewein_T/0/1/0/all/0/1"&gt;Tim Genewein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grau_Moya_J/0/1/0/all/0/1"&gt;Jordi Grau-Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deletang_G/0/1/0/all/0/1"&gt;Gr&amp;#xe9;goire Del&amp;#xe9;tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunesch_M/0/1/0/all/0/1"&gt;Markus Kunesch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1"&gt;Shane Legg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_P/0/1/0/all/0/1"&gt;Pedro Ortega&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Artificial Intelligence for Exhaust Gas Temperature of Turbofan Engines. (arXiv:2203.13108v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.13108</id>
        <link href="http://arxiv.org/abs/2203.13108"/>
        <updated>2022-03-26T00:46:07.924Z</updated>
        <summary type="html"><![CDATA[Data-driven modeling is an imperative tool in various industrial
applications, including many applications in the sectors of aeronautics and
commercial aviation. These models are in charge of providing key insights, such
as which parameters are important on a specific measured outcome or which
parameter values we should expect to observe given a set of input parameters.
At the same time, however, these models rely heavily on assumptions (e.g.,
stationarity) or are "black box" (e.g., deep neural networks), meaning that
they lack interpretability of their internal working and can be viewed only in
terms of their inputs and outputs. An interpretable alternative to the "black
box" models and with considerably less assumptions is symbolic regression (SR).
SR searches for the optimal model structure while simultaneously optimizing the
model's parameters without relying on an a-priori model structure. In this
work, we apply SR on real-life exhaust gas temperature (EGT) data, collected at
high frequencies through the entire flight, in order to uncover meaningful
algebraic relationships between the EGT and other measurable engine parameters.
The experimental results exhibit promising model accuracy, as well as
explainability returning an absolute difference of 3{\deg}C compared to the
ground truth and demonstrating consistency from an engineering perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kefalas_M/0/1/0/all/0/1"&gt;Marios Kefalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rojo_J/0/1/0/all/0/1"&gt;Juan de Santiago Rojo Jr.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apostolidis_A/0/1/0/all/0/1"&gt;Asteris Apostolidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herik_D/0/1/0/all/0/1"&gt;Dirk van den Herik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1"&gt;Bas van Stein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1"&gt;Thomas B&amp;#xe4;ck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Representation Learning for Reliable Robotic Monitoring of Fruit Anomalies. (arXiv:2109.10135v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.10135</id>
        <link href="http://arxiv.org/abs/2109.10135"/>
        <updated>2022-03-26T00:46:07.917Z</updated>
        <summary type="html"><![CDATA[Data augmentation can be a simple yet powerful tool for autonomous robots to
fully utilise available data for selfsupervised identification of atypical
scenes or objects. State-of-the-art augmentation methods arbitrarily embed
"structural" peculiarity on typical images so that classifying these artefacts
can provide guidance for learning representations for the detection of
anomalous visual signals. In this paper, however, we argue that learning such
structure-sensitive representations can be a suboptimal approach to some
classes of anomaly (e.g., unhealthy fruits) which could be better recognised by
a different type of visual element such as "colour". We thus propose Channel
Randomisation as a novel data augmentation method for restricting neural
networks to learn encoding of "colour irregularity" whilst predicting
channel-randomised images to ultimately build reliable fruit-monitoring robots
identifying atypical fruit qualities. Our experiments show that (1) this
colour-based alternative can better learn representations for consistently
accurate identification of fruit anomalies in various fruit species, and also,
(2) unlike other methods, the validation accuracy can be utilised as a
criterion for early stopping of training in practice due to positive
correlation between the performance in the self-supervised
colour-differentiation task and the subsequent detection rate of actual
anomalous fruits. Also, the proposed approach is evaluated on a new
agricultural dataset, Riseholme-2021, consisting of 3.5K strawberry images
gathered by a mobile robot, which we share online to encourage active
agri-robotics research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1"&gt;Taeyeong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Would_O/0/1/0/all/0/1"&gt;Owen Would&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salazar_Gomez_A/0/1/0/all/0/1"&gt;Adrian Salazar-Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1"&gt;Grzegorz Cielniak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems. (arXiv:2107.07508v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07508</id>
        <link href="http://arxiv.org/abs/2107.07508"/>
        <updated>2022-03-26T00:46:07.897Z</updated>
        <summary type="html"><![CDATA[Real-world decision-making systems are often subject to uncertainties that
have to be resolved through observational data. Therefore, we are frequently
confronted with combinatorial optimization problems of which the objective
function is unknown and thus has to be debunked using empirical evidence. In
contrast to the common practice that relies on a learning-and-optimization
strategy, we consider the regression between combinatorial spaces, aiming to
infer high-quality optimization solutions from samples of input-solution pairs
-- without the need to learn the objective function. Our main deliverable is a
universal solver that is able to handle abstract undetermined stochastic
combinatorial optimization problems. For learning foundations, we present
learning-error analysis under the PAC-Bayesian framework using a new
margin-based analysis. In empirical studies, we demonstrate our design using
proof-of-concept experiments, and compare it with other methods that are
potentially applicable. Overall, we obtain highly encouraging experimental
results for several classic combinatorial problems on both synthetic and
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_G/0/1/0/all/0/1"&gt;Guangmo Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning. (arXiv:2112.10982v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.10982</id>
        <link href="http://arxiv.org/abs/2112.10982"/>
        <updated>2022-03-26T00:46:07.847Z</updated>
        <summary type="html"><![CDATA[Generalized few-shot semantic segmentation was introduced to move beyond only
evaluating few-shot segmentation models on novel classes to include testing
their ability to remember base classes. While the current state-of-the-art
approach is based on meta-learning, it performs poorly and saturates in
learning after observing only a few shots. We propose the first fine-tuning
solution, and demonstrate that it addresses the saturation problem while
achieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We
also show that it outperforms existing methods, whether fine-tuning multiple
final layers or only the final layer. Finally, we present a triplet loss
regularization that shows how to redistribute the balance of performance
between novel and base categories so that there is a smaller gap between them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Myers_Dean_J/0/1/0/all/0/1"&gt;Josh Myers-Dean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yinan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1"&gt;Brian Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Scott Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1"&gt;Danna Gurari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepEverest: Accelerating Declarative Top-K Queries for Deep Neural Network Interpretation [Technical Report]. (arXiv:2104.02234v7 [cs.DB] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2104.02234</id>
        <link href="http://arxiv.org/abs/2104.02234"/>
        <updated>2022-03-26T00:46:07.841Z</updated>
        <summary type="html"><![CDATA[We design, implement, and evaluate DeepEverest, a system for the efficient
execution of interpretation by example queries over the activation values of a
deep neural network. DeepEverest consists of an efficient indexing technique
and a query execution algorithm with various optimizations. We prove that the
proposed query execution algorithm is instance optimal. Experiments with our
prototype show that DeepEverest, using less than 20% of the storage of full
materialization, significantly accelerates individual queries by up to 63x and
consistently outperforms other methods on multi-query workloads that simulate
DNN interpretation processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daum_M/0/1/0/all/0/1"&gt;Maureen Daum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Walter Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balazinska_M/0/1/0/all/0/1"&gt;Magdalena Balazinska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Portrait Delighting. (arXiv:2203.12088v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.12088</id>
        <link href="http://arxiv.org/abs/2203.12088"/>
        <updated>2022-03-26T00:46:07.818Z</updated>
        <summary type="html"><![CDATA[We present a deep neural network for removing undesirable shading features
from an unconstrained portrait image, recovering the underlying texture. Our
training scheme incorporates three regularization strategies: masked loss, to
emphasize high-frequency shading features; soft-shadow loss, which improves
sensitivity to subtle changes in lighting; and shading-offset estimation, to
supervise separation of shading and texture. Our method demonstrates improved
delighting quality and generalization when compared with the state-of-the-art.
We further demonstrate how our delighting method can enhance the performance of
light-sensitive computer vision tasks such as face relighting and semantic
parsing, allowing them to handle extreme lighting conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weir_J/0/1/0/all/0/1"&gt;Joshua Weir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Junhong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chalmers_A/0/1/0/all/0/1"&gt;Andrew Chalmers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_T/0/1/0/all/0/1"&gt;Taehyun Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep-Discrete Learning Framework for Spherical Surface Registration. (arXiv:2203.12999v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12999</id>
        <link href="http://arxiv.org/abs/2203.12999"/>
        <updated>2022-03-26T00:46:07.813Z</updated>
        <summary type="html"><![CDATA[Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suliman_M/0/1/0/all/0/1"&gt;Mohamed A. Suliman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1"&gt;Logan Z. J. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaz_A/0/1/0/all/0/1"&gt;Abdulah Fawaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_E/0/1/0/all/0/1"&gt;Emma C. Robinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bioformers: Embedding Transformers for Ultra-Low Power sEMG-based Gesture Recognition. (arXiv:2203.12932v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2203.12932</id>
        <link href="http://arxiv.org/abs/2203.12932"/>
        <updated>2022-03-26T00:46:07.807Z</updated>
        <summary type="html"><![CDATA[Human-machine interaction is gaining traction in rehabilitation tasks, such
as controlling prosthetic hands or robotic arms. Gesture recognition exploiting
surface electromyographic (sEMG) signals is one of the most promising
approaches, given that sEMG signal acquisition is non-invasive and is directly
related to muscle contraction. However, the analysis of these signals still
presents many challenges since similar gestures result in similar muscle
contractions. Thus the resulting signal shapes are almost identical, leading to
low classification accuracy. To tackle this challenge, complex neural networks
are employed, which require large memory footprints, consume relatively high
energy and limit the maximum battery life of devices used for classification.
This work addresses this problem with the introduction of the Bioformers. This
new family of ultra-small attention-based architectures approaches
state-of-the-art performance while reducing the number of parameters and
operations of 4.9X. Additionally, by introducing a new inter-subjects
pre-training, we improve the accuracy of our best Bioformer by 3.39%, matching
state-of-the-art accuracy without any additional inference cost. Deploying our
best performing Bioformer on a Parallel, Ultra-Low Power (PULP) microcontroller
unit (MCU), the GreenWaves GAP8, we achieve an inference latency and energy of
2.72 ms and 0.14 mJ, respectively, 8.0X lower than the previous
state-of-the-art neural network, while occupying just 94.2 kB of memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Burrello_A/0/1/0/all/0/1"&gt;Alessio Burrello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Morghet_F/0/1/0/all/0/1"&gt;Francesco Bianco Morghet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scherer_M/0/1/0/all/0/1"&gt;Moritz Scherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benatti_S/0/1/0/all/0/1"&gt;Simone Benatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Macii_E/0/1/0/all/0/1"&gt;Enrico Macii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Poncino_M/0/1/0/all/0/1"&gt;Massimo Poncino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pagliari_D/0/1/0/all/0/1"&gt;Daniele Jahier Pagliari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position Tracking using Likelihood Modeling of Channel Features with Gaussian Processes. (arXiv:2203.13110v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2203.13110</id>
        <link href="http://arxiv.org/abs/2203.13110"/>
        <updated>2022-03-26T00:46:07.792Z</updated>
        <summary type="html"><![CDATA[Recent localization frameworks exploit spatial information of complex channel
measurements (CMs) to estimate accurate positions even in multipath propagation
scenarios. State-of-the art CM fingerprinting(FP)-based methods employ
convolutional neural networks (CNN) to extract the spatial information.
However, they need spatially dense data sets (associated with high acquisition
and maintenance efforts) to work well -- which is rarely the case in practical
applications. If such data is not available (or its quality is low), we cannot
compensate the performance degradation of CNN-based FP as they do not provide
statistical position estimates, which prevents a fusion with other sources of
information on the observation level.

We propose a novel localization framework that adapts well to sparse datasets
that only contain CMs of specific areas within the environment with strong
multipath propagation. Our framework compresses CMs into informative features
to unravel spatial information. It then regresses Gaussian processes (GPs) for
each of them, which imply statistical observation models based on
distance-dependent covariance kernels. Our framework combines the trained GPs
with line-of-sight ranges and a dynamics model in a particle filter. Our
measurements show that our approach outperforms state-of-the-art CNN
fingerprinting (0.52 m vs. 1.3 m MAE) on spatially sparse data collected in a
realistic industrial indoor environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kram_S/0/1/0/all/0/1"&gt;Sebastian Kram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kraus_C/0/1/0/all/0/1"&gt;Christopher Kraus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feigl_T/0/1/0/all/0/1"&gt;Tobias Feigl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stahlke_M/0/1/0/all/0/1"&gt;Maximilian Stahlke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Robert_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg Robert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mutschler_C/0/1/0/all/0/1"&gt;Christopher Mutschler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks. (arXiv:2112.06825v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.06825</id>
        <link href="http://arxiv.org/abs/2112.06825"/>
        <updated>2022-03-26T00:46:07.769Z</updated>
        <summary type="html"><![CDATA[Recently, fine-tuning language models pre-trained on large text corpora have
provided huge improvements on vision-and-language (V&L) tasks as well as on
pure language tasks. However, fine-tuning the entire parameter set of
pre-trained models becomes impractical since the model size is growing rapidly.
Hence, in this paper, we introduce adapter-based parameter-efficient transfer
learning techniques to V&L models such as VL-BART and VLT5. We evaluate our
methods in a unified multi-task setup on both image-text and video-text
benchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,
GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,
How2QA, TVC, and YC2C. With careful training and thorough experiments, we
benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)
against the standard full fine-tuning and the recently proposed prompt-tuning
approach. We also enhance the efficiency and performance of adapters by sharing
their weights to attain knowledge across tasks. Our results demonstrate that
training the adapter with the weight-sharing technique (4.18% of total
parameters for image-text tasks and 3.39% for video-text tasks) can match the
performance of fine-tuning the entire model. Lastly, we present a comprehensive
analysis including the combination of adapter and task-specific prompts and the
impact of V&L pre-training on adapters. Our code is available at:
https://github.com/ylsung/VL_adapter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Yi-Lin Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Jaemin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Maximum Likelihood Difference Scaling method to measure inter content scale. (arXiv:2203.13186v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2203.13186</id>
        <link href="http://arxiv.org/abs/2203.13186"/>
        <updated>2022-03-26T00:46:07.750Z</updated>
        <summary type="html"><![CDATA[The goal of most subjective studies is to place a set of stimuli on a
perceptual scale. This is mostly done directly by rating, e.g. using single or
double stimulus methodologies, or indirectly by ranking or pairwise comparison.
All these methods estimate the perceptual magnitudes of the stimuli on a scale.
However, procedures such as Maximum Likelihood Difference Scaling (MLDS) have
shown that considering perceptual distances can bring benefits in terms of
discriminatory power, observers' cognitive load, and the number of trials
required. One of the disadvantages of the MLDS method is that the perceptual
scales obtained for stimuli created from different source content are generally
not comparable. In this paper, we propose an extension of the MLDS method that
ensures inter-content comparability of the results and shows its usefulness
especially in the presence of observer errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Andreas_P/0/1/0/all/0/1"&gt;Pastor Andr&amp;#xe9;as&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Krasula_L/0/1/0/all/0/1"&gt;Luk&amp;#xe1;&amp;#x161; Krasula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Callet_P/0/1/0/all/0/1"&gt;Patrick Le Callet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Asynchronous Stochastic Gradient Descent for Decentralised Deep Learning. (arXiv:2203.13085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.13085</id>
        <link href="http://arxiv.org/abs/2203.13085"/>
        <updated>2022-03-26T00:46:07.743Z</updated>
        <summary type="html"><![CDATA[Distributed training algorithms of deep neural networks show impressive
convergence speedup properties on very large problems. However, they inherently
suffer from communication related slowdowns and communication topology becomes
a crucial design choice. Common approaches supported by most machine learning
frameworks are: 1) Synchronous decentralized algorithms relying on a
peer-to-peer All Reduce topology that is sensitive to stragglers and
communication delays. 2) Asynchronous centralised algorithms with a server
based topology that is prone to communication bottleneck. Researchers also
suggested asynchronous decentralized algorithms designed to avoid the
bottleneck and speedup training, however, those commonly use inexact sparse
averaging that may lead to a degradation in accuracy. In this paper, we propose
Local Asynchronous SGD (LASGD), an asynchronous decentralized algorithm that
relies on All Reduce for model synchronization.

We empirically validate LASGD's performance on image classification tasks on
the ImageNet dataset. Our experiments demonstrate that LASGD accelerates
training compared to SGD and state of the art gossip based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avidor_T/0/1/0/all/0/1"&gt;Tomer Avidor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Israel_N/0/1/0/all/0/1"&gt;Nadav Tal Israel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Dutch Draw: Constructing a Universal Baseline for Binary Prediction Models. (arXiv:2203.13084v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.13084</id>
        <link href="http://arxiv.org/abs/2203.13084"/>
        <updated>2022-03-26T00:46:07.727Z</updated>
        <summary type="html"><![CDATA[Novel prediction methods should always be compared to a baseline to know how
well they perform. Without this frame of reference, the performance score of a
model is basically meaningless. What does it mean when a model achieves an
$F_1$ of 0.8 on a test set? A proper baseline is needed to evaluate the
`goodness' of a performance score. Comparing with the latest state-of-the-art
model is usually insightful. However, being state-of-the-art can change rapidly
when newer models are developed. Contrary to an advanced model, a simple dummy
classifier could be used. However, the latter could be beaten too easily,
making the comparison less valuable. This paper presents a universal baseline
method for all binary classification models, named the Dutch Draw (DD). This
approach weighs simple classifiers and determines the best classifier to use as
a baseline. We theoretically derive the DD baseline for many commonly used
evaluation measures and show that in most situations it reduces to (almost)
always predicting either zero or one. Summarizing, the DD baseline is: (1)
general, as it is applicable to all binary classification problems; (2) simple,
as it is quickly determined without training or parameter-tuning; (3)
informative, as insightful conclusions can be drawn from the results. The DD
baseline serves two purposes. First, to enable comparisons across research
papers by this robust and universal baseline. Secondly, to provide a sanity
check during the development process of a prediction model. It is a major
warning sign when a model is outperformed by the DD baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bijl_E/0/1/0/all/0/1"&gt;Etienne van de Bijl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1"&gt;Jan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pries_J/0/1/0/all/0/1"&gt;Joris Pries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhulai_S/0/1/0/all/0/1"&gt;Sandjai Bhulai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1"&gt;Mark Hoogendoorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_R/0/1/0/all/0/1"&gt;Rob van der Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.13167</id>
        <link href="http://arxiv.org/abs/2203.13167"/>
        <updated>2022-03-26T00:46:07.713Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the continual learning of Vision Transformers
(ViT) for the challenging exemplar-free scenario, with special focus on how to
efficiently distill the knowledge of its crucial self-attention mechanism
(SAM). Our work takes an initial step towards a surgical investigation of SAM
for designing coherent continual learning methods in ViTs. We first carry out
an evaluation of established continual learning regularization techniques. We
then examine the effect of regularization when applied to two key enablers of
SAM: (a) the contextualized embedding layers, for their ability to capture
well-scaled representations with respect to the values, and (b) the prescaled
attention maps, for carrying value-independent global contextual information.
We depict the perks of each distilling strategy on two image recognition
benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall
accuracy, (b) helps enhance the rigidity by maintaining competitive
performances. Furthermore, we identify the limitation imposed by the symmetric
nature of regularization losses. To alleviate this, we propose an asymmetric
variant and apply it to the pooled output distillation (POD) loss adapted for
ViTs. Our experiments confirm that introducing asymmetry to POD boosts its
plasticity while retaining stability across (a) and (b). Moreover, we
acknowledge low forgetting measures for all the compared methods, indicating
that ViTs might be naturally inclined continual learner]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1"&gt;Francesco Pelosin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Saurav Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1"&gt;Andrea Torsello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1"&gt;Bogdan Raducanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost van de Weijer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?. (arXiv:2203.12881v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.12881</id>
        <link href="http://arxiv.org/abs/2203.12881"/>
        <updated>2022-03-26T00:46:07.706Z</updated>
        <summary type="html"><![CDATA[Identifying argument components from unstructured texts and predicting the
relationships expressed among them are two primary steps of argument mining.
The intrinsic complexity of these tasks demands powerful learning models. While
pretrained Transformer-based Language Models (LM) have been shown to provide
state-of-the-art results over different NLP tasks, the scarcity of manually
annotated data and the highly domain-dependent nature of argumentation restrict
the capabilities of such models. In this work, we propose a novel transfer
learning strategy to overcome these challenges. We utilize argumentation-rich
social discussions from the ChangeMyView subreddit as a source of unsupervised,
argumentative discourse-aware knowledge by finetuning pretrained LMs on a
selectively masked language modeling task. Furthermore, we introduce a novel
prompt-based strategy for inter-component relation prediction that compliments
our proposed finetuning method while leveraging on the discourse context.
Exhaustive experiments show the generalization capability of our method on
these two tasks over within-domain as well as out-of-domain datasets,
outperforming several existing and employed strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Subhabrata Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1"&gt;Jeevesh Juneja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Dipankar Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1"&gt;Tanmoy Chakraborty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey. (arXiv:2108.04462v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04462</id>
        <link href="http://arxiv.org/abs/2108.04462"/>
        <updated>2022-03-26T00:46:07.686Z</updated>
        <summary type="html"><![CDATA[Recent technology development brings the booming of numerous new
Demand-Driven Services (DDS) into urban lives, including ridesharing, on-demand
delivery, express systems and warehousing. In DDS, a service loop is an
elemental structure, including its service worker, the service providers and
corresponding service targets. The service workers should transport either
humans or parcels from the providers to the target locations. Various planning
tasks within DDS can thus be classified into two individual stages: 1)
Dispatching, which is to form service loops from demand/supply distributions,
and 2)Routing, which is to decide specific serving orders within the
constructed loops. Generating high-quality strategies in both stages is
important to develop DDS but faces several challenging. Meanwhile, deep
reinforcement learning (DRL) has been developed rapidly in recent years. It is
a powerful tool to solve these problems since DRL can learn a parametric model
without relying on too many problem-based assumptions and optimize long-term
effect by learning sequential decisions. In this survey, we first define DDS,
then highlight common applications and important decision/control problems
within. For each problem, we comprehensively introduce the existing DRL
solutions. We also introduce open simulation environments for development and
evaluation of DDS applications. Finally, we analyze remaining challenges and
discuss further research opportunities in DRL solutions for DDS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Zefang Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1"&gt;Tao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Depeng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identification of high order closure terms from fully kinetic simulations using machine learning. (arXiv:2110.09916v2 [physics.plasm-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.09916</id>
        <link href="http://arxiv.org/abs/2110.09916"/>
        <updated>2022-03-26T00:46:07.679Z</updated>
        <summary type="html"><![CDATA[Simulations of large-scale plasma systems are typically based on a fluid
approximation approach. These models construct a moment-based system of
equations that approximate the particle-based physics as a fluid, but as a
result lack the small-scale physical processes available to fully kinetic
models. Traditionally, empirical closure relations are used to close the
moment-based system of equations, which typically approximate the pressure
tensor or heat flux. The more accurate the closure relation, the stronger the
simulation approaches kinetic-based results. In this paper, new closure terms
are constructed using machine learning techniques. Two different machine
learning models, a multi-layer perceptron and a gradient boosting regressor,
synthesize a local closure relation for the pressure tensor and heat flux
vector from fully kinetic simulations of a 2D magnetic reconnection problem.
The models are compared to an existing closure relation for the pressure
tensor, and the applicability of the models is discussed. The initial results
show that the models can capture the diagonal components of the pressure tensor
accurately, and show promising results for the heat flux, opening the way for
new experiments in multi-scale modeling. We find that the sampling of the
points used to train both models play a capital role in their accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Laperre_B/0/1/0/all/0/1"&gt;Brecht Laperre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Amaya_J/0/1/0/all/0/1"&gt;Jorge Amaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jamal_S/0/1/0/all/0/1"&gt;Sara Jamal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lapenta_G/0/1/0/all/0/1"&gt;Giovanni Lapenta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Smells in Public Datasets. (arXiv:2203.08007v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.08007</id>
        <link href="http://arxiv.org/abs/2203.08007"/>
        <updated>2022-03-26T00:46:07.673Z</updated>
        <summary type="html"><![CDATA[The adoption of Artificial Intelligence (AI) in high-stakes domains such as
healthcare, wildlife preservation, autonomous driving and criminal justice
system calls for a data-centric approach to AI. Data scientists spend the
majority of their time studying and wrangling the data, yet tools to aid them
with data analysis are lacking. This study identifies the recurrent data
quality issues in public datasets. Analogous to code smells, we introduce a
novel catalogue of data smells that can be used to indicate early signs of
problems or technical debt in machine learning systems. To understand the
prevalence of data quality issues in datasets, we analyse 25 public datasets
and identify 14 data smells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shome_A/0/1/0/all/0/1"&gt;Arumoy Shome&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1"&gt;Luis Cruz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1"&gt;Arie van Deursen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.11834</id>
        <link href="http://arxiv.org/abs/2203.11834"/>
        <updated>2022-03-26T00:46:07.667Z</updated>
        <summary type="html"><![CDATA[Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caldarola_D/0/1/0/all/0/1"&gt;Debora Caldarola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1"&gt;Marco Ciccone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.12865</id>
        <link href="http://arxiv.org/abs/2203.12865"/>
        <updated>2022-03-26T00:46:07.661Z</updated>
        <summary type="html"><![CDATA[The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation
of NLP systems has revealed high failure rates for basic capabilities for
multiple state-of-the-art and commercial models. However, the CheckList
creation process is manual which creates a bottleneck towards creation of
multilingual CheckLists catering 100s of languages. In this work, we explore
multiple approaches to generate and evaluate the quality of Multilingual
CheckList. We device an algorithm -- Automated Multilingual Checklist
Generation (AMCG) for automatically transferring a CheckList from a source to a
target language that relies on a reasonable machine translation system. We then
compare the CheckList generated by AMCG with CheckLists generated with
different levels of human intervention. Through in-depth crosslingual
experiments between English and Hindi, and broad multilingual experiments
spanning 11 languages, we show that the automatic approach can provide accurate
estimates of failure rates of a model across capabilities, as would a
human-verified CheckList, and better than CheckLists generated by humans from
scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1"&gt;Karthikeyan K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1"&gt;Shaily Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Pankaj Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1"&gt;Somak Aditya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1"&gt;Sandipan Dandapat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1"&gt;Sunayana Sitaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_M/0/1/0/all/0/1"&gt;Monojit Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.11894</id>
        <link href="http://arxiv.org/abs/2203.11894"/>
        <updated>2022-03-26T00:46:07.641Z</updated>
        <summary type="html"><![CDATA[In this work we demonstrate the vulnerability of vision transformers (ViTs)
to gradient-based inversion attacks. During this attack, the original data
batch is reconstructed given model weights and the corresponding gradients. We
introduce a method, named GradViT, that optimizes random noise into naturally
looking images via an iterative process. The optimization objective consists of
(i) a loss on matching the gradients, (ii) image prior in the form of distance
to batch-normalization statistics of a pretrained CNN model, and (iii) a total
variation regularization on patches to guide correct recovery locations. We
propose a unique loss scheduling function to overcome local minima during
optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and
observe unprecedentedly high fidelity and closeness to the original (hidden)
data. During the analysis we find that vision transformers are significantly
more vulnerable than previously studied CNNs due to the presence of the
attention mechanism. Our method demonstrates new state-of-the-art results for
gradient inversion in both qualitative and quantitative metrics. Project page
at https://gradvit.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1"&gt;Ali Hatamizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongxu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1"&gt;Holger Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Daguang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Orientation to Distinguish Overlapping Chromosomes. (arXiv:2203.13004v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.13004</id>
        <link href="http://arxiv.org/abs/2203.13004"/>
        <updated>2022-03-26T00:46:07.630Z</updated>
        <summary type="html"><![CDATA[A difficult step in the process of karyotyping is segmenting chromosomes that
touch or overlap. In an attempt to automate the process, previous studies
turned to Deep Learning methods, with some formulating the task as a semantic
segmentation problem. These models treat separate chromosome instances as
semantic classes, which we show to be problematic, since it is uncertain which
chromosome should be classed as #1 and #2. Assigning class labels based on
comparison rules, such as the shorter/longer chromosome alleviates, but does
not fully resolve the issue. Instead, we separate the chromosome instances in a
second stage, predicting the orientation of the chromosomes by the model and
use it as one of the key distinguishing factors of the chromosomes. We
demonstrate this method to be effective. Furthermore, we introduce a novel
Double-Angle representation that a neural network can use to predict the
orientation. The representation maps any direction and its reverse to the same
point. Lastly, we present a new expanded synthetic dataset, which is based on
Pommier's dataset, but addresses its issues with insufficient separation
between its training and testing sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kluvanec_D/0/1/0/all/0/1"&gt;Daniel Kluvanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_T/0/1/0/all/0/1"&gt;Thomas B. Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCaffrey_K/0/1/0/all/0/1"&gt;Kenneth J. W. McCaffrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named Entity Recognition. (arXiv:2203.12907v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.12907</id>
        <link href="http://arxiv.org/abs/2203.12907"/>
        <updated>2022-03-26T00:46:07.599Z</updated>
        <summary type="html"><![CDATA[Named entity recognition (NER) is the process of recognising and classifying
important information (entities) in text. Proper nouns, such as a person's
name, an organization's name, or a location's name, are examples of entities.
The NER is one of the important modules in applications like human resources,
customer support, search engines, content classification, and academia. In this
work, we consider NER for low-resource Indian languages like Hindi and Marathi.
The transformer-based models have been widely used for NER tasks. We consider
different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark
them on publicly available Hindi and Marathi NER datasets. We provide an
exhaustive comparison of different monolingual and multilingual
transformer-based models and establish simple baselines currently missing in
the literature. We show that the monolingual MahaRoBERTa model performs the
best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for
Hindi NER. We also perform cross-language evaluation and present mixed
observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1"&gt;Onkar Litake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabane_M/0/1/0/all/0/1"&gt;Maithili Sabane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patil_P/0/1/0/all/0/1"&gt;Parth Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranade_A/0/1/0/all/0/1"&gt;Aparna Ranade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Raviraj Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12861</id>
        <link href="http://arxiv.org/abs/2203.12861"/>
        <updated>2022-03-26T00:46:07.593Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNN) have demonstrated outstanding Compressed
Sensing (CS) performance compared to traditional, hand-crafted methods.
However, they are broadly limited in terms of generalisability, inductive bias
and difficulty to model long distance relationships. Transformer neural
networks (TNN) overcome such issues by implementing an attention mechanism
designed to capture dependencies between inputs. However, high-resolution tasks
typically require vision Transformers (ViT) to decompose an image into
patch-based tokens, limiting inputs to inherently local contexts. We propose a
novel image decomposition that naturally embeds images into low-resolution
inputs. These Kaleidoscope tokens (KD) provide a mechanism for global
attention, at the same computational cost as a patch-based approach. To
showcase this development, we replace CNN components in a well-known CS-MRI
neural network with TNN blocks and demonstrate the improvements afforded by KD.
We also propose an ensemble of image tokens, which enhance overall image
quality and reduces model size. Supplementary material is available:
https://github.com/uqmarlonbran/TCS.git}{https://github.com/uqmarlonbran/TCS.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lorenzana_M/0/1/0/all/0/1"&gt;Marlon Bran Lorenzana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engstrom_C/0/1/0/all/0/1"&gt;Craig Engstrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1"&gt;Shekhar S. Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized incentives as feedback design in generalized Nash equilibrium problems. (arXiv:2203.12948v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2203.12948</id>
        <link href="http://arxiv.org/abs/2203.12948"/>
        <updated>2022-03-26T00:46:07.573Z</updated>
        <summary type="html"><![CDATA[We investigate both stationary and time-varying, nonmonotone generalized Nash
equilibrium problems that exhibit symmetric interactions among the agents,
which are known to be potential. As may happen in practical cases, however, we
envision a scenario in which the formal expression of the underlying potential
function is not available, and we design a semi-decentralized Nash equilibrium
seeking algorithm. In the proposed two-layer scheme, a coordinator iteratively
integrates the (possibly noisy and sporadic) agents' feedback to learn the
pseudo-gradients of the agents, and then design personalized incentives for
them. On their side, the agents receive those personalized incentives, compute
a solution to an extended game, and then return feedback measurements to the
coordinator. In the stationary setting, our algorithm returns a Nash
equilibrium in case the coordinator is endowed with standard learning policies,
while it returns a Nash equilibrium up to a constant, yet adjustable, error in
the time-varying case. As a motivating application, we consider the ridehailing
service provided by several companies with mobility as a service orchestration,
necessary to both handle competition among firms and avoid traffic congestion,
which is also adopted to run numerical experiments verifying our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Fabiani_F/0/1/0/all/0/1"&gt;Filippo Fabiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simonetto_A/0/1/0/all/0/1"&gt;Andrea Simonetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Goulart_P/0/1/0/all/0/1"&gt;Paul J. Goulart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Optimization via Ball Oracle Acceleration. (arXiv:2203.13225v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2203.13225</id>
        <link href="http://arxiv.org/abs/2203.13225"/>
        <updated>2022-03-26T00:46:07.561Z</updated>
        <summary type="html"><![CDATA[We develop and analyze algorithms for distributionally robust optimization
(DRO) of convex losses. In particular, we consider group-structured and bounded
$f$-divergence uncertainty sets. Our approach relies on an accelerated method
that queries a ball optimization oracle, i.e., a subroutine that minimizes the
objective within a small ball around the query point. Our main contribution is
efficient implementations of this oracle for DRO objectives. For DRO with $N$
non-smooth loss functions, the resulting algorithms find an $\epsilon$-accurate
solution with $\widetilde{O}\left(N\epsilon^{-2/3} + \epsilon^{-2}\right)$
first-order oracle queries to individual loss functions. Compared to existing
algorithms for this problem, we improve complexity by a factor of up to
$\epsilon^{-4/3}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Carmon_Y/0/1/0/all/0/1"&gt;Yair Carmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hausler_D/0/1/0/all/0/1"&gt;Danielle Hausler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Development of a Vertex Finding Algorithm using Recurrent Neural Network. (arXiv:2101.11906v4 [physics.data-an] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11906</id>
        <link href="http://arxiv.org/abs/2101.11906"/>
        <updated>2022-03-26T00:46:07.555Z</updated>
        <summary type="html"><![CDATA[Deep learning is a rapidly-evolving technology with possibility to
significantly improve physics reach of collider experiments. In this study we
developed a novel algorithm of vertex finding for future lepton colliders such
as the International Linear Collider. We deploy two networks; one is simple
fully-connected layers to look for vertex seeds from track pairs, and the other
is a customized Recurrent Neural Network with an attention mechanism and an
encoder-decoder structure to associate tracks to the vertex seeds. The
performance of the vertex finder is compared with the standard ILC
reconstruction algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Goto_K/0/1/0/all/0/1"&gt;Kiichi Goto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Suehara_T/0/1/0/all/0/1"&gt;Taikan Suehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yoshioka_T/0/1/0/all/0/1"&gt;Tamaki Yoshioka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kurata_M/0/1/0/all/0/1"&gt;Masakazu Kurata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nagahara_H/0/1/0/all/0/1"&gt;Hajime Nagahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nakashima_Y/0/1/0/all/0/1"&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Takemura_N/0/1/0/all/0/1"&gt;Noriko Takemura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Iwasaki_M/0/1/0/all/0/1"&gt;Masako Iwasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Sparse Decision Tree Optimization via Reference Ensembles. (arXiv:2112.00798v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.00798</id>
        <link href="http://arxiv.org/abs/2112.00798"/>
        <updated>2022-03-26T00:46:07.553Z</updated>
        <summary type="html"><![CDATA[Sparse decision tree optimization has been one of the most fundamental
problems in AI since its inception and is a challenge at the core of
interpretable machine learning. Sparse decision tree optimization is
computationally hard, and despite steady effort since the 1960's, breakthroughs
have only been made on the problem within the past few years, primarily on the
problem of finding optimal sparse decision trees. However, current
state-of-the-art algorithms often require impractical amounts of computation
time and memory to find optimal or near-optimal trees for some real-world
datasets, particularly those having several continuous-valued features. Given
that the search spaces of these decision tree optimization problems are
massive, can we practically hope to find a sparse decision tree that competes
in accuracy with a black box machine learning model? We address this problem
via smart guessing strategies that can be applied to any optimal
branch-and-bound-based decision tree algorithm. We show that by using these
guesses, we can reduce the run time by multiple orders of magnitude, while
providing bounds on how far the resulting trees can deviate from the black
box's accuracy and expressive power. Our approach enables guesses about how to
bin continuous features, the size of the tree, and lower bounds on the error
for the optimal decision tree. Our experiments show that in many cases we can
rapidly construct sparse decision trees that match the accuracy of black box
models. To summarize: when you are having trouble optimizing, just guess.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McTavish_H/0/1/0/all/0/1"&gt;Hayden McTavish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Chudi Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achermann_R/0/1/0/all/0/1"&gt;Reto Achermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimalis_I/0/1/0/all/0/1"&gt;Ilias Karimalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jacques Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Margo Seltzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. (arXiv:2203.13131v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.13131</id>
        <link href="http://arxiv.org/abs/2203.13131"/>
        <updated>2022-03-26T00:46:07.535Z</updated>
        <summary type="html"><![CDATA[Recent text-to-image generation methods provide a simple yet exciting
conversion capability between text and image domains. While these methods have
incrementally improved the generated image fidelity and text relevancy, several
pivotal gaps remain unanswered, limiting applicability and quality. We propose
a novel text-to-image method that addresses these gaps by (i) enabling a simple
control mechanism complementary to text in the form of a scene, (ii)
introducing elements that substantially improve the tokenization process by
employing domain-specific knowledge over key image regions (faces and salient
objects), and (iii) adapting classifier-free guidance for the transformer use
case. Our model achieves state-of-the-art FID and human evaluation results,
unlocking the ability to generate high fidelity images in a resolution of
512x512 pixels, significantly improving visual quality. Through scene
controllability, we introduce several new capabilities: (i) Scene editing, (ii)
text editing with anchor scenes, (iii) overcoming out-of-distribution text
prompts, and (iv) story illustration generation, as demonstrated in the story
we wrote.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gafni_O/0/1/0/all/0/1"&gt;Oran Gafni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1"&gt;Adam Polyak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashual_O/0/1/0/all/0/1"&gt;Oron Ashual&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheynin_S/0/1/0/all/0/1"&gt;Shelly Sheynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taigman_Y/0/1/0/all/0/1"&gt;Yaniv Taigman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedCor: Correlation-Based Active Client Selection Strategy for Heterogeneous Federated Learning. (arXiv:2103.13822v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13822</id>
        <link href="http://arxiv.org/abs/2103.13822"/>
        <updated>2022-03-26T00:46:07.532Z</updated>
        <summary type="html"><![CDATA[Client-wise data heterogeneity is one of the major issues that hinder
effective training in federated learning (FL). Since the data distribution on
each client may vary dramatically, the client selection strategy can
significantly influence the convergence rate of the FL process. Active client
selection strategies are popularly proposed in recent studies. However, they
neglect the loss correlations between the clients and achieve only marginal
improvement compared to the uniform selection strategy. In this work, we
propose FedCor -- an FL framework built on a correlation-based client selection
strategy, to boost the convergence rate of FL. Specifically, we first model the
loss correlations between the clients with a Gaussian Process (GP). Based on
the GP model, we derive a client selection strategy with a significant
reduction of expected global loss in each round. Besides, we develop an
efficient GP training method with a low communication overhead in the FL
scenario by utilizing the covariance stationarity. Our experimental results
show that compared to the state-of-the-art method, FedCorr can improve the
convergence rates by $34\%\sim 99\%$ and $26\%\sim 51\%$ on FMNIST and
CIFAR-10, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Minxue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1"&gt;Xuefei Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yitu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jingwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiran Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Implicit Bias Towards Minimal Depth of Deep Neural Networks. (arXiv:2202.09028v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.09028</id>
        <link href="http://arxiv.org/abs/2202.09028"/>
        <updated>2022-03-26T00:46:07.531Z</updated>
        <summary type="html"><![CDATA[We study the implicit bias of gradient based training methods to favor
low-depth solutions when training deep neural networks. Recent results in the
literature suggest that penultimate layer representations learned by a
classifier over multiple classes exhibit a clustering property, called neural
collapse. We demonstrate empirically that the neural collapse property extends
beyond the penultimate layer and tends to emerge in intermediate layers as
well. In this regards, we hypothesize that gradient based methods are
implicitly biased towards selecting neural networks of minimal depth for
achieving this clustering property.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1"&gt;Tomer Galanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galanti_L/0/1/0/all/0/1"&gt;Liane Galanti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LHNN: Lattice Hypergraph Neural Network for VLSI Congestion Prediction. (arXiv:2203.12831v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12831</id>
        <link href="http://arxiv.org/abs/2203.12831"/>
        <updated>2022-03-26T00:46:07.530Z</updated>
        <summary type="html"><![CDATA[Precise congestion prediction from a placement solution plays a crucial role
in circuit placement. This work proposes the lattice hypergraph (LH-graph), a
novel graph formulation for circuits, which preserves netlist data during the
whole learning process, and enables the congestion information propagated
geometrically and topologically. Based on the formulation, we further developed
a heterogeneous graph neural network architecture LHNN, jointing the routing
demand regression to support the congestion spot classification. LHNN
constantly achieves more than 35% improvements compared with U-nets and Pix2Pix
on the F1 score. We expect our work shall highlight essential procedures using
machine learning for congestion prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bowen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1"&gt;Guibao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1"&gt;Jianye Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wulong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hongzhong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yibo Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[gACSON software for automated segmentation and morphology analyses of myelinated axons in 3D electron microscopy. (arXiv:2112.06476v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.06476</id>
        <link href="http://arxiv.org/abs/2112.06476"/>
        <updated>2022-03-26T00:46:07.528Z</updated>
        <summary type="html"><![CDATA[Background and Objective: Advances in electron microscopy (EM) now allow
three-dimensional (3D) imaging of hundreds of micrometers of tissue with
nanometer-scale resolution, providing new opportunities to study the
ultrastructure of the brain. In this work, we introduce a freely available
Matlab-based gACSON software for visualization, segmentation, assessment, and
morphology analysis of myelinated axons in 3D-EM volumes of brain tissue
samples. Methods: The software is equipped with a graphical user interface
(GUI). It automatically segments the intra-axonal space of myelinated axons and
their corresponding myelin sheaths and allows manual segmentation,
proofreading, and interactive correction of the segmented components. gACSON
analyzes the morphology of myelinated axons, such as axonal diameter, axonal
eccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of
the software by segmenting and analyzing myelinated axons in six 3D-EM volumes
of rat somatosensory cortex after sham surgery or traumatic brain injury (TBI).
Our results suggest that the equivalent diameter of myelinated axons in
somatosensory cortex was decreased in TBI animals five months after the injury.
Conclusions: Our results indicate that gACSON is a valuable tool for
visualization, segmentation, assessment, and morphology analysis of myelinated
axons in 3D-EM volumes. It is freely available at
https://github.com/AndreaBehan/g-ACSON under the MIT license.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Behanova_A/0/1/0/all/0/1"&gt;Andrea Behanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdollahzadeh_A/0/1/0/all/0/1"&gt;Ali Abdollahzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Belevich_I/0/1/0/all/0/1"&gt;Ilya Belevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jokitalo_E/0/1/0/all/0/1"&gt;Eija Jokitalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sierra_A/0/1/0/all/0/1"&gt;Alejandra Sierra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1"&gt;Jussi Tohka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Training of Siamese Spiking Neural Networks with Earth's Mover Distance. (arXiv:2203.13207v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2203.13207</id>
        <link href="http://arxiv.org/abs/2203.13207"/>
        <updated>2022-03-26T00:46:07.527Z</updated>
        <summary type="html"><![CDATA[This study adapts the highly-versatile siamese neural network model to the
event data domain. We introduce a supervised training framework for optimizing
Earth's Mover Distance (EMD) between spike trains with spiking neural networks
(SNN). We train this model on images of the MNIST dataset converted into
spiking domain with novel conversion schemes. The quality of the siamese
embeddings of input images was evaluated by measuring the classifier
performance for different dataset coding types. The models achieved performance
similar to existing SNN-based approaches (F1-score of up to 0.9386) while using
only about 15% of hidden layer neurons to classify each example. Furthermore,
models which did not employ a sparse neural code were about 45% slower than
their sparse counterparts. These properties make the model suitable for low
energy consumption and low prediction latency applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pabian_M/0/1/0/all/0/1"&gt;Mateusz Pabian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rzepka_D/0/1/0/all/0/1"&gt;Dominik Rzepka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawlak_M/0/1/0/all/0/1"&gt;Miros&amp;#x142;aw Pawlak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High pressure hydrogen by machine learning and quantum Monte Carlo. (arXiv:2112.11099v2 [cond-mat.str-el] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.11099</id>
        <link href="http://arxiv.org/abs/2112.11099"/>
        <updated>2022-03-26T00:46:07.525Z</updated>
        <summary type="html"><![CDATA[We have developed a technique combining the accuracy of quantum Monte Carlo
in describing the electron correlation with the efficiency of a Machine
Learning Potential (MLP). We use kernel regression in combination with SOAP
(Smooth Overlap of Atomic Position) features, implemented here in a very
efficient way. The key ingredients are: i) a sparsification technique, based on
farthest point sampling, ensuring generality and transferability of our MLPs
and ii) the so called $\Delta$-learning, allowing a small training data set, a
fundamental property for highly accurate but computationally demanding
calculations, such as the ones based on quantum Monte Carlo. As the first
application we present a benchmark study of the liquid-liquid transition of
high-pressure hydrogen and show the quality of our MLP, by emphasizing the
importance of high accuracy for this very debated subject, where experiments
are difficult in the lab, and theory is still far from being conclusive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Tirelli_A/0/1/0/all/0/1"&gt;Andrea Tirelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Tenti_G/0/1/0/all/0/1"&gt;Giacomo Tenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Nakano_K/0/1/0/all/0/1"&gt;Kousuke Nakano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sorella_S/0/1/0/all/0/1"&gt;Sandro Sorella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Missing Sources with Adversarial Support-Matching. (arXiv:2203.13154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.13154</id>
        <link href="http://arxiv.org/abs/2203.13154"/>
        <updated>2022-03-26T00:46:07.524Z</updated>
        <summary type="html"><![CDATA[When trained on diverse labeled data, machine learning models have proven
themselves to be a powerful tool in all facets of society. However, due to
budget limitations, deliberate or non-deliberate censorship, and other problems
during data collection and curation, the labeled training set might exhibit a
systematic shortage of data for certain groups. We investigate a scenario in
which the absence of certain data is linked to the second level of a two-level
hierarchy in the data. Inspired by the idea of protected groups from
algorithmic fairness, we refer to the partitions carved by this second level as
"subgroups"; we refer to combinations of subgroups and classes, or leaves of
the hierarchy, as "sources". To characterize the problem, we introduce the
concept of classes with incomplete subgroup support. The representational bias
in the training set can give rise to spurious correlations between the classes
and the subgroups which render standard classification models ungeneralizable
to unseen sources. To overcome this bias, we make use of an additional, diverse
but unlabeled dataset, called the "deployment set", to learn a representation
that is invariant to subgroup. This is done by adversarially matching the
support of the training and deployment sets in representation space. In order
to learn the desired invariance, it is paramount that the sets of samples
observed by the discriminator are balanced by class; this is easily achieved
for the training set, but requires using semi-supervised clustering for the
deployment set. We demonstrate the effectiveness of our method with experiments
on several datasets and variants of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kehrenberg_T/0/1/0/all/0/1"&gt;Thomas Kehrenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bartlett_M/0/1/0/all/0/1"&gt;Myles Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharmanska_V/0/1/0/all/0/1"&gt;Viktoriia Sharmanska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Quadrianto_N/0/1/0/all/0/1"&gt;Novi Quadrianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions. (arXiv:2109.04386v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.04386</id>
        <link href="http://arxiv.org/abs/2109.04386"/>
        <updated>2022-03-26T00:46:07.522Z</updated>
        <summary type="html"><![CDATA[An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends also on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct and Pserf. Experiments suggest that the proposed functions improve the
network performance significantly compared to the widely used activations like
ReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and
5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in
CIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet
V2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1"&gt;Koushik Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sandeep Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1"&gt;Shilpak Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1"&gt;Ashish Kumar Pandey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation of binary classification trees with binary features by quantum circuits. (arXiv:2108.13207v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.13207</id>
        <link href="http://arxiv.org/abs/2108.13207"/>
        <updated>2022-03-26T00:46:07.514Z</updated>
        <summary type="html"><![CDATA[We propose a quantum representation of binary classification trees with
binary features based on a probabilistic approach. By using the quantum
computer as a processor for probability distributions, a probabilistic
traversal of the decision tree can be realized via measurements of a quantum
circuit. We describe how tree inductions and the prediction of class labels of
query data can be integrated into this framework. An on-demand sampling method
enables predictions with a constant number of classical memory slots,
independent of the tree depth. We experimentally study our approach using both
a quantum computing simulator and actual IBM quantum hardware. To our
knowledge, this is the first realization of a decision tree classifier on a
quantum device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Heese_R/0/1/0/all/0/1"&gt;Raoul Heese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bickert_P/0/1/0/all/0/1"&gt;Patricia Bickert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Niederle_A/0/1/0/all/0/1"&gt;Astrid Elisa Niederle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.13151</id>
        <link href="http://arxiv.org/abs/2203.13151"/>
        <updated>2022-03-26T00:46:07.513Z</updated>
        <summary type="html"><![CDATA[Transformer-based language models (TLMs) provide state-of-the-art performance
in many modern natural language processing applications. TLM training is
conducted in two phases. First, the model is pre-trained over large volumes of
text to minimize a generic objective function, such as the Masked Language
Model (MLM). Second, the model is fine-tuned in specific downstream tasks.
Pre-training requires large volumes of data and high computational resources,
while introducing many still unresolved design choices. For instance, selecting
hyperparameters for language model pre-training is often carried out based on
heuristics or grid-based searches. In this work, we propose a multi-armed
bandit-based online optimization framework for the sequential selection of
pre-training hyperparameters to optimize language model performance. We pose
the pre-training procedure as a sequential decision-making task, where at each
pre-training step, an agent must determine what hyperparameters to use towards
optimizing the pre-training objective. We propose a Thompson sampling bandit
algorithm, based on a surrogate Gaussian process reward model of the MLM
pre-training objective, for its sequential minimization. We empirically show
how the proposed Gaussian process based Thompson sampling pre-trains robust and
well-performing language models. Namely, by sequentially selecting masking
hyperparameters of the TLM, we achieve satisfactory performance in less epochs,
not only in terms of the pre-training MLM objective, but in diverse downstream
fine-tuning tasks. The proposed bandit-based technique provides an automated
hyperparameter selection method for pre-training TLMs of interest to
practitioners. In addition, our results indicate that, instead of MLM
pre-training with fixed masking probabilities, sequentially adapting the
masking hyperparameters improves both pre-training loss and downstream task
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Urteaga_I/0/1/0/all/0/1"&gt;I&amp;#xf1;igo Urteaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Draidia_M/0/1/0/all/0/1"&gt;Moulay-Za&amp;#xef;dane Dra&amp;#xef;dia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1"&gt;Tomer Lancewicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1"&gt;Shahram Khadivi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-distribution Generalization with Causal Invariant Transformations. (arXiv:2203.11528v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.11528</id>
        <link href="http://arxiv.org/abs/2203.11528"/>
        <updated>2022-03-26T00:46:07.512Z</updated>
        <summary type="html"><![CDATA[In real-world applications, it is important and desirable to learn a model
that performs well on out-of-distribution (OOD) data. Recently, causality has
become a powerful tool to tackle the OOD generalization problem, with the idea
resting on the causal mechanism that is invariant across domains of interest.
To leverage the generally unknown causal mechanism, existing works assume a
linear form of causal feature or require sufficiently many and diverse training
domains, which are usually restrictive in practice. In this work, we obviate
these assumptions and tackle the OOD problem without explicitly recovering the
causal feature. Our approach is based on transformations that modify the
non-causal feature but leave the causal part unchanged, which can be either
obtained from prior knowledge or learned from the training data in the
multi-domain scenario. Under the setting of invariant causal mechanism, we
theoretically show that if all such transformations are available, then we can
learn a minimax optimal model across the domains using only single domain data.
Noticing that knowing a complete set of these causal invariant transformations
may be impractical, we further show that it suffices to know only a subset of
these transformations. Based on the theoretical findings, a regularized
training procedure is proposed to improve the OOD generalization capability.
Extensive experimental results on both synthetic and real datasets verify the
effectiveness of the proposed algorithm, even with only a few causal invariant
transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yi_M/0/1/0/all/0/1"&gt;Mingyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhitang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shengyu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Variational Representations of Divergences and Accelerating their Statistical Estimation. (arXiv:2006.08781v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08781</id>
        <link href="http://arxiv.org/abs/2006.08781"/>
        <updated>2022-03-26T00:46:07.508Z</updated>
        <summary type="html"><![CDATA[Variational representations of divergences and distances between
high-dimensional probability distributions offer significant theoretical
insights and practical advantages in numerous research areas. Recently, they
have gained popularity in machine learning as a tractable and scalable approach
for training probabilistic models and for statistically differentiating between
data distributions. Their advantages include: 1) They can be estimated from
data as statistical averages. 2) Such representations can leverage the ability
of neural networks to efficiently approximate optimal solutions in function
spaces. However, a systematic and practical approach to improving the tightness
of such variational formulas, and accordingly accelerate statistical learning
and estimation from data, is currently lacking. Here we develop such a
methodology for building new, tighter variational representations of
divergences. Our approach relies on improved objective functionals constructed
via an auxiliary optimization problem. Furthermore, the calculation of the
functional Hessian of objective functionals unveils the local curvature
differences around the common optimal variational solution; this quantifies and
orders the tightness gains between different variational representations.
Finally, numerical simulations utilizing neural network optimization
demonstrate that tighter representations can result in significantly faster
learning and more accurate estimation of divergences in both synthetic and real
datasets (of more than 1000 dimensions), often accelerated by nearly an order
of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Birrell_J/0/1/0/all/0/1"&gt;Jeremiah Birrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsoulakis_M/0/1/0/all/0/1"&gt;Markos A. Katsoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantazis_Y/0/1/0/all/0/1"&gt;Yannis Pantazis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spoofing Generalization: When Can't You Trust Proprietary Models?. (arXiv:2106.08393v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08393</id>
        <link href="http://arxiv.org/abs/2106.08393"/>
        <updated>2022-03-26T00:46:07.508Z</updated>
        <summary type="html"><![CDATA[In this work, we study the computational complexity of determining whether a
machine learning model that perfectly fits the training data will generalizes
to unseen data. In particular, we study the power of a malicious agent whose
goal is to construct a model g that fits its training data and nothing else,
but is indistinguishable from an accurate model f. We say that g strongly
spoofs f if no polynomial-time algorithm can tell them apart. If instead we
restrict to algorithms that run in $n^c$ time for some fixed $c$, we say that g
c-weakly spoofs f. Our main results are

1. Under cryptographic assumptions, strong spoofing is possible and

2. For any c> 0, c-weak spoofing is possible unconditionally

While the assumption of a malicious agent is an extreme scenario (hopefully
companies training large models are not malicious), we believe that it sheds
light on the inherent difficulties of blindly trusting large proprietary models
or data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mossel_E/0/1/0/all/0/1"&gt;Elchanan Mossel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandon_C/0/1/0/all/0/1"&gt;Colin Sandon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation. (arXiv:2203.13251v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2203.13251</id>
        <link href="http://arxiv.org/abs/2203.13251"/>
        <updated>2022-03-26T00:46:07.490Z</updated>
        <summary type="html"><![CDATA[Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arunachalam_S/0/1/0/all/0/1"&gt;Sridhar Pandian Arunachalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1"&gt;Sneha Silwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1"&gt;Ben Evans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1"&gt;Lerrel Pinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPST: De Novo Peptide Sequencing with Amino-Acid-Aware Transformers. (arXiv:2203.13132v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2203.13132</id>
        <link href="http://arxiv.org/abs/2203.13132"/>
        <updated>2022-03-26T00:46:07.255Z</updated>
        <summary type="html"><![CDATA[De novo peptide sequencing aims to recover amino acid sequences of a peptide
from tandem mass spectrometry (MS) data. Existing approaches for de novo
analysis enumerate MS evidence for all amino acid classes during inference. It
leads to over-trimming on receptive fields of MS data and restricts MS evidence
associated with following undecoded amino acids. Our approach, DPST,
circumvents these limitations with two key components: (1) A confidence value
aggregation encoder to sketch spectrum representations according to
amino-acid-based connectivity among MS; (2) A global-local fusion decoder to
progressively assimilate contextualized spectrum representations with a
predefined preconception of localized MS evidence and amino acid priors. Our
components originate from a closed-form solution and selectively attend to
informative amino-acid-aware MS representations. Through extensive empirical
studies, we demonstrate the superiority of DPST, showing that it outperforms
state-of-the-art approaches by a margin of 12% - 19% peptide accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hossain_Z/0/1/0/all/0/1"&gt;Zakir Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Asif_K/0/1/0/all/0/1"&gt;Khandaker Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Pan_L/0/1/0/all/0/1"&gt;Liyuan Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Rahman_S/0/1/0/all/0/1"&gt;Shafin Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Stone_E/0/1/0/all/0/1"&gt;Eric Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rubik's Cube Operator: A Plug And Play Permutation Module for Better Arranging High Dimensional Industrial Data in Deep Convolutional Processes. (arXiv:2203.12921v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12921</id>
        <link href="http://arxiv.org/abs/2203.12921"/>
        <updated>2022-03-26T00:46:07.252Z</updated>
        <summary type="html"><![CDATA[The convolutional neural network (CNN) has been widely applied to process the
industrial data based tensor input, which integrates data records of
distributed industrial systems from the spatial, temporal, and system dynamics
aspects. However, unlike images, information in the industrial data based
tensor is not necessarily spatially ordered. Thus, directly applying CNN is
ineffective. To tackle such issue, we propose a plug and play module, the
Rubik's Cube Operator (RCO), to adaptively permutate the data organization of
the industrial data based tensor to an optimal or suboptimal order of
attributes before being processed by CNNs, which can be updated with subsequent
CNNs together via the gradient-based optimizer. The proposed RCO maintains K
binary and right stochastic permutation matrices to permutate attributes of K
axes of the input industrial data based tensor. A novel learning process is
proposed to enable learning permutation matrices from data, where the
Gumbel-Softmax is employed to reparameterize elements of permutation matrices,
and the soft regularization loss is proposed and added to the task-specific
loss to ensure the feature diversity of the permuted data. We verify the
effectiveness of the proposed RCO via considering two representative learning
tasks processing industrial data via CNNs, the wind power prediction (WPP) and
the wind speed prediction (WSP) from the renewable energy domain. Computational
experiments are conducted based on four datasets collected from different wind
farms and the results demonstrate that the proposed RCO can improve the
performance of CNN based networks significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Luoxiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Kernel-Based Approach to Non-Stationary Reinforcement Learning in Metric Spaces. (arXiv:2007.05078v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05078</id>
        <link href="http://arxiv.org/abs/2007.05078"/>
        <updated>2022-03-26T00:46:07.248Z</updated>
        <summary type="html"><![CDATA[In this work, we propose KeRNS: an algorithm for episodic reinforcement
learning in non-stationary Markov Decision Processes (MDPs) whose state-action
set is endowed with a metric. Using a non-parametric model of the MDP built
with time-dependent kernels, we prove a regret bound that scales with the
covering dimension of the state-action space and the total variation of the MDP
with time, which quantifies its level of non-stationarity. Our method
generalizes previous approaches based on sliding windows and exponential
discounting used to handle changing environments. We further propose a
practical implementation of KeRNS, we analyze its regret and validate it
experimentally.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingues_O/0/1/0/all/0/1"&gt;Omar Darwiche Domingues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menard_P/0/1/0/all/0/1"&gt;Pierre M&amp;#xe9;nard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Emilie Kaufmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary Policies. (arXiv:2203.12922v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12922</id>
        <link href="http://arxiv.org/abs/2203.12922"/>
        <updated>2022-03-26T00:46:07.246Z</updated>
        <summary type="html"><![CDATA[This paper gives the first polynomial-time algorithm for tabular Markov
Decision Processes (MDP) that enjoys a regret bound \emph{independent on the
planning horizon}. Specifically, we consider tabular MDP with $S$ states, $A$
actions, a planning horizon $H$, total reward bounded by $1$, and the agent
plays for $K$ episodes. We design an algorithm that achieves an
$O\left(\mathrm{poly}(S,A,\log K)\sqrt{K}\right)$ regret in contrast to
existing bounds which either has an additional $\mathrm{polylog}(H)$
dependency~\citep{zhang2020reinforcement} or has an exponential dependency on
$S$~\citep{li2021settling}. Our result relies on a sequence of new structural
lemmas establishing the approximation power, stability, and concentration
property of stationary policies, which can have applications in other problems
related to Markov chains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zihan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LAFITE: Towards Language-Free Training for Text-to-Image Generation. (arXiv:2111.13792v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.13792</id>
        <link href="http://arxiv.org/abs/2111.13792"/>
        <updated>2022-03-26T00:46:07.245Z</updated>
        <summary type="html"><![CDATA[One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yufan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changyou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tensmeyer_C/0/1/0/all/0/1"&gt;Chris Tensmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiuxiang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinhui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Avalanche RL: a Continual Reinforcement Learning Library. (arXiv:2202.13657v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.13657</id>
        <link href="http://arxiv.org/abs/2202.13657"/>
        <updated>2022-03-26T00:46:07.245Z</updated>
        <summary type="html"><![CDATA[Continual Reinforcement Learning (CRL) is a challenging setting where an
agent learns to interact with an environment that is constantly changing over
time (the stream of experiences). In this paper, we describe Avalanche RL, a
library for Continual Reinforcement Learning which allows to easily train
agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and
supports any OpenAI Gym environment. Its design is based on Avalanche, one of
the more popular continual learning libraries, which allow us to reuse a large
number of continual learning strategies and improve the interaction between
reinforcement learning and continual learning researchers. Additionally, we
propose Continual Habitat-Lab, a novel benchmark and a high-level library which
enables the usage of the photorealistic simulator Habitat-Sim for CRL research.
Overall, Avalanche RL attempts to unify under a common framework continual
reinforcement learning applications, which we hope will foster the growth of
the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lucchesi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Lucchesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Explanations for Entity Resolution Models. (arXiv:2203.12978v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2203.12978</id>
        <link href="http://arxiv.org/abs/2203.12978"/>
        <updated>2022-03-26T00:46:07.244Z</updated>
        <summary type="html"><![CDATA[Entity resolution (ER) aims at matching records that refer to the same
real-world entity. Although widely studied for the last 50 years, ER still
represents a challenging data management problem, and several recent works have
started to investigate the opportunity of applying deep learning (DL)
techniques to solve this problem. In this paper, we study the fundamental
problem of explainability of the DL solution for ER. Understanding the matching
predictions of an ER solution is indeed crucial to assess the trustworthiness
of the DL model and to discover its biases. We treat the DL model as a black
box classifier and - while previous approaches to provide explanations for DL
predictions are agnostic to the classification task. we propose the CERTA
approach that is aware of the semantics of the ER problem. Our approach
produces both saliency explanations, which associate each attribute with a
saliency score, and counterfactual explanations, which provide examples of
values that can flip the prediction. CERTA builds on a probabilistic framework
that aims at computing the explanations evaluating the outcomes produced by
using perturbed copies of the input records. We experimentally evaluate CERTA's
explanations of state-of-the-art ER solutions based on DL models using publicly
available datasets, and demonstrate the effectiveness of CERTA over recently
proposed methods for this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teofili_T/0/1/0/all/0/1"&gt;Tommaso Teofili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firmani_D/0/1/0/all/0/1"&gt;Donatella Firmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koudas_N/0/1/0/all/0/1"&gt;Nick Koudas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martello_V/0/1/0/all/0/1"&gt;Vincenzo Martello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merialdo_P/0/1/0/all/0/1"&gt;Paolo Merialdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_D/0/1/0/all/0/1"&gt;Divesh Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Optimal Strategies for Temporal Tasks in Stochastic Games. (arXiv:2102.04307v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04307</id>
        <link href="http://arxiv.org/abs/2102.04307"/>
        <updated>2022-03-26T00:46:07.244Z</updated>
        <summary type="html"><![CDATA[Synthesis from linear temporal logic (LTL) specifications provides assured
controllers for autonomous systems operating in stochastic and potentially
adversarial environments. Automatic synthesis tools, however, require a model
of the environment to construct controllers. In this work, we introduce a
model-free reinforcement learning (RL) approach that derives controllers from
given LTL specifications even when the environment is completely unknown. We
model the problem of satisfying the LTL specifications as a stochastic game
(SG) between the controller and the adversarial environment; we then learn
optimal controller strategies that maximize the probability of satisfying the
LTL specifications against the worst-case environment behavior. We first
construct a product game using the deterministic parity automaton (DPA)
translated from the given LTL specification. By deriving distinct rewards and
discount factors from the acceptance condition of the DPA, we reduce the
maximization of the worst-case probability of satisfying the LTL specification
into the maximization of a discounted reward objective in the product game;
this allows for the use of model-free RL algorithms to learn an optimal
controller strategy. To deal with the common scalability problems when the
number of colors defining the acceptance condition of the DPA is large, we
propose a lazy color generation method where distinct rewards and discount
factors are utilized only when needed, and an approximate method where the
controller eventually focuses on only one color. In several case studies, we
show that our approach is scalable to a wide range of LTL formulas,
significantly outperforming existing methods for learning controllers from LTL
specifications in SGs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bozkurt_A/0/1/0/all/0/1"&gt;Alper Kamil Bozkurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zavlanos_M/0/1/0/all/0/1"&gt;Michael M. Zavlanos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1"&gt;Miroslav Pajic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SwiftAgg+: Achieving Asymptotically Optimal Communication Load in Secure Aggregation for Federated Learning. (arXiv:2203.13060v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2203.13060</id>
        <link href="http://arxiv.org/abs/2203.13060"/>
        <updated>2022-03-26T00:46:07.238Z</updated>
        <summary type="html"><![CDATA[We propose SwiftAgg+, a novel secure aggregation protocol for federated
learning systems, where a central server aggregates local models of
$N\in\mathbb{N}$ distributed users, each of size $L \in \mathbb{N}$, trained on
their local data, in a privacy-preserving manner. SwiftAgg+ can significantly
reduce the communication overheads without any compromise on security, and
achieve the optimum communication load within a diminishing gap. Specifically,
in presence of at most $D$ dropout users, SwiftAgg+ achieves average per-user
communication load of $(1+\mathcal{O}(\frac{1}{N}))L$ and the server
communication load of $(1+\mathcal{O}(\frac{1}{N}))L$, with a worst-case
information-theoretic security guarantee, against any subset of up to $T$
semi-honest users who may also collude with the curious server. The proposed
SwiftAgg+ has also a flexibility to reduce the number of active communication
links at the cost of increasing the the communication load between the users
and the server. In particular, for any $K\in\mathbb{N}$, SwiftAgg+ can achieve
the uplink communication load of $(1+\frac{T}{K})L$, and per-user communication
load of up to $(1-\frac{1}{N})(1+\frac{T+D}{K})L$, where the number of
pair-wise active connections in the network is $\frac{N}{2}(K+T+D+1)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jahani_Nezhad_T/0/1/0/all/0/1"&gt;Tayyebeh Jahani-Nezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddah_Ali_M/0/1/0/all/0/1"&gt;Mohammad Ali Maddah-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songze Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caire_G/0/1/0/all/0/1"&gt;Giuseppe Caire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm Fairness in AI for Medicine and Healthcare. (arXiv:2110.00603v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.00603</id>
        <link href="http://arxiv.org/abs/2110.00603"/>
        <updated>2022-03-26T00:46:07.238Z</updated>
        <summary type="html"><![CDATA[In the current development and deployment of many artificial intelligence
(AI) systems in healthcare, algorithm fairness is a challenging problem in
delivering equitable care. Recent evaluation of AI models stratified across
race sub-populations have revealed inequalities in how patients are diagnosed,
given treatments, and billed for healthcare costs. In this perspective article,
we summarize the intersectional field of fairness in machine learning through
the context of current issues in healthcare, outline how algorithmic biases
(e.g. - image acquisition, genetic variation, intra-observer labeling
variability) arise in current clinical workflows and their resulting healthcare
disparities. Lastly, we also review emerging technology for mitigating bias via
federated learning, disentanglement, and model explainability, and their role
in AI-SaMD development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Richard J. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tiffany Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipkova_J/0/1/0/all/0/1"&gt;Jana Lipkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Judy J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1"&gt;Drew F.K. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahai_S/0/1/0/all/0/1"&gt;Sharifa Sahai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1"&gt;Faisal Mahmood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data. (arXiv:2109.04260v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.04260</id>
        <link href="http://arxiv.org/abs/2109.04260"/>
        <updated>2022-03-26T00:46:07.237Z</updated>
        <summary type="html"><![CDATA[With the vigorous development of multimedia equipment and applications,
efficient retrieval of large-scale multi-modal data has become a trendy
research topic. Thereinto, hashing has become a prevalent choice due to its
retrieval efficiency and low storage cost. Although multi-modal hashing has
drawn lots of attention in recent years, there still remain some problems. The
first point is that existing methods are mainly designed in batch mode and not
able to efficiently handle streaming multi-modal data. The second point is that
all existing online multi-modal hashing methods fail to effectively handle
unseen new classes which come continuously with streaming data chunks. In this
paper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).
We design novel semantic-enhanced representation for data, which could help
handle the new coming classes, and thereby construct the enhanced semantic
objective function. An efficient and effective discrete online optimization
algorithm is further proposed for OASIS. Extensive experiments show that our
method can exceed the state-of-the-art models. For good reproducibility and
benefiting the community, our code and data are already available in
supplementary material and will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiao-Ming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1"&gt;Yu-Wei Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Chen-Lu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhen-Duo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xin-Shun Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TCN Mapping Optimization for Ultra-Low Power Time-Series Edge Inference. (arXiv:2203.12925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12925</id>
        <link href="http://arxiv.org/abs/2203.12925"/>
        <updated>2022-03-26T00:46:07.236Z</updated>
        <summary type="html"><![CDATA[Temporal Convolutional Networks (TCNs) are emerging lightweight Deep Learning
models for Time Series analysis. We introduce an automated exploration approach
and a library of optimized kernels to map TCNs on Parallel Ultra-Low Power
(PULP) microcontrollers. Our approach minimizes latency and energy by
exploiting a layer tiling optimizer to jointly find the tiling dimensions and
select among alternative implementations of the causal and dilated
1D-convolution operations at the core of TCNs. We benchmark our approach on a
commercial PULP device, achieving up to 103X lower latency and 20.3X lower
energy than the Cube-AI toolkit executed on the STM32L4 and from 2.9X to 26.6X
lower energy compared to commercial closed-source and academic open-source
approaches on the same hardware target.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1"&gt;Alessio Burrello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dequino_A/0/1/0/all/0/1"&gt;Alberto Dequino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagliari_D/0/1/0/all/0/1"&gt;Daniele Jahier Pagliari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1"&gt;Francesco Conti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanghieri_M/0/1/0/all/0/1"&gt;Marcello Zanghieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1"&gt;Enrico Macii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poncino_M/0/1/0/all/0/1"&gt;Massimo Poncino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decouple-and-Sample: Protecting sensitive information in task agnostic data release. (arXiv:2203.13204v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2203.13204</id>
        <link href="http://arxiv.org/abs/2203.13204"/>
        <updated>2022-03-26T00:46:07.235Z</updated>
        <summary type="html"><![CDATA[We propose sanitizer, a framework for secure and task-agnostic data release.
While releasing datasets continues to make a big impact in various applications
of computer vision, its impact is mostly realized when data sharing is not
inhibited by privacy concerns. We alleviate these concerns by sanitizing
datasets in a two-stage process. First, we introduce a global decoupling stage
for decomposing raw data into sensitive and non-sensitive latent
representations. Secondly, we design a local sampling stage to synthetically
generate sensitive information with differential privacy and merge it with
non-sensitive latent features to create a useful representation while
preserving the privacy. This newly formed latent information is a task-agnostic
representation of the original dataset with anonymized sensitive information.
While most algorithms sanitize data in a task-dependent manner, a few
task-agnostic sanitization techniques sanitize data by censoring sensitive
information. In this work, we show that a better privacy-utility trade-off is
achieved if sensitive information can be synthesized privately. We validate the
effectiveness of the sanitizer by outperforming state-of-the-art baselines on
the existing benchmark tasks and demonstrating tasks that are not possible
using existing techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Abhishek Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garza_E/0/1/0/all/0/1"&gt;Ethan Garza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1"&gt;Ayush Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vepakomma_P/0/1/0/all/0/1"&gt;Praneeth Vepakomma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1"&gt;Vivek Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1"&gt;Ramesh Raskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.08368</id>
        <link href="http://arxiv.org/abs/2203.08368"/>
        <updated>2022-03-26T00:46:07.234Z</updated>
        <summary type="html"><![CDATA[The exponentially large discrete search space in mixed-precision quantization
(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous
works usually resort to iterative search methods on the training set, which
consume hundreds or even thousands of GPU-hours. In this study, we reveal that
some unique learnable parameters in quantization, namely the scale factors in
the quantizer, can serve as importance indicators of a layer, reflecting the
contribution of that layer to the final accuracy at certain bit-widths. These
importance indicators naturally perceive the numerical transformation during
quantization-aware training, which can precisely and correctly provide
quantization sensitivity metrics of layers. However, a deep network always
contains hundreds of such indicators, and training them one by one would lead
to an excessive time cost. To overcome this issue, we propose a joint training
scheme that can obtain all indicators at once. It considerably speeds up the
indicators training process by parallelizing the original sequential training
processes. With these learned importance indicators, we formulate the MPQ
search problem as a one-time integer linear programming (ILP) problem. That
avoids the iterative search and significantly reduces search time without
limiting the bit-width search space. For example, MPQ search on ResNet18 with
our indicators takes only 0.06 seconds. Also, extensive experiments show our
approach can achieve SOTA accuracy on ImageNet for far-ranging models with
various constraints (e.g., BitOps, compress rate).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_K/0/1/0/all/0/1"&gt;Kai Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yifei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaowei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wen Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer. (arXiv:2203.13248v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.13248</id>
        <link href="http://arxiv.org/abs/2203.13248"/>
        <updated>2022-03-26T00:46:07.215Z</updated>
        <summary type="html"><![CDATA[Recent studies on StyleGAN show high performance on artistic portrait
generation by transfer learning with limited data. In this paper, we explore
more challenging exemplar-based high-resolution portrait style transfer by
introducing a novel DualStyleGAN with flexible control of dual styles of the
original face domain and the extended artistic portrait domain. Different from
StyleGAN, DualStyleGAN provides a natural way of style transfer by
characterizing the content and style of a portrait with an intrinsic style path
and a new extrinsic style path, respectively. The delicately designed extrinsic
style path enables our model to modulate both the color and complex structural
styles hierarchically to precisely pastiche the style example. Furthermore, a
novel progressive fine-tuning scheme is introduced to smoothly transform the
generative space of the model to the target domain, even with the above
modifications on the network architecture. Experiments demonstrate the
superiority of DualStyleGAN over state-of-the-art methods in high-quality
portrait style transfer and flexible style control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Implicit Bias of Gradient Descent for Temporal Extrapolation. (arXiv:2202.04302v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.04302</id>
        <link href="http://arxiv.org/abs/2202.04302"/>
        <updated>2022-03-26T00:46:07.195Z</updated>
        <summary type="html"><![CDATA[When using recurrent neural networks (RNNs) it is common practice to apply
trained models to sequences longer than those seen in training. This
"extrapolating" usage deviates from the traditional statistical learning setup
where guarantees are provided under the assumption that train and test
distributions are identical. Here we set out to understand when RNNs can
extrapolate, focusing on a simple case where the data generating distribution
is memoryless. We first show that even with infinite training data, there exist
RNN models that interpolate perfectly (i.e., they fit the training data) yet
extrapolate poorly to longer sequences. We then show that if gradient descent
is used for training, learning will converge to perfect extrapolation under
certain assumptions on initialization. Our results complement recent studies on
the implicit bias of gradient descent, showing that it plays a key role in
extrapolation when learning temporal prediction models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Karlik_E/0/1/0/all/0/1"&gt;Edo Cohen-Karlik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Avichai Ben David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1"&gt;Nadav Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1"&gt;Amir Globerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kullback-Leibler control for discrete-time nonlinear systems on continuous spaces. (arXiv:2203.12864v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2203.12864</id>
        <link href="http://arxiv.org/abs/2203.12864"/>
        <updated>2022-03-26T00:46:07.192Z</updated>
        <summary type="html"><![CDATA[Kullback-Leibler (KL) control enables efficient numerical methods for
nonlinear optimal control problems. The crucial assumption of KL control is the
full controllability of the transition distribution. However, this assumption
is often violated when the dynamics evolves in a continuous space.
Consequently, applying KL control to problems with continuous spaces requires
some approximation, which leads to the lost of the optimality. To avoid such
approximation, in this paper, we reformulate the KL control problem for
continuous spaces so that it does not require unrealistic assumptions. The key
difference between the original and reformulated KL control is that the former
measures the control effort by KL divergence between controlled and
uncontrolled transition distributions while the latter replaces the
uncontrolled transition by a noise-driven transition. We show that the
reformulated KL control admits efficient numerical algorithms like the original
one without unreasonable assumptions. Specifically, the associated value
function can be computed by using a Monte Carlo method based on its path
integral representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ito_K/0/1/0/all/0/1"&gt;Kaito Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kashima_K/0/1/0/all/0/1"&gt;Kenji Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Microfossil Identification via Deep Metric Learning. (arXiv:2112.09490v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.09490</id>
        <link href="http://arxiv.org/abs/2112.09490"/>
        <updated>2022-03-26T00:46:07.172Z</updated>
        <summary type="html"><![CDATA[We apply deep metric learning for the first time to the problem of
classifying planktic foraminifer shells on microscopic images. This species
recognition task is an important information source and scientific pillar for
reconstructing past climates. All foraminifer CNN recognition pipelines in the
literature produce black-box classifiers that lack visualization options for
human experts and cannot be applied to open-set problems. Here, we benchmark
metric learning against these pipelines, produce the first scientific
visualization of the phenotypic planktic foraminifer morphology space, and
demonstrate that metric learning can be used to cluster species unseen during
training. We show that metric learning outperforms all published CNN-based
state-of-the-art benchmarks in this domain. We evaluate our approach on the
34,640 expert-annotated images of the Endless Forams public library of 35
modern planktic foraminifera species. Our results on this data show leading 92%
accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,
and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered
in training. We conclude that metric learning is highly effective for this
domain and serves as an important tool towards expert-in-the-loop automation of
microfossil identification. Keycode, network weights, and data splits are
published with this paper for full reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karaderi_T/0/1/0/all/0/1"&gt;Tayfun Karaderi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1"&gt;Tilo Burghardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiang_A/0/1/0/all/0/1"&gt;Allison Y. Hsiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramaer_J/0/1/0/all/0/1"&gt;Jacob Ramaer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1"&gt;Daniela N. Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extended critical regimes of deep neural networks. (arXiv:2203.12967v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12967</id>
        <link href="http://arxiv.org/abs/2203.12967"/>
        <updated>2022-03-26T00:46:07.163Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have been successfully applied to many real-world
problems, but a complete understanding of their dynamical and computational
principles is still lacking. Conventional theoretical frameworks for analysing
DNNs often assume random networks with coupling weights obeying Gaussian
statistics. However, non-Gaussian, heavy-tailed coupling is a ubiquitous
phenomenon in DNNs. Here, by weaving together theories of heavy-tailed random
matrices and non-equilibrium statistical physics, we develop a new type of mean
field theory for DNNs which predicts that heavy-tailed weights enable the
emergence of an extended critical regime without fine-tuning parameters. In
this extended critical regime, DNNs exhibit rich and complex propagation
dynamics across layers. We further elucidate that the extended criticality
endows DNNs with profound computational advantages: balancing the contraction
as well as expansion of internal neural representations and speeding up
training processes, hence providing a theoretical guide for the design of
efficient neural architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1"&gt;Cheng Kevin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wardak_A/0/1/0/all/0/1"&gt;Asem Wardak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1"&gt;Pulin Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HiFi++: a Unified Framework for Neural Vocoding, Bandwidth Extension and Speech Enhancement. (arXiv:2203.13086v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.13086</id>
        <link href="http://arxiv.org/abs/2203.13086"/>
        <updated>2022-03-26T00:46:07.163Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks have recently demonstrated outstanding
performance in neural vocoding outperforming best autoregressive and flow-based
models. In this paper, we show that this success can be extended to other tasks
of conditional audio generation. In particular, building upon HiFi vocoders, we
propose a novel HiFi++ general framework for neural vocoding, bandwidth
extension, and speech enhancement. We show that with the improved generator
architecture and simplified multi-discriminator training, HiFi++ performs on
par with the state-of-the-art in these tasks while spending significantly less
memory and computational resources. The effectiveness of our approach is
validated through a series of extensive experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andreev_P/0/1/0/all/0/1"&gt;Pavel Andreev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alanov_A/0/1/0/all/0/1"&gt;Aibek Alanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ivanov_O/0/1/0/all/0/1"&gt;Oleg Ivanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token Dropping for Efficient BERT Pretraining. (arXiv:2203.13240v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.13240</id>
        <link href="http://arxiv.org/abs/2203.13240"/>
        <updated>2022-03-26T00:46:07.163Z</updated>
        <summary type="html"><![CDATA[Transformer-based models generally allocate the same amount of computation
for each token in a given sequence. We develop a simple but effective "token
dropping" method to accelerate the pretraining of transformer models, such as
BERT, without degrading its performance on downstream tasks. In short, we drop
unimportant tokens starting from an intermediate layer in the model to make the
model focus on important tokens; the dropped tokens are later picked up by the
last layer of the model so that the model still produces full-length sequences.
We leverage the already built-in masked language modeling (MLM) loss to
identify unimportant tokens with practically no computational overhead. In our
experiments, this simple approach reduces the pretraining cost of BERT by 25%
while achieving similar overall fine-tuning performance on standard downstream
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1"&gt;Le Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1"&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuexin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xinying Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xiaodan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Denny Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Model Aggregation for Fast and Robust Federated Learning in Edge Computing. (arXiv:2203.12738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12738</id>
        <link href="http://arxiv.org/abs/2203.12738"/>
        <updated>2022-03-26T00:46:07.135Z</updated>
        <summary type="html"><![CDATA[Federated learning is a prime candidate for distributed machine learning at
the network edge due to the low communication complexity and privacy protection
among other attractive properties. However, existing algorithms face issues
with slow convergence and/or robustness of performance due to the considerable
heterogeneity of data distribution, computation and communication capability at
the edge. In this work, we tackle both of these issues by focusing on the key
component of model aggregation in federated learning systems and studying
optimal algorithms to perform this task. Particularly, we propose a contextual
aggregation scheme that achieves the optimal context-dependent bound on loss
reduction in each round of optimization. The aforementioned context-dependent
bound is derived from the particular participating devices in that round and an
assumption on smoothness of the overall loss function. We show that this
aggregation leads to a definite reduction of loss function at every round.
Furthermore, we can integrate our aggregation with many existing algorithms to
obtain the contextual versions. Our experimental results demonstrate
significant improvements in convergence speed and robustness of the contextual
versions compared to the original algorithms. We also consider different
variants of the contextual aggregation and show robust performance even in the
most extreme settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hung T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1"&gt;Mung Chiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Supervised Machine Learning Approach for Sequence Based Protein-protein Interaction (PPI) Prediction. (arXiv:2203.12659v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12659</id>
        <link href="http://arxiv.org/abs/2203.12659"/>
        <updated>2022-03-26T00:46:07.132Z</updated>
        <summary type="html"><![CDATA[Computational protein-protein interaction (PPI) prediction techniques can
contribute greatly in reducing time, cost and false-positive interactions
compared to experimental approaches. Sequence is one of the key and primary
information of proteins that plays a crucial role in PPI prediction. Several
machine learning approaches have been applied to exploit the characteristics of
PPI datasets. However, these datasets greatly influence the performance of
predicting models. So, care should be taken on both dataset curation as well as
design of predictive models. Here, we have described our submitted solution
with the results of the SeqPIP competition whose objective was to develop
comprehensive PPI predictive models from sequence information with high-quality
bias-free interaction datasets. A training set of 2000 positive and 2000
negative interactions with sequences was given to us. Our method was evaluated
with three independent high-quality interaction test datasets and with other
competitors solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Debnath_S/0/1/0/all/0/1"&gt;Soumyadeep Debnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mollah_A/0/1/0/all/0/1"&gt;Ayatullah Faruk Mollah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computed Tomography Reconstruction using Generative Energy-Based Priors. (arXiv:2203.12658v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.12658</id>
        <link href="http://arxiv.org/abs/2203.12658"/>
        <updated>2022-03-26T00:46:07.123Z</updated>
        <summary type="html"><![CDATA[In the past decades, Computed Tomography (CT) has established itself as one
of the most important imaging techniques in medicine. Today, the applicability
of CT is only limited by the deposited radiation dose, reduction of which
manifests in noisy or incomplete measurements. Thus, the need for robust
reconstruction algorithms arises. In this work, we learn a parametric
regularizer with a global receptive field by maximizing it's likelihood on
reference CT data. Due to this unsupervised learning strategy, our trained
regularizer truly represents higher-level domain statistics, which we
empirically demonstrate by synthesizing CT images. Moreover, this regularizer
can easily be applied to different CT reconstruction problems by embedding it
in a variational framework, which increases flexibility and interpretability
compared to feed-forward learning-based approaches. In addition, the
accompanying probabilistic perspective enables experts to explore the full
posterior distribution and may quantify uncertainty of the reconstruction
approach. We apply the regularizer to limited-angle and few-view CT
reconstruction problems, where it outperforms traditional reconstruction
algorithms by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zach_M/0/1/0/all/0/1"&gt;Martin Zach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kobler_E/0/1/0/all/0/1"&gt;Erich Kobler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pock_T/0/1/0/all/0/1"&gt;Thomas Pock&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming. (arXiv:2203.01382v2 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.01382</id>
        <link href="http://arxiv.org/abs/2203.01382"/>
        <updated>2022-03-26T00:46:07.122Z</updated>
        <summary type="html"><![CDATA[Weak Supervision (WS) techniques allow users to efficiently create large
training datasets by programmatically labeling data with heuristic sources of
supervision. While the success of WS relies heavily on the provided labeling
heuristics, the process of how these heuristics are created in practice has
remained under-explored. In this work, we formalize the development process of
labeling heuristics as an interactive procedure, built around the existing
workflow where users draw ideas from a selected set of development data for
designing the heuristic sources. With the formalism, we study two core problems
of how to strategically select the development data to guide users in
efficiently creating informative heuristics, and how to exploit the information
within the development process to contextualize and better learn from the
resultant heuristics. Building upon two novel methodologies that effectively
tackle the respective problems considered, we present Nemo, an end-to-end
interactive system that improves the overall productivity of WS learning
pipeline by an average 20% (and up to 47% in one task) compared to the
prevailing WS approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cheng-Yu Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1"&gt;Alexander Ratner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample-efficient Iterative Lower Bound Optimization of Deep Reactive Policies for Planning in Continuous MDPs. (arXiv:2203.12679v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2203.12679</id>
        <link href="http://arxiv.org/abs/2203.12679"/>
        <updated>2022-03-26T00:46:07.121Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have enabled optimization of deep reactive
policies (DRPs) for continuous MDP planning by encoding a parametric policy as
a deep neural network and exploiting automatic differentiation in an end-to-end
model-based gradient descent framework. This approach has proven effective for
optimizing DRPs in nonlinear continuous MDPs, but it requires a large number of
sampled trajectories to learn effectively and can suffer from high variance in
solution quality. In this work, we revisit the overall model-based DRP
objective and instead take a minorization-maximization perspective to
iteratively optimize the DRP w.r.t. a locally tight lower-bounded objective.
This novel formulation of DRP learning as iterative lower bound optimization
(ILBO) is particularly appealing because (i) each step is structurally easier
to optimize than the overall objective, (ii) it guarantees a monotonically
improving objective under certain theoretical conditions, and (iii) it reuses
samples between iterations thus lowering sample complexity. Empirical
evaluation confirms that ILBO is significantly more sample-efficient than the
state-of-the-art DRP planner and consistently produces better solution quality
with lower variance. We additionally demonstrate that ILBO generalizes well to
new problem instances (i.e., different initial states) without requiring
retraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Low_S/0/1/0/all/0/1"&gt;Siow Meng Low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Akshat Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1"&gt;Scott Sanner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Evolutionary Algorithms Safe Optimizers?. (arXiv:2203.12622v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2203.12622</id>
        <link href="http://arxiv.org/abs/2203.12622"/>
        <updated>2022-03-26T00:46:07.120Z</updated>
        <summary type="html"><![CDATA[We consider a type of constrained optimization problem, where the violation
of a constraint leads to an irrevocable loss, such as breakage of a valuable
experimental resource/platform or loss of human life. Such problems are
referred to as safe optimization problems (SafeOPs). While SafeOPs have
received attention in the machine learning community in recent years, there was
little interest in the evolutionary computation (EC) community despite some
early attempts between 2009 and 2011. Moreover, there is a lack of acceptable
guidelines on how to benchmark different algorithms for SafeOPs, an area where
the EC community has significant experience in. Driven by the need for more
efficient algorithms and benchmark guidelines for SafeOPs, the objective of
this paper is to reignite the interest of this problem class in the EC
community. To achieve this we (i) provide a formal definition of SafeOPs and
contrast it to other types of optimization problems that the EC community is
familiar with, (ii) investigate the impact of key SafeOP parameters on the
performance of selected safe optimization algorithms, (iii) benchmark EC
against state-of-the-art safe optimization algorithms from the machine learning
community, and (iv) provide an open-source Python framework to replicate and
extend our work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngmin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allmendinger_R/0/1/0/all/0/1"&gt;Richard Allmendinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Ibanez_M/0/1/0/all/0/1"&gt;Manuel L&amp;#xf3;pez-Ib&amp;#xe1;&amp;#xf1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Efficient Exploration through Human Seeded Rapidly-exploring Random Trees. (arXiv:2203.12774v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12774</id>
        <link href="http://arxiv.org/abs/2203.12774"/>
        <updated>2022-03-26T00:46:07.120Z</updated>
        <summary type="html"><![CDATA[Modern day computer games have extremely large state and action spaces. To
detect bugs in these games' models, human testers play the games repeatedly to
explore the game and find errors in the games. Such game play is exhaustive and
time consuming. Moreover, since robotics simulators depend on similar methods
of model specification and debugging, the problem of finding errors in the
model is of interest for the robotics community to ensure robot behaviors and
interactions are consistent in simulators. Previous methods have used
reinforcement learning and search based methods including Rapidly-exploring
Random Trees (RRT) to explore a game's state-action space to find bugs.
However, such search and exploration based methods are not efficient at
exploring the state-action space without a pre-defined heuristic. In this work
we attempt to combine a human-tester's expertise in solving games, and the
exhaustiveness of RRT to search a game's state space efficiently with high
coverage. This paper introduces human-seeded RRT (HS-RRT) and
behavior-cloning-assisted RRT (CA-RRT) in testing the number of game states
searched and the time taken to explore those game states. We compare our
methods to an existing weighted RRT baseline for game exploration testing
studied. We find HS-RRT and CA-RRT both explore more game states in fewer tree
expansions/iterations when compared to the existing baseline. In each test,
CA-RRT reached more states on average in the same number of iterations as RRT.
In our tested environments, CA-RRT was able to reach the same number of states
as RRT by more than 5000 fewer iterations on average, almost a 50% reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_M/0/1/0/all/0/1"&gt;Max Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schick_L/0/1/0/all/0/1"&gt;Logan Schick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1"&gt;Matthew Gombolay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1"&gt;Nakul Gopalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-FW: A Hybrid Classical-Quantum Frank-Wolfe for Quadratic Binary Optimization. (arXiv:2203.12633v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12633</id>
        <link href="http://arxiv.org/abs/2203.12633"/>
        <updated>2022-03-26T00:46:07.119Z</updated>
        <summary type="html"><![CDATA[We present a hybrid classical-quantum framework based on the Frank-Wolfe
algorithm, Q-FW, for solving quadratic, linearly-constrained, binary
optimization problems on quantum annealers (QA). The computational premise of
quantum computers has cultivated the re-design of various existing vision
problems into quantum-friendly forms. Experimental QA realizations can solve a
particular non-convex problem known as the quadratic unconstrained binary
optimization (QUBO). Yet a naive-QUBO cannot take into account the restrictions
on the parameters. To introduce additional structure in the parameter space,
researchers have crafted ad-hoc solutions incorporating (linear) constraints in
the form of regularizers. However, this comes at the expense of a
hyper-parameter, balancing the impact of regularization. To date, a true
constrained solver of quadratic binary optimization (QBO) problems has lacked.
Q-FW first reformulates constrained-QBO as a copositive program (CP), then
employs Frank-Wolfe iterations to solve CP while satisfying linear (in)equality
constraints. This procedure unrolls the original constrained-QBO into a set of
unconstrained QUBOs all of which are solved, in a sequel, on a QA. We use
D-Wave Advantage QA to conduct synthetic and real experiments on two important
computer vision problems, graph matching and permutation synchronization, which
demonstrate that our approach is effective in alleviating the need for an
explicit regularization coefficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yurtsever_A/0/1/0/all/0/1"&gt;Alp Yurtsever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1"&gt;Tolga Birdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1"&gt;Vladislav Golyanik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bellman Residual Orthogonalization for Offline Reinforcement Learning. (arXiv:2203.12786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12786</id>
        <link href="http://arxiv.org/abs/2203.12786"/>
        <updated>2022-03-26T00:46:07.119Z</updated>
        <summary type="html"><![CDATA[We introduce a new reinforcement learning principle that approximates the
Bellman equations by enforcing their validity only along an user-defined space
of test functions. Focusing on applications to model-free offline RL with
function approximation, we exploit this principle to derive confidence
intervals for off-policy evaluation, as well as to optimize over policies
within a prescribed policy class. We prove an oracle inequality on our policy
optimization procedure in terms of a trade-off between the value and
uncertainty of an arbitrary comparator policy. Different choices of test
function spaces allow us to tackle different problems within a common
framework. We characterize the loss of efficiency in moving from on-policy to
off-policy data using our procedures, and establish connections to
concentrability coefficients studied in past work. We examine in depth the
implementation of our methods with linear function approximation, and provide
theoretical guarantees with polynomial-time implementations even when Bellman
closure does not hold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1"&gt;Andrea Zanette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1"&gt;Martin J. Wainwright&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Possibility Before Utility: Learning And Using Hierarchical Affordances. (arXiv:2203.12686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12686</id>
        <link href="http://arxiv.org/abs/2203.12686"/>
        <updated>2022-03-26T00:46:07.118Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning algorithms struggle on tasks with complex hierarchical
dependency structures. Humans and other intelligent agents do not waste time
assessing the utility of every high-level action in existence, but instead only
consider ones they deem possible in the first place. By focusing only on what
is feasible, or "afforded", at the present moment, an agent can spend more time
both evaluating the utility of and acting on what matters. To this end, we
present Hierarchical Affordance Learning (HAL), a method that learns a model of
hierarchical affordances in order to prune impossible subtasks for more
effective learning. Existing works in hierarchical reinforcement learning
provide agents with structural representations of subtasks but are not
affordance-aware, and by grounding our definition of hierarchical affordances
in the present state, our approach is more flexible than the multitude of
approaches that ground their subtask dependencies in a symbolic history. While
these logic-based methods often require complete knowledge of the subtask
hierarchy, our approach is able to utilize incomplete and varying symbolic
specifications. Furthermore, we demonstrate that relative to
non-affordance-aware methods, HAL agents are better able to efficiently learn
complex tasks, navigate environment stochasticity, and acquire diverse skills
in the absence of extrinsic supervision -- all of which are hallmarks of human
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costales_R/0/1/0/all/0/1"&gt;Robby Costales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1"&gt;Shariq Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1"&gt;Fei Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asynchronous Collaborative Learning Across Data Silos. (arXiv:2203.12637v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12637</id>
        <link href="http://arxiv.org/abs/2203.12637"/>
        <updated>2022-03-26T00:46:07.117Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms can perform well when trained on large datasets.
While large organisations often have considerable data assets, it can be
difficult for these assets to be unified in a manner that makes training
possible. Data is very often 'siloed' in different parts of the organisation,
with little to no access between silos. This fragmentation of data assets is
especially prevalent in heavily regulated industries like financial services or
healthcare. In this paper we propose a framework to enable asynchronous
collaborative training of machine learning models across data silos. This
allows data science teams to collaboratively train a machine learning model,
without sharing data with one another. Our proposed approach enhances
conventional federated learning techniques to make them suitable for this
asynchronous training in this intra-organisation, cross-silo setting. We
validate our proposed approach via extensive experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuor_T/0/1/0/all/0/1"&gt;Tiffany Tuor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lockhart_J/0/1/0/all/0/1"&gt;Joshua Lockhart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1"&gt;Daniele Magazzeni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shared Data and Algorithms for Deep Learning in Fundamental Physics. (arXiv:2107.00656v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00656</id>
        <link href="http://arxiv.org/abs/2107.00656"/>
        <updated>2022-03-26T00:46:07.116Z</updated>
        <summary type="html"><![CDATA[We introduce a Python package that provides simply and unified access to a
collection of datasets from fundamental physics research - including particle
physics, astroparticle physics, and hadron- and nuclear physics - for
supervised machine learning studies. The datasets contain hadronic top quarks,
cosmic-ray induced air showers, phase transitions in hadronic matter, and
generator-level histories. While public datasets from multiple fundamental
physics disciplines already exist, the common interface and provided reference
models simplify future work on cross-disciplinary machine learning and transfer
learning in fundamental physics. We discuss the design and structure and line
out how additional datasets can be submitted for inclusion.

As showcase application, we present a simple yet flexible graph-based neural
network architecture that can easily be applied to a wide range of supervised
learning tasks. We show that our approach reaches performance close to
dedicated methods on all datasets. To simplify adaptation for various problems,
we provide easy-to-follow instructions on how graph-based representations of
data structures, relevant for fundamental physics, can be constructed and
provide code implementations for several of them. Implementations are also
provided for our proposed method and all reference algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benato_L/0/1/0/all/0/1"&gt;Lisa Benato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buhmann_E/0/1/0/all/0/1"&gt;Erik Buhmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdmann_M/0/1/0/all/0/1"&gt;Martin Erdmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fackeldey_P/0/1/0/all/0/1"&gt;Peter Fackeldey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glombitza_J/0/1/0/all/0/1"&gt;Jonas Glombitza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartmann_N/0/1/0/all/0/1"&gt;Nikolai Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasieczka_G/0/1/0/all/0/1"&gt;Gregor Kasieczka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korcari_W/0/1/0/all/0/1"&gt;William Korcari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuhr_T/0/1/0/all/0/1"&gt;Thomas Kuhr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinheimer_J/0/1/0/all/0/1"&gt;Jan Steinheimer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stocker_H/0/1/0/all/0/1"&gt;Horst St&amp;#xf6;cker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plehn_T/0/1/0/all/0/1"&gt;Tilman Plehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kai Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamically-Scaled Deep Canonical Correlation Analysis. (arXiv:2203.12377v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.12377</id>
        <link href="http://arxiv.org/abs/2203.12377"/>
        <updated>2022-03-26T00:46:07.115Z</updated>
        <summary type="html"><![CDATA[Canonical Correlation Analysis (CCA) is a method for feature extraction of
two views by finding maximally correlated linear projections of them. Several
variants of CCA have been introduced in the literature, in particular, variants
based on deep neural networks for learning highly correlated nonlinear
transformations of two views. As these models are parameterized conventionally,
their learnable parameters remain independent of the inputs after the training
process, which may limit their capacity for learning highly correlated
representations. We introduce a novel dynamic scaling method for training an
input-dependent canonical correlation model. In our deep-CCA models, the
parameters of the last layer are scaled by a second neural network that is
conditioned on the model's input, resulting in a parameterization that is
dependent on the input samples. We evaluate our model on multiple datasets and
demonstrate that the learned representations are more correlated in comparison
to the conventionally-parameterized CCA-based models and also obtain preferable
retrieval results. Our code is available at
https://github.com/tomerfr/DynamicallyScaledDeepCCA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Understanding the Influence of Controllable Factors with a Feature Attribution Algorithm: a Medical Case Study. (arXiv:2203.12701v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2203.12701</id>
        <link href="http://arxiv.org/abs/2203.12701"/>
        <updated>2022-03-26T00:46:07.109Z</updated>
        <summary type="html"><![CDATA[Feature attribution XAI algorithms enable their users to gain insight into
the underlying patterns of large datasets through their feature importance
calculation. Existing feature attribution algorithms treat all features in a
dataset homogeneously, which may lead to misinterpretation of consequences of
changing feature values. In this work, we consider partitioning features into
controllable and uncontrollable parts and propose the Controllable fActor
Feature Attribution (CAFA) approach to compute the relative importance of
controllable features. We carried out experiments applying CAFA to two existing
datasets and our own COVID-19 non-pharmaceutical control measures dataset.
Experimental results show that with CAFA, we are able to exclude influences
from uncontrollable features in our explanation while keeping the full dataset
for prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovvuri_V/0/1/0/all/0/1"&gt;Veera Raghava Reddy Kovvuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seisenberger_M/0/1/0/all/0/1"&gt;Monika Seisenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1"&gt;Berndt M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiuyi Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exploration of Learnt Representations of W Jets. (arXiv:2109.10919v2 [hep-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.10919</id>
        <link href="http://arxiv.org/abs/2109.10919"/>
        <updated>2022-03-26T00:46:07.108Z</updated>
        <summary type="html"><![CDATA[I present a Variational Autoencoder (VAE) trained on collider physics data
(specifically boosted $W$ jets), with reconstruction error given by an
approximation to the Earth Movers Distance (EMD) between input and output jets.
This VAE learns a concrete representation of the data manifold, with
semantically meaningful and interpretable latent space directions which are
hierarchically organized in terms of their relation to physical EMD scales in
the underlying physical generative process. A hyperparameter $\beta$ controls
the resolution at which the VAE is sensitive to structures in the data
manifold. The variation of the latent space structure with $\beta$, and the
scaling of some VAE properties, provide insight into scale dependent structure
of the dataset and its information complexity. I introduce two measures of the
dimensionality of the learnt representation that are calculated from this
scaling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Collins_J/0/1/0/all/0/1"&gt;Jack H. Collins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Applicability of ML Fairness Notions. (arXiv:2006.16745v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16745</id>
        <link href="http://arxiv.org/abs/2006.16745"/>
        <updated>2022-03-26T00:46:07.091Z</updated>
        <summary type="html"><![CDATA[Fairness emerged as an important requirement to guarantee that Machine
Learning (ML) predictive systems do not discriminate against specific
individuals or entire sub-populations, in particular, minorities. Given the
inherent subjectivity of viewing the concept of fairness, several notions of
fairness have been introduced in the literature. This paper is a survey that
illustrates the subtleties between fairness notions through a large number of
examples and scenarios. In addition, unlike other surveys in the literature, it
addresses the question of: which notion of fairness is most suited to a given
real-world scenario and why? Our attempt to answer this question consists in
(1) identifying the set of fairness-related characteristics of the real-world
scenario at hand, (2) analyzing the behavior of each fairness notion, and then
(3) fitting these two elements to recommend the most suitable fairness notion
in every specific setup. The results are summarized in a decision diagram that
can be used by practitioners and policymakers to navigate the relatively large
catalog of ML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makhlouf_K/0/1/0/all/0/1"&gt;Karima Makhlouf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhioua_S/0/1/0/all/0/1"&gt;Sami Zhioua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Prediction of Pulmonary Hypertension in Newborns using Echocardiograms. (arXiv:2203.13038v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.13038</id>
        <link href="http://arxiv.org/abs/2203.13038"/>
        <updated>2022-03-26T00:46:07.089Z</updated>
        <summary type="html"><![CDATA[Pulmonary hypertension (PH) in newborns and infants is a complex condition
associated with several pulmonary, cardiac, and systemic diseases contributing
to morbidity and mortality. Therefore, accurate and early detection of PH is
crucial for successful management. Using echocardiography, the primary
diagnostic tool in pediatrics, human assessment is both time-consuming and
expertise-demanding, raising the need for an automated approach. In this work,
we present an interpretable multi-view video-based deep learning approach to
predict PH for a cohort of 194 newborns using echocardiograms. We use
spatio-temporal convolutional architectures for the prediction of PH from each
view, and aggregate the predictions of the different views using majority
voting. To the best of our knowledge, this is the first work for an automated
assessment of PH in newborns using echocardiograms. Our results show a mean
F1-score of 0.84 for severity prediction and 0.92 for binary detection using
10-fold cross-validation. We complement our predictions with saliency maps and
show that the learned model focuses on clinically relevant cardiac structures,
motivating its usage in clinical practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ragnarsdottir_H/0/1/0/all/0/1"&gt;Hanna Ragnarsdottir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manduchi_L/0/1/0/all/0/1"&gt;Laura Manduchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Michel_H/0/1/0/all/0/1"&gt;Holger Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Laumer_F/0/1/0/all/0/1"&gt;Fabian Laumer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wellmann_S/0/1/0/all/0/1"&gt;Sven Wellmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozkan_E/0/1/0/all/0/1"&gt;Ece Ozkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Parameter Inference as a Principle for Learning. (arXiv:2203.13203v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2203.13203</id>
        <link href="http://arxiv.org/abs/2203.13203"/>
        <updated>2022-03-26T00:46:07.085Z</updated>
        <summary type="html"><![CDATA[Learning in biological and artificial neural networks is often framed as a
problem in which targeted error signals guide parameter updating for more
optimal network behaviour. Backpropagation of error (BP) is an example of such
an approach and has proven to be a highly successful application of stochastic
gradient descent to deep neural networks. However, BP relies on the global
transmission of gradient information and has therefore been criticised for its
biological implausibility. We propose constrained parameter inference (COPI) as
a new principle for learning. COPI allows for the estimation of network
parameters under the constraints of decorrelated neural inputs and top-down
perturbations of neural states. We show that COPI not only is more biologically
plausible but also provides distinct advantages for fast learning, compared
with standard backpropagation of error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1"&gt;Nasir Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schrader_E/0/1/0/all/0/1"&gt;Ellen Schrader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerven_M/0/1/0/all/0/1"&gt;Marcel van Gerven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DyRep: Bootstrapping Training with Dynamic Re-parameterization. (arXiv:2203.12868v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12868</id>
        <link href="http://arxiv.org/abs/2203.12868"/>
        <updated>2022-03-26T00:46:07.080Z</updated>
        <summary type="html"><![CDATA[Structural re-parameterization (Rep) methods achieve noticeable improvements
on simple VGG-style networks. Despite the prevalence, current Rep methods
simply re-parameterize all operations into an augmented network, including
those that rarely contribute to the model's performance. As such, the price to
pay is an expensive computational overhead to manipulate these unnecessary
behaviors. To eliminate the above caveats, we aim to bootstrap the training
with minimal cost by devising a dynamic re-parameterization (DyRep) method,
which encodes Rep technique into the training process that dynamically evolves
the network structures. Concretely, our proposal adaptively finds the
operations which contribute most to the loss in the network, and applies Rep to
enhance their representational capacity. Besides, to suppress the noisy and
redundant operations introduced by Rep, we devise a de-parameterization
technique for a more compact re-parameterization. With this regard, DyRep is
more efficient than Rep since it smoothly evolves the given network instead of
constructing an over-parameterized network. Experimental results demonstrate
our effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by $2.04\%$
on ImageNet and reduces $22\%$ runtime over the baseline. Code is available at:
https://github.com/hunto/DyRep.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Shan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bohan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuxuan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1"&gt;Chen Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00567</id>
        <link href="http://arxiv.org/abs/2104.00567"/>
        <updated>2022-03-26T00:46:07.015Z</updated>
        <summary type="html"><![CDATA[Text-to-image synthesis (T2I) aims to generate photo-realistic images which
are semantically consistent with the text descriptions. Existing methods are
usually built upon conditional generative adversarial networks (GANs) and
initialize an image from noise with sentence embedding, and then refine the
features with fine-grained word embedding iteratively. A close inspection of
their generated images reveals a major limitation: even though the generated
image holistically matches the description, individual image regions or parts
of somethings are often not recognizable or consistent with words in the
sentence, e.g. "a white crown". To address this problem, we propose a novel
framework Semantic-Spatial Aware GAN for synthesizing images from input text.
Concretely, we introduce a simple and effective Semantic-Spatial Aware block,
which (1) learns semantic-adaptive transformation conditioned on text to
effectively fuse text features and image features, and (2) learns a semantic
mask in a weakly-supervised way that depends on the current text-image fusion
process in order to guide the transformation spatially. Experiments on the
challenging COCO and CUB bird datasets demonstrate the advantage of our method
over the recent state-of-the-art approaches, regarding both visual fidelity and
alignment with input text description.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1"&gt;Kai Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wentong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Michael Ying Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks in Particle Physics: Implementations, Innovations, and Challenges. (arXiv:2203.12852v1 [hep-ex])]]></title>
        <id>http://arxiv.org/abs/2203.12852</id>
        <link href="http://arxiv.org/abs/2203.12852"/>
        <updated>2022-03-26T00:46:07.005Z</updated>
        <summary type="html"><![CDATA[Many physical systems can be best understood as sets of discrete data with
associated relationships. Where previously these sets of data have been
formulated as series or image data to match the available machine learning
architectures, with the advent of graph neural networks (GNNs), these systems
can be learned natively as graphs. This allows a wide variety of high- and
low-level physical features to be attached to measurements and, by the same
token, a wide variety of HEP tasks to be accomplished by the same GNN
architectures. GNNs have found powerful use-cases in reconstruction, tagging,
generation and end-to-end analysis. With the wide-spread adoption of GNNs in
industry, the HEP community is well-placed to benefit from rapid improvements
in GNN latency and memory usage. However, industry use-cases are not perfectly
aligned with HEP and much work needs to be done to best match unique GNN
capabilities to unique HEP obstacles. We present here a range of these
capabilities, predictions of which are currently being well-adopted in HEP
communities, and which are still immature. We hope to capture the landscape of
graph techniques in machine learning as well as point out the most significant
gaps that are inhibiting potentially large leaps in research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ex/1/au:+Thais_S/0/1/0/all/0/1"&gt;Savannah Thais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Calafiura_P/0/1/0/all/0/1"&gt;Paolo Calafiura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Chachamis_G/0/1/0/all/0/1"&gt;Grigorios Chachamis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+DeZoort_G/0/1/0/all/0/1"&gt;Gage DeZoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Ganguly_S/0/1/0/all/0/1"&gt;Sanmay Ganguly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Kagan_M/0/1/0/all/0/1"&gt;Michael Kagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Murnane_D/0/1/0/all/0/1"&gt;Daniel Murnane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Neubauer_M/0/1/0/all/0/1"&gt;Mark S. Neubauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ex/1/au:+Terao_K/0/1/0/all/0/1"&gt;Kazuhiro Terao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data. (arXiv:2203.12369v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.12369</id>
        <link href="http://arxiv.org/abs/2203.12369"/>
        <updated>2022-03-26T00:46:06.996Z</updated>
        <summary type="html"><![CDATA[Training of speech enhancement systems often does not incorporate knowledge
of human perception and thus can lead to unnatural sounding results.
Incorporating psychoacoustically motivated speech perception metrics as part of
model training via a predictor network has recently gained interest. However,
the performance of such predictors is limited by the distribution of metric
scores that appear in the training data. In this work, we propose MetricGAN+/-
(an extension of MetricGAN+, one such metric-motivated system) which introduces
an additional network - a "de-generator" which attempts to improve the
robustness of the prediction network (and by extension of the generator) by
ensuring observation of a wider range of metric scores in training.
Experimental results on the VoiceBank-DEMAND dataset show relative improvement
in PESQ score of 3.8% (3.05 vs 3.22 PESQ score), as well as better
generalisation to unseen noise and speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Close_G/0/1/0/all/0/1"&gt;George Close&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1"&gt;Thomas Hain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1"&gt;Stefan Goetze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. (arXiv:2203.08411v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.08411</id>
        <link href="http://arxiv.org/abs/2203.08411"/>
        <updated>2022-03-26T00:46:06.982Z</updated>
        <summary type="html"><![CDATA[Sequence modeling has demonstrated state-of-the-art performance on natural
language and document understanding tasks. However, it is challenging to
correctly serialize tokens in form-like documents in practice due to their
variety of layout patterns. We propose FormNet, a structure-aware sequence
model to mitigate the suboptimal serialization of forms. First, we design Rich
Attention that leverages the spatial relationship between tokens in a form for
more precise attention score calculation. Second, we construct Super-Tokens for
each word by embedding representations from their neighboring tokens through
graph convolutions. FormNet therefore explicitly recovers local syntactic
information that may have been lost during serialization. In experiments,
FormNet outperforms existing methods with a more compact model size and less
pre-training data, establishing new state-of-the-art performance on CORD, FUNSD
and Payment benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chen-Yu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1"&gt;Timothy Dozat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1"&gt;Vincent Perot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1"&gt;Guolong Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1"&gt;Nan Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1"&gt;Joshua Ainslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renshen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1"&gt;Yasuhisa Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct evaluation of progression or regression of disease burden in brain metastatic disease with Deep Neuroevolution. (arXiv:2203.12853v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2203.12853</id>
        <link href="http://arxiv.org/abs/2203.12853"/>
        <updated>2022-03-26T00:46:06.970Z</updated>
        <summary type="html"><![CDATA[Purpose: A core component of advancing cancer treatment research is assessing
response to therapy. Doing so by hand, for example as per RECIST or RANO
criteria, is tedious, time-consuming, and can miss important tumor response
information; most notably, they exclude non-target lesions. We wish to assess
change in a holistic fashion that includes all lesions, obtaining simple,
informative, and automated assessments of tumor progression or regression. Due
to often low patient enrolments in clinical trials, we wish to make response
assessments with small training sets. Deep neuroevolution (DNE) can produce
radiology artificial intelligence (AI) that performs well on small training
sets. Here we use DNE for function approximation that predicts progression
versus regression of metastatic brain disease.

Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training
set. Half of these pairs, separated in time, qualified as disease progression,
while the other 25 images constituted regression. We trained the parameters of
a relatively small CNN via mutations that consisted of random CNN weight
adjustments and mutation fitness. We then incorporated the best mutations into
the next generations CNN, repeating this process for approximately 50,000
generations. We applied the CNNs to our training set, as well as a separate
testing set with the same class balance of 25 progression and 25 regression
images.

Results: DNE achieved monotonic convergence to 100% training set accuracy.
DNE also converged monotonically to 100% testing set accuracy.

Conclusion: DNE can accurately classify brain-metastatic disease progression
versus regression. Future work will extend the input from 2D image slices to
full 3D volumes, and include the category of no change. We believe that an
approach such as our could ultimately provide a useful adjunct to RANO/RECIST
assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stember_J/0/1/0/all/0/1"&gt;Joseph Stember&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1"&gt;Robert Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders. (arXiv:2203.12742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12742</id>
        <link href="http://arxiv.org/abs/2203.12742"/>
        <updated>2022-03-26T00:46:06.946Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization is a gold standard for query-efficient continuous
optimization. However, its adoption for drug and antibody sequence design has
been hindered by the discrete, high-dimensional nature of the decision
variables. We develop a new approach (LaMBO) which jointly trains a denoising
autoencoder with a discriminative multi-task Gaussian process head, enabling
gradient-based optimization of multi-objective acquisition functions in the
latent space of the autoencoder. These acquisition functions allow LaMBO to
balance the explore-exploit trade-off over multiple design rounds, and to
balance objective tradeoffs by optimizing sequences at many different points on
the Pareto frontier. We evaluate LaMBO on a small-molecule task based on the
ZINC dataset and introduce a new large-molecule task targeting fluorescent
proteins. In our experiments, LaMBO outperforms genetic optimizers and does not
require a large pretraining corpus, demonstrating that Bayesian optimization is
practical and effective for biological sequence design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stanton_S/0/1/0/all/0/1"&gt;Samuel Stanton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1"&gt;Wesley Maddox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruver_N/0/1/0/all/0/1"&gt;Nate Gruver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maffettone_P/0/1/0/all/0/1"&gt;Phillip Maffettone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1"&gt;Emily Delaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenside_P/0/1/0/all/0/1"&gt;Peyton Greenside&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphCoCo: Graph Complementary Contrastive Learning. (arXiv:2203.12821v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12821</id>
        <link href="http://arxiv.org/abs/2203.12821"/>
        <updated>2022-03-26T00:46:06.946Z</updated>
        <summary type="html"><![CDATA[Graph Contrastive Learning (GCL) has shown promising performance in graph
representation learning (GRL) without the supervision of manual annotations.
GCL can generate graph-level embeddings by maximizing the Mutual Information
(MI) between different augmented views of the same graph (positive pairs).
However, we identify an obstacle that the optimization of InfoNCE loss only
concentrates on a few embeddings dimensions, limiting the distinguishability of
embeddings in downstream graph classification tasks. This paper proposes an
effective graph complementary contrastive learning approach named GraphCoCo to
tackle the above issue. Specifically, we set the embedding of the first
augmented view as the anchor embedding to localize "highlighted" dimensions
(i.e., the dimensions contribute most in similarity measurement). Then remove
these dimensions in the embeddings of the second augmented view to discover
neglected complementary representations. Therefore, the combination of anchor
and complementary embeddings significantly improves the performance in
downstream tasks. Comprehensive experiments on various benchmark datasets are
conducted to demonstrate the effectiveness of GraphCoCo, and the results show
that our model outperforms the state-of-the-art methods. Source code will be
made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiawei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chentao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yue Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Ruoxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xinyu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Removal in Sampling-based Bayesian Inference. (arXiv:2203.12964v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12964</id>
        <link href="http://arxiv.org/abs/2203.12964"/>
        <updated>2022-03-26T00:46:06.945Z</updated>
        <summary type="html"><![CDATA[The right to be forgotten has been legislated in many countries, but its
enforcement in the AI industry would cause unbearable costs. When single data
deletion requests come, companies may need to delete the whole models learned
with massive resources. Existing works propose methods to remove knowledge
learned from data for explicitly parameterized models, which however are not
appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte
Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we
propose the first machine unlearning algorithm for MCMC. We first convert the
MCMC unlearning problem into an explicit optimization problem. Based on this
problem conversion, an {\it MCMC influence function} is designed to provably
characterize the learned knowledge from data, which then delivers the MCMC
unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not
compromise the generalizability of the MCMC models. Experiments on Gaussian
mixture models and Bayesian neural networks confirm the effectiveness of the
proposed algorithm. The code is available at
\url{https://github.com/fshp971/mcmc-unlearning}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1"&gt;Shaopeng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Dense Correspondence from Synthetic Environments. (arXiv:2203.12919v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12919</id>
        <link href="http://arxiv.org/abs/2203.12919"/>
        <updated>2022-03-26T00:46:06.944Z</updated>
        <summary type="html"><![CDATA[Estimation of human shape and pose from a single image is a challenging task.
It is an even more difficult problem to map the identified human shape onto a
3D human model. Existing methods map manually labelled human pixels in real 2D
images onto the 3D surface, which is prone to human error, and the sparsity of
available annotated data often leads to sub-optimal results. We propose to
solve the problem of data scarcity by training 2D-3D human mapping algorithms
using automatically generated synthetic data for which exact and dense 2D-3D
correspondence is known. Such a learning strategy using synthetic environments
has a high generalisation potential towards real-world data. Using different
camera parameter variations, background and lighting settings, we created
precise ground truth data that constitutes a wider distribution. We evaluate
the performance of models trained on synthetic using the COCO dataset and
validation framework. Results show that training 2D-3D mapping network models
on synthetic data is a viable alternative to using real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lal_M/0/1/0/all/0/1"&gt;Mithun Lal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paproki_A/0/1/0/all/0/1"&gt;Anthony Paproki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habili_N/0/1/0/all/0/1"&gt;Nariman Habili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1"&gt;Lars Petersson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salvado_O/0/1/0/all/0/1"&gt;Olivier Salvado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1"&gt;Clinton Fookes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Bidirectional Transformers for SoC Flow Specification Mining. (arXiv:2203.13182v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.13182</id>
        <link href="http://arxiv.org/abs/2203.13182"/>
        <updated>2022-03-26T00:46:06.935Z</updated>
        <summary type="html"><![CDATA[High-quality system-level message flow specifications can lead to
comprehensive validation of system-on-chip (SoC) designs. We propose a
disruptive method that utilizes an attention mechanism to produce accurate flow
specifications from SoC IP communication traces. The proposed method can
overcome the inherent complexity of SoC traces induced by the concurrency and
parallelism of multicore designs that existing flow specification mining tools
often find extremely challenging. Experiments on highly interleaved traces show
promising flow reconstruction compared to several tools dedicated to the flow
specification mining problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1"&gt;Md Rubel Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hao Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Local Convergence Theory for the Stochastic Gradient Descent Method in Non-Convex Optimization With Non-isolated Local Minima. (arXiv:2203.10973v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.10973</id>
        <link href="http://arxiv.org/abs/2203.10973"/>
        <updated>2022-03-26T00:46:06.928Z</updated>
        <summary type="html"><![CDATA[Non-convex loss functions arise frequently in modern machine learning, and
for the theoretical analysis of stochastic optimization methods, the presence
of non-isolated minima presents a unique challenge that has remained
under-explored. In this paper, we study the local convergence of the stochastic
gradient descent method to non-isolated global minima. Under mild assumptions,
we estimate the probability for the iterations to stay near the minima by
adopting the notion of stochastic stability. After establishing such stability,
we present the lower bound complexity in terms of various error criteria for a
given error tolerance $\epsilon$ and a failure probability $\gamma$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1"&gt;Taehee Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiantao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Classifier Conservativeness and Robustness by Polynomiality. (arXiv:2203.12693v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12693</id>
        <link href="http://arxiv.org/abs/2203.12693"/>
        <updated>2022-03-26T00:46:06.919Z</updated>
        <summary type="html"><![CDATA[We illustrate the detrimental effect, such as overconfident decisions, that
exponential behavior can have in methods like classical LDA and logistic
regression. We then show how polynomiality can remedy the situation. This,
among others, leads purposefully to random-level performance in the tails, away
from the bulk of the training data. A directly related, simple, yet important
technical novelty we subsequently present is softRmax: a reasoned alternative
to the standard softmax function employed in contemporary (deep) neural
networks. It is derived through linking the standard softmax to Gaussian
class-conditional models, as employed in LDA, and replacing those by a
polynomial alternative. We show that two aspects of softRmax, conservativeness
and inherent gradient regularization, lead to robustness against adversarial
attacks without gradient obfuscation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1"&gt;Marco Loog&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion. (arXiv:2203.12621v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.12621</id>
        <link href="http://arxiv.org/abs/2203.12621"/>
        <updated>2022-03-26T00:46:06.916Z</updated>
        <summary type="html"><![CDATA[Patient scans from MRI often suffer from noise, which hampers the diagnostic
capability of such images. As a method to mitigate such artifact, denoising is
largely studied both within the medical imaging community and beyond the
community as a general subject. However, recent deep neural network-based
approaches mostly rely on the minimum mean squared error (MMSE) estimates,
which tend to produce a blurred output. Moreover, such models suffer when
deployed in real-world sitautions: out-of-distribution data, and complex noise
distributions that deviate from the usual parametric noise models. In this
work, we propose a new denoising method based on score-based reverse diffusion
sampling, which overcomes all the aforementioned drawbacks. Our network,
trained only with coronal knee scans, excels even on out-of-distribution in
vivo liver MRI data, contaminated with complex mixture of noise. Even more, we
propose a method to enhance the resolution of the denoised image with the same
network. With extensive experiments, we show that our method establishes
state-of-the-art performance, while having desirable properties which prior
MMSE denoisers did not have: flexibly choosing the extent of denoising, and
quantifying uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1"&gt;Hyungjin Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eun Sun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-Based Manipulators Need to Also See from Their Hands. (arXiv:2203.12677v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2203.12677</id>
        <link href="http://arxiv.org/abs/2203.12677"/>
        <updated>2022-03-26T00:46:06.900Z</updated>
        <summary type="html"><![CDATA[We study how the choice of visual perspective affects learning and
generalization in the context of physical manipulation from raw sensor
observations. Compared with the more commonly used global third-person
perspective, a hand-centric (eye-in-hand) perspective affords reduced
observability, but we find that it consistently improves training efficiency
and out-of-distribution generalization. These benefits hold across a variety of
learning algorithms, experimental settings, and distribution shifts, and for
both simulated and real robot apparatuses. However, this is only the case when
hand-centric observability is sufficient; otherwise, including a third-person
perspective is necessary for learning, but also harms out-of-distribution
generalization. To mitigate this, we propose to regularize the third-person
information stream via a variational information bottleneck. On six
representative manipulation tasks with varying hand-centric observability
adapted from the Meta-World benchmark, this results in a state-of-the-art
reinforcement learning agent operating from both perspectives improving its
out-of-distribution generalization on every task. While some practitioners have
long put cameras in the hands of robots, our work systematically analyzes the
benefits of doing so and provides simple and broadly applicable insights for
improving end-to-end learned vision-based robotic manipulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_K/0/1/0/all/0/1"&gt;Kyle Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Moo Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1"&gt;Rafael Rafailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Non-Invasive Thermal Imaging for detection of Viability of Onchocerciasis worms. (arXiv:2203.12620v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.12620</id>
        <link href="http://arxiv.org/abs/2203.12620"/>
        <updated>2022-03-26T00:46:06.894Z</updated>
        <summary type="html"><![CDATA[Onchocerciasis is causing blindness in over half a million people in the
world today. Drug development for the disease is crippled as there is no way of
measuring effectiveness of the drug without an invasive procedure. Drug
efficacy measurement through assessment of viability of onchocerca worms
requires the patients to undergo nodulectomy which is invasive, expensive,
time-consuming, skill-dependent, infrastructure dependent and lengthy process.
In this paper, we discuss the first-ever study that proposes use of machine
learning over thermal imaging to non-invasively and accurately predict the
viability of worms. The key contributions of the paper are (i) a unique thermal
imaging protocol along with pre-processing steps such as alignment,
registration and segmentation to extract interpretable features (ii) extraction
of relevant semantic features (iii) development of accurate classifiers for
detecting the existence of viable worms in a nodule. When tested on a
prospective test data of 30 participants with 48 palpable nodules, we achieved
an Area Under the Curve (AUC) of 0.85.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dedhiya_R/0/1/0/all/0/1"&gt;Ronak Dedhiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kakileti_S/0/1/0/all/0/1"&gt;Siva Teja Kakileti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deepu_G/0/1/0/all/0/1"&gt;Goutham Deepu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gopinath_K/0/1/0/all/0/1"&gt;Kanchana Gopinath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Opoku_N/0/1/0/all/0/1"&gt;Nicholas Opoku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+King_C/0/1/0/all/0/1"&gt;Christopher King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manjunath_G/0/1/0/all/0/1"&gt;Geetha Manjunath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (arXiv:2203.12915v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12915</id>
        <link href="http://arxiv.org/abs/2203.12915"/>
        <updated>2022-03-26T00:46:06.892Z</updated>
        <summary type="html"><![CDATA[Deep learning has recently been widely applied to many applications across
different domains, e.g., image classification and audio recognition. However,
the quality of Deep Neural Networks (DNNs) still raises concerns in the
practical operational environment, which calls for systematic testing,
especially in safety-critical scenarios. Inspired by software testing, a number
of structural coverage criteria are designed and proposed to measure the test
adequacy of DNNs. However, due to the blackbox nature of DNN, the existing
structural coverage criteria are difficult to interpret, making it hard to
understand the underlying principles of these criteria. The relationship
between the structural coverage and the decision logic of DNNs is unknown.
Moreover, recent studies have further revealed the non-existence of correlation
between the structural coverage and DNN defect detection, which further posts
concerns on what a suitable DNN testing criterion should be.

In this paper, we propose the interpretable coverage criteria through
constructing the decision structure of a DNN. Mirroring the control flow graph
of the traditional program, we first extract a decision graph from a DNN based
on its interpretation, where a path of the decision graph represents a decision
logic of the DNN. Based on the control flow and data flow of the decision
graph, we propose two variants of path coverage to measure the adequacy of the
test cases in exercising the decision logic. The higher the path coverage, the
more diverse decision logic the DNN is expected to be explored. Our large-scale
evaluation results demonstrate that: the path in the decision graph is
effective in characterizing the decision of the DNN, and the proposed coverage
criteria are also sensitive with errors including natural errors and
adversarial examples, and strongly correlated with the output impartiality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEMA: An open-source Python library for self-organizing-maps. (arXiv:2203.13190v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2203.13190</id>
        <link href="http://arxiv.org/abs/2203.13190"/>
        <updated>2022-03-26T00:46:06.885Z</updated>
        <summary type="html"><![CDATA[Organizations have realized the importance of data analysis and its benefits.
This in combination with Machine Learning algorithms has allowed to solve
problems more easily, making these processes less time-consuming. Neural
networks are the Machine Learning technique that is recently obtaining very
good best results. This paper describes an open-source Python library called
GEMA developed to work with a type of neural network model called
Self-Organizing-Maps. GEMA is freely available under GNU General Public License
at GitHub (https://github.com/ufvceiec/GEMA). The library has been evaluated in
different a particular use case obtaining accurate results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Tejedor_A/0/1/0/all/0/1"&gt;Alvaro J. Garcia-Tejedor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nogales_A/0/1/0/all/0/1"&gt;Alberto Nogales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel-Based Reinforcement Learning: A Finite-Time Analysis. (arXiv:2004.05599v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05599</id>
        <link href="http://arxiv.org/abs/2004.05599"/>
        <updated>2022-03-26T00:46:06.879Z</updated>
        <summary type="html"><![CDATA[We consider the exploration-exploitation dilemma in finite-horizon
reinforcement learning problems whose state-action space is endowed with a
metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that
leverages the smoothness of the MDP and a non-parametric kernel estimator of
the rewards and transitions to efficiently balance exploration and
exploitation. For problems with $K$ episodes and horizon $H$, we provide a
regret bound of $\widetilde{O}\left( H^3 K^{\frac{2d}{2d+1}}\right)$, where $d$
is the covering dimension of the joint state-action space. This is the first
regret bound for kernel-based RL using smoothing kernels, which requires very
weak assumptions on the MDP and has been previously applied to a wide range of
tasks. We empirically validate our approach in continuous MDPs with sparse
rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingues_O/0/1/0/all/0/1"&gt;Omar Darwiche Domingues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menard_P/0/1/0/all/0/1"&gt;Pierre M&amp;#xe9;nard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Emilie Kaufmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Rates of (Locally) Differentially Private Heavy-tailed Multi-Armed Bandits. (arXiv:2106.02575v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02575</id>
        <link href="http://arxiv.org/abs/2106.02575"/>
        <updated>2022-03-26T00:46:06.871Z</updated>
        <summary type="html"><![CDATA[In this paper we investigate the problem of stochastic multi-armed bandits
(MAB) in the (local) differential privacy (DP/LDP) model. Unlike previous
results that assume bounded/sub-Gaussian reward distributions, we focus on the
setting where each arm's reward distribution only has $(1+v)$-th moment with
some $v\in (0, 1]$. In the first part, we study the problem in the central
$\epsilon$-DP model. We first provide a near-optimal result by developing a
private and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the
result via a private and robust version of the Successive Elimination (SE)
algorithm. Finally, we establish the lower bound to show that the
instance-dependent regret of our improved algorithm is optimal. In the second
part, we study the problem in the $\epsilon$-LDP model. We propose an algorithm
that can be seen as locally private and robust version of SE algorithm, which
provably achieves (near) optimal rates for both instance-dependent and
instance-independent regret. Our results reveal differences between the problem
of private MAB with bounded/sub-Gaussian rewards and heavy-tailed rewards. To
achieve these (near) optimal rates, we develop several new hard instances and
private robust estimators as byproducts, which might be used to other related
problems. Finally, experiments also support our theoretical findings and show
the effectiveness of our algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Youming Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction. (arXiv:2203.13088v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2203.13088</id>
        <link href="http://arxiv.org/abs/2203.13088"/>
        <updated>2022-03-26T00:46:06.850Z</updated>
        <summary type="html"><![CDATA[Recent progress in neural information retrieval has demonstrated large gains
in effectiveness, while often sacrificing the efficiency and interpretability
of the neural model compared to classical approaches. This paper proposes
ColBERTer, a neural retrieval model using contextualized late interaction
(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,
ColBERTer's reductions dramatically lower ColBERT's storage requirements while
simultaneously improving the interpretability of its token-matching scores. To
this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and
optional lexical matching components into one model. For its multi-vector
component, ColBERTer reduces the number of stored vectors per document by
learning unique whole-word representations for the terms in each document and
learning to identify and remove word representations that are not essential to
effective scoring. We employ an explicit multi-task, multi-stage training to
facilitate using very small vector dimensions. Results on the MS MARCO and
TREC-DL collection show that ColBERTer can reduce the storage footprint by up
to 2.5x, while maintaining effectiveness. With just one dimension per token in
its smallest setting, ColBERTer achieves index storage parity with the
plaintext size, with very strong effectiveness results. Finally, we demonstrate
ColBERTer's robustness on seven high-quality out-of-domain collections,
yielding statistically significant gains over traditional retrieval baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1"&gt;Sebastian Hofst&amp;#xe4;tter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1"&gt;Omar Khattab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1"&gt;Sophia Althammer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sertkan_M/0/1/0/all/0/1"&gt;Mete Sertkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate Shapley Values for explaining tree-based models. (arXiv:2106.03820v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03820</id>
        <link href="http://arxiv.org/abs/2106.03820"/>
        <updated>2022-03-26T00:46:06.839Z</updated>
        <summary type="html"><![CDATA[Although Shapley Values (SV) are widely used in explainable AI, they can be
poorly understood and estimated, implying that their analysis may lead to
spurious inferences and explanations. As a starting point, we remind an
invariance principle for SV and derive the correct approach for computing the
SV of categorical variables that are particularly sensitive to the encoding
used. In the case of tree-based models, we introduce two estimators of Shapley
Values that exploit the tree structure efficiently and are more accurate than
state-of-the-art methods. Simulations and comparisons are performed with
state-of-the-art algorithms and show the practical gain of our approach.
Finally, we discuss the ability of SV to provide reliable local explanations.
We also provide a Python package that computes our estimators at
https://github.com/salimamoukou/acv00.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Amoukou_S/0/1/0/all/0/1"&gt;Salim I. Amoukou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brunel_N/0/1/0/all/0/1"&gt;Nicolas J-B. Brunel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salaun_T/0/1/0/all/0/1"&gt;Tangi Sala&amp;#xfc;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Finite-Horizon Restless Multi-Armed Multi-Action Bandits. (arXiv:2109.09855v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.09855</id>
        <link href="http://arxiv.org/abs/2109.09855"/>
        <updated>2022-03-26T00:46:06.830Z</updated>
        <summary type="html"><![CDATA[We study a finite-horizon restless multi-armed bandit problem with multiple
actions, dubbed R(MA)^2B. The state of each arm evolves according to a
controlled Markov decision process (MDP), and the reward of pulling an arm
depends on both the current state of the corresponding MDP and the action
taken. The goal is to sequentially choose actions for arms so as to maximize
the expected value of the cumulative rewards collected. Since finding the
optimal policy is typically intractable, we propose a computationally appealing
index policy which we call Occupancy-Measured-Reward Index Policy. Our policy
is well-defined even if the underlying MDPs are not indexable. We prove that it
is asymptotically optimal when the activation budget and number of arms are
scaled up, while keeping their ratio as a constant. For the case when the
system parameters are unknown, we develop a learning algorithm. Our learning
algorithm uses the principle of optimism in the face of uncertainty and further
uses a generative model in order to fully exploit the structure of
Occupancy-Measured-Reward Index Policy. We call it the R(MA)^2B-UCB algorithm.
As compared with the existing algorithms, R(MA)^2B-UCB performs close to an
offline optimum policy, and also achieves a sub-linear regret with a low
computational complexity. Experimental results show that R(MA)^2B-UCB
outperforms the existing algorithms in both regret and run time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1"&gt;Guojun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Waveform Learning for Next-Generation Wireless Communication Systems. (arXiv:2109.00998v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.00998</id>
        <link href="http://arxiv.org/abs/2109.00998"/>
        <updated>2022-03-26T00:46:06.823Z</updated>
        <summary type="html"><![CDATA[We propose a learning-based method for the joint design of a transmit and
receive filter, the constellation geometry and associated bit labeling, as well
as a neural network (NN)-based detector. The method maximizes an achievable
information rate, while simultaneously satisfying constraints on the adjacent
channel leakage ratio (ACLR) and peak-to-average power ratio (PAPR). This
allows control of the tradeoff between spectral containment, peak power, and
communication rate. Evaluation on an additive white Gaussian noise (AWGN)
channel shows significant reduction of ACLR and PAPR compared to a conventional
baseline relying on quadrature amplitude modulation (QAM) and
root-raised-cosine (RRC), without significant loss of information rate. When
considering a 3rd Generation Partnership Project (3GPP) multipath channel, the
learned waveform and neural receiver enable competitive or higher rates than an
orthogonal frequency division multiplexing (OFDM) baseline, while reducing the
ACLR by 10 dB and the PAPR by 2 dB. The proposed method incurs no additional
complexity on the transmitter side and might be an attractive tool for waveform
design of beyond-5G systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12668</id>
        <link href="http://arxiv.org/abs/2203.12668"/>
        <updated>2022-03-26T00:46:06.797Z</updated>
        <summary type="html"><![CDATA[State-of-the-art automatic speech recognition (ASR) systems are trained with
tens of thousands of hours of labeled speech data. Human transcription is
expensive and time consuming. Factors such as the quality and consistency of
the transcription can greatly affect the performance of the ASR models trained
with these data. In this paper, we show that we can train a strong teacher
model to produce high quality pseudo labels by utilizing recent self-supervised
and semi-supervised learning techniques. Specifically, we use JUST (Joint
Unsupervised/Supervised Training) and iterative noisy student teacher training
to train a 600 million parameter bi-directional teacher model. This model
achieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively
better than a baseline. We further show that by using this strong teacher model
to generate high-quality pseudo labels for training, we can achieve 13.6%
relative WER reduction (5.9% to 5.1%) for a streaming model compared to using
human labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Dongseong Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1"&gt;Khe Chai Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1"&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1"&gt;Trevor Strohman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Consistent Multi-Class Learning from Label Proportions. (arXiv:2203.12836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12836</id>
        <link href="http://arxiv.org/abs/2203.12836"/>
        <updated>2022-03-26T00:46:06.787Z</updated>
        <summary type="html"><![CDATA[This study addresses a multiclass learning from label proportions (MCLLP)
setting in which training instances are provided in bags and only the
proportion of each class within the bags is provided. Most existing MCLLP
methods impose bag-wise constraints on the prediction of instances or assign
them pseudo-labels; however, none of these methods have a theoretical
consistency. To solve this problem, a risk-consistent method is proposed for
instance classification using the empirical risk minimization framework, and
its estimation error bound is derived. An approximation method is proposed for
the proposed risk estimator, to apply it to large bags, by diverting the
constraints on bags in existing research. The proposed method can be applied to
any deep model or loss and is compatible with stochastic optimization.
Experiments are conducted on benchmarks to verify the effectiveness of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_R/0/1/0/all/0/1"&gt;Ryoma Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1"&gt;Yusuke Mukuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Accuracy Meets Privacy: Two-Stage Federated Transfer Learning Framework in Classification of Medical Images on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.12803</id>
        <link href="http://arxiv.org/abs/2203.12803"/>
        <updated>2022-03-26T00:46:06.781Z</updated>
        <summary type="html"><![CDATA[COVID-19 pandemic has spread rapidly and caused a shortage of global medical
resources. The efficiency of COVID-19 diagnosis has become highly significant.
As deep learning and convolutional neural network (CNN) has been widely
utilized and been verified in analyzing medical images, it has become a
powerful tool for computer-assisted diagnosis. However, there are two most
significant challenges in medical image classification with the help of deep
learning and neural networks, one of them is the difficulty of acquiring enough
samples, which may lead to model overfitting. Privacy concerns mainly bring the
other challenge since medical-related records are often deemed patients'
private information and protected by laws such as GDPR and HIPPA. Federated
learning can ensure the model training is decentralized on different devices
and no data is shared among them, which guarantees privacy. However, with data
located on different devices, the accessible data of each device could be
limited. Since transfer learning has been verified in dealing with limited data
with good performance, therefore, in this paper, We made a trial to implement
federated learning and transfer learning techniques using CNNs to classify
COVID-19 using lung CT scans. We also explored the impact of dataset
distribution at the client-side in federated learning and the number of
training epochs a model is trained. Finally, we obtained very high performance
with federated learning, demonstrating our success in leveraging accuracy and
privacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Alexandros Shikun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1"&gt;Naomi Fengqi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linearizing Transformer with Key-Value Memory Bank. (arXiv:2203.12644v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.12644</id>
        <link href="http://arxiv.org/abs/2203.12644"/>
        <updated>2022-03-26T00:46:06.774Z</updated>
        <summary type="html"><![CDATA[Transformer has brought great success to a wide range of natural language
processing tasks. Nevertheless, the computational overhead of the vanilla
transformer scales quadratically with sequence length. Many efforts have been
made to develop more efficient transformer variants. A line of work (e.g.,
Linformer) projects the input sequence into a low-rank space, achieving linear
time complexity. However, Linformer does not suit well for text generation
tasks as the sequence length must be pre-specified. We propose MemSizer, an
approach also projects the source sequence into lower dimension representation
but can take input with dynamic length, with a different perspective of the
attention mechanism. MemSizer not only achieves the same linear time complexity
but also enjoys efficient recurrent-style autoregressive generation, which
yields constant memory complexity and reduced computation at inference. We
demonstrate that MemSizer provides an improved tradeoff between efficiency and
accuracy over the vanilla transformer and other linear variants in language
modeling and machine translation tasks, revealing a viable direction towards
further inference efficiency improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPar2: Fast and Scalable PARAFAC2 Decomposition for Irregular Dense Tensors. (arXiv:2203.12798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12798</id>
        <link href="http://arxiv.org/abs/2203.12798"/>
        <updated>2022-03-26T00:46:06.768Z</updated>
        <summary type="html"><![CDATA[Given an irregular dense tensor, how can we efficiently analyze it? An
irregular tensor is a collection of matrices whose columns have the same size
and rows have different sizes from each other. PARAFAC2 decomposition is a
fundamental tool to deal with an irregular tensor in applications including
phenotype discovery and trend analysis. Although several PARAFAC2 decomposition
methods exist, their efficiency is limited for irregular dense tensors due to
the expensive computations involved with the tensor. In this paper, we propose
DPar2, a fast and scalable PARAFAC2 decomposition method for irregular dense
tensors. DPar2 achieves high efficiency by effectively compressing each slice
matrix of a given irregular tensor, careful reordering of computations with the
compression results, and exploiting the irregularity of the tensor. Extensive
experiments show that DPar2 is up to 6.0x faster than competitors on real-world
irregular tensors while achieving comparable accuracy. In addition, DPar2 is
scalable with respect to the tensor size and target rank.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1"&gt;Jun-Gi Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_U/0/1/0/all/0/1"&gt;U Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Competency Assessment for Autonomous Agents using Deep Generative Models. (arXiv:2203.12670v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12670</id>
        <link href="http://arxiv.org/abs/2203.12670"/>
        <updated>2022-03-26T00:46:06.748Z</updated>
        <summary type="html"><![CDATA[For autonomous agents to act as trustworthy partners to human users, they
must be able to reliably communicate their competency for the tasks they are
asked to perform. Towards this objective, we develop probabilistic world models
based on deep generative modelling that allow for the simulation of agent
trajectories and accurate calculation of tasking outcome probabilities. By
combining the strengths of conditional variational autoencoders with recurrent
neural networks, the deep generative world model can probabilistically forecast
trajectories over long horizons to task completion. We show how these
forecasted trajectories can be used to calculate outcome probability
distributions, which enable the precise assessment of agent competency for
specific tasks and initial settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Aastha Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_R/0/1/0/all/0/1"&gt;Rebecca Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1"&gt;Nisar R. Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applications of physics informed neural operators. (arXiv:2203.12634v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2203.12634</id>
        <link href="http://arxiv.org/abs/2203.12634"/>
        <updated>2022-03-26T00:46:06.741Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end framework to learn partial differential equations
that brings together initial data production, selection of boundary conditions,
and the use of physics-informed neural operators to solve partial differential
equations that are ubiquitous in the study and modeling of physics phenomena.
We first demonstrate that our methods reproduce the accuracy and performance of
other neural operators published elsewhere in the literature to learn the 1D
wave equation and the 1D Burgers equation. Thereafter, we apply our
physics-informed neural operators to learn new types of equations, including
the 2D Burgers equation in the scalar, inviscid and vector types. Finally, we
show that our approach is also applicable to learn the physics of the 2D linear
and nonlinear shallow water equations, which involve three coupled partial
differential equations. We release our artificial intelligence surrogates and
scientific software to produce initial data and boundary conditions to study a
broad range of physically motivated scenarios. We provide the source code, an
interactive website to visualize the predictions of our physics informed neural
operators, and a tutorial for their use at the Data and Learning Hub for
Science.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Rosofsky_S/0/1/0/all/0/1"&gt;Shawn G. Rosofsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Huerta_E/0/1/0/all/0/1"&gt;E. A. Huerta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Reinforcement Learning-Based Caching Strategy for IoT Networks with Transient Data. (arXiv:2203.12674v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2203.12674</id>
        <link href="http://arxiv.org/abs/2203.12674"/>
        <updated>2022-03-26T00:46:06.726Z</updated>
        <summary type="html"><![CDATA[The Internet of Things (IoT) has been continuously rising in the past few
years, and its potentials are now more apparent. However, transient data
generation and limited energy resources are the major bottlenecks of these
networks. Besides, minimum delay and other conventional quality of service
measurements are still valid requirements to meet. An efficient caching policy
can help meet the standard quality of service requirements while bypassing IoT
networks' specific limitations. Adopting deep reinforcement learning (DRL)
algorithms enables us to develop an effective caching scheme without the need
for any prior knowledge or contextual information. In this work, we propose a
DRL-based caching scheme that improves the cache hit rate and reduces energy
consumption of the IoT networks, in the meanwhile, taking data freshness and
limited lifetime of IoT data into account. To better capture the
regional-different popularity distribution, we propose a hierarchical
architecture to deploy edge caching nodes in IoT networks. The results of
comprehensive experiments show that our proposed method outperforms the
well-known conventional caching policies and an existing DRL-based solution in
terms of cache hit rate and energy consumption of the IoT networks by
considerable margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hongda Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasehzadeh_A/0/1/0/all/0/1"&gt;Ali Nasehzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards All-Purpose Domain Adaptation Under Confounding. (arXiv:2203.12720v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.12720</id>
        <link href="http://arxiv.org/abs/2203.12720"/>
        <updated>2022-03-26T00:46:06.705Z</updated>
        <summary type="html"><![CDATA[Current domain adaptation methods address the problems of covariate shift or
label shift, but are not applicable to the setting where they occur
simultaneously and interact with each other. In this paper, we propose an
assumption, confounded shift, to begin to address this problem. We also propose
a framework for this task, based on minimizing the expected divergence between
the source and target conditional distributions. Within this framework, we
propose using the reverse KL divergence, demonstrating the use of both
parametric linear Gaussian and nonparametric nonlinear Gaussian Process
estimators of the conditional distribution. We also propose using the Maximum
Mean Discrepancy (MMD) within our framework. To make confounded domain
adaptation with the MMD effective, we propose an intelligent dynamic strategy
for choosing the kernel bandwidth, which may be of independent interest even
outside of the confounded shift context. Finally, we show that our approach is
advantageous on a variety of synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+McCarter_C/0/1/0/all/0/1"&gt;Calvin McCarter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Multi-Antenna Frequency-Selective Channels via Meta-Learned Linear Filters based on Long-Short Term Channel Decomposition. (arXiv:2203.12715v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2203.12715</id>
        <link href="http://arxiv.org/abs/2203.12715"/>
        <updated>2022-03-26T00:46:06.686Z</updated>
        <summary type="html"><![CDATA[An efficient data-driven prediction strategy for multi-antenna
frequency-selective channels must operate based on a small number of pilot
symbols. This paper proposes novel channel prediction algorithms that address
this goal by integrating transfer and meta-learning with a reduced-rank
parametrization of the channel. The proposed methods optimize linear predictors
by utilizing data from previous frames, which are generally characterized by
distinct propagation characteristics, in order to enable fast training on the
time slots of the current frame. The proposed predictors rely on a novel
long-short-term decomposition (LSTD) of the linear prediction model that
leverages the disaggregation of the channel into long-term space-time
signatures and fading amplitudes. We first develop predictors for
single-antenna frequency-flat channels based on transfer/meta-learned quadratic
regularization. Then, we introduce transfer and meta-learning algorithms for
LSTD-based prediction models that build on equilibrium propagation (EP) and
alternating least squares (ALS). Numerical results under the 3GPP 5G standard
channel model demonstrate the impact of transfer and meta-learning on reducing
the number of pilots for channel prediction, as well as the merits of the
proposed LSTD parametrization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mokey: Enabling Narrow Fixed-Point Inference for Out-of-the-Box Floating-Point Transformer Models. (arXiv:2203.12758v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12758</id>
        <link href="http://arxiv.org/abs/2203.12758"/>
        <updated>2022-03-26T00:46:06.677Z</updated>
        <summary type="html"><![CDATA[Increasingly larger and better Transformer models keep advancing
state-of-the-art accuracy and capability for Natural Language Processing
applications. These models demand more computational power, storage, and
energy. Mokey reduces the footprint of state-of-the-art 32-bit or 16-bit
floating-point transformer models by quantizing all values to 4-bit indexes
into dictionaries of representative 16-bit fixed-point centroids. Mokey does
not need fine-tuning, an essential feature as often the training resources or
datasets are not available to many. Exploiting the range of values that
naturally occur in transformer models, Mokey selects centroid values to also
fit an exponential curve. This unique feature enables Mokey to replace the bulk
of the original multiply-accumulate operations with narrow 3b fixed-point
additions resulting in an area- and energy-efficient hardware accelerator
design. Over a set of state-of-the-art transformer models, the Mokey
accelerator delivers an order of magnitude improvements in energy efficiency
over a Tensor Cores-based accelerator while improving performance by at least
$4\times$ and as much as $15\times$ depending on the model and on-chip
buffering capacity. Optionally, Mokey can be used as a memory compression
assist for any other accelerator, transparently stashing wide floating-point or
fixed-point activations or weights into narrow 4-bit indexes. Mokey proves
superior to prior state-of-the-art quantization methods for Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zadeh_A/0/1/0/all/0/1"&gt;Ali Hadi Zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmoud_M/0/1/0/all/0/1"&gt;Mostafa Mahmoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelhadi_A/0/1/0/all/0/1"&gt;Ameer Abdelhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moshovos_A/0/1/0/all/0/1"&gt;Andreas Moshovos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12667</id>
        <link href="http://arxiv.org/abs/2203.12667"/>
        <updated>2022-03-26T00:46:06.664Z</updated>
        <summary type="html"><![CDATA[A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jing Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefani_E/0/1/0/all/0/1"&gt;Eliana Stefani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1"&gt;Jesse Thomason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Challenges of Continuous Self-Supervised Learning. (arXiv:2203.12710v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.12710</id>
        <link href="http://arxiv.org/abs/2203.12710"/>
        <updated>2022-03-26T00:46:06.657Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1"&gt;Senthil Purushwalkam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1"&gt;Pedro Morgado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhinav Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Search for Feedback in Reinforcement Learning. (arXiv:2002.09478v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09478</id>
        <link href="http://arxiv.org/abs/2002.09478"/>
        <updated>2022-03-26T00:46:06.644Z</updated>
        <summary type="html"><![CDATA[The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical
system is equivalent to the search for an optimal feedback law utilizing the
simulations/ rollouts of the dynamical system. Most RL techniques search over a
complex global nonlinear feedback parametrization making them suffer from high
training times as well as variance. Instead, we advocate searching over a local
feedback representation consisting of an open-loop sequence, and an associated
optimal linear feedback law completely determined by the open-loop. We show
that this alternate approach results in highly efficient training, the answers
obtained are repeatable and hence reliable, and the resulting closed
performance is superior to global state-of-the-art RL techniques. Finally, if
we replan, whenever required, which is feasible due to the fast and reliable
local solution, it allows us to recover global optimality of the resulting
feedback law.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parunandi_K/0/1/0/all/0/1"&gt;Karthikeya S. Parunandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Aayushman Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1"&gt;Raman Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1"&gt;Suman Chakravorty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning. (arXiv:2203.12748v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12748</id>
        <link href="http://arxiv.org/abs/2203.12748"/>
        <updated>2022-03-26T00:46:06.642Z</updated>
        <summary type="html"><![CDATA[Deep metric learning (DML) enables learning with less supervision through its
emphasis on the similarity structure of representations. There has been much
work on improving generalization of DML in settings like zero-shot retrieval,
but little is known about its implications for fairness. In this paper, we are
the first to evaluate state-of-the-art DML methods trained on imbalanced data,
and to show the negative impact these representations have on minority subgroup
performance when used for downstream tasks. In this work, we first define
fairness in DML through an analysis of three properties of the representation
space -- inter-class alignment, intra-class alignment, and uniformity -- and
propose finDML, the fairness in non-balanced DML benchmark to characterize
representation fairness. Utilizing finDML, we find bias in DML representations
to propagate to common downstream classification tasks. Surprisingly, this bias
is propagated even when training data in the downstream task is re-balanced. To
address this problem, we present Partial Attribute De-correlation (PARADE) to
de-correlate feature representations from sensitive attributes and reduce
performance gaps between subgroups in both embedding space and downstream
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dullerud_N/0/1/0/all/0/1"&gt;Natalie Dullerud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidieh_K/0/1/0/all/0/1"&gt;Kimia Hamidieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling. (arXiv:2203.12940v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.12940</id>
        <link href="http://arxiv.org/abs/2203.12940"/>
        <updated>2022-03-26T00:46:06.624Z</updated>
        <summary type="html"><![CDATA[Zero-shot slot filling has received considerable attention to cope with the
problem of limited available data for the target domain. One of the important
factors in zero-shot learning is to make the model learn generalized and
reliable representations. For this purpose, we present mcBERT, which stands for
momentum contrastive learning with BERT, to develop a robust zero-shot slot
filling model. mcBERT uses BERT to initialize the two encoders, the query
encoder and key encoder, and is trained by applying momentum contrastive
learning. Our experimental results on the SNIPS benchmark show that mcBERT
substantially outperforms the previous models, recording a new
state-of-the-art. Besides, we also show that each component composing mcBERT
contributes to the performance improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1"&gt;Seong-Hwan Heo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1"&gt;WonKee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jong-Hyeok Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation of binary classification trees with binary features by quantum circuits. (arXiv:2108.13207v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.13207</id>
        <link href="http://arxiv.org/abs/2108.13207"/>
        <updated>2022-03-26T00:46:05.894Z</updated>
        <summary type="html"><![CDATA[We propose a quantum representation of binary classification trees with
binary features based on a probabilistic approach. By using the quantum
computer as a processor for probability distributions, a probabilistic
traversal of the decision tree can be realized via measurements of a quantum
circuit. We describe how tree inductions and the prediction of class labels of
query data can be integrated into this framework. An on-demand sampling method
enables predictions with a constant number of classical memory slots,
independent of the tree depth. We experimentally study our approach using both
a quantum computing simulator and actual IBM quantum hardware. To our
knowledge, this is the first realization of a decision tree classifier on a
quantum device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Heese_R/0/1/0/all/0/1"&gt;Raoul Heese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Bickert_P/0/1/0/all/0/1"&gt;Patricia Bickert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Niederle_A/0/1/0/all/0/1"&gt;Astrid Elisa Niederle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-Visual Speech Enhancement using Multimodal Deep Convolutional Neural Network. (arXiv:1709.00944v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1709.00944</id>
        <link href="http://arxiv.org/abs/1709.00944"/>
        <updated>2022-03-26T00:46:05.874Z</updated>
        <summary type="html"><![CDATA[Speech enhancement (SE) aims to reduce noise in speech signals. Most SE
techniques focus on addressing audio information only. In this work, inspired
by multimodal learning, which utilizes data from different modalities, and the
recent success of convolutional neural networks (CNNs) in SE, we propose an
audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual
streams into a unified network model. In the proposed AVDCNN SE model, audio
and visual data are first processed using individual CNNs, and then, fused into
a joint network to generate enhanced speech at the output layer. The AVDCNN
model is trained in an end-to-end manner, and parameters are jointly learned
through back-propagation. We evaluate enhanced speech using five objective
criteria. Results show that the AVDCNN yields notably better performance,
compared with an audio-only CNN-based SE model and two conventional SE
approaches, confirming the effectiveness of integrating visual information into
the SE process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jen-Cheng Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Syu-Siang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Ying-Hui Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hsiu-Wen Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Variational Representations of Divergences and Accelerating their Statistical Estimation. (arXiv:2006.08781v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08781</id>
        <link href="http://arxiv.org/abs/2006.08781"/>
        <updated>2022-03-26T00:46:05.867Z</updated>
        <summary type="html"><![CDATA[Variational representations of divergences and distances between
high-dimensional probability distributions offer significant theoretical
insights and practical advantages in numerous research areas. Recently, they
have gained popularity in machine learning as a tractable and scalable approach
for training probabilistic models and for statistically differentiating between
data distributions. Their advantages include: 1) They can be estimated from
data as statistical averages. 2) Such representations can leverage the ability
of neural networks to efficiently approximate optimal solutions in function
spaces. However, a systematic and practical approach to improving the tightness
of such variational formulas, and accordingly accelerate statistical learning
and estimation from data, is currently lacking. Here we develop such a
methodology for building new, tighter variational representations of
divergences. Our approach relies on improved objective functionals constructed
via an auxiliary optimization problem. Furthermore, the calculation of the
functional Hessian of objective functionals unveils the local curvature
differences around the common optimal variational solution; this quantifies and
orders the tightness gains between different variational representations.
Finally, numerical simulations utilizing neural network optimization
demonstrate that tighter representations can result in significantly faster
learning and more accurate estimation of divergences in both synthetic and real
datasets (of more than 1000 dimensions), often accelerated by nearly an order
of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Birrell_J/0/1/0/all/0/1"&gt;Jeremiah Birrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsoulakis_M/0/1/0/all/0/1"&gt;Markos A. Katsoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantazis_Y/0/1/0/all/0/1"&gt;Yannis Pantazis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Dutch Draw: Constructing a Universal Baseline for Binary Prediction Models. (arXiv:2203.13084v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.13084</id>
        <link href="http://arxiv.org/abs/2203.13084"/>
        <updated>2022-03-26T00:46:05.860Z</updated>
        <summary type="html"><![CDATA[Novel prediction methods should always be compared to a baseline to know how
well they perform. Without this frame of reference, the performance score of a
model is basically meaningless. What does it mean when a model achieves an
$F_1$ of 0.8 on a test set? A proper baseline is needed to evaluate the
`goodness' of a performance score. Comparing with the latest state-of-the-art
model is usually insightful. However, being state-of-the-art can change rapidly
when newer models are developed. Contrary to an advanced model, a simple dummy
classifier could be used. However, the latter could be beaten too easily,
making the comparison less valuable. This paper presents a universal baseline
method for all binary classification models, named the Dutch Draw (DD). This
approach weighs simple classifiers and determines the best classifier to use as
a baseline. We theoretically derive the DD baseline for many commonly used
evaluation measures and show that in most situations it reduces to (almost)
always predicting either zero or one. Summarizing, the DD baseline is: (1)
general, as it is applicable to all binary classification problems; (2) simple,
as it is quickly determined without training or parameter-tuning; (3)
informative, as insightful conclusions can be drawn from the results. The DD
baseline serves two purposes. First, to enable comparisons across research
papers by this robust and universal baseline. Secondly, to provide a sanity
check during the development process of a prediction model. It is a major
warning sign when a model is outperformed by the DD baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bijl_E/0/1/0/all/0/1"&gt;Etienne van de Bijl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1"&gt;Jan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pries_J/0/1/0/all/0/1"&gt;Joris Pries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhulai_S/0/1/0/all/0/1"&gt;Sandjai Bhulai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1"&gt;Mark Hoogendoorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_R/0/1/0/all/0/1"&gt;Rob van der Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel-Based Reinforcement Learning: A Finite-Time Analysis. (arXiv:2004.05599v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05599</id>
        <link href="http://arxiv.org/abs/2004.05599"/>
        <updated>2022-03-26T00:46:05.845Z</updated>
        <summary type="html"><![CDATA[We consider the exploration-exploitation dilemma in finite-horizon
reinforcement learning problems whose state-action space is endowed with a
metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that
leverages the smoothness of the MDP and a non-parametric kernel estimator of
the rewards and transitions to efficiently balance exploration and
exploitation. For problems with $K$ episodes and horizon $H$, we provide a
regret bound of $\widetilde{O}\left( H^3 K^{\frac{2d}{2d+1}}\right)$, where $d$
is the covering dimension of the joint state-action space. This is the first
regret bound for kernel-based RL using smoothing kernels, which requires very
weak assumptions on the MDP and has been previously applied to a wide range of
tasks. We empirically validate our approach in continuous MDPs with sparse
rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingues_O/0/1/0/all/0/1"&gt;Omar Darwiche Domingues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menard_P/0/1/0/all/0/1"&gt;Pierre M&amp;#xe9;nard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Emilie Kaufmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.10750</id>
        <link href="http://arxiv.org/abs/2203.10750"/>
        <updated>2022-03-26T00:46:05.826Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop a new multi-singer Chinese neural singing voice
synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness
of synthesized singing voice, we design several specifical modules and
techniques: 1) A deep bi-directional LSTM based duration model with multi-scale
rhythm loss and post-processing step; 2) A Transformer-alike acoustic model
with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet
neural vocoder to produce high-quality singing waveforms; 4) A novel data
augmentation method with multi-singer pre-training for stronger robustness and
naturalness. Both quantitative and qualitative evaluation results demonstrate
the effectiveness of WeSinger in terms of accuracy and naturalness, and
WeSinger achieves state-of-the-art performance on the public corpus Opencpop.
Some synthesized singing samples are available online
(https://zzw922cn.github.io/WeSinger/).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zewang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yibin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinhui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Li Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilevel Bayesin Deep Neural Networks. (arXiv:2203.12961v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2203.12961</id>
        <link href="http://arxiv.org/abs/2203.12961"/>
        <updated>2022-03-26T00:46:05.820Z</updated>
        <summary type="html"><![CDATA[In this article we consider Bayesian inference associated to deep neural
networks (DNNs) and in particular, trace-class neural network (TNN) priors
which were proposed by Sell et al. [39]. Such priors were developed as more
robust alternatives to classical architectures in the context of inference
problems. For this work we develop multilevel Monte Carlo (MLMC) methods for
such models. MLMC is a popular variance reduction technique, with particular
applications in Bayesian statistics and uncertainty quantification. We show how
a particular advanced MLMC method that was introduced in [4] can be applied to
Bayesian inference from DNNs and establish mathematically, that the
computational cost to achieve a particular mean square error, associated to
posterior expectation computation, can be reduced by several orders, versus
more conventional techniques. To verify such results we provide numerous
numerical experiments on model problems arising in machine learning. These
include Bayesian regression, as well as Bayesian classification and
reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chada_N/0/1/0/all/0/1"&gt;Neil K. Chada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jasra_A/0/1/0/all/0/1"&gt;Ajay Jasra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Law_K/0/1/0/all/0/1"&gt;Kody J. H. Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sumeetpal S. Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Kernel-Based Approach to Non-Stationary Reinforcement Learning in Metric Spaces. (arXiv:2007.05078v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05078</id>
        <link href="http://arxiv.org/abs/2007.05078"/>
        <updated>2022-03-26T00:46:05.813Z</updated>
        <summary type="html"><![CDATA[In this work, we propose KeRNS: an algorithm for episodic reinforcement
learning in non-stationary Markov Decision Processes (MDPs) whose state-action
set is endowed with a metric. Using a non-parametric model of the MDP built
with time-dependent kernels, we prove a regret bound that scales with the
covering dimension of the state-action space and the total variation of the MDP
with time, which quantifies its level of non-stationarity. Our method
generalizes previous approaches based on sliding windows and exponential
discounting used to handle changing environments. We further propose a
practical implementation of KeRNS, we analyze its regret and validate it
experimentally.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingues_O/0/1/0/all/0/1"&gt;Omar Darwiche Domingues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menard_P/0/1/0/all/0/1"&gt;Pierre M&amp;#xe9;nard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Emilie Kaufmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1"&gt;Michal Valko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Missing Sources with Adversarial Support-Matching. (arXiv:2203.13154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.13154</id>
        <link href="http://arxiv.org/abs/2203.13154"/>
        <updated>2022-03-26T00:46:05.807Z</updated>
        <summary type="html"><![CDATA[When trained on diverse labeled data, machine learning models have proven
themselves to be a powerful tool in all facets of society. However, due to
budget limitations, deliberate or non-deliberate censorship, and other problems
during data collection and curation, the labeled training set might exhibit a
systematic shortage of data for certain groups. We investigate a scenario in
which the absence of certain data is linked to the second level of a two-level
hierarchy in the data. Inspired by the idea of protected groups from
algorithmic fairness, we refer to the partitions carved by this second level as
"subgroups"; we refer to combinations of subgroups and classes, or leaves of
the hierarchy, as "sources". To characterize the problem, we introduce the
concept of classes with incomplete subgroup support. The representational bias
in the training set can give rise to spurious correlations between the classes
and the subgroups which render standard classification models ungeneralizable
to unseen sources. To overcome this bias, we make use of an additional, diverse
but unlabeled dataset, called the "deployment set", to learn a representation
that is invariant to subgroup. This is done by adversarially matching the
support of the training and deployment sets in representation space. In order
to learn the desired invariance, it is paramount that the sets of samples
observed by the discriminator are balanced by class; this is easily achieved
for the training set, but requires using semi-supervised clustering for the
deployment set. We demonstrate the effectiveness of our method with experiments
on several datasets and variants of the problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kehrenberg_T/0/1/0/all/0/1"&gt;Thomas Kehrenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bartlett_M/0/1/0/all/0/1"&gt;Myles Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharmanska_V/0/1/0/all/0/1"&gt;Viktoriia Sharmanska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Quadrianto_N/0/1/0/all/0/1"&gt;Novi Quadrianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shared Data and Algorithms for Deep Learning in Fundamental Physics. (arXiv:2107.00656v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00656</id>
        <link href="http://arxiv.org/abs/2107.00656"/>
        <updated>2022-03-26T00:46:05.801Z</updated>
        <summary type="html"><![CDATA[We introduce a Python package that provides simply and unified access to a
collection of datasets from fundamental physics research - including particle
physics, astroparticle physics, and hadron- and nuclear physics - for
supervised machine learning studies. The datasets contain hadronic top quarks,
cosmic-ray induced air showers, phase transitions in hadronic matter, and
generator-level histories. While public datasets from multiple fundamental
physics disciplines already exist, the common interface and provided reference
models simplify future work on cross-disciplinary machine learning and transfer
learning in fundamental physics. We discuss the design and structure and line
out how additional datasets can be submitted for inclusion.

As showcase application, we present a simple yet flexible graph-based neural
network architecture that can easily be applied to a wide range of supervised
learning tasks. We show that our approach reaches performance close to
dedicated methods on all datasets. To simplify adaptation for various problems,
we provide easy-to-follow instructions on how graph-based representations of
data structures, relevant for fundamental physics, can be constructed and
provide code implementations for several of them. Implementations are also
provided for our proposed method and all reference algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benato_L/0/1/0/all/0/1"&gt;Lisa Benato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buhmann_E/0/1/0/all/0/1"&gt;Erik Buhmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdmann_M/0/1/0/all/0/1"&gt;Martin Erdmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fackeldey_P/0/1/0/all/0/1"&gt;Peter Fackeldey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glombitza_J/0/1/0/all/0/1"&gt;Jonas Glombitza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartmann_N/0/1/0/all/0/1"&gt;Nikolai Hartmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasieczka_G/0/1/0/all/0/1"&gt;Gregor Kasieczka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korcari_W/0/1/0/all/0/1"&gt;William Korcari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuhr_T/0/1/0/all/0/1"&gt;Thomas Kuhr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinheimer_J/0/1/0/all/0/1"&gt;Jan Steinheimer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stocker_H/0/1/0/all/0/1"&gt;Horst St&amp;#xf6;cker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plehn_T/0/1/0/all/0/1"&gt;Tilman Plehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kai Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-distribution Generalization with Causal Invariant Transformations. (arXiv:2203.11528v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.11528</id>
        <link href="http://arxiv.org/abs/2203.11528"/>
        <updated>2022-03-26T00:46:05.780Z</updated>
        <summary type="html"><![CDATA[In real-world applications, it is important and desirable to learn a model
that performs well on out-of-distribution (OOD) data. Recently, causality has
become a powerful tool to tackle the OOD generalization problem, with the idea
resting on the causal mechanism that is invariant across domains of interest.
To leverage the generally unknown causal mechanism, existing works assume a
linear form of causal feature or require sufficiently many and diverse training
domains, which are usually restrictive in practice. In this work, we obviate
these assumptions and tackle the OOD problem without explicitly recovering the
causal feature. Our approach is based on transformations that modify the
non-causal feature but leave the causal part unchanged, which can be either
obtained from prior knowledge or learned from the training data in the
multi-domain scenario. Under the setting of invariant causal mechanism, we
theoretically show that if all such transformations are available, then we can
learn a minimax optimal model across the domains using only single domain data.
Noticing that knowing a complete set of these causal invariant transformations
may be impractical, we further show that it suffices to know only a subset of
these transformations. Based on the theoretical findings, a regularized
training procedure is proposed to improve the OOD generalization capability.
Extensive experimental results on both synthetic and real datasets verify the
effectiveness of the proposed algorithm, even with only a few causal invariant
transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yi_M/0/1/0/all/0/1"&gt;Mingyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhitang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shengyu Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Removal in Sampling-based Bayesian Inference. (arXiv:2203.12964v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12964</id>
        <link href="http://arxiv.org/abs/2203.12964"/>
        <updated>2022-03-26T00:46:05.774Z</updated>
        <summary type="html"><![CDATA[The right to be forgotten has been legislated in many countries, but its
enforcement in the AI industry would cause unbearable costs. When single data
deletion requests come, companies may need to delete the whole models learned
with massive resources. Existing works propose methods to remove knowledge
learned from data for explicitly parameterized models, which however are not
appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte
Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we
propose the first machine unlearning algorithm for MCMC. We first convert the
MCMC unlearning problem into an explicit optimization problem. Based on this
problem conversion, an {\it MCMC influence function} is designed to provably
characterize the learned knowledge from data, which then delivers the MCMC
unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not
compromise the generalizability of the MCMC models. Experiments on Gaussian
mixture models and Bayesian neural networks confirm the effectiveness of the
proposed algorithm. The code is available at
\url{https://github.com/fshp971/mcmc-unlearning}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1"&gt;Shaopeng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Search for Feedback in Reinforcement Learning. (arXiv:2002.09478v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09478</id>
        <link href="http://arxiv.org/abs/2002.09478"/>
        <updated>2022-03-26T00:46:05.768Z</updated>
        <summary type="html"><![CDATA[The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical
system is equivalent to the search for an optimal feedback law utilizing the
simulations/ rollouts of the dynamical system. Most RL techniques search over a
complex global nonlinear feedback parametrization making them suffer from high
training times as well as variance. Instead, we advocate searching over a local
feedback representation consisting of an open-loop sequence, and an associated
optimal linear feedback law completely determined by the open-loop. We show
that this alternate approach results in highly efficient training, the answers
obtained are repeatable and hence reliable, and the resulting closed
performance is superior to global state-of-the-art RL techniques. Finally, if
we replan, whenever required, which is feasible due to the fast and reliable
local solution, it allows us to recover global optimality of the resulting
feedback law.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parunandi_K/0/1/0/all/0/1"&gt;Karthikeya S. Parunandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Aayushman Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1"&gt;Raman Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1"&gt;Suman Chakravorty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your Policy Regularizer is Secretly an Adversary. (arXiv:2203.12592v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.12592</id>
        <link href="http://arxiv.org/abs/2203.12592"/>
        <updated>2022-03-26T00:46:05.762Z</updated>
        <summary type="html"><![CDATA[Policy regularization methods such as maximum entropy regularization are
widely used in reinforcement learning to improve the robustness of a learned
policy. In this paper, we show how this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a
limited set by an imagined adversary. Using convex duality, we characterize
this robust set of adversarial reward perturbations under KL and
alpha-divergence regularization, which includes Shannon and Tsallis entropy
regularization as special cases. Importantly, generalization guarantees can be
given within this robust set. We provide detailed discussion of the worst-case
reward perturbations, and present intuitive empirical examples to illustrate
this robustness and its relationship with generalization. Finally, we discuss
how our analysis complements and extends previous results on adversarial reward
robustness and path consistency optimality conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brekelmans_R/0/1/0/all/0/1"&gt;Rob Brekelmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genewein_T/0/1/0/all/0/1"&gt;Tim Genewein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grau_Moya_J/0/1/0/all/0/1"&gt;Jordi Grau-Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deletang_G/0/1/0/all/0/1"&gt;Gr&amp;#xe9;goire Del&amp;#xe9;tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunesch_M/0/1/0/all/0/1"&gt;Markus Kunesch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1"&gt;Shane Legg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_P/0/1/0/all/0/1"&gt;Pedro Ortega&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Local Convergence Theory for the Stochastic Gradient Descent Method in Non-Convex Optimization With Non-isolated Local Minima. (arXiv:2203.10973v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.10973</id>
        <link href="http://arxiv.org/abs/2203.10973"/>
        <updated>2022-03-26T00:46:05.742Z</updated>
        <summary type="html"><![CDATA[Non-convex loss functions arise frequently in modern machine learning, and
for the theoretical analysis of stochastic optimization methods, the presence
of non-isolated minima presents a unique challenge that has remained
under-explored. In this paper, we study the local convergence of the stochastic
gradient descent method to non-isolated global minima. Under mild assumptions,
we estimate the probability for the iterations to stay near the minima by
adopting the notion of stochastic stability. After establishing such stability,
we present the lower bound complexity in terms of various error criteria for a
given error tolerance $\epsilon$ and a failure probability $\gamma$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1"&gt;Taehee Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiantao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Applicability of ML Fairness Notions. (arXiv:2006.16745v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16745</id>
        <link href="http://arxiv.org/abs/2006.16745"/>
        <updated>2022-03-26T00:46:05.736Z</updated>
        <summary type="html"><![CDATA[Fairness emerged as an important requirement to guarantee that Machine
Learning (ML) predictive systems do not discriminate against specific
individuals or entire sub-populations, in particular, minorities. Given the
inherent subjectivity of viewing the concept of fairness, several notions of
fairness have been introduced in the literature. This paper is a survey that
illustrates the subtleties between fairness notions through a large number of
examples and scenarios. In addition, unlike other surveys in the literature, it
addresses the question of: which notion of fairness is most suited to a given
real-world scenario and why? Our attempt to answer this question consists in
(1) identifying the set of fairness-related characteristics of the real-world
scenario at hand, (2) analyzing the behavior of each fairness notion, and then
(3) fitting these two elements to recommend the most suitable fairness notion
in every specific setup. The results are summarized in a decision diagram that
can be used by practitioners and policymakers to navigate the relatively large
catalog of ML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makhlouf_K/0/1/0/all/0/1"&gt;Karima Makhlouf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhioua_S/0/1/0/all/0/1"&gt;Sami Zhioua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Finite-Horizon Restless Multi-Armed Multi-Action Bandits. (arXiv:2109.09855v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.09855</id>
        <link href="http://arxiv.org/abs/2109.09855"/>
        <updated>2022-03-26T00:46:05.730Z</updated>
        <summary type="html"><![CDATA[We study a finite-horizon restless multi-armed bandit problem with multiple
actions, dubbed R(MA)^2B. The state of each arm evolves according to a
controlled Markov decision process (MDP), and the reward of pulling an arm
depends on both the current state of the corresponding MDP and the action
taken. The goal is to sequentially choose actions for arms so as to maximize
the expected value of the cumulative rewards collected. Since finding the
optimal policy is typically intractable, we propose a computationally appealing
index policy which we call Occupancy-Measured-Reward Index Policy. Our policy
is well-defined even if the underlying MDPs are not indexable. We prove that it
is asymptotically optimal when the activation budget and number of arms are
scaled up, while keeping their ratio as a constant. For the case when the
system parameters are unknown, we develop a learning algorithm. Our learning
algorithm uses the principle of optimism in the face of uncertainty and further
uses a generative model in order to fully exploit the structure of
Occupancy-Measured-Reward Index Policy. We call it the R(MA)^2B-UCB algorithm.
As compared with the existing algorithms, R(MA)^2B-UCB performs close to an
offline optimum policy, and also achieves a sub-linear regret with a low
computational complexity. Experimental results show that R(MA)^2B-UCB
outperforms the existing algorithms in both regret and run time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1"&gt;Guojun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[k-Rater Reliability: The Correct Unit of Reliability for Aggregated Human Annotations. (arXiv:2203.12913v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2203.12913</id>
        <link href="http://arxiv.org/abs/2203.12913"/>
        <updated>2022-03-26T00:46:05.723Z</updated>
        <summary type="html"><![CDATA[Since the inception of crowdsourcing, aggregation has been a common strategy
for dealing with unreliable data. Aggregate ratings are more reliable than
individual ones. However, many natural language processing (NLP) applications
that rely on aggregate ratings only report the reliability of individual
ratings, which is the incorrect unit of analysis. In these instances, the data
reliability is under-reported, and a proposed k-rater reliability (kRR) should
be used as the correct data reliability for aggregated datasets. It is a
multi-rater generalization of inter-rater reliability (IRR). We conducted two
replications of the WordSim-353 benchmark, and present empirical, analytical,
and bootstrap-based methods for computing kRR on WordSim-353. These methods
produce very similar results. We hope this discussion will nudge researchers to
report kRR in addition to IRR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;Ka Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paritosh_P/0/1/0/all/0/1"&gt;Praveen Paritosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Kullback-Leibler divergence between pairwise isotropic Gaussian-Markov random fields. (arXiv:2203.13164v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2203.13164</id>
        <link href="http://arxiv.org/abs/2203.13164"/>
        <updated>2022-03-26T00:46:05.716Z</updated>
        <summary type="html"><![CDATA[The Kullback-Leibler divergence or relative entropy is an
information-theoretic measure between statistical models that play an important
role in measuring a distance between random variables. In the study of complex
systems, random fields are mathematical structures that models the interaction
between these variables by means of an inverse temperature parameter,
responsible for controlling the spatial dependence structure along the field.
In this paper, we derive closed-form expressions for the Kullback-Leibler
divergence between two pairwise isotropic Gaussian-Markov random fields in both
univariate and multivariate cases. The proposed equation allows the development
of novel similarity measures in image processing and machine learning
applications, such as image denoising and unsupervised metric learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levada_A/0/1/0/all/0/1"&gt;Alexandre L. M. Levada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Dynamics of Autonomous Linear Systems From Multiple Trajectories. (arXiv:2203.12794v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2203.12794</id>
        <link href="http://arxiv.org/abs/2203.12794"/>
        <updated>2022-03-26T00:46:05.696Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning the dynamics of autonomous linear systems
(i.e., systems that are not affected by external control inputs) from
observations of multiple trajectories of those systems, with finite sample
guarantees. Existing results on learning rate and consistency of autonomous
linear system identification rely on observations of steady state behaviors
from a single long trajectory, and are not applicable to unstable systems. In
contrast, we consider the scenario of learning system dynamics based on
multiple short trajectories, where there are no easily observed steady state
behaviors. We provide a finite sample analysis, which shows that the dynamics
can be learned at a rate $\mathcal{O}(\frac{1}{\sqrt{N}})$ for both stable and
unstable systems, where $N$ is the number of trajectories, when the initial
state of the system has zero mean (which is a common assumption in the existing
literature). We further generalize our result to the case where the initial
state has non-zero mean. We show that one can adjust the length of the
trajectories to achieve a learning rate of
$\mathcal{O}(\sqrt{\frac{\log{N}}{N})}$ for strictly stable systems and a
learning rate of $\mathcal{O}(\frac{(\log{N})^d}{\sqrt{N}})$ for marginally
stable systems, where $d$ is some constant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xin_L/0/1/0/all/0/1"&gt;Lei Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chiu_G/0/1/0/all/0/1"&gt;George Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Shreyas Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.13151</id>
        <link href="http://arxiv.org/abs/2203.13151"/>
        <updated>2022-03-26T00:46:05.689Z</updated>
        <summary type="html"><![CDATA[Transformer-based language models (TLMs) provide state-of-the-art performance
in many modern natural language processing applications. TLM training is
conducted in two phases. First, the model is pre-trained over large volumes of
text to minimize a generic objective function, such as the Masked Language
Model (MLM). Second, the model is fine-tuned in specific downstream tasks.
Pre-training requires large volumes of data and high computational resources,
while introducing many still unresolved design choices. For instance, selecting
hyperparameters for language model pre-training is often carried out based on
heuristics or grid-based searches. In this work, we propose a multi-armed
bandit-based online optimization framework for the sequential selection of
pre-training hyperparameters to optimize language model performance. We pose
the pre-training procedure as a sequential decision-making task, where at each
pre-training step, an agent must determine what hyperparameters to use towards
optimizing the pre-training objective. We propose a Thompson sampling bandit
algorithm, based on a surrogate Gaussian process reward model of the MLM
pre-training objective, for its sequential minimization. We empirically show
how the proposed Gaussian process based Thompson sampling pre-trains robust and
well-performing language models. Namely, by sequentially selecting masking
hyperparameters of the TLM, we achieve satisfactory performance in less epochs,
not only in terms of the pre-training MLM objective, but in diverse downstream
fine-tuning tasks. The proposed bandit-based technique provides an automated
hyperparameter selection method for pre-training TLMs of interest to
practitioners. In addition, our results indicate that, instead of MLM
pre-training with fixed masking probabilities, sequentially adapting the
masking hyperparameters improves both pre-training loss and downstream task
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Urteaga_I/0/1/0/all/0/1"&gt;I&amp;#xf1;igo Urteaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Draidia_M/0/1/0/all/0/1"&gt;Moulay-Za&amp;#xef;dane Dra&amp;#xef;dia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1"&gt;Tomer Lancewicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1"&gt;Shahram Khadivi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extended critical regimes of deep neural networks. (arXiv:2203.12967v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12967</id>
        <link href="http://arxiv.org/abs/2203.12967"/>
        <updated>2022-03-26T00:46:05.681Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have been successfully applied to many real-world
problems, but a complete understanding of their dynamical and computational
principles is still lacking. Conventional theoretical frameworks for analysing
DNNs often assume random networks with coupling weights obeying Gaussian
statistics. However, non-Gaussian, heavy-tailed coupling is a ubiquitous
phenomenon in DNNs. Here, by weaving together theories of heavy-tailed random
matrices and non-equilibrium statistical physics, we develop a new type of mean
field theory for DNNs which predicts that heavy-tailed weights enable the
emergence of an extended critical regime without fine-tuning parameters. In
this extended critical regime, DNNs exhibit rich and complex propagation
dynamics across layers. We further elucidate that the extended criticality
endows DNNs with profound computational advantages: balancing the contraction
as well as expansion of internal neural representations and speeding up
training processes, hence providing a theoretical guide for the design of
efficient neural architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1"&gt;Cheng Kevin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wardak_A/0/1/0/all/0/1"&gt;Asem Wardak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1"&gt;Pulin Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamically-Scaled Deep Canonical Correlation Analysis. (arXiv:2203.12377v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.12377</id>
        <link href="http://arxiv.org/abs/2203.12377"/>
        <updated>2022-03-26T00:46:05.674Z</updated>
        <summary type="html"><![CDATA[Canonical Correlation Analysis (CCA) is a method for feature extraction of
two views by finding maximally correlated linear projections of them. Several
variants of CCA have been introduced in the literature, in particular, variants
based on deep neural networks for learning highly correlated nonlinear
transformations of two views. As these models are parameterized conventionally,
their learnable parameters remain independent of the inputs after the training
process, which may limit their capacity for learning highly correlated
representations. We introduce a novel dynamic scaling method for training an
input-dependent canonical correlation model. In our deep-CCA models, the
parameters of the last layer are scaled by a second neural network that is
conditioned on the model's input, resulting in a parameterization that is
dependent on the input samples. We evaluate our model on multiple datasets and
demonstrate that the learned representations are more correlated in comparison
to the conventionally-parameterized CCA-based models and also obtain preferable
retrieval results. Our code is available at
https://github.com/tomerfr/DynamicallyScaledDeepCCA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate Shapley Values for explaining tree-based models. (arXiv:2106.03820v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03820</id>
        <link href="http://arxiv.org/abs/2106.03820"/>
        <updated>2022-03-26T00:46:05.656Z</updated>
        <summary type="html"><![CDATA[Although Shapley Values (SV) are widely used in explainable AI, they can be
poorly understood and estimated, implying that their analysis may lead to
spurious inferences and explanations. As a starting point, we remind an
invariance principle for SV and derive the correct approach for computing the
SV of categorical variables that are particularly sensitive to the encoding
used. In the case of tree-based models, we introduce two estimators of Shapley
Values that exploit the tree structure efficiently and are more accurate than
state-of-the-art methods. Simulations and comparisons are performed with
state-of-the-art algorithms and show the practical gain of our approach.
Finally, we discuss the ability of SV to provide reliable local explanations.
We also provide a Python package that computes our estimators at
https://github.com/salimamoukou/acv00.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Amoukou_S/0/1/0/all/0/1"&gt;Salim I. Amoukou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brunel_N/0/1/0/all/0/1"&gt;Nicolas J-B. Brunel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salaun_T/0/1/0/all/0/1"&gt;Tangi Sala&amp;#xfc;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two Stage Curvature Identification with Machine Learning: Causal Inference with Possibly Invalid Instrumental Variables. (arXiv:2203.12808v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2203.12808</id>
        <link href="http://arxiv.org/abs/2203.12808"/>
        <updated>2022-03-26T00:46:05.650Z</updated>
        <summary type="html"><![CDATA[Instrumental variables regression is a popular causal inference method for
endogenous treatment. A significant concern in practical applications is the
validity and strength of instrumental variables. This paper aims to perform
causal inference when all instruments are possibly invalid. To do this, we
propose a novel methodology called two stage curvature identification (TSCI)
together with a generalized concept to measure the strengths of possibly
invalid instruments: such invalid instruments can still be used for inference
in our framework. We fit the treatment model with a general machine learning
method and propose a novel bias correction method to remove the overfitting
bias from machine learning methods. Among a collection of spaces of violation
functions, we choose the best one by evaluating invalid instrumental variables'
strength. We demonstrate our proposed TSCI methodology in a large-scale
simulation study and revisit the important economics question on the effect of
education on earnings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zijian Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Buhlmann_P/0/1/0/all/0/1"&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards All-Purpose Domain Adaptation Under Confounding. (arXiv:2203.12720v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.12720</id>
        <link href="http://arxiv.org/abs/2203.12720"/>
        <updated>2022-03-26T00:46:05.630Z</updated>
        <summary type="html"><![CDATA[Current domain adaptation methods address the problems of covariate shift or
label shift, but are not applicable to the setting where they occur
simultaneously and interact with each other. In this paper, we propose an
assumption, confounded shift, to begin to address this problem. We also propose
a framework for this task, based on minimizing the expected divergence between
the source and target conditional distributions. Within this framework, we
propose using the reverse KL divergence, demonstrating the use of both
parametric linear Gaussian and nonparametric nonlinear Gaussian Process
estimators of the conditional distribution. We also propose using the Maximum
Mean Discrepancy (MMD) within our framework. To make confounded domain
adaptation with the MMD effective, we propose an intelligent dynamic strategy
for choosing the kernel bandwidth, which may be of independent interest even
outside of the confounded shift context. Finally, we show that our approach is
advantageous on a variety of synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+McCarter_C/0/1/0/all/0/1"&gt;Calvin McCarter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning. (arXiv:2203.12748v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12748</id>
        <link href="http://arxiv.org/abs/2203.12748"/>
        <updated>2022-03-26T00:46:05.620Z</updated>
        <summary type="html"><![CDATA[Deep metric learning (DML) enables learning with less supervision through its
emphasis on the similarity structure of representations. There has been much
work on improving generalization of DML in settings like zero-shot retrieval,
but little is known about its implications for fairness. In this paper, we are
the first to evaluate state-of-the-art DML methods trained on imbalanced data,
and to show the negative impact these representations have on minority subgroup
performance when used for downstream tasks. In this work, we first define
fairness in DML through an analysis of three properties of the representation
space -- inter-class alignment, intra-class alignment, and uniformity -- and
propose finDML, the fairness in non-balanced DML benchmark to characterize
representation fairness. Utilizing finDML, we find bias in DML representations
to propagate to common downstream classification tasks. Surprisingly, this bias
is propagated even when training data in the downstream task is re-balanced. To
address this problem, we present Partial Attribute De-correlation (PARADE) to
de-correlate feature representations from sensitive attributes and reduce
performance gaps between subgroups in both embedding space and downstream
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dullerud_N/0/1/0/all/0/1"&gt;Natalie Dullerud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidieh_K/0/1/0/all/0/1"&gt;Kimia Hamidieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Regularization of B-Spline Models for Scientific Data. (arXiv:2203.12730v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.12730</id>
        <link href="http://arxiv.org/abs/2203.12730"/>
        <updated>2022-03-26T00:46:05.609Z</updated>
        <summary type="html"><![CDATA[B-spline models are a powerful way to represent scientific data sets with a
functional approximation. However, these models can suffer from spurious
oscillations when the data to be approximated are not uniformly distributed.
Model regularization (i.e., smoothing) has traditionally been used to minimize
these oscillations; unfortunately, it is sometimes impossible to sufficiently
remove unwanted artifacts without smoothing away key features of the data set.
In this article, we present a method of model regularization that preserves
significant features of a data set while minimizing artificial oscillations.
Our method varies the strength of a smoothing parameter throughout the domain
automatically, removing artifacts in poorly-constrained regions while leaving
other regions unchanged. The behavior of our method is validated on a
collection of two- and three-dimensional data sets produced by scientific
simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lenz_D/0/1/0/all/0/1"&gt;David Lenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yeh_R/0/1/0/all/0/1"&gt;Raine Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mahadevan_V/0/1/0/all/0/1"&gt;Vijay Mahadevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grindeanu_I/0/1/0/all/0/1"&gt;Iulian Grindeanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peterka_T/0/1/0/all/0/1"&gt;Tom Peterka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Possibility Before Utility: Learning And Using Hierarchical Affordances. (arXiv:2203.12686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12686</id>
        <link href="http://arxiv.org/abs/2203.12686"/>
        <updated>2022-03-26T00:46:05.592Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning algorithms struggle on tasks with complex hierarchical
dependency structures. Humans and other intelligent agents do not waste time
assessing the utility of every high-level action in existence, but instead only
consider ones they deem possible in the first place. By focusing only on what
is feasible, or "afforded", at the present moment, an agent can spend more time
both evaluating the utility of and acting on what matters. To this end, we
present Hierarchical Affordance Learning (HAL), a method that learns a model of
hierarchical affordances in order to prune impossible subtasks for more
effective learning. Existing works in hierarchical reinforcement learning
provide agents with structural representations of subtasks but are not
affordance-aware, and by grounding our definition of hierarchical affordances
in the present state, our approach is more flexible than the multitude of
approaches that ground their subtask dependencies in a symbolic history. While
these logic-based methods often require complete knowledge of the subtask
hierarchy, our approach is able to utilize incomplete and varying symbolic
specifications. Furthermore, we demonstrate that relative to
non-affordance-aware methods, HAL agents are better able to efficiently learn
complex tasks, navigate environment stochasticity, and acquire diverse skills
in the absence of extrinsic supervision -- all of which are hallmarks of human
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costales_R/0/1/0/all/0/1"&gt;Robby Costales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1"&gt;Shariq Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1"&gt;Fei Sha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders. (arXiv:2203.12742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.12742</id>
        <link href="http://arxiv.org/abs/2203.12742"/>
        <updated>2022-03-26T00:46:05.585Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization is a gold standard for query-efficient continuous
optimization. However, its adoption for drug and antibody sequence design has
been hindered by the discrete, high-dimensional nature of the decision
variables. We develop a new approach (LaMBO) which jointly trains a denoising
autoencoder with a discriminative multi-task Gaussian process head, enabling
gradient-based optimization of multi-objective acquisition functions in the
latent space of the autoencoder. These acquisition functions allow LaMBO to
balance the explore-exploit trade-off over multiple design rounds, and to
balance objective tradeoffs by optimizing sequences at many different points on
the Pareto frontier. We evaluate LaMBO on a small-molecule task based on the
ZINC dataset and introduce a new large-molecule task targeting fluorescent
proteins. In our experiments, LaMBO outperforms genetic optimizers and does not
require a large pretraining corpus, demonstrating that Bayesian optimization is
practical and effective for biological sequence design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stanton_S/0/1/0/all/0/1"&gt;Samuel Stanton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1"&gt;Wesley Maddox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruver_N/0/1/0/all/0/1"&gt;Nate Gruver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maffettone_P/0/1/0/all/0/1"&gt;Phillip Maffettone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1"&gt;Emily Delaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenside_P/0/1/0/all/0/1"&gt;Peyton Greenside&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NERFs To Make 3D As Simple As Shooting a Video]]></title>
        <id>https://www.datasciencecentral.com/?p=56934</id>
        <link href="https://www.datasciencecentral.com/nerfs-to-make-3d-as-simple-as-making-a-video/"/>
        <updated>2022-03-25T23:49:23.000Z</updated>
        <summary type="html"><![CDATA[Gaming, creating CGI movies, building shared worlds, and creating digital twins are exciting in principle, but the complexity of building 3D models usually serves to limit the ambition of even the most dedicated auteur. However, recent innovations by nVidia, announced earlier this year for their RTX 3090 line of GPUs, are very likely to change… Read More »NERFs To Make 3D As Simple As Shooting a Video
The post NERFs To Make 3D As Simple As Shooting a Video appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kurt Cagle</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] keras-genetic: Train Keras Models Using Genetic Algorithms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/to4gf3/p_kerasgenetic_train_keras_models_using_genetic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/to4gf3/p_kerasgenetic_train_keras_models_using_genetic/"/>
        <updated>2022-03-25T22:51:24.000Z</updated>
        <summary type="html"><![CDATA[Hey r/machinelearning! Recently when working on a WorldModels implementation for keras.io I realized that I needed a genetic algorithm implementation to train the "controller" module. Instead of writing a one off solution, I decided to write Keras Genetic, a full package to train keras models using genetic algorithms.
 Please note genetic algorithms are not good for training neural networks outside of some niche use cases; typically training a controller with <1k parameters. The ConvNet MNIST example scores *horribly* when compared to comparable backprop examples.
 Please give the package a try and let me know if you find this interesting or useful:
 https://github.com/lukewood/keras-genetic
    submitted by    /u/puppet_pals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Applying Keras ImageDataGenerator to the features AND labels of a dataset]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/to4a2c/d_applying_keras_imagedatagenerator_to_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/to4a2c/d_applying_keras_imagedatagenerator_to_the/"/>
        <updated>2022-03-25T22:42:23.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 For a personal project, I intend to develop a little CNN - TransposeCNN model to colorize images of portraits. The idea is simply, from a grayscale image, to give the RGB version. To do so, I constructed a dataset of colored portraits and I would like to use the keras' ImageDataGenerator tool to specify that the feature to be the grayscale version and the label the original one.
 I could simply duplicate the current dataset and convert one of them into grayscales, and that may be easier to do, but then I would like to do something else :
 I would like to apply the same data augmentation functions from ImageDataGenerator (rotations, flipping... ) to the features and their corresponding labels.
 Do you know if it is possible or will I have to construct the augmented dataset explicitely ?
 Thank you for your advice
    submitted by    /u/Arioxel_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the motivation of checkpoint averaging for Transformers? [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/to2qde/what_is_the_motivation_of_checkpoint_averaging/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/to2qde/what_is_the_motivation_of_checkpoint_averaging/"/>
        <updated>2022-03-25T22:07:55.000Z</updated>
        <summary type="html"><![CDATA[According to the original "Attention is all you need" paper (Section 6), for the base models, they used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. 
 Is it about improving the performance? But if other works didn't do the checkpoint averaging, it wouldn't be a fair comparision. However, I seldom see the recently transformer works highlighting this technique.
 What's more, I have not heard any ViT (Vision Transformer) works utilizing such a checkpoint averaging trick. My background is computer vision so I was wondering if it makes sense to try this...
 Could someone provide some guidance on this? Thanks.
    submitted by    /u/AaronSpalding  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Improving on time-averaging RNN or Transformer features for sequence classification? (few sample regime)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/to00fi/d_improving_on_timeaveraging_rnn_or_transformer/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/to00fi/d_improving_on_timeaveraging_rnn_or_transformer/"/>
        <updated>2022-03-25T21:28:41.000Z</updated>
        <summary type="html"><![CDATA[Currently, for a sequence classification problem I'm using a pretrained network (Transformer or RNN) as a feature extractor. I average across time to obtain a d-dimensional vector per training example and train classifier on top of these feature vectors.
 I see two improvements to simple averaging of the features:
  
Add an average pooling layer and train the network end-to-end.
 Add a weighted average pooling layer and train the network end-to-end
  
Are there any ways that I can do better here? I am under sample size limitations, samples per class can be as low as 20, total data set size under 300, with 2-3 classes.
 I've tried method 1. and it barely changes the classification accuracy.
    submitted by    /u/PK_thundr  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] ML CPU Benchmarking - When to upgrade CPU?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tnzecn/d_ml_cpu_benchmarking_when_to_upgrade_cpu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tnzecn/d_ml_cpu_benchmarking_when_to_upgrade_cpu/"/>
        <updated>2022-03-25T21:19:20.000Z</updated>
        <summary type="html"><![CDATA[Are there any standard benchmarking tools or charts which compares CPU's to compare how much faster one performs than the other relative to ML training/predicting? For example, a "5600X" will train models 30% faster than a "2600X, etc. which may be seconds saved on a small model our hours on a larger model.
 In the example above, I've actually a 2600X for the last 2 years and am considering to buy a 5600X, wait until AM5 CPU later this year, or switch back to Intel 12600K although Intel is at least $200 more because their MB are overpriced. It would be nice to know whether it would save me incredible amount of time so I can justify getting more experience quicker or if it's marginal and to save the money on a nice 3080 GPU, etc. and get into Deep Learning sooner.
    submitted by    /u/bugsysiegals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a mental health machine learning risk model using Amazon SageMaker Data Wrangler]]></title>
        <id>dea7590ac337b5e2d649942249f932503c4d46bd</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-mental-health-machine-learning-risk-model-using-amazon-sagemaker-data-wrangler/"/>
        <updated>2022-03-25T20:37:56.000Z</updated>
        <summary type="html"><![CDATA[This post is co-written by Shibangi Saha, Data Scientist, and Graciela Kravtzov, Co-Founder and CTO, of Equilibrium Point. Many individuals are experiencing new symptoms of mental illness, such as stress, anxiety, depression, substance use, and post-traumatic stress disorder (PTSD). According to Kaiser Family Foundation, about half of adults (47%) nationwide have reported negative mental health […]]]></summary>
        <author>
            <name>Shibhangi Saha</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Video Paper Review - Typical Decoding for Natural Language Generation (More human-like sampling from language models)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tny5ko/d_video_paper_review_typical_decoding_for_natural/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tny5ko/d_video_paper_review_typical_decoding_for_natural/"/>
        <updated>2022-03-25T20:20:50.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/_EDr3ryrT_Y
 Modern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.
 ​
 OUTLINE:
 0:00 - Intro
 1:50 - Sponsor: Fully Connected by Weights & Biases
 4:10 - Paper Overview
 7:40 - What's the problem with sampling?
 11:45 - Beam Search: The good and the bad
 14:10 - Top-k and Nucleus Sampling
 16:20 - Why the most likely things might not be the best
 21:30 - The expected information content of the next word
 25:00 - How to trade off information and likelihood
 31:25 - Connections to information theory and psycholinguistics
 36:40 - Introducing Typical Sampling
 43:00 - Experimental Evaluation
 44:40 - My thoughts on this paper
 ​
 Paper: https://arxiv.org/abs/2202.00666
 Code: https://github.com/cimeister/typical-sampling
    submitted by    /u/ykilcher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I wrote a GPT-3 based web application to help myself write more effectively - and it worked!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnxqrq/i_wrote_a_gpt3_based_web_application_to_help/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnxqrq/i_wrote_a_gpt3_based_web_application_to_help/"/>
        <updated>2022-03-25T20:01:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/data-gig  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about calculating Entropy of a Decision Tree]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tnxgjf/question_about_calculating_entropy_of_a_decision/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tnxgjf/question_about_calculating_entropy_of_a_decision/"/>
        <updated>2022-03-25T19:48:51.000Z</updated>
        <summary type="html"><![CDATA[Hi! 
 I hope this is the right subreddit...
 I am currently studying Electrical Engineering at University and studying for an exam that is (partly) about Neural NEtworks. I am struggling a bit with an example given about calculating the entropy of a decision tree and hope someone here can help me out. 
 I have the following information:
 Given information 
 The table is the given dataset and the tree is the resulting tree. I am trying to understand how the calculated Entropy values came to place, since I get different answers when I try it myself. 
 This is my way (for example for "level"):
 p1 (Senior) = 5/14
 p2 (Mid) = 4/14
 p3 (Junior) = 5/14
 Entropy = -(5/14*log_2(5/14)+4/14*log_2(4/14)+5/14*log_2(5/14)) = 1.577 (which makes no sense, since Entropy should be between 0 and 1??)
 Thanks a lot for any help!
    submitted by    /u/inc0mingst0rm  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What's the MVM (minimum viable model) for node classification?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tnx9vo/d_whats_the_mvm_minimum_viable_model_for_node/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tnx9vo/d_whats_the_mvm_minimum_viable_model_for_node/"/>
        <updated>2022-03-25T19:39:50.000Z</updated>
        <summary type="html"><![CDATA[One core idea in ML is to use/build a simple model at first to get some minimum threshold, and then find better models or refine the existing model, improve the data, etc. 
 I'd say for computer vision, in most cases, a Resnet / EfficientNet would get a good result, given enough data. 
 In NLP, if it's about something very simple, Naive Bayes methods can be decent. If the task is harder, BERT would do the trick up to a certain level for many tasks
 However, choosing the first model is not obvious for many graph-related tasks where a node has more than 1 feature. For example, in node classification problems, what's a model easy to implement that guarantees decent results?
    submitted by    /u/adenml  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[my meme generating AI just came up with this (not technically AI)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnwb4q/my_meme_generating_ai_just_came_up_with_this_not/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnwb4q/my_meme_generating_ai_just_came_up_with_this_not/"/>
        <updated>2022-03-25T18:53:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/snoggel  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can I use TensorFlow just for one function inside my PyTorch model?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tnvqdr/can_i_use_tensorflow_just_for_one_function_inside/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tnvqdr/can_i_use_tensorflow_just_for_one_function_inside/"/>
        <updated>2022-03-25T18:36:14.000Z</updated>
        <summary type="html"><![CDATA[There is a Tensorflow function that I cannot convert into PyTorch. Can I use that function from tensorflow but still use my entire architecture in PyTorch?
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve search accuracy with Spell Checker in Amazon Kendra]]></title>
        <id>99ad186f1cc45ebb6c59a41d94d1ffd3662359d5</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/improve-search-accuracy-with-spell-checker-in-amazon-kendra/"/>
        <updated>2022-03-25T18:08:51.000Z</updated>
        <summary type="html"><![CDATA[Amazon Kendra is an intelligent search service powered by machine learning. You can receive spelling suggestions for misspelled terms in your queries by utilizing the Amazon Kendra Spell Checker. Spell Checker helps reduce the frequency of queries returning irrelevant results by providing spelling suggestions for unrecognized terms. In this post, we explore how to use […]]]></summary>
        <author>
            <name>Matthew Peretick</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This Latest Paper From Twitter and Oxford Research Shows That Feature Propagation is an Efficient and Scalable Approach for Handling Missing Features in Graph Machine Learning Applications]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnsnot/this_latest_paper_from_twitter_and_oxford/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnsnot/this_latest_paper_from_twitter_and_oxford/"/>
        <updated>2022-03-25T17:33:24.000Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have proved to be effective in a wide range of issues and fields. GNNs commonly use a message-passing mechanism, in which nodes communicate feature representations (“messages”) to their neighbors at each layer. Each node’s feature representation is initialized to its original features, and it is updated by aggregating incoming messages from neighbors on a regular basis. GNNs are distinguished from other purely topological learning systems such as random walks or label propagation by their ability to mix topological and feature information, which is arguably what contributes to their success.
 Typically, GNN models assume a fully observed feature matrix, with rows representing nodes and columns representing channels. In real-world circumstances, however, each trait is frequently only observable for a subset of nodes. Demographic information, for example, maybe exposed to only a small percentage of social network users, while content features are typically only available to the most active users. 
 Continue Reading
 Paper: https://arxiv.org/pdf/2111.12128.pdf
 https://preview.redd.it/nylt2m19fkp81.png?width=1024&format=png&auto=webp&s=639c61207d4bffaa4c67f97263fcd5527a849f85
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do DQN and DDQN learn to not perform an action that gives a small reward for another action in the future that gives a bigger reward]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tnrvw5/how_do_dqn_and_ddqn_learn_to_not_perform_an/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tnrvw5/how_do_dqn_and_ddqn_learn_to_not_perform_an/"/>
        <updated>2022-03-25T17:15:43.000Z</updated>
        <summary type="html"><![CDATA[To clarify, deep q networks and double q networks
 would it ever discover such actions if its always favouring higher goals in the short term? what if another action has to be performed that may give a loss (potentially a significant loss) in the short term but in fact sets a path for a greater reward?
 are there any papers I could look at?
    submitted by    /u/clockface99  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there a AI which is able to gereate images which people would buy?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnpd3g/is_there_a_ai_which_is_able_to_gereate_images/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnpd3g/is_there_a_ai_which_is_able_to_gereate_images/"/>
        <updated>2022-03-25T16:12:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/xXLisa28Xx  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] how do you defend the choice of ML algorithm depending the distribution of the data?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tnowi9/d_how_do_you_defend_the_choice_of_ml_algorithm/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tnowi9/d_how_do_you_defend_the_choice_of_ml_algorithm/"/>
        <updated>2022-03-25T15:51:59.000Z</updated>
        <summary type="html"><![CDATA[For example, when is it better to use decision trees instead of SVM or KNN, based on underlying theory/distribution of the data?
 I would appreciate any empirical/theoritical advice or references. Thank you.
    submitted by    /u/Redditagonist  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] TorchMetrics -- How do we use it, and what's the difference between .update() and .forward()?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tno47l/p_torchmetrics_how_do_we_use_it_and_whats_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tno47l/p_torchmetrics_how_do_we_use_it_and_whats_the/"/>
        <updated>2022-03-25T15:14:02.000Z</updated>
        <summary type="html"><![CDATA[TorchMetrics is a really nice and convenient library that lets us compute the performance of models in an iterative fashion. It's designed with PyTorch (and PyTorch Lightning) in mind, but it is a general-purpose library compatible with other libraries and workflows. This iterative computation is useful if we want to track a model during iterative training or evaluation on minibatches (and optionally across on multiple GPUs). In deep learning, that's essentially all the time. However, when using TorchMetrics, one common question is whether we should use .update() or .forward()? (And that's also a question I certainly had when I started using it.). Here's a hands-on example and explanation.
 https://sebastianraschka.com/blog/2022/torchmetrics.html
    submitted by    /u/seraschka  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Is a Transformer Model?]]></title>
        <id>https://blogs.nvidia.com/?p=56338</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/"/>
        <updated>2022-03-25T15:00:27.000Z</updated>
        <summary type="html"><![CDATA[If you want to ride the next big wave in AI, grab a transformer. They’re not the shape-shifting toy robots on TV or the trash-can-sized tubs on telephone poles. So, What’s a Transformer Model? A transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the Read article >
The post What Is a Transformer Model? appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Rick Merritt</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to apply Deep RL in Arcade Learning Environment?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tnnmng/how_to_apply_deep_rl_in_arcade_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tnnmng/how_to_apply_deep_rl_in_arcade_learning/"/>
        <updated>2022-03-25T14:51:36.000Z</updated>
        <summary type="html"><![CDATA[I am new to RL but have experience on Deep Learning. Would you please guide me in the right direction as to how can I apply Deep Reinforcement Learning in Arcade Learning Environment? I also have basic knowledge of OpenAI gym environment.
    submitted by    /u/AvailableBike9260  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Should expert opinion be a bigger part of the Machine Learning world?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tnmx1h/d_should_expert_opinion_be_a_bigger_part_of_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tnmx1h/d_should_expert_opinion_be_a_bigger_part_of_the/"/>
        <updated>2022-03-25T14:17:32.000Z</updated>
        <summary type="html"><![CDATA[I came across this Twitter thread which shows some interesting results when trying to recolor historical photos that have been decolored. The recolored photos are a lot drabber than the original, and the thread author says that this gives us a skewed view of the past, making us think the past was a lot more boring than it was, downplaying how vibrant and diverse certain societies were.
 https://preview.redd.it/fctveek4cjp81.png?width=3064&format=png&auto=webp&s=9aadecef7c12a4dd13f5197a69ffd155084be75e
 This made me think of some questions that I thought could lead to a good discussion here. I've put some below in no particular order!
  
Do you think that expert opinion should be consulted in the Machine Learning process more? If so, where? (perhaps omitting Expert Systems)
 Is there too much faith that a result from an ML model is the "right" result? (a phenomenon that maybe isn't specific to ML but a result of human tendencies?)
 Do ML practitioners have a responsibility to clearly communicate to the general public the limitations and degree-of-confidence in these systems?
 Am I reading too much into this, and this colorization model is just a fun model to play with, and the conclusions of the Twitter thread are too speculative or conjectural?
 Is this colorization issue just another form of bias that needs to be ironed out?
 The thread concludes by saying that colorization should be left to experts who can use context to pick accurate colors. I think this is too extreme, and that ML systems can incorporate expertise when training, or after during evaluation. Do you think there are any jobs/problems that ML methods could be applied to but should be left to experts (some considerations might be safety, privacy, ethics, etc.)
  
I know that ultimately a lot of these questions can simply boil down to statistics and their interpretation, so I'm not sure exactly where discussion could/should/will lead, but I'm looking forward to hearing your opinions :)
    submitted by    /u/SleekEagle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence Helps Cut Miss Rate of Colorectal Polyps]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnlobi/artificial_intelligence_helps_cut_miss_rate_of/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnlobi/artificial_intelligence_helps_cut_miss_rate_of/"/>
        <updated>2022-03-25T13:14:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Beautiful-Credit-868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I want to create an A.I. to tell me what to do next. based on my geo location and a wave of factors i will punch in(obvs i know nothing)all guided by a premise (ex..i want to help save our species from destroying itself) any thoughts?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnlikk/i_want_to_create_an_ai_to_tell_me_what_to_do_next/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnlikk/i_want_to_create_an_ai_to_tell_me_what_to_do_next/"/>
        <updated>2022-03-25T13:05:57.000Z</updated>
        <summary type="html"><![CDATA[I dont really know how a i is done i imagine with lots of code i dont know and loads of statistics i really dont know. Can someone out in the land of 1's aand 0's point me in the right direction?
    submitted by    /u/143openyourmind  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Research Turns 2D Photos Into 3D Scenes in the Blink of an AI]]></title>
        <id>https://blogs.nvidia.com/?p=55984</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/25/instant-nerf-research-3d-ai/"/>
        <updated>2022-03-25T13:00:23.000Z</updated>
        <summary type="html"><![CDATA[When the first instant photo was taken 75 years ago with a Polaroid camera, it was groundbreaking to rapidly capture the 3D world in a realistic 2D image. Today, AI researchers are working on the opposite: turning a collection of still images into a digital 3D scene in a matter of seconds. Known as inverse Read article >
The post NVIDIA Research Turns 2D Photos Into 3D Scenes in the Blink of an AI appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Isha Salian</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[python NEAT neural network not evolving...]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tnl6dm/python_neat_neural_network_not_evolving/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tnl6dm/python_neat_neural_network_not_evolving/"/>
        <updated>2022-03-25T12:48:22.000Z</updated>
        <summary type="html"><![CDATA[I've been trying to program a "game" where drones collect points floating around using a neural network with NEAT. I've tried tinkering a bit with the config file, but the drones just don't seem to evolve... 
 https://stackoverflow.com/questions/71611034/python-neat-neural-network-not-evolving
 I guess i just want to know what i'm doing wrong, since nothing appears to be "evolving" when i run the program... (NEAT v0.92 and python v3.9.2 btw) link to the github repo is here aswell
    submitted by    /u/LukasSchulte  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doubts in TRPO's objective and constraint approximations.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tnkp7d/doubts_in_trpos_objective_and_constraint/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tnkp7d/doubts_in_trpos_objective_and_constraint/"/>
        <updated>2022-03-25T12:20:18.000Z</updated>
        <summary type="html"><![CDATA[Correct me if I am wrong.
 Mathematically, Linear Approximation L = f(X_0) + ∇f(X_0)(X-X_0) and Quadratic Approximation Q = f(X_0) + ∇f(X_0)(X-X_0) + 0.5 * (X-X_0)^T H_f(X_0) (X-X_0).
 Then in TRPO algorithm, why are we ignoring the constant term f(X_0) for importance sampling term and f(X_0) + ∇f(X_0)(X-X_0) for Quadratic Approximation in KL Term.
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nvidia stock rallies 9% with a $1 Trillion opportunity]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnk4fg/nvidia_stock_rallies_9_with_a_1_trillion/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnk4fg/nvidia_stock_rallies_9_with_a_1_trillion/"/>
        <updated>2022-03-25T11:45:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/PM_ME_YOUR_PC_DEALS  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combine Lidar and Cameras for 3D object detection - Waymo & Google Research]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnjlrb/combine_lidar_and_cameras_for_3d_object_detection/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnjlrb/combine_lidar_and_cameras_for_3d_object_detection/"/>
        <updated>2022-03-25T11:15:22.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ML Researchers From Oxford Propose a Forward Mode Method to Compute Gradients Without Backpropagation]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tnfx18/ml_researchers_from_oxford_propose_a_forward_mode/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tnfx18/ml_researchers_from_oxford_propose_a_forward_mode/"/>
        <updated>2022-03-25T06:37:47.000Z</updated>
        <summary type="html"><![CDATA[The amount of money and energy necessary to train AI models has become a hot-button issue as they grow in size. Leaders in the AI field have been pouring money towards training increasingly bigger models since GPT-3 proved the considerable gains in performance that can be achieved by merely increasing model size. However, this is prohibitively expensive, necessitates tremendous computational resources, consumes enormous energy, and is becoming more recognized as an issue, not just because of the environmental implications, but also because it makes it harder for smaller AI companies to compete, concentrating power in the hands of industry titans. A new technique that rewrites one of the discipline’s core building pieces could give a workaround.
 Oxford University researchers have proposed a novel method that might cut training time in half. This is accomplished by redesigning backpropagation, one of the essential components of today’s neural network-based AI systems. Backpropagation has remained a mainstay of machine learning for computing gradients of objective functions for optimization. Backpropagation, also known as reverse-mode differentiation, is a subset of the general family of automatic differentiation algorithms that includes forward mode differentiation. A method for computing gradients based purely on the directional derivative, which may be done in the forward mode with precision and efficiency, is designed. The method is known as the forward gradient, which is an unbiased estimate of the gradient that can be assessed in a single forward run of the function, obviating the necessity for backpropagation in gradient descent completely.
 Continue Reading The Research Summary
 Paper: https://arxiv.org/pdf/2202.08587.pdf
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Robot peels banana with goal-conditioned dual-action deep imitation learning", Kim et al 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tnau1d/robot_peels_banana_with_goalconditioned/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tnau1d/robot_peels_banana_with_goalconditioned/"/>
        <updated>2022-03-25T01:27:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is there any state-of-the-art RL method based on REINFORCE?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tnaaja/is_there_any_stateoftheart_rl_method_based_on/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tnaaja/is_there_any_stateoftheart_rl_method_based_on/"/>
        <updated>2022-03-25T00:58:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Blasphemer666  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Testing Large Models for Convergence?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tna0u6/d_testing_large_models_for_convergence/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tna0u6/d_testing_large_models_for_convergence/"/>
        <updated>2022-03-25T00:44:04.000Z</updated>
        <summary type="html"><![CDATA[Hi, I am wondering if there are any ways to test large neural networks (to see if they work) before training the entire model? In any case, I don't really want to train the model (which would take a long time for me) and find out that it does not converge or converges poorly.
    submitted by    /u/ChunkOfAir  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Author Interview - BLIP: Bootstrapping Language-Image Pre-training (Video)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tna06n/d_author_interview_blip_bootstrapping/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tna06n/d_author_interview_blip_bootstrapping/"/>
        <updated>2022-03-25T00:43:04.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/Z3knUzwuIgo
 This is an interview with Junnan Li and Dongxu Li, authors of BLIP and members of Salesforce research.
 Cross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!
 ​
 OUTLINE:
 0:00 - Intro
 0:40 - Sponsor: Assembly AI
 1:30 - Start of Interview
 2:30 - What's the pitch?
 4:40 - How did data bootstrapping come into the project?
 7:10 - How big of a problem is data quality?
 11:10 - Are the captioning & filtering models biased towards COCO data?
 14:40 - Could the data bootstrapping be done multiple times?
 16:20 - What was the evolution of the BLIP architecture?
 21:15 - Are there additional benefits to adding language modelling?
 23:50 - Can we imagine a modular future for pre-training?
 29:45 - Diving into the experimental results
 42:40 - What did and did not work out during the research?
 45:00 - How is research life at Salesforce?
 46:45 - Where do we go from here?
 ​
 Paper: https://arxiv.org/abs/2201.12086
 Code: https://github.com/salesforce/BLIP
 Demo: https://huggingface.co/spaces/Salesforce/BLIP
    submitted by    /u/ykilcher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apple ML Researchers Introduce ‘MobileViT’: A Light-Weight And General-Purpose Vision Transformer For Mobile Devices]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tn87vf/apple_ml_researchers_introduce_mobilevit_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tn87vf/apple_ml_researchers_introduce_mobilevit_a/"/>
        <updated>2022-03-24T23:12:01.000Z</updated>
        <summary type="html"><![CDATA[To learn visual representations, self-attention-based models, particularly vision transformers, offer an alternative to convolutional neural networks (CNNs). ViT breaks an image into a series of non-overlapping patches and then uses multi-headed self-attention in transformers to learn interpatch representations. The typical trend in ViT networks is to increase the number of parameters to improve performance.
 These gains in performance come at the expense of model size (network parameters) and latency. Many real-world applications (such as augmented reality and autonomous wheelchairs) necessitate the timely execution of visual recognition tasks (such as object detection and semantic segmentation) on resource-constrained mobile devices.
 ViT models for such jobs should be light-weight and quick to be effective. ViT models’ performance is much inferior to light-weight CNNs, even when the model size is reduced to fit the resource limits of mobile devices. DeIT, for example, is 3% less accurate than MobileNetv3 for a parameter budget of roughly 5-6 million. As a result, designing light-weight ViT models is critical.
 Many mobile vision tasks have been powered by light-weight CNNs. ViT-based networks, on the other hand, are still a long way from being utilized on such devices. Unlike light-weight CNNs that are simple to optimize and integrate with task-specific networks, ViTs are large (e.g., ViT-B/16 vs. MobileNetv3: 86 vs. 7.5 million parameters), difficult to optimize, require extensive data augmentation and L2 regularisation to avoid over-fitting, and require expensive decoders for downstream tasks, particularly dense prediction tasks.
 Continue Reading The Research Summary
 Paper: https://arxiv.org/pdf/2110.02178.pdf
 Github: https://github.com/apple/ml-cvnets
 https://preview.redd.it/u1m6udvqyep81.png?width=1198&format=png&auto=webp&s=b12153662902056706dd2ff92459c625233bb785
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Explainability for Semantic Search]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tn78l9/p_explainability_for_semantic_search/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tn78l9/p_explainability_for_semantic_search/"/>
        <updated>2022-03-24T22:24:44.000Z</updated>
        <summary type="html"><![CDATA[​
 https://preview.redd.it/7ztp9xcapep81.png?width=1410&format=png&auto=webp&s=370f3d2554e06343996c9e69a223272a25268e16
 The following notebook demonstrates a method to provide explainability for semantic search. This method masks/replaces each token for a search result to determine the importance of each term for a query. The score is then compared with the score for the full text to determine how much of a delta the permutation caused. 
 https://colab.research.google.com/github/neuml/txtai/blob/master/examples/32_Model_explainability.ipynb
    submitted by    /u/davidmezzetti  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Would you trust an AI COVID diagnosis? - PhD research, <5m survey, £25 Amazon voucher raffle]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tn78gs/r_would_you_trust_an_ai_covid_diagnosis_phd/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tn78gs/r_would_you_trust_an_ai_covid_diagnosis_phd/"/>
        <updated>2022-03-24T22:24:34.000Z</updated>
        <summary type="html"><![CDATA[Hi guys, I hope this is allowed!
 I'm a PhD researcher at the University of Bristol in the UK, focussing on Human-In-The-Loop AI. I (and the others in my group) work a lot in XAI and tricky, grey-area ethical issues. Currently I'm researching how different groups, including those familiar with AI, view AI in high-stakes settings.
 As part of this I'm trying to find people to take part in my survey: 
 https://forms.office.com/r/xVFu2f8qrJ
 It takes about 3m on average to complete, and there's the option to enter a raffle for one of four £25 Amazon vouchers. All of your data is completely anonymous, and the raffle is separated from the questions, so there's no way that I can associate your answers with your email. 
 If you have any questions feel free to drop them in the comments!
    submitted by    /u/Odd_Beautiful2592  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] [P] Patch Spatial Interaction for Long-Range Dependencies in CNNs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tn772b/d_p_patch_spatial_interaction_for_longrange/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tn772b/d_p_patch_spatial_interaction_for_longrange/"/>
        <updated>2022-03-24T22:22:44.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 An idea for modelling long-range interactions in CNNs came to my mind, and I thought to share it with you folks and seek your opinions. 
 On a high level, it functions akin to squeeze-and-excitation, with the chief difference being instead of calibrating channel-wise features, it operates on spatial features. Concretely, assume we start with a 3 X 256 X 256 tensor; the first step is to squeeze the channels via a convolution to get a 256 X 256 tensor. However, owing to the computational costs of the multilayer perceptron that is about to be described, it is necessary to downscale the spatial dimensions, which can be done by setting the convolution's kernel size and stride to, say, 16 (i.e., the input is being "patchified"). The resulting 16 X 16 matrix is flattened to yield a 25…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training a MultiInput policy using Stablebaselines3]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tn76qe/pretraining_a_multiinput_policy_using/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tn76qe/pretraining_a_multiinput_policy_using/"/>
        <updated>2022-03-24T22:22:17.000Z</updated>
        <summary type="html"><![CDATA[Hi! 
 Newbie here! 
 I am attempting to pre-train a custom policy in stable baselines. The policy takes multiple inputs, each with a different shape. 
 I tried using this as reference https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pretraining.ipynb#scrollTo=toKEQE9i8aof but failed miserably. 
 Has anyone successfully pretrained a multi input? Would love some guidance.
 Currently each of my observations are stored as a list of lists with different sizes.. For example [ list of size(7), list of size(3)...] where each list is one of the inputs.
 Thank you!
    submitted by    /u/Alpha-Seirra  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm from scratch?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tn4rrz/algorithm_from_scratch/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tn4rrz/algorithm_from_scratch/"/>
        <updated>2022-03-24T21:06:03.000Z</updated>
        <summary type="html"><![CDATA[Does it pay off to make an algorithm from scratch if there are libs like ray?
 I implemented an own distributed algorithm. Anyway i wounder if it has any worth because there are libs like ray where popular algos are already implemented. It even has hyper parameter tuning. What do you guys think?
    submitted by    /u/Willing-Classroom735  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Suggest some Scientific machine learning for modeling and simulations]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tn4amb/d_suggest_some_scientific_machine_learning_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tn4amb/d_suggest_some_scientific_machine_learning_for/"/>
        <updated>2022-03-24T20:58:46.000Z</updated>
        <summary type="html"><![CDATA[-Basically I want to learn ML, to apply in scientific computing. -Learn Scientific computing, visualization and Simulations -it will be more helpful if someone knows pathway to computational biology and guide me
    submitted by    /u/Grapes_icecream  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nick Walton on AI Dungeon and the Future of AI in Games]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tn2gga/nick_walton_on_ai_dungeon_and_the_future_of_ai_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tn2gga/nick_walton_on_ai_dungeon_and_the_future_of_ai_in/"/>
        <updated>2022-03-24T20:29:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unpopular opinion: R is actually better than Python]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tn0bij/unpopular_opinion_r_is_actually_better_than_python/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tn0bij/unpopular_opinion_r_is_actually_better_than_python/"/>
        <updated>2022-03-24T19:59:41.000Z</updated>
        <summary type="html"><![CDATA[I love R. It's simple in syntax and very easy to install and run with Rstudio. Some of the data transformation libraries are very good. And even analytics libraries provide more statistical insights.
 Not to say that Python does not edge out at some places. Like having lots of libraries for almost everything, being multi-functional and huge community.
 But yet, I enjoy R more than Python. I personally find that I can get most of work done quickly in R.
 Or is it because I've been working with R, that I'm better at it!?
    submitted by    /u/maverick_css  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are you a Machine Learning Engineer interested in Supporting Ethics in your Everyday work? [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tmxvxe/are_you_a_machine_learning_engineer_interested_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tmxvxe/are_you_a_machine_learning_engineer_interested_in/"/>
        <updated>2022-03-24T19:25:39.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, I am a Researcher at UXP2 Lab Purdue University working on an NSF funded project to support practitioners to improve ethics in their everyday practice.
 Are a Machine Learning Engineer who likes to think about ethics in your everyday work? I would love for you to attend one of our virtual workshops. This three-hour workshop will engage you in co-design activities with 3-5 other technology practitioners to build, iterate on, and implement ethics-focused action plans based on provided prompts and your personal experiences. A $50 incentive will be provided for your participation. Fill out the following form to express your interest in this study:https://forms.gle/Ac89zyJLdTrnaHVS9
 We are looking for practitioners who are currently employed in roles that include (but are not limited to): Software Engineering, Data Science, Front/Back-end Development, Product Management, User Experience (UX), and other design or technology personnel responsible for the development of digital systems in any industry or governmental context.
 You can learn more about the overall grant project at https://everydayethics.uxp2.com. I am looking forward to seeing you at the virtual workshop. Thank you! If you have any questions, please feel free to message me!
    submitted by    /u/rikeob  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nebullvm, an open-source library to accelerate AI inference by 5-20x in a few lines of code]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tmwwx5/nebullvm_an_opensource_library_to_accelerate_ai/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tmwwx5/nebullvm_an_opensource_library_to_accelerate_ai/"/>
        <updated>2022-03-24T19:12:08.000Z</updated>
        <summary type="html"><![CDATA[How does nebullvm work?
 It takes your AI model as input and outputs an optimized version that runs 5-20 times faster on your hardware. In other words, nebullvm tests multiple deep learning compilers to identify the best possible way to execute your model on your specific machine, without impacting the accuracy of your model.
 And that's it. In just a few lines of code.
 And a big thank you to everyone for supporting this open-source project! The library received 250+ Github stars⭐ on release day, and that's just amazing 🚀
 ORIENTATION MAP
 Let's learn more about nebullvm and AI optimization. Where should we start? From...
  
Some CONTEXT on why few developers optimize AI and related negative consequences
 An overview of how the LIBRARY works
 Some USE CASES, technology demonstrations and…]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is it smart of me to do a ML postdoc?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tmux3x/d_is_it_smart_of_me_to_do_a_ml_postdoc/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tmux3x/d_is_it_smart_of_me_to_do_a_ml_postdoc/"/>
        <updated>2022-03-24T18:43:58.000Z</updated>
        <summary type="html"><![CDATA[A bit of my background: I completed a Ph.D. in statistics in Europe last year. For my Ph.D., I did some work on DL (and also had some papers published). Directly after finishing my Ph.D. I started working as a data scientist in the financial service industry. However, my goal is to move into a pure tech company and hopefully work as a research data scientist or a research scientist. Recently I have been offered a 2-year postdoc in ML in a good group in Europe. I am considering taking this postdoc since I get more time to do "pure" research (and to complete some of the research from my Ph.D.). Thus, would it make sense to take this postdoc with a lower salary (and worse work-life balance) for a few years if the goal is to get a "research" job in the industry, or would I be better of to try to "climb the ladder" from my current data science job?
 PS: I am new to this subreddit so I hope this post is ok.
    submitted by    /u/Other_Hotel_6099  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI: AI for Algorithms, Chemical Weapons, Zelenskyy Deepfake, Border Control]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tmutkc/last_week_in_ai_ai_for_algorithms_chemical/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tmutkc/last_week_in_ai_ai_for_algorithms_chemical/"/>
        <updated>2022-03-24T18:42:49.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[using 5 different ai generated images i created 1 artwork, second picture show the original pics i made/used, what do you guys think]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tmrdy4/using_5_different_ai_generated_images_i_created_1/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tmrdy4/using_5_different_ai_generated_images_i_created_1/"/>
        <updated>2022-03-24T17:53:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bigshinna  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Which GAN is Jon Rafman using?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tmr5fc/d_which_gan_is_jon_rafman_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tmr5fc/d_which_gan_is_jon_rafman_using/"/>
        <updated>2022-03-24T17:50:19.000Z</updated>
        <summary type="html"><![CDATA[Any ideas which GAN he is using in his latest works? Resolution and detail are insane..https://www.instagram.com/ronjafman/
 https://preview.redd.it/gxzu3ielddp81.png?width=1164&format=png&auto=webp&s=a0e3ad2d345989524d271a15df62e1d8245ffd7f
    submitted by    /u/janni95  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Signs of Disease from External Images of the Eye]]></title>
        <id>http://ai.googleblog.com/2022/03/detecting-signs-of-disease-from.html</id>
        <link href="http://ai.googleblog.com/2022/03/detecting-signs-of-disease-from.html"/>
        <updated>2022-03-24T17:10:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Boris Babenko, Software Engineer and Naama Hammel, Clinical Research Scientist, Google Health 
Three years ago we wrote about our work on predicting a number of cardiovascular risk factors from fundus photos (i.e., photos of the back of the eye)1 using deep learning. That such risk factors could be extracted from fundus photos was a novel discovery and thus a surprising outcome to clinicians and laypersons alike. Since then, we and other researchers have discovered additional novel biomarkers from fundus photos, such as markers for chronic kidney disease and diabetes, and hemoglobin levels to detect anemia.  
A unifying goal of work like this is to develop new disease detection or monitoring approaches that are less invasive, more accurate, cheaper and more readily available. How…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Any tools like streamlit etc. using which I can make a clustering based image tagging tool for classification. Anyone here made their own data tagging tool?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tmnkhh/d_any_tools_like_streamlit_etc_using_which_i_can/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tmnkhh/d_any_tools_like_streamlit_etc_using_which_i_can/"/>
        <updated>2022-03-24T16:57:50.000Z</updated>
        <summary type="html"><![CDATA[I have a lot of untagged images, I want to show the data annotator close looking patches together, which they can just select and then give label at once. Steamlit doesn't work because it doesn't allow for batch selection of images. Is there any easy to use framework available in python using which I can make a frontend for data tagging?
    submitted by    /u/lMAObigZEDONG  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anyone knows to code lenet-5 architecture from scratch using numpy only]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tmniqy/anyone_knows_to_code_lenet5_architecture_from/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tmniqy/anyone_knows_to_code_lenet5_architecture_from/"/>
        <updated>2022-03-24T16:56:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Actual-Performer-832  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I hope this is accurate enough for monte Carlo to accept me]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tmkmzp/i_hope_this_is_accurate_enough_for_monte_carlo_to/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tmkmzp/i_hope_this_is_accurate_enough_for_monte_carlo_to/"/>
        <updated>2022-03-24T16:17:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/FurryMachine  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why is using an estimate to update another estimate called Bootstrapping?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tmibg3/why_is_using_an_estimate_to_update_another/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tmibg3/why_is_using_an_estimate_to_update_another/"/>
        <updated>2022-03-24T15:45:27.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/FurryMachine  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Incremental DBSCAN]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tmg2id/p_incremental_dbscan/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tmg2id/p_incremental_dbscan/"/>
        <updated>2022-03-24T15:10:42.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 It might be of interest that I created a Python implementation of IncrementalDBSCAN.
 The repository, including documentation, is here: https://github.com/DataOmbudsman/incdbscan
 And this is the original paper: https://www.dbs.ifi.lmu.de/Publikationen/Papers/VLDB-98-IncDBSCAN.pdf
 The paper (from the authors of DBSCAN) describes how to make DBSCAN work with an incremental strategy, in which one can add new data points to an already existing clustering and doesn't have to re-cluster every data point. I couldn't find any implementation of the algorithm so I ended up writing my own one. I'm glad it's actually working and has a lot of unit tests covering common and special cases. The performance gain (vs reclustering with DBSCAN) is not huge though, could be worked on more. 
 Happy to take any feedback!
    submitted by    /u/DataOmbudsman  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based chatbot / stylized text generation model that responds to images and text?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tmg2ij/transformerbased_chatbot_stylized_text_generation/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tmg2ij/transformerbased_chatbot_stylized_text_generation/"/>
        <updated>2022-03-24T15:10:42.000Z</updated>
        <summary type="html"><![CDATA[I did a quick search of huggingface but couldn't find anything. Has anyone tried making a transformer-based reply bot or chatbot that responds to both images and text, like how a person would be able to chat in messaging apps? I thought about this because I figured tweet generation models would be much more interesting if they could also generate replies instead of standalone tweets, and replies by tweeters more often than not are replies to tweets with images. 
 I was thinking not only do you need an image-to-text model, you probably also need finetuning data that includes an image-to-text model? I guess understanding context would be major obstacle.
 Thanks!
    submitted by    /u/IndicatorGlobe679  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BMW Group, Qualcomm and Arriver to form long-lasting strategic cooperation for joint development of Automated Driving software solutions]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tmej7f/bmw_group_qualcomm_and_arriver_to_form/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tmej7f/bmw_group_qualcomm_and_arriver_to_form/"/>
        <updated>2022-03-24T14:39:43.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/dannylenwinn  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Career advice for a soon-to-be master's graduate in AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tmcm2z/career_advice_for_a_soontobe_masters_graduate_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tmcm2z/career_advice_for_a_soontobe_masters_graduate_in/"/>
        <updated>2022-03-24T14:10:58.000Z</updated>
        <summary type="html"><![CDATA[I'm a soon-to-be master's graduate in artificial intelligence and I'm looking for some career advice. I most likely want to go for a PhD because I enjoy thinking about challenging problems and trying out new things. My dream job would be to work as a research scientist in the industry, which would guarantee a good salary (as opposed to academia), applied value in the research I do, and some creativity. Currently, I am considering different research avenues while keeping in mind a potential trade-off between utility, potential to be creative, intrinsic motivation, and future job prospects. While intrinsic motivation is definitely important to me, I really see a PhD as an investment in my future. That is, I want my job prospects to signficantly improvea after completing it. Furthermore, I am…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Take Control This GFN Thursday With New Stratus+ Controller From SteelSeries]]></title>
        <id>https://blogs.nvidia.com/?p=56286</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/24/geforce-now-thursday-march-24/"/>
        <updated>2022-03-24T13:00:59.000Z</updated>
        <summary type="html"><![CDATA[GeForce NOW gives you the power to game almost anywhere, at GeForce quality. And with the latest controller from SteelSeries, members can stay in control of the action on Android and Chromebook devices. This GFN Thursday takes a look at the SteelSeries Stratus+, now part of the GeForce NOW Recommended program. And it wouldn’t be Read article >
The post Take Control This GFN Thursday With New Stratus+ Controller From SteelSeries appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orchestrated to Perfection: NVIDIA Data Center Grooves to Tune of Millionfold Speedups]]></title>
        <id>https://blogs.nvidia.com/?p=56306</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/24/data-center-millionfold-speedups-orchestrated-to-perfection/"/>
        <updated>2022-03-24T13:00:51.000Z</updated>
        <summary type="html"><![CDATA[The hum of a bustling data center is music to an AI developer’s ears — and NVIDIA data centers have found a rhythm of their own, grooving to the swing classic “Sing, Sing, Sing” in this week’s GTC keynote address. The lighthearted video, created with the NVIDIA Omniverse platform, features Louis Prima’s iconic music track, Read article >
The post Orchestrated to Perfection: NVIDIA Data Center Grooves to Tune of Millionfold Speedups appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Michael Kagan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Few-shot NER: entity extraction without annotation and training based on GPT]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tm46h9/d_fewshot_ner_entity_extraction_without/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tm46h9/d_fewshot_ner_entity_extraction_without/"/>
        <updated>2022-03-24T11:52:39.000Z</updated>
        <summary type="html"><![CDATA[Hello all,
 After 1 year working extensively with GPT models (GPT-3, GPT-J, and GPT-NeoX), I think I now have a good view on what these NLP models are capable of. It appears that many traditional NLP tasks can now be achieved thanks to these large language models thanks to few-shot learning (aka "prompting", or "prompt engineering").
 NER is a very good candidate because, thanks to these models, it is possible to extract any type of entity without ever annotating and training a new model. Annotation has always been a challenge that has caused many entity extraction projects to simply fail, because it is a long and tedious process.
 In this article, I'm showing how easy it is to perform NER thanks to GPT and few-shot learning, without any annotation process: https://nlpcloud.io/few-shot-ner-entity-extraction-without-annotation-training-based-on-gpt.html
 If you also experimented with entity extraction with GPT models, I would love to hear your thoughts. Are you, like I am, impressed by the results? And do you think it means that annotation is a thing from the past?
 Thanks!
    submitted by    /u/juliensalinas  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need Help Better Understanding A Neural Net]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tm4127/need_help_better_understanding_a_neural_net/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tm4127/need_help_better_understanding_a_neural_net/"/>
        <updated>2022-03-24T11:43:14.000Z</updated>
        <summary type="html"><![CDATA[Hey , not sure if this is the right place to ask or not, i think itd be. 
 Basically i need to create a neural net from scratch in c# for intent classification. I have modelled the neurons as a class with attributes : weights(moddelled as an array, each value is a weight for a branch connecting to the neuron) a , a value(result after the activation),and a bias(a different bias per neuron. 
 My question is that , my input will be an array of words, obviously hashed or something so it has a numerical value, each value in the array will be multiplied by the weight. Is the bias added to each of these values and then the activation function applied to each value in the array, OR is it that the activation is applied to the array there is one value as an output and the bias is added to that.
 thank you any help appreciated , if more info is needed then please ask.
    submitted by    /u/Tubhalooter  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What MLOps platform do you use, and how helpful are they?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tm39o4/d_what_mlops_platform_do_you_use_and_how_helpful/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tm39o4/d_what_mlops_platform_do_you_use_and_how_helpful/"/>
        <updated>2022-03-24T10:58:11.000Z</updated>
        <summary type="html"><![CDATA[Lots of options out there. I'm wondering how useful they are and whether they help you build better apps/services?
    submitted by    /u/dmart89  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Resources to Learn Data Science 2022 (courses, books, Blogs) -]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tm2s08/best_resources_to_learn_data_science_2022_courses/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tm2s08/best_resources_to_learn_data_science_2022_courses/"/>
        <updated>2022-03-24T10:25:35.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Following: Is Geometry a Language That Only Humans Know? " How Do Spiders Build Their Webs? "]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tm0ozh/d_following_is_geometry_a_language_that_only/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tm0ozh/d_following_is_geometry_a_language_that_only/"/>
        <updated>2022-03-24T07:54:50.000Z</updated>
        <summary type="html"><![CDATA[After reading the post on Is Geometry a Language That Only Humans Know? I see comments about spiders and their fascinating capabilities to use their legs and vibration sensor and not their eyes to build the web. I had an episode about it and one of the questions about the error assessment and how they know when they should stop building or whether it is right? How they can know if there is a damage in the web. Also the interesting part about their brain behavior.
 Links in the comment in case you are interested.
    submitted by    /u/meldiwin  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some A.I.-type thing is being used to cause Severe Emotional Trauma]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tlz9ve/some_aitype_thing_is_being_used_to_cause_severe/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tlz9ve/some_aitype_thing_is_being_used_to_cause_severe/"/>
        <updated>2022-03-24T06:14:13.000Z</updated>
        <summary type="html"><![CDATA[This is not an attempt to bend any forum rules, I am not looking for someone in this forum to hack for us or anything. We just need advice as to where to look more than the avenues we have taken.
 TL;DR: Girlfriend is being stalked by some A.I.-type thing that very specifically, and very repeatedly, targets her and reminds her of abuse and we don't know where to turn for more help, or what it could be, and where it is coming from.
 Now the details:
 Over the last year or so, her phone was hacked and started acting funny, apps not working right, etc. During the same time, someone she knew got a hold of her bank information and ID. Someone also logged into Github on her phone (we are only familiar with generally what they do or what can be done there). People started following her around in …]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rainbow Forests AI Art]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tlz6fp/rainbow_forests_ai_art/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tlz6fp/rainbow_forests_ai_art/"/>
        <updated>2022-03-24T06:07:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Recent_Coffee_2551  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Inferring Articulated Rigid Body Dynamics from RGBD Video]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tly88s/r_inferring_articulated_rigid_body_dynamics_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tly88s/r_inferring_articulated_rigid_body_dynamics_from/"/>
        <updated>2022-03-24T05:04:50.000Z</updated>
        <summary type="html"><![CDATA[They introduce a new method to learn simulators from depth and RGB videos. The "URDF" of an articulated rigid-body mechanism is reconstructed, and the parameters of the simulator inferred through Bayesian inference.
 Website: https://eric-heiden.github.io/video2sim/
 Paper: https://arxiv.org/abs/2203.10488
 Twitter Summary: https://twitter.com/eric_heiden/status/1506756429285191682
    submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Business Analytics from Application Logs and Database using Splunk]]></title>
        <id>https://www.datasciencecentral.com/?p=56798</id>
        <link href="https://www.datasciencecentral.com/business-analytics-from-application-logs-and-database-using-splunk/"/>
        <updated>2022-03-24T04:57:27.000Z</updated>
        <summary type="html"><![CDATA[Introduction Splunk is a well-known log management tool. Splunk mines log from different machines in real-time and can be used to monitor, search, and analyze gathered data. It is a Big Data log management tool that can give insight from the unstructured data stored in the Splunk indexes. Splunk analytics helps turn unstructured log data… Read More »Business Analytics from Application Logs and Database using Splunk
The post Business Analytics from Application Logs and Database using Splunk appeared first on Data Science Central.]]></summary>
        <author>
            <name>Roopesh Uniyal</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Series for π]]></title>
        <id>https://www.johndcook.com/blog/?p=100720</id>
        <link href="https://www.johndcook.com/blog/2022/03/23/series-for-%cf%80/"/>
        <updated>2022-03-24T03:00:13.000Z</updated>
        <summary type="html"><![CDATA[Here’s a curious series for π that I ran across on Math Overflow. In case you’re unfamiliar with the notation, n!! is n double factorial, the product of the positive integers up to n with the same parity as n. More on that here. When n is 0 or -1, n!! is defined to be […]
Series for π first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[cyberpunk breakdown #3. How to draw perspective from cyberpunk 2077]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tlvodv/cyberpunk_breakdown_3_how_to_draw_perspective/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tlvodv/cyberpunk_breakdown_3_how_to_draw_perspective/"/>
        <updated>2022-03-24T02:39:32.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Puzzleheaded-Gas-906  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is Geometry a Language That Only Humans Know?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tltji3/d_is_geometry_a_language_that_only_humans_know/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tltji3/d_is_geometry_a_language_that_only_humans_know/"/>
        <updated>2022-03-24T01:56:09.000Z</updated>
        <summary type="html"><![CDATA[A general public paper in the New-York Times explores whether humans are the only species able to master geometry. Neuroscientists like Stanislas Dehaene at Collège de France, Moira Dillon at New York University, and Josh Tenenbaum at MIT are exploring whether human ability to reason about geometric shapes like squares and rectangles are part of what makes humans special. Dehaene said. “I love the progress in A.I. It’s very impressive. But I believe that there is a deep aspect missing, which is symbol processing”. Yoshua Bengio, at the Université de Montréal, agreed that state-of-the-art machine learning lacks something related to symbols or abstract reasoning. It’s not impossible to do abstract reasoning with neural networks, Bengio said, “it’s just that we don’t know yet how to do it.” It’s an indication, Bengio said, of where A.I. research needs to go.
    submitted by    /u/ClaudeCoulombe  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is evaluation faster on GPU or CPU?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tloweh/d_is_evaluation_faster_on_gpu_or_cpu/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tloweh/d_is_evaluation_faster_on_gpu_or_cpu/"/>
        <updated>2022-03-24T00:36:03.000Z</updated>
        <summary type="html"><![CDATA[I work with NLP primarily, and find that sometimes it's faster on one or the other. 
 Surprisingly, evaluation on GPU takes as much time as training, even if I do .eval(). I don't understand why. 
 Is there a general rule of thumb on if evaluation should be done on GPU or CPU? 
 Any tips? 
 Thanks!
    submitted by    /u/cuddle_cuddle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["CrossBeam: Learning to Search in Bottom-Up Program Synthesis", Shi et al 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tljz0m/crossbeam_learning_to_search_in_bottomup_program/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tljz0m/crossbeam_learning_to_search_in_bottomup_program/"/>
        <updated>2022-03-23T23:05:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To help Ukraine, Berkeley AI Researchers Provide Machine Learning Methods And Pretrained Models To Interchangeably Use Any Imagery]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tljxox/to_help_ukraine_berkeley_ai_researchers_provide/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tljxox/to_help_ukraine_berkeley_ai_researchers_provide/"/>
        <updated>2022-03-23T23:04:50.000Z</updated>
        <summary type="html"><![CDATA[Extracting knowledge and actionable insights by manually processing hundreds of terabytes of data downlinked from satellites to data centers has become difficult.
 Synthetic aperture radar (SAR) imaging is a type of active remote sensing in which a satellite sends microwave radar wave pulses down to the Earth’s surface. These radar signals return to the satellite after reflecting off the Earth and any objects. A SAR image is created by processing these pulses over time and space, with each pixel representing the superposition of multiple radar scatters. Radar waves penetrate clouds and illuminate the Earth’s surface even during nights because the satellite is actively creating them.
 They produce visual that is sometimes contradictory and incompatible with modern computer vision systems. Three common effects are polarisation, layover, and multi-path.
  
The layover effect occurs when radar beams reach the top of a structure before reaching the bottom. This causes the top of the object to appear to be overlapping with the bottom. 
 When radar waves reflect off objects on the ground and bounce numerous times before returning to the SAR sensor, this is known as multi-path effects. Multi-path effects cause things in the picture to appear in multiple transformations in the final image.
  
Continue reading this research summary here
 BAIR Blog: https://bair.berkeley.edu/blog/2022/03/21/ukraine-sar-maers/
 https://i.redd.it/1r05r2mks7p81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DSC Weekly Newsletter 22 March 2022: The Shape of Data]]></title>
        <id>https://www.datasciencecentral.com/?p=56700</id>
        <link href="https://www.datasciencecentral.com/dsc-weekly-newsletter-22-march-2022-the-shape-of-data/"/>
        <updated>2022-03-23T22:40:38.000Z</updated>
        <summary type="html"><![CDATA[It is easy, when dealing with data, to think of it as something tangible. More precisely, data as water (or as oil) has become such a powerful metaphor that many managers, and even more than a few more technically-oriented people, have come to accept the fact that data engineering is largely about moving data from… Read More »DSC Weekly Newsletter 22 March 2022: The Shape of Data
The post DSC Weekly Newsletter 22 March 2022: The Shape of Data appeared first on Data Science Central.]]></summary>
        <author>
            <name>Kurt Cagle</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Set up a text summarization project with Hugging Face Transformers: Part 2]]></title>
        <id>3d6c5da9163112cfa37e93445c4e2b60a26d3c64</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/part-2-set-up-a-text-summarization-project-with-hugging-face-transformers/"/>
        <updated>2022-03-23T21:14:03.000Z</updated>
        <summary type="html"><![CDATA[This is the second post in a two-part series in which I propose a practical guide for organizations so you can assess the quality of text summarization models for your domain. For an introduction to text summarization, an overview of this tutorial, and the steps to create a baseline for our project (also referred to […]]]></summary>
        <author>
            <name>Heiko Hotz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Set up a text summarization project with Hugging Face Transformers: Part 1]]></title>
        <id>29be3c5b971aa0ada6bec541c65f04d985b9f9fd</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/part-1-set-up-a-text-summarization-project-with-hugging-face-transformers/"/>
        <updated>2022-03-23T21:13:30.000Z</updated>
        <summary type="html"><![CDATA[When OpenAI released the third generation of their machine learning (ML) model that specializes in text generation in July 2020, I knew something was different. This model struck a nerve like no one that came before it. Suddenly I heard friends and colleagues, who might be interested in technology but usually don’t care much about […]]]></summary>
        <author>
            <name>Heiko Hotz</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suggestions for board game reinforcement learning methods, frameworks]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tlf2qn/suggestions_for_board_game_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tlf2qn/suggestions_for_board_game_reinforcement_learning/"/>
        <updated>2022-03-23T21:08:54.000Z</updated>
        <summary type="html"><![CDATA[Hello, long time lurker here. I've applied to a hackathlon where the goal is to implement an AI for a 1v1 turn based game and was hoping to hear some thoughts, suggestions, recommendations.
 The game itself will be simple, likely a turn based board game. The AIs battle each other to win the competition. Last year's competition had a simplified version of Catan.
 I have taken some university courses in the AI field (AI 101, Machine learning, Soft Computing etc.) and am familiar with basic concepts, but I don't know much about RL (I have a general notion on what it's about). I was hoping to learn more about it some day and this competition seems like a perfect opportunity. 
 I'm looking to prepare a bit before the competition and started exploring reinforcement learning methods and frameworks. Is there a framework which would be suitable for this? A framework which has an interface for a custom game that I can build?
 Could Q learning be used in environments where there is more than one agent? Are there some methods to learn the best heuristic for the minmax algorithm?
    submitted by    /u/ludibog  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimize customer engagement with reinforcement learning]]></title>
        <id>a30a1487c091be6a2f19fba15a0ca11760ac910b</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/optimize-customer-engagement-with-reinforcement-learning/"/>
        <updated>2022-03-23T21:07:15.000Z</updated>
        <summary type="html"><![CDATA[This is a guest post co-authored by Taylor Names, Staff Machine Learning Engineer, Dev Gupta, Machine Learning Manager, and Argie Angeleas, Senior Product Manager at Ibotta. Ibotta is an American technology company that enables users with its desktop and mobile apps to earn cash back on in-store, mobile app, and online purchases with receipt submission, […]]]></summary>
        <author>
            <name>Taylor Names</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the state of the art for poker AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tldie4/what_is_the_state_of_the_art_for_poker_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tldie4/what_is_the_state_of_the_art_for_poker_ai/"/>
        <updated>2022-03-23T20:43:04.000Z</updated>
        <summary type="html"><![CDATA[I see a few articles about Facebook's Pluribus claiming to outperform multiple human pros two years back, but that looks a little dubious to me:
  
The paper explaining it is super basic, which is not what you'd expect from the people who finally solved poker. It has entire pages dedicated to illustrations about the game theory of tic-tac-toe
 
I've seen people arguing about whether it's merely counting cards (which isn't the same as out-strategizing humans)
 
Their metrics for success seem questionable, like they have a graph of cumulative chips won that goes up - which seems great but would also happen if some of the human pros could still outperform it on average
 
 What is the actual best we've done at teaching computers poker strategy?
    submitted by    /u/WouldThatIKnew0  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-generated Summaries in Google Docs]]></title>
        <id>http://ai.googleblog.com/2022/03/auto-generated-summaries-in-google-docs.html</id>
        <link href="http://ai.googleblog.com/2022/03/auto-generated-summaries-in-google-docs.html"/>
        <updated>2022-03-23T20:20:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Mohammad Saleh, Software Engineer, Google Research, Brain Team and Anjuli Kannan, Software Engineer, Google Docs  
For many of us, it can be challenging to keep up with the volume of documents that arrive in our inboxes every day: reports, reviews, briefs, policies and the list goes on. When a new document is received, readers often wish it included a brief summary of the main points in order to effectively prioritize it. However, composing a document summary can be cognitively challenging and time-consuming, especially when a document writer is starting from scratch. 
 To help with this, we recently announced that Google Docs now automatically generates suggestions to aid document writers in creating content summaries, when they are available. Today we describe how this was enab…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What kind of explainability techniques exist for Reinforcement learning?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tlc1vr/what_kind_of_explainability_techniques_exist_for/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tlc1vr/what_kind_of_explainability_techniques_exist_for/"/>
        <updated>2022-03-23T20:11:43.000Z</updated>
        <summary type="html"><![CDATA[I am looking for techniques and libraries to interpret/explain the reinforcement learning model.
    submitted by    /u/Mariam_Dundua  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a story about AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tlbdmw/looking_for_a_story_about_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tlbdmw/looking_for_a_story_about_ai/"/>
        <updated>2022-03-23T20:02:22.000Z</updated>
        <summary type="html"><![CDATA[I'm looking for a story about a company that makes a machine trained to write letters. The company (against their rules) decides to connect the machine to the internet and it rapidly becomes smarter than all of humanity and wipes them out in a handful of days.
 If anyone knows where this story is at, please let me know. Trying to show it to someone, ty!
    submitted by    /u/V4locity  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Paper Explained Video - BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tlauw4/d_paper_explained_video_blip_bootstrapping/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tlauw4/d_paper_explained_video_blip_bootstrapping/"/>
        <updated>2022-03-23T19:55:16.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/X2k7n4FuI7c
 Cross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!
 ​
 OUTLINE:
 0:00 - Intro
 0:50 - Sponsor: Zeta Alpha
 3:40 - Paper Overview
 6:40 - Vision-Language Pre-Training
 11:15 - Contributions of the paper
 14:30 - Model architecture: many parts for many tasks
 19:50 - How data flows in the model
 26:50 - Parameter sharing between the modules
 29:45 - Captioning & Filtering bootstrapping
 41:10 - Fine-tuning the model for downstream tasks
 ​
 Paper: https://arxiv.org/abs/2201.12086
 Code: https://github.com/salesforce/BLIP
 Demo: https://huggingface.co/spaces/Salesforce/BLIP
    submitted by    /u/ykilcher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Hosting AI Art Generative ML Model]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tl9q82/d_hosting_ai_art_generative_ml_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tl9q82/d_hosting_ai_art_generative_ml_model/"/>
        <updated>2022-03-23T19:36:22.000Z</updated>
        <summary type="html"><![CDATA[I am looking for a solution to host an AI art generative ML model for use with an external API. What is the best practice or simplest solution?
 Here's what I've considered so far:
  
Hugging Face (Gradio) - the VQGAN_CLIP model is down, which doesn't instill confidence...
 Docker, cog, and replicate.com - it doesn't seem to have API capability.
 RunwayML - they seem to be pivoting away from such a service.
 NodeJS and some cloud hosting service (Amazon?) - may be complex to set up, are there any straightforward tutorials to host say VQGAN+CLIP? Would the process be different for a diffusion model?
  
How do tools like WOMBO host their models? Anyone have advice before I start going down a dead end?
 I have some software engineering and machine learning experience, but none with deployment.
 Thanks!
    submitted by    /u/WildRaccoon8427  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where to publish a short or demo paper?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tl3sv8/where_to_publish_a_short_or_demo_paper/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tl3sv8/where_to_publish_a_short_or_demo_paper/"/>
        <updated>2022-03-23T17:50:36.000Z</updated>
        <summary type="html"><![CDATA[I've implemented a slightly novel distributed DRL system, which I'd like to submit to a conference as a short or demo paper. Does anybody know a conference where this is possible? ICML and Neurips do not support short papers. ICLR has a blog post track.
 Concerning a full technical paper, I lack the resources (time, computational resources) to thoroughly benchmark my system against seed, ray, apex, r2d2 and so on...
    submitted by    /u/LilHairdy  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Is Path Tracing?]]></title>
        <id>https://blogs.nvidia.com/?p=56003</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/23/what-is-path-tracing/"/>
        <updated>2022-03-23T17:00:10.000Z</updated>
        <summary type="html"><![CDATA[Turn on your TV. Fire up your favorite streaming service. Grab a Coke. A demo of the most important visual technology of our time is as close as your living room couch. Propelled by an explosion in computing power over the past decade and a half, path tracing has swept through visual media. It brings Read article >
The post What Is Path Tracing? appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[For those who want to automate SMS with chatbots]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tl0e5l/for_those_who_want_to_automate_sms_with_chatbots/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tl0e5l/for_those_who_want_to_automate_sms_with_chatbots/"/>
        <updated>2022-03-23T16:44:09.000Z</updated>
        <summary type="html"><![CDATA[A regular SMS message sent from a business – like a promotion or a welcome message – is a common one-way communication. A chatbot, instead, provides a two-way conversation: it can ask questions, process the answer, and continue the communicational path to subscribe, qualify leads and even sell. 
 But here are things to keep in mind:
  
You'll pay for every message.
 You should also keep in mind the character limit. A single SMS message should include no more than 160 characters encoded using the GSM-7 character set.
 If using SMS, you’ll also need to update relevant legal pages like terms of service and privacy policy, with details on how SMS consent will be handled.
 Many countries may have additional regulations. For the US, all SMS messaging in The United States of America is governed by the CTIA – The Wireless Association.
 Another point is media limits. SMS does not allow sending videos or attachments. You may send an image with an MMS but keep in mind the cost of it is different than for a text-only SMS. 
  
Overall, there are more benefits to SMS chatbots, and if you've been thinking on them for a long time, here's a guide to help you starting out https://botscrew.com/blog/sms-chatbot-a-complete-guide-for-business-use-cases/?utm_source=Reddit&utm_medium=artificial&utm_campaign=44643&utm_term=&utm_content=
    submitted by    /u/Avandegraund  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Showcases Novel AI Tools in DRIVE Sim to Advance Autonomous Vehicle Development]]></title>
        <id>https://blogs.nvidia.com/?p=55947</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/23/drive-sim-omniverse-neural-ai-digital-twin/"/>
        <updated>2022-03-23T16:18:32.000Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicle development and validation require the ability to replicate real-world scenarios in simulation. At GTC, NVIDIA founder and CEO Jensen Huang showcased new AI-based tools for NVIDIA DRIVE Sim that accurately reconstruct and modify actual driving scenarios. These tools are enabled by breakthroughs from NVIDIA Research that leverage technologies such as NVIDIA Omniverse platform Read article >
The post NVIDIA Showcases Novel AI Tools in DRIVE Sim to Advance Autonomous Vehicle Development appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Matt Cragun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just Tech: Centering Community-Driven Innovation at the Margins episode 1 with Desmond Patton and Mary Gray]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=824392</id>
        <link href="https://www.microsoft.com/en-us/research/podcast/just-tech-centering-community-driven-innovation-at-the-margins-episode-1-with-desmond-patton-and-mary-gray/"/>
        <updated>2022-03-23T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 133 | March 23, 2022 In “Just Tech: Centering Community-Driven Innovation at the Margins,” Senior Principal Researcher Mary Gray explores how technology and community intertwine and the role technology can play in supporting community-driven innovation and community-based organizations. Dr. Gray and her team are working to bring computer science, engineering, social science, and community […]
The post Just Tech: Centering Community-Driven Innovation at the Margins episode 1 with Desmond Patton and Mary Gray appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Alyssa Hughes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is Project Z-code?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tkwsbu/what_is_project_zcode/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tkwsbu/what_is_project_zcode/"/>
        <updated>2022-03-23T15:42:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Beautiful-Credit-868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Inception Introduces New and Updated Benefits for Startup Members to Accelerate Computing]]></title>
        <id>https://blogs.nvidia.com/?p=55971</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/23/inception-expands-beyond-10k-with-omniverse-climate-startups/"/>
        <updated>2022-03-23T15:00:45.000Z</updated>
        <summary type="html"><![CDATA[This week at GTC, we’re celebrating – celebrating the amazing and impactful work that developers and startups are doing around the world. Nowhere is that more apparent than among the members of our global NVIDIA Inception program, designed to nurture cutting-edge startups who are revolutionizing industries. The program is free for startups of all sizes Read article >
The post NVIDIA Inception Introduces New and Updated Benefits for Startup Members to Accelerate Computing appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Serge Lemonde</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Morse code numbers and abbreviations]]></title>
        <id>https://www.johndcook.com/blog/?p=100670</id>
        <link href="https://www.johndcook.com/blog/2022/03/23/morse-code-numbers/"/>
        <updated>2022-03-23T14:52:54.000Z</updated>
        <summary type="html"><![CDATA[Numbers in Morse code seem a little strange. Here they are: |-------+-------| | Digit | Code | |-------+-------| | 1 | .---- | | 2 | ..--- | | 3 | ...-- | | 4 | ....- | | 5 | ..... | | 6 | -.... | | 7 | --... | | 8 […]
Morse code numbers and abbreviations first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a training & testing set with Neural Networks]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tkuwp9/using_a_training_testing_set_with_neural_networks/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tkuwp9/using_a_training_testing_set_with_neural_networks/"/>
        <updated>2022-03-23T14:15:50.000Z</updated>
        <summary type="html"><![CDATA[Let me start this by saying I am a complete rookie on neural networks. I had one semester when we touched on them and I did a project that involved one. Beyond that, my skill is very limited. Now I am out of school and want to improve on my project. Looking for some expert advice. 
 My project was based on the program found here:
 Price-Forecaster/Stock-RNN-Deep-Learning-TechIndicators.ipynb at master · marcosan93/Price-Forecaster · GitHub 
 I posted an issue and someone added the comment that the programmer has not split the data to X-train,X-test, y-train,y-test so the model will be always biased due to peek ahead. I added early stopping to the program to help with overfitting. Am I understanding things right on where would you add splitting the data? Would you split the data before running the network and use the training data in the epochs, run the network, then recall the best weights for your test set? Just unsure how you do this when the accuracy of the test set may be garbage. 
 My goal is to send a list of stocks through the network, then give me a "report" at the end that shows which stock is most likely to meet or exceed its prediction. Would this just be a confidence interval? Ideally, I would include a goal return and it would say which stock was most likely to reach that return. Since each stock has its own cycles, I was planning to run each stock through the network to find the individual weights. Any advice on this section?
 I hope you can help with some advice. I'd love to see how well my added code ideas would do. Thanks.
    submitted by    /u/charlieexcel  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Roadmap feedback on DS open-source]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tkuszz/d_roadmap_feedback_on_ds_opensource/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tkuszz/d_roadmap_feedback_on_ds_opensource/"/>
        <updated>2022-03-23T14:11:11.000Z</updated>
        <summary type="html"><![CDATA[Hey All!
 So we've been working on an open-source product to help data scientists build faster. We've revamped our long term road map and we'd like to hear thoughts on our direction and vision. This is the roadmap.
 Any insights/feedback is much appreciated!
    submitted by    /u/idomic  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can anyone explain what is the role of MM algorithm in TRPO?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tku6l6/can_anyone_explain_what_is_the_role_of_mm/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tku6l6/can_anyone_explain_what_is_the_role_of_mm/"/>
        <updated>2022-03-23T13:41:20.000Z</updated>
        <summary type="html"><![CDATA[I was reading this post about the TRPO algorithm. But I couldn't understand how we use MM algorithm in TRPO. I also watched some videos, they talked something about maximizing lower bound but I am not able to catch up what they are explaining. Can anyone explain this to me?
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Dueling DQN prerequisite of Prioritized Experience Replay? or which paper I have to start reading first, I already done DQN and DDQN.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tktwy4/is_dueling_dqn_prerequisite_of_prioritized/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tktwy4/is_dueling_dqn_prerequisite_of_prioritized/"/>
        <updated>2022-03-23T13:27:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Professional_Card176  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Use AI to Write Creative Stories in Seconds (Case Study)]]></title>
        <id>https://medium.com/p/c1b6ede3c737</id>
        <link href="https://becominghuman.ai/how-to-use-ai-to-write-creative-stories-in-seconds-case-study-c1b6ede3c737?source=rss----5e5bef33608a---4"/>
        <updated>2022-03-23T13:14:27.000Z</updated>
        <summary type="html"><![CDATA[The use of AI to write creative stories is increasing in popularity.]]></summary>
        <author>
            <name>A. Noah</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Is It Possible to Train an AI to Decode Encrypted Content?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tksq25/d_is_it_possible_to_train_an_ai_to_decode/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tksq25/d_is_it_possible_to_train_an_ai_to_decode/"/>
        <updated>2022-03-23T12:23:26.000Z</updated>
        <summary type="html"><![CDATA[With a theoretically large enough dataset of encrypted content corresponding to their unencrypted versions, it it possible to train an AI to develop a method of decryption that’s better than brute-force? If not all, can this work with some encryption methods?
    submitted by    /u/Plane_Bite3639  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why do GANS work knowing that the discriminator does not take spatial information into account]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tkq4ou/d_why_do_gans_work_knowing_that_the_discriminator/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tkq4ou/d_why_do_gans_work_knowing_that_the_discriminator/"/>
        <updated>2022-03-23T09:32:57.000Z</updated>
        <summary type="html"><![CDATA[f CNNS do not take spatial information into account since filters are looking for features in different locations and output the same feature maps, no matter where the feature is located in the input image, what makes the discriminator robust to real features appearing in random locations ?
    submitted by    /u/Wonderful-Donut-6687  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weird vanilla policy gradient behaviour after adding lstm to the network.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tknadh/weird_vanilla_policy_gradient_behaviour_after/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tknadh/weird_vanilla_policy_gradient_behaviour_after/"/>
        <updated>2022-03-23T05:58:44.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I am trying vanilla policy gradient algorithm of gym's bipedal walker. My algorithm was not converging so I added lstm to the network. I expected that somehow the agent will use the memory to converge even faster but after adding it I observed that LSTM is somehow making the agent worse. Below are the graphs of without and with lstm.
 ​
 without lstm
 ​
 with lstm
 ​
 orange-with lstm and yellow-without lstm
 Can anyone explain to me why is this happening?
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft improves its AI translations with Z-Code – TechCrunch]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tkmvbd/microsoft_improves_its_ai_translations_with_zcode/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tkmvbd/microsoft_improves_its_ai_translations_with_zcode/"/>
        <updated>2022-03-23T05:29:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nonaime7777777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Avoid RegTech myopia with a data-centric approach]]></title>
        <id>https://www.datasciencecentral.com/?p=56771</id>
        <link href="https://www.datasciencecentral.com/avoid-regtech-myopia-with-a-data-centric-approach/"/>
        <updated>2022-03-23T03:29:31.000Z</updated>
        <summary type="html"><![CDATA[The US Securities and Exchange Commission (SEC), which regulates public company securities, recently proposed its own climate impact reporting requirement. Many US public companies already voluntarily publish information on this topic for shareholders who have been asking for those details. But various state GOP attorneys general are already questioning the SEC’s ability to impose the… Read More »Avoid RegTech myopia with a data-centric approach
The post Avoid RegTech myopia with a data-centric approach appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The long game: Feedback loops and desiloed systems by design (Part II of II)]]></title>
        <id>https://www.datasciencecentral.com/?p=56776</id>
        <link href="https://www.datasciencecentral.com/the-long-game-feedback-loops-and-desiloed-systems-by-design-part-ii-of-ii/"/>
        <updated>2022-03-23T03:26:26.000Z</updated>
        <summary type="html"><![CDATA[The previous post covered the problem of oversiloing. Systems thinking, I pointed out, can help reduce the practice of siloing when it’s not necessary.  In earlier posts, I’ve contrasted the difference between provincial IT and data-centric IT:  Provincial IT is no longer necessary given the advances in compute, networking and storage we’ve seen over the… Read More »The long game: Feedback loops and desiloed systems by design (Part II of II)
The post The long game: Feedback loops and desiloed systems by design (Part II of II) appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alan Morrison</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to check if a paper has been plagiarized?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tkjbsw/d_how_to_check_if_a_paper_has_been_plagiarized/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tkjbsw/d_how_to_check_if_a_paper_has_been_plagiarized/"/>
        <updated>2022-03-23T02:03:43.000Z</updated>
        <summary type="html"><![CDATA[Any recommendations for tools to check if a paper has been plagiarized? I have an unfortunate situation that may require a thorough examination of another researcher’s work. For example, how did they determine that Siraj Raval’s work was plagiarized? I assume the big ML conferences perform some sort of automated check for plagiarism, right?
    submitted by    /u/purplebrown_updown  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expedite IVR development with industry grammars on Amazon Lex]]></title>
        <id>1eb961f694869c6a2187d10121ba0a13554186d1</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/expedite-ivr-development-with-industry-grammars-on-amazon-lex/"/>
        <updated>2022-03-23T01:17:18.000Z</updated>
        <summary type="html"><![CDATA[Amazon Lex is a service for building conversational interfaces into any application using voice and text. With Amazon Lex, you can easily build sophisticated, natural language, conversational bots (chatbots), virtual agents, and interactive voice response (IVR) systems. You can now use industry grammars to accelerate IVR development on Amazon Lex as part of your IVR […]]]></summary>
        <author>
            <name>John Heater</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Metadata Improves Security, Quality, and Transparency]]></title>
        <id>https://www.datasciencecentral.com/?p=56737</id>
        <link href="https://www.datasciencecentral.com/how-metadata-improves-security-quality-and-transparency/"/>
        <updated>2022-03-22T22:48:54.000Z</updated>
        <summary type="html"><![CDATA[How does Spotify battle against a giant like Apple? One word: data. With machine learning and AI, Spotify creates value for its users by providing a more personalized and bespoke experience. Let’s take a quick look at the layers of aggregate information that are used to enhance their platform: Spotify uses natural language processing (NLP)… Read More »How Metadata Improves Security, Quality, and Transparency
The post How Metadata Improves Security, Quality, and Transparency appeared first on Data Science Central.]]></summary>
        <author>
            <name>Lewis Wynne-Jones</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Easily migrate your IVR flows to Amazon Lex using the IVR migration tool]]></title>
        <id>229dd128c741e3f461444de5ff03100092aa8119</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/easily-migrate-your-ivr-flows-to-amazon-lex-using-the-ivr-migration-tool/"/>
        <updated>2022-03-22T22:40:21.000Z</updated>
        <summary type="html"><![CDATA[This post was co-written by John Heater, SVP of the Contact Center Practice at NeuraFlash. NeuraFlash is an Advanced AWS Partner with over 40 collective years of experience in the voice and automation space. With a dedicated team of conversation designers, data engineers, and AWS developers, NeuraFlash helps customers take advantage of the power of Amazon […]]]></summary>
        <author>
            <name>John Heater</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Relative error metrics comparing Model complexity versus error]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tkda9c/d_relative_error_metrics_comparing_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tkda9c/d_relative_error_metrics_comparing_model/"/>
        <updated>2022-03-22T21:11:51.000Z</updated>
        <summary type="html"><![CDATA[Is anyone aware of metrics or measures that classify the error relative the complexity of the model. If so could anyone provide some references where people have introduced such a notion?
    submitted by    /u/AbjectDrink3276  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dopamine Helps Signal to Neurons When to Start a Movement]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tkcmvw/dopamine_helps_signal_to_neurons_when_to_start_a/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tkcmvw/dopamine_helps_signal_to_neurons_when_to_start_a/"/>
        <updated>2022-03-22T20:43:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSTM encoder in the policy?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tkbc2a/lstm_encoder_in_the_policy/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tkbc2a/lstm_encoder_in_the_policy/"/>
        <updated>2022-03-22T19:46:03.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I'm trying to code an actor net that consists of an LSTM followed by two fully connected layers. The policy is therefore parameterized using an LSTM. The LSTM is meant to enable agents to remember the history of states in which they have observed other agents, and improve their ability to model other agents’ policies.
 Can someone refer me to PyTorch code for such an architecture?
 Thanks!
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["We've got incredible potential here to use the Sophiaverse virtual world, not just as a fun way for people to play or make money by selling NFTs they create, but also as a way to get human values and culture and the pragmatics of human interaction across to AI."- Ben Goertzel, CEO at SingularityNET]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tkakgw/weve_got_incredible_potential_here_to_use_the/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tkakgw/weve_got_incredible_potential_here_to_use_the/"/>
        <updated>2022-03-22T19:11:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/thedyezwfl  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Google Research: Self-Consistency Improves Chain of Thought Reasoning in Language Models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tkaihr/r_google_research_selfconsistency_improves_chain/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tkaihr/r_google_research_selfconsistency_improves_chain/"/>
        <updated>2022-03-22T19:09:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/ReasonablyBadass  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Business Analytics from Application Logs and SQL Server Database using Splunk]]></title>
        <id>https://www.datasciencecentral.com/?p=56741</id>
        <link href="https://www.datasciencecentral.com/business-analytics-from-application-logs-and-sql-server-database-using-splunk/"/>
        <updated>2022-03-22T19:05:47.000Z</updated>
        <summary type="html"><![CDATA[Introduction/ Problem  Splunk is a well-known log management tool. Splunk mines log from different machines in real-time and can be used to monitor, search, and analyze gathered data. It is a Big Data log management tool that can give insight from the unstructured data stored in the Splunk indexes. Splunk analytics helps turn unstructured log… Read More »Business Analytics from Application Logs and SQL Server Database using Splunk 
The post Business Analytics from Application Logs and SQL Server Database using Splunk  appeared first on Data Science Central.]]></summary>
        <author>
            <name>Roopesh Uniyal</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P][R] Hear what you see: Autoencoder to convert video to audio]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tk8ika/pr_hear_what_you_see_autoencoder_to_convert_video/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tk8ika/pr_hear_what_you_see_autoencoder_to_convert_video/"/>
        <updated>2022-03-22T17:41:16.000Z</updated>
        <summary type="html"><![CDATA[GitHub: https://github.com/muxamilian/wysiwyh
 WYSIWHY is a neural network that transforms an input video into an audio sequence in real time. It does so by compressing each image using an autoencoder and interpreting the resulting code as a frequency range.
 This could potentially be useful for the visually impaired, helping with indoor navigation. It could also be used to transform an infrared or ultraviolet video to sound and thus enable one to perceive otherwise invisible colors.
 Demo; recommended with sound turned on
 In a usual autoencoder, the elements of the code vector are independent of each other. This makes subtle differences between adjacent elements of the code vector hard to perceive for humans. The resulting audio sequence would sound like indistinguishable white noise. For WYSIWYH thus an encoder is used which hierarchically structures the code, meaning that large differences in the input image result in large differences in the output code. This also results in adjacent elements of the code vector being correlated. This makes the autoencoder’s code more friendly to human perception.
 Overall approach
    submitted by    /u/muxamilian  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research Proposes MaskGIT: A New Deep Learning Technique Based on Bi-Directional Generative Transformers For High-Quality and Fast Image Synthesis]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk8emn/google_research_proposes_maskgit_a_new_deep/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk8emn/google_research_proposes_maskgit_a_new_deep/"/>
        <updated>2022-03-22T17:36:25.000Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs), with their capacity of producing high-quality images, have been the leading technology in image generation for the past couple of years. Nevertheless, their minimax learning mechanism brought out different limits, such as training instability and mode collapse (i.e., when all the produced samples belong to a small set of samples).
 Recently, Generative Transformer models are beginning to match, or even surpass, the performances of GANs. The simple idea is to learn a function to encode the input image into a quantized sequence and then train an autoregressive Transformer on a sequence prediction task (i.e., predict an image token, given all the previous image tokens). This learning is based on maximum likelihood and thus not affected by the same issue…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Omniverse Upgrade Delivers Extraordinary Benefits to 3D Content Creators]]></title>
        <id>https://blogs.nvidia.com/?p=56162</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/gtc-omniverse-create-view-machinima-update/"/>
        <updated>2022-03-22T17:00:56.000Z</updated>
        <summary type="html"><![CDATA[At GTC, NVIDIA announced significant updates for millions of creators using the NVIDIA Omniverse real-time 3D design collaboration platform. The announcements kicked off with updates to the Omniverse apps Create, Machinima and Showroom, with an immement View release. Powered by GeForce RTX and NVIDIA RTX GPUs, they dramatically accelerate 3D creative workflows. New Omniverse Connections Read article >
The post NVIDIA Omniverse Upgrade Delivers Extraordinary Benefits to 3D Content Creators appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>TJ Galda</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[At GTC: NVIDIA RTX Professional Laptop GPUs Debut, New NVIDIA Studio Laptops, a Massive Omniverse Upgrade and NVIDIA Canvas Update]]></title>
        <id>https://blogs.nvidia.com/?p=56102</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/gtc-rtx-studio-updates/"/>
        <updated>2022-03-22T17:00:39.000Z</updated>
        <summary type="html"><![CDATA[Digital artists and creative professionals have plenty to be excited about at NVIDIA GTC. Impressive NVIDIA Studio laptop offerings from ASUS and MSI launch with upgraded RTX GPUs, providing more options for professional content creators to elevate and expand creative possibilities. NVIDIA Omniverse gets a significant upgrade — including updates to the Omniverse Create, Machinima Read article >
The post At GTC: NVIDIA RTX Professional Laptop GPUs Debut, New NVIDIA Studio Laptops, a Massive Omniverse Upgrade and NVIDIA Canvas Update appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Gerardo Delgado</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft Translator enhanced with Z-code Mixture of Experts models]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=827599</id>
        <link href="https://www.microsoft.com/en-us/research/blog/microsoft-translator-enhanced-with-z-code-mixture-of-experts-models/"/>
        <updated>2022-03-22T17:00:00.000Z</updated>
        <summary type="html"><![CDATA[Translator, a Microsoft Azure Cognitive Service, is adopting Z-code Mixture of Experts models, a breakthrough AI technology that significantly improves the quality of production translation models. As a component of Microsoft’s larger XYZ-code initiative to combine AI models for text, vision, audio, and language, Z-code supports the creation of AI systems that can speak, see, […]
The post Microsoft Translator enhanced with Z-code Mixture of Experts models appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Powering the next generation of trustworthy AI in a confidential cloud using NVIDIA GPUs]]></title>
        <id>https://www.microsoft.com/en-us/research/?p=828337</id>
        <link href="https://www.microsoft.com/en-us/research/blog/powering-the-next-generation-of-trustworthy-ai-in-a-confidential-cloud-using-nvidia-gpus/"/>
        <updated>2022-03-22T17:00:00.000Z</updated>
        <summary type="html"><![CDATA[Cloud computing is powering a new age of data and AI by democratizing access to scalable compute, storage, and networking infrastructure and services. Thanks to the cloud, organizations can now collect data at an unprecedented scale and use it to train complex models and generate insights.   While this increasing demand for data has unlocked new […]
The post Powering the next generation of trustworthy AI in a confidential cloud using NVIDIA GPUs appeared first on Microsoft Research.]]></summary>
        <author>
            <name>Brenda Potts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I decided to make roads out of the Lorenz attractor…]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk7g4r/i_decided_to_make_roads_out_of_the_lorenz/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk7g4r/i_decided_to_make_roads_out_of_the_lorenz/"/>
        <updated>2022-03-22T16:53:49.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/TheRealMandelbrotSet  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Amazon Search achieves low-latency, high-throughput T5 inference with NVIDIA Triton on AWS]]></title>
        <id>32a555920fe3b0dc5224f950b745f309fba00220</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/how-amazon-search-achieves-low-latency-high-throughput-t5-inference-with-nvidia-triton-on-aws/"/>
        <updated>2022-03-22T16:47:40.000Z</updated>
        <summary type="html"><![CDATA[Amazon Search’s vision is to enable customers to search effortlessly. Our spelling correction helps you find what you want even if you don’t know the exact spelling of the intended words. In the past, we used classical machine learning (ML) algorithms with manual feature engineering for spelling correction. To make the next generational leap in […]]]></summary>
        <author>
            <name>RJ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keynote Wrap Up: Turning Data Centers into ‘AI Factories,’ NVIDIA CEO Intros Hopper Architecture, H100 GPU, New Supercomputers, Software]]></title>
        <id>https://blogs.nvidia.com/?p=55990</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/ai-factories-hopper-h100-nvidia-ceo-jensen-huang/"/>
        <updated>2022-03-22T16:45:40.000Z</updated>
        <summary type="html"><![CDATA[Promising to transform trillion-dollar industries and address the “grand challenges” of our time, NVIDIA founder and CEO Jensen Huang Tuesday shared a vision of an era where intelligence is created on an industrial scale and woven into real and virtual worlds. Kicking off NVIDIA’s GTC conference, Huang introduced new silicon — including the new Hopper Read article >
The post Keynote Wrap Up: Turning Data Centers into ‘AI Factories,’ NVIDIA CEO Intros Hopper Architecture, H100 GPU, New Supercomputers, Software appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlimited Data, Unlimited Possibilities: UF Health and NVIDIA Build World’s Largest Clinical Language Generator]]></title>
        <id>https://blogs.nvidia.com/?p=55986</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/uf-health-syngatortron-ai-synthetic-clinical-data/"/>
        <updated>2022-03-22T16:45:38.000Z</updated>
        <summary type="html"><![CDATA[The University of Florida’s academic health center, UF Health, has teamed up with NVIDIA to develop a neural network that generates synthetic clinical data — a powerful resource that researchers can use to train other AI models in healthcare. Trained on a decade of data representing more than 2 million patients, SynGatorTron is a language Read article >
The post Unlimited Data, Unlimited Possibilities: UF Health and NVIDIA Build World’s Largest Clinical Language Generator appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Anthony Costa</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New NVIDIA RTX GPUs Tackle Demanding Professional Workflows and Hybrid Work, Enabling Creation From Anywhere]]></title>
        <id>https://blogs.nvidia.com/?p=56037</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/rtx-tackle-pro-workflows/"/>
        <updated>2022-03-22T16:45:36.000Z</updated>
        <summary type="html"><![CDATA[Remote work and hybrid workplaces are the new normal for professionals in many industries. Teams spread throughout the world are expected to create and collaborate while maintaining top productivity and performance. Businesses use the NVIDIA RTX platform to enable their workers to keep up with the most demanding workloads, from anywhere. And today, NVIDIA is Read article >
The post New NVIDIA RTX GPUs Tackle Demanding Professional Workflows and Hybrid Work, Enabling Creation From Anywhere appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Bob Pette</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First Wave of Startups Harnesses UK’s Most Powerful Supercomputer to Power Digital Biology Breakthroughs]]></title>
        <id>https://blogs.nvidia.com/?p=55987</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/startups-harness-cambridge-1-supercomputer/"/>
        <updated>2022-03-22T16:45:30.000Z</updated>
        <summary type="html"><![CDATA[Four NVIDIA Inception members have been selected as the first cohort of startups to access Cambridge-1, the U.K.’s most powerful supercomputer. The system will help British companies Alchemab Therapeutics, InstaDeep, Peptone and Relation Therapeutics enable breakthroughs in digital biology. Officially launched in July, Cambridge-1 — an NVIDIA DGX SuperPOD cluster powered by NVIDIA DGX A100 Read article >
The post First Wave of Startups Harnesses UK’s Most Powerful Supercomputer to Power Digital Biology Breakthroughs appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Craig Rhodes</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nvidia unveils latest chip to speed up AI computing]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk75y0/nvidia_unveils_latest_chip_to_speed_up_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk75y0/nvidia_unveils_latest_chip_to_speed_up_ai/"/>
        <updated>2022-03-22T16:40:45.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nonaime7777777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Software Company Releases Chrome Extension That Detects AI-Generated Profile Pictures]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk75t9/software_company_releases_chrome_extension_that/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk75t9/software_company_releases_chrome_extension_that/"/>
        <updated>2022-03-22T16:40:33.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/nonaime7777777  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Omniverse Ecosystem Expands 10x, Amid New Features and Services for Developers, Enterprises and Creators]]></title>
        <id>https://blogs.nvidia.com/?p=56065</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/omniverse-ecosystem-expands/"/>
        <updated>2022-03-22T16:31:44.000Z</updated>
        <summary type="html"><![CDATA[When it comes to creating and connecting virtual worlds, over 150,000 individuals have downloaded NVIDIA Omniverse to make huge leaps in transforming 3D design workflows and achieve new heights of real-time, physically accurate simulations. At GTC, NVIDIA today announced new releases and updates for Omniverse — including the latest Omniverse Connectors and libraries — expanding Read article >
The post NVIDIA Omniverse Ecosystem Expands 10x, Amid New Features and Services for Developers, Enterprises and Creators appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Richard Kerris</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Combines With Decentralized Autonomous Organization]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk6u6j/ai_combines_with_decentralized_autonomous/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk6u6j/ai_combines_with_decentralized_autonomous/"/>
        <updated>2022-03-22T16:26:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/getrich_or_diemining  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Unveils Isaac Nova Orin to Accelerate Development of Autonomous Mobile Robots]]></title>
        <id>https://blogs.nvidia.com/?p=55992</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/nvidia-isaac-nova-orin-amrs/"/>
        <updated>2022-03-22T16:21:27.000Z</updated>
        <summary type="html"><![CDATA[Next time socks, cereal or sandpaper shows up in hours delivered to your doorstep, consider the behind-the-scenes logistics acrobatics that help get them there so fast. Order fulfillment is a massive industry of moving parts. Heavily supported by autonomous mobile robots (AMRs), warehouses can span 1 million square feet, expanding and reconfiguring to meet demands. Read article >
The post NVIDIA Unveils Isaac Nova Orin to Accelerate Development of Autonomous Mobile Robots appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Gerard Andrews</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Driving on Air: Lucid Group Builds Intelligent EVs on NVIDIA DRIVE]]></title>
        <id>https://blogs.nvidia.com/?p=55948</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/lucid-motors-intelligent-evs-nvidia-drive/"/>
        <updated>2022-03-22T16:15:54.000Z</updated>
        <summary type="html"><![CDATA[Lucid Group may be a newcomer to the electric vehicle market, but its entrance has been grand. The electric automaker announced at GTC that its current and future fleets are built on NVIDIA DRIVE Hyperion for programmable, intelligent capabilities. By developing on the scalable, software-defined platform, Lucid ensures its vehicles are always at the cutting Read article >
The post Driving on Air: Lucid Group Builds Intelligent EVs on NVIDIA DRIVE appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Danny Shapiro</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA DRIVE Continues Industry Momentum With $11 Billion Pipeline as DRIVE Orin Enters Production]]></title>
        <id>https://blogs.nvidia.com/?p=55946</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/drive-orin-byd-production/"/>
        <updated>2022-03-22T16:14:54.000Z</updated>
        <summary type="html"><![CDATA[NVIDIA DRIVE Hyperion and DRIVE Orin are gaining ground in the industry. At NVIDIA GTC, BYD, the world’s second-largest electric vehicle maker, announced it is building its next-generation fleets on the DRIVE Hyperion architecture. This platform, based on DRIVE Orin, is now in production, and powering a wide ecosystem of 25 EV makers building software-defined Read article >
The post NVIDIA DRIVE Continues Industry Momentum With $11 Billion Pipeline as DRIVE Orin Enters Production appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Ali Kani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing NVIDIA DRIVE Map: Scalable, Multi-Modal Mapping Engine Accelerates Deployment of Level 3 and Level 4 Autonomy]]></title>
        <id>https://blogs.nvidia.com/?p=55943</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/drive-map-multi-modal-mapping-engine/"/>
        <updated>2022-03-22T16:11:16.000Z</updated>
        <summary type="html"><![CDATA[With a detailed knowledge of the world and everything in it, maps provide the foresight AI uses to make advanced and safe driving decisions. At his GTC keynote, NVIDIA founder and CEO Jensen Huang introduced NVIDIA DRIVE Map, a multimodal mapping platform designed to enable the highest levels of autonomy while improving safety. It combines Read article >
The post Announcing NVIDIA DRIVE Map: Scalable, Multi-Modal Mapping Engine Accelerates Deployment of Level 3 and Level 4 Autonomy appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Zvi Greenstein</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing NVIDIA DRIVE Hyperion 9: Next-Generation Platform for Software-Defined Autonomous Vehicle Fleets]]></title>
        <id>https://blogs.nvidia.com/?p=55945</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/drive-hyperion-9-atlan/"/>
        <updated>2022-03-22T16:10:24.000Z</updated>
        <summary type="html"><![CDATA[NVIDIA DRIVE Hyperion is taking software-defined vehicle architectures to the next level. At his GTC keynote, NVIDIA founder and CEO Jensen Huang announced DRIVE Hyperion 9, the next generation of the open platform for automated and autonomous vehicles. The programmable architecture, slated for 2026 production vehicles, is built on multiple DRIVE Atlan computers to achieve Read article >
The post Introducing NVIDIA DRIVE Hyperion 9: Next-Generation Platform for Software-Defined Autonomous Vehicle Fleets appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Gary Hicok</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alphabet is Spinning out Division X 'Sandbox AQ']]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk6akk/alphabet_is_spinning_out_division_x_sandbox_aq/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk6akk/alphabet_is_spinning_out_division_x_sandbox_aq/"/>
        <updated>2022-03-22T16:02:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Beautiful-Credit-868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranking System]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tk668j/ranking_system/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tk668j/ranking_system/"/>
        <updated>2022-03-22T15:57:26.000Z</updated>
        <summary type="html"><![CDATA[I am making a mobile app which contains newsfeed page, where I want to show the users post in ranking order, posts can be ranked based on the post content and the user rating count and feedback who has posted it, I don't want to spend more time on building a ranking system so what way or pathway I can take? Is there any API, system or anything already built which I can integrate into my app directly to rank post?
    submitted by    /u/aliazlanaziz  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Siemens Gamesa Taps NVIDIA Digital Twin Platform for Scientific Computing to Accelerate Clean Energy Transition]]></title>
        <id>https://blogs.nvidia.com/?p=55989</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/siemens-gamesa-wind-farms-digital-twins/"/>
        <updated>2022-03-22T15:57:21.000Z</updated>
        <summary type="html"><![CDATA[Siemens Gamesa Renewable Energy is working with NVIDIA to create physics-informed digital twins of wind farms — groups of wind turbines used to produce electricity. The company has thousands of turbines around the globe that light up schools, homes, hospitals and factories with clean energy. In total they generate over 100 gigawatts of wind power, Read article >
The post Siemens Gamesa Taps NVIDIA Digital Twin Platform for Scientific Computing to Accelerate Clean Energy Transition appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Bhoomi Gadhia</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tk61vh/r_improving_anatomical_plausibility_in_medical/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tk61vh/r_improving_anatomical_plausibility_in_medical/"/>
        <updated>2022-03-22T15:52:00.000Z</updated>
        <summary type="html"><![CDATA[https://arxiv.org/abs/2203.10977
    submitted by    /u/gaggi_94  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Unveils Onramp to Hybrid Quantum Computing]]></title>
        <id>https://blogs.nvidia.com/?p=55950</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/hybrid-quantum-computing-ecosystem/"/>
        <updated>2022-03-22T15:47:16.000Z</updated>
        <summary type="html"><![CDATA[We’re working with leaders in quantum computing to build the tools developers will need to program tomorrow’s ultrahigh performance systems. Today’s high-performance computers are simulating quantum computing jobs at scale and with performance far beyond what’s possible on today’s smaller and error-prone quantum systems. In this way, classical HPC systems are helping quantum researchers chart Read article >
The post NVIDIA Unveils Onramp to Hybrid Quantum Computing appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Timothy Costa</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speed Dialer: How AT&T Rings Up New Opportunities With Data Science]]></title>
        <id>https://blogs.nvidia.com/?p=55977</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/att-data-science-rapids/"/>
        <updated>2022-03-22T15:45:47.000Z</updated>
        <summary type="html"><![CDATA[AT&T’s wireless network connects more than 100 million subscribers from the Aleutian Islands to the Florida Keys, spawning a big data sea. Abhay Dabholkar runs a research group that acts like a lighthouse on the lookout for the best tools to navigate it. “It’s fun, we get to play with new tools that can make Read article >
The post Speed Dialer: How AT&T Rings Up New Opportunities With Data Science appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Karthikeyan Rajendran</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This Is The Reason Why Sleep Robots Are Becoming The Perfect Bed Mates]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk5tgc/this_is_the_reason_why_sleep_robots_are_becoming/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk5tgc/this_is_the_reason_why_sleep_robots_are_becoming/"/>
        <updated>2022-03-22T15:41:06.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sopadebombillas  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Hopper GPU Architecture Accelerates Dynamic Programming Up to 40x Using New DPX Instructions]]></title>
        <id>https://blogs.nvidia.com/?p=55985</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/nvidia-hopper-accelerates-dynamic-programming-using-dpx-instructions/"/>
        <updated>2022-03-22T15:33:11.000Z</updated>
        <summary type="html"><![CDATA[The NVIDIA Hopper GPU architecture unveiled today at GTC will accelerate dynamic programming — a problem-solving technique used in algorithms for genomics, quantum  computing, route optimization and more — by up to 40x with new DPX instructions. An instruction set built into NVIDIA H100 GPUs, DPX will help developers write code to achieve speedups on Read article >
The post NVIDIA Hopper GPU Architecture Accelerates Dynamic Programming Up to 40x Using New DPX Instructions appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Dion Harris</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[H100 Transformer Engine Supercharges AI Training, Delivering Up to 6x Higher Performance Without Losing Accuracy]]></title>
        <id>https://blogs.nvidia.com/?p=56129</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/"/>
        <updated>2022-03-22T15:31:37.000Z</updated>
        <summary type="html"><![CDATA[The largest AI models can require months to train on today’s computing platforms. That’s too slow for businesses. AI, high performance computing and data analytics are growing in complexity with some models, like large language ones, reaching trillions of parameters. The NVIDIA Hopper architecture is built from the ground up to accelerate these next-generation AI Read article >
The post H100 Transformer Engine Supercharges AI Training, Delivering Up to 6x Higher Performance Without Losing Accuracy appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Dave Salvator</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVIDIA Maxine Reinvents Real-Time Communication With AI]]></title>
        <id>https://blogs.nvidia.com/?p=56050</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/22/maxine-reinvents-communication-ai/"/>
        <updated>2022-03-22T15:25:39.000Z</updated>
        <summary type="html"><![CDATA[Everyone wants to be heard. And with more people than ever in video calls or live streaming from their home offices, rich audio free from echo hiccups and background noises like barking dogs is key to better sounding online experiences. NVIDIA Maxine offers GPU-accelerated, AI-enabled software development kits to help developers build scalable, low-latency audio Read article >
The post NVIDIA Maxine Reinvents Real-Time Communication With AI appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Rick Champagne</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Hugging Face 🤗 can contribute to the Deep Reinforcement Learning Ecosystem?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tk5f00/how_hugging_face_can_contribute_to_the_deep/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tk5f00/how_hugging_face_can_contribute_to_the_deep/"/>
        <updated>2022-03-22T15:22:31.000Z</updated>
        <summary type="html"><![CDATA[Hey there! 👋
 I'm Thomas Simonini from Hugging Face 🤗. I work on building tools, environments and integrating RL libraries to empower researchers and RL enthusiasts. I was wondering how Hugging Face can be useful to you in the Deep Reinforcement Learning Ecosystem? What do you need as RL researcher/enthusiast/engineer and how we can help you?
 For now:
  
We integrated Stable-baselines3 to the Hub** such that you can: 
 Easily host and test your saved models.
 Load powerful, trained models from the community
 
  
https://preview.redd.it/n0b2s1gndyo81.jpg?width=1920&format=pjpg&auto=webp&s=eb62a1f4323b12a5c1eb9d7bcb44ebbc6bae579f
  
We're currently integrating more libraries (RL-Zoo, CleanRL...)
 We're working on building tools that allow you to generate a replay video of your agent and test it.
 We're building open-source RL environments such as snowball-fight
 And finally, we're working on state of the art's research with Decision Transformers, Embodied environments, etc.
  
But I would love to know what do you need as RL researcher/enthusiast/engineer and how we can help you?
 Thanks for your feedback,
 📢 To keep in touch is to join our discord server to exchange with us and with the community.
    submitted by    /u/cranthir_  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tips for Weaving and Implementing a Successful Data Mesh]]></title>
        <id>https://www.datasciencecentral.com/?p=56731</id>
        <link href="https://www.datasciencecentral.com/tips-for-weaving-and-implementing-a-successful-data-mesh/"/>
        <updated>2022-03-22T14:56:32.000Z</updated>
        <summary type="html"><![CDATA[For years, organizations have been working steadily to consolidate their data into a single place via a data warehouse, or more recently, a data lake. Data lakes offer key advantages over data warehouses, data marts, and traditional databases that require data to be structured and organized in particular ways. However, businesses have found that they… Read More »Tips for Weaving and Implementing a Successful Data Mesh
The post Tips for Weaving and Implementing a Successful Data Mesh appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alberto Pan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Frequency Shift Keying]]></title>
        <id>https://www.johndcook.com/blog/?p=100600</id>
        <link href="https://www.johndcook.com/blog/2022/03/22/multiple-frequency-shift-keying/"/>
        <updated>2022-03-22T14:51:00.000Z</updated>
        <summary type="html"><![CDATA[A few days ago I wrote about Frequency Shift Keying (FSK), a way to encode digital data in an analog signal using two frequencies. The extension to multiple frequencies is called, unsurprisingly, Multiple Frequency Shift Keying (MFSK). What is surprising is how MFSK sounds. When I first heard MFSK I immediately recognized it as an […]
Multiple Frequency Shift Keying first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] STUMPY v1.11.0 Released for Modern Time Series Analysis]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tk41hz/d_stumpy_v1110_released_for_modern_time_series/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tk41hz/d_stumpy_v1110_released_for_modern_time_series/"/>
        <updated>2022-03-22T14:18:33.000Z</updated>
        <summary type="html"><![CDATA[We're happy to announce the release of STUMPY v1.11.0! This version includes the oft requested Minkowski (p-norm) Distance, support for Multi-dimensional Motif Discovery, new Annotation vector tutorials, and enhancements for Pan Matrix Profiles!
 https://github.com/TDAmeritrade/stumpy
    submitted by    /u/slaw07  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unraveling the Mystery Behind Background Filters in Video Calling Apps]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk4141/unraveling_the_mystery_behind_background_filters/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk4141/unraveling_the_mystery_behind_background_filters/"/>
        <updated>2022-03-22T14:18:01.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/VikasOjha666  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[how to set the distance_threshold value and make it effective]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tk3juz/how_to_set_the_distance_threshold_value_and_make/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tk3juz/how_to_set_the_distance_threshold_value_and_make/"/>
        <updated>2022-03-22T13:54:56.000Z</updated>
        <summary type="html"><![CDATA[I use the [RL Robotics library](https://github.com/Farama-Foundation/Gym-Robotics) to simulate robots. In the Fetchxxx-Env, the variable distance_threshold is set as 0.05 default to determine whether it completed successfully a task. I try to change it by using 
 env.distance_threshold = 0.01(or other value) 
 Because I think 0.05(m) is too rough for some tasks. But it doesn't work! (When the success_rate is 1.0, the distance between the achieved_goal and the desired_goal is 0.04x, which is larger than I set.) 
 What should I do to change the conditions for determining whether a task is complete? Can anyone give me some advice or information?
    submitted by    /u/Due_Advertising6542  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Need help trying to implement a paper]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tk2ya5/need_help_trying_to_implement_a_paper/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tk2ya5/need_help_trying_to_implement_a_paper/"/>
        <updated>2022-03-22T13:24:58.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I'm an undergraduate CS student trying to re-implement this paper for my University project.
 I have some experience in Machine Learning (Supervised Learning) and implemented some basic projects myself, but I'm a little bit confused about where to begin, I've found two different implementations online but I don't think that's gonna help me a lot as I'm not that experienced in this field.
 I would appreciate any suggestions on how I can understand the paper and implement it myself (not just a copy/paste from someone else's code) or the libraries that can help me, etc.
 Sorry for my bad English, I'm not a native speaker
 thanks a lot :)
    submitted by    /u/ahmadreza_hadi  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digit recogniser — Artificial Neural Network (ANN)]]></title>
        <id>https://medium.com/p/f8ca23426478</id>
        <link href="https://becominghuman.ai/digit-recogniser-artificial-neural-network-ann-f8ca23426478?source=rss----5e5bef33608a---4"/>
        <updated>2022-03-22T13:14:02.000Z</updated>
        <summary type="html"><![CDATA[Using the Artificial Neural Network (ANN) to make a churn model, we will create a model that predicts a handwritten digit. (with source…]]></summary>
        <author>
            <name>Aviral Bhardwaj</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Modernize Enterprise Data and Analytics Platform (Part 2 of 4)]]></title>
        <id>https://www.datasciencecentral.com/?p=56657</id>
        <link href="https://www.datasciencecentral.com/how-to-modernize-enterprise-data-and-analytics-platform-part-2-of-4/"/>
        <updated>2022-03-22T13:06:47.000Z</updated>
        <summary type="html"><![CDATA[In many cases, for an enterprise to build its digital business technology platform, it must modernize its traditional data and analytics architecture. A modern data and analytics platform should be built on services-based principles and architecture. Introduction part 1, provided a conceptual-level reference architecture of a traditional Data and Analytics (D&A) platform. This part, provides… Read More »How to Modernize Enterprise Data and Analytics Platform (Part 2 of 4)
The post How to Modernize Enterprise Data and Analytics Platform (Part 2 of 4) appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alaa Mahjoub, M.Sc. Eng.</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in the News (the period of March 21st, 2022)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk1szs/artificial_intelligence_in_the_news_the_period_of/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk1szs/artificial_intelligence_in_the_news_the_period_of/"/>
        <updated>2022-03-22T12:25:12.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Beautiful-Credit-868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Best Machine Learning Books to read in 2022 -]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk0woa/best_machine_learning_books_to_read_in_2022/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk0woa/best_machine_learning_books_to_read_in_2022/"/>
        <updated>2022-03-22T11:34:18.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Could it be, that people spend a lot of money on AIs, which are generating realistic images and then sell the copyright of these images?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tk0w2q/could_it_be_that_people_spend_a_lot_of_money_on/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tk0w2q/could_it_be_that_people_spend_a_lot_of_money_on/"/>
        <updated>2022-03-22T11:33:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/xXNOdrugsForMEXx  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Call for submissions: Foundations of Digital Games 2022, 5-8 September (Athens, GR & hybrid)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tk07jt/n_call_for_submissions_foundations_of_digital/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tk07jt/n_call_for_submissions_foundations_of_digital/"/>
        <updated>2022-03-22T10:50:28.000Z</updated>
        <summary type="html"><![CDATA[FDG2022: Foundations of Digital Games 2022Athens, Greece, September 5-8, 2022Conference website: http://fdg2022.org/
 Foundations of Digital Games (FDG) 2022 invites research contributions in the form of papers, posters and demos, doctoral consortium applications, as well as panel, competition, and workshop proposals.
 We invite contributions from within and across any discipline committed to advancing knowledge on the foundations of games: computer science and engineering, humanities and social sciences, arts and design, mathematics and natural sciences. As was the case in the previous years, we aim to publish the FDG 2022 proceedings in the ACM Digital Library. ​FDG invites authors to submit short or full papers reporting new research. Both short and full papers need to be anonymized and…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robotics control : DRL vs classical approaches]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tk00hv/robotics_control_drl_vs_classical_approaches/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tk00hv/robotics_control_drl_vs_classical_approaches/"/>
        <updated>2022-03-22T10:36:47.000Z</updated>
        <summary type="html"><![CDATA[I'm curious about your opinion on this. There are impressive recent advances in DRL for humanoid whole-body control (Deepmimic, Drecon, etc...). Do you think these methods will one day overtake the "classical" approaches from control theory? Or we will see more and more methods combining these two approaches?
    submitted by    /u/thejackforest  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nebullvm, an open-source library to accelerate AI inference by 5-20x in a few lines of code]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjzxm5/nebullvm_an_opensource_library_to_accelerate_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjzxm5/nebullvm_an_opensource_library_to_accelerate_ai/"/>
        <updated>2022-03-22T10:31:22.000Z</updated>
        <summary type="html"><![CDATA[How does nebullvm work?
 It takes your AI model as input and outputs an optimized version that runs 5-20 times faster on your hardware. In other words, nebullvm tests multiple deep learning compilers to identify the best possible way to execute your model on your specific machine, without impacting the accuracy of your model.
 And that's it. In just a few lines of code.
 And a big thank you to everyone for supporting this open-source project! The library received 250+ Github stars⭐ on release day, and that's just amazing 🚀
 ORIENTATION MAP
 Let's learn more about nebullvm and AI optimization. Where should we start? From...
  
Some CONTEXT on why few developers optimize AI and related negative consequences
 An overview of how the LIBRARY works
 Some USE CASES, technology demonstrations and…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are there any ai's to try and recreate the voice you hear in your head?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjzeqb/are_there_any_ais_to_try_and_recreate_the_voice/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjzeqb/are_there_any_ais_to_try_and_recreate_the_voice/"/>
        <updated>2022-03-22T09:54:27.000Z</updated>
        <summary type="html"><![CDATA[Im wondering if there is any ai that you can feed your voice to and it will try to make an approximation of what you sound like to yourself in your head?
    submitted by    /u/alidan  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] JAX meets Flower - Federated Learning with JAX]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjz1wd/r_jax_meets_flower_federated_learning_with_jax/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjz1wd/r_jax_meets_flower_federated_learning_with_jax/"/>
        <updated>2022-03-22T09:28:16.000Z</updated>
        <summary type="html"><![CDATA[Flower and its community are growing. Since Flower is a friendly federated learning framework, the goal is always to get an easy start to federated learning for every data scientist. 
 This involves having Flower coding examples for different machine learning frameworks.
 One of the frameworks is JAX which was developed by Google researchers to run NumPy programs on GPUs and TPUs. It is quickly rising in popularity and is used by DeepMind to support and accelerate its research.
 We couldn’t miss the opportunity to create a code example and a blog post about “JAX meets Flower - Federated Learning with JAX”. 
 It takes always some time to get into a new machine learning framework and its syntax but it is easy to combine it with Flower. 
 You can check out the blog post here: https://flower.dev/blog/2022-03-22-running-jax-federated-jax-meets-flower/ 
 If you are interested in writing a blog post for our Flower community, contact me.
    submitted by    /u/burnai  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dial tone and busy signal]]></title>
        <id>https://www.johndcook.com/blog/?p=100543</id>
        <link href="https://www.johndcook.com/blog/2022/03/22/dial-tone-and-busy-signal/"/>
        <updated>2022-03-22T09:23:56.000Z</updated>
        <summary type="html"><![CDATA[Henry Lowengard left a comment on my post Phone tones in musical notation mentioning dial tones and busy signals, so I looked these up. Tones According to this page, a dial tone in DTMF [1] is a chord of two sine waves at 350 Hz and 440 Hz. In musical notation: According to the same […]
Dial tone and busy signal first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] With experience deploying batch models, how to learn about streaming?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjx4t5/d_with_experience_deploying_batch_models_how_to/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjx4t5/d_with_experience_deploying_batch_models_how_to/"/>
        <updated>2022-03-22T07:04:09.000Z</updated>
        <summary type="html"><![CDATA[Hi!
 I have some experience deploying batch machine learning models and now I want to learn about real-time models. More specifically, how to put them in production and what are the best practices and tools for different use-cases.
 Any ideas? I was thinking of reading the book "Designing Event-Driven Systems" by Ben Stopford (I think it's based on Kafka which seems quite popular), but would like to hear your thoughts or if someone has any other reference.
 Thanks and I hope this is the right sub!
    submitted by    /u/Silver_Book_938  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time (Language) Model]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjwzn9/realtime_language_model/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjwzn9/realtime_language_model/"/>
        <updated>2022-03-22T06:54:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/BeginningInfluence55  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking for a Speaker for a Webinar about Artificial Intelligence]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjwrla/looking_for_a_speaker_for_a_webinar_about/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjwrla/looking_for_a_speaker_for_a_webinar_about/"/>
        <updated>2022-03-22T06:38:31.000Z</updated>
        <summary type="html"><![CDATA[Hi guys!, we are University Students and we're currently looking for a speaker that can discuss Artificial Intelligence on an introductory level and the Opportunities it can bring for Engineering Students. Thank you! Please DM asap me for more details.
    submitted by    /u/BigBadToilet  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] A Social Platform to Rate and Review Machine Learning Research Papers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjvttx/p_a_social_platform_to_rate_and_review_machine/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjvttx/p_a_social_platform_to_rate_and_review_machine/"/>
        <updated>2022-03-22T05:33:57.000Z</updated>
        <summary type="html"><![CDATA[We developed a social platform for everyone to rate and review machine learning research papers. One can think of it as the IMDB database for the research community.
 Link: https://doublind.com
 Features we think that are important:
  
Search all machine learning related papers, old and new.
 Rate and review the paper you've just read, open or anonymous.
 A profile page is created for all users to display their paper reviews.
 Each paper has a rating score that help users find good papers to read.
 Users can like, comment and share a paper review, helping it reaching more readers.
  
​
 We'd love to hear your feedback and suggestions. Thank you all and we appreciate the support.
    submitted by    /u/DouBlindDotCOM  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] always behind Sota]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjv2tl/d_always_behind_sota/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjv2tl/d_always_behind_sota/"/>
        <updated>2022-03-22T04:44:21.000Z</updated>
        <summary type="html"><![CDATA[How do you guys deal with the stress of developing a new models and being off from SOTA but also needing to publish papers? 
 Do people publish results that aren’t quite SOTA? Do you tweak results? Do you do something else?
    submitted by    /u/AbjectDrink3276  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI SEO: How AI Helps You Optimize Content for Search Results]]></title>
        <id>https://www.datasciencecentral.com/?p=56708</id>
        <link href="https://www.datasciencecentral.com/ai-seo-how-ai-helps-you-optimize-content-for-search-results/"/>
        <updated>2022-03-22T04:03:00.000Z</updated>
        <summary type="html"><![CDATA[AI has been making its way into the marketing world over the last few years. Businesses have touted it as the solution to their problems and a way to incorporate technology into their processes. But, how is AI changing SEO? How can you use AI to improve your business? Machine learning and artificial intelligence are… Read More »AI SEO: How AI Helps You Optimize Content for Search Results
The post AI SEO: How AI Helps You Optimize Content for Search Results appeared first on Data Science Central.]]></summary>
        <author>
            <name>Karen Anthony</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Role of Augmented Data Catalogs in Data Governance]]></title>
        <id>https://www.datasciencecentral.com/?p=56705</id>
        <link href="https://www.datasciencecentral.com/understanding-the-role-of-augmented-data-catalogs-in-data-governance/"/>
        <updated>2022-03-22T03:58:08.000Z</updated>
        <summary type="html"><![CDATA[Every time we think we have grasped a new technology and its use. Sometimes that shift is an increase in the technology itself that seemingly intensifies the original version. Sometimes something happens that causes a significant transformation in the technology’s nature. As the technology’s significance is increasingly understood, the name is altered to better reflect… Read More »Understanding the Role of Augmented Data Catalogs in Data Governance
The post Understanding the Role of Augmented Data Catalogs in Data Governance appeared first on Data Science Central.]]></summary>
        <author>
            <name>Indhu</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security Issues in The Metaverse]]></title>
        <id>https://www.datasciencecentral.com/?p=56714</id>
        <link href="https://www.datasciencecentral.com/security-issues-in-the-metaverse/"/>
        <updated>2022-03-22T03:15:53.000Z</updated>
        <summary type="html"><![CDATA[Issues that will affect the metaverse range from hacking to cyberbullying. Proposed solutions are far from perfect. We may see end-to-end encryption disappear soon. The only real solution might be not to enter the metaverse at all. The advent of the metaverse compounds a list of existing security concerns. The addition of another dimension to… Read More »Security Issues in The Metaverse
The post Security Issues in The Metaverse appeared first on Data Science Central.]]></summary>
        <author>
            <name>Stephanie Glen</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjsd7w/r_transencoder_unsupervised_sentencepair/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjsd7w/r_transencoder_unsupervised_sentencepair/"/>
        <updated>2022-03-22T02:10:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hardmaru  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Will a PhD in Biomedical Engineering limit opportunities if I want to become a research scientist in ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjrtb9/d_will_a_phd_in_biomedical_engineering_limit/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjrtb9/d_will_a_phd_in_biomedical_engineering_limit/"/>
        <updated>2022-03-22T01:42:01.000Z</updated>
        <summary type="html"><![CDATA[I’m a tech in an academic computational lab at a very large US flagship R1 university. My current lab is a neuroscience lab that does lots of ML theory stuff (bio inspired ML). My primary goal is to get a PhD and find a research scientist position in industry at a non-FAANG company. 
 Because my PI is a neuroscientist, I might not be able to work with him as a PhD student if I apply to the EE or CS program at this university, but he has an affiliate appointment at the Biomedical Engineering department. 
 Would having a biomedical engineering degree in any way affect my ability to get a research scientist ML job? As long as I have a productive PhD with ML publications etc., and get internships at relevant places, I should be fine right? Will I have a worse chance at non-biotech companies? If BME is an issue, I’m sure my PI can get affiliate status in EE, because he has multiple collaborators in EE.
    submitted by    /u/throawaythroaway11  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NN AI vs projectile motion physics]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tjqnpb/nn_ai_vs_projectile_motion_physics/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tjqnpb/nn_ai_vs_projectile_motion_physics/"/>
        <updated>2022-03-22T00:43:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/hotcodist  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weekly China AI Newsletter: Cambricon Stocks Plunge After Sudden Exit of Huawei Veteran CTO; US-Banned DeepGlint Goes Public; China-US AI Collaboration Rises Fivefold Since 2010]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjqch7/weekly_china_ai_newsletter_cambricon_stocks/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjqch7/weekly_china_ai_newsletter_cambricon_stocks/"/>
        <updated>2022-03-22T00:27:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/trcytony  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A mini-conversation with Albert Einstein's AI persona]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjpcax/a_miniconversation_with_albert_einsteins_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjpcax/a_miniconversation_with_albert_einsteins_ai/"/>
        <updated>2022-03-21T23:37:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kuasha7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate new designs based on provided dataset?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjp7lu/generate_new_designs_based_on_provided_dataset/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjp7lu/generate_new_designs_based_on_provided_dataset/"/>
        <updated>2022-03-21T23:31:23.000Z</updated>
        <summary type="html"><![CDATA[I am looking to generate new car wheel designs based on a dataset I have of my own 30-50 wheels. I was wondering if there is a tool/GAN that would suit this?
    submitted by    /u/jrobin51  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The advantages and disadvantages of AI in healthcare]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjmsq4/the_advantages_and_disadvantages_of_ai_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjmsq4/the_advantages_and_disadvantages_of_ai_in/"/>
        <updated>2022-03-21T21:41:31.000Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence is generally praised for their use in many sectors. One of the most notable achievements of AI in science is its breakthrough in molecular biology as it helped reduce the time to identify the structure of complicated 3D protein folds. AI has also helped advance the efficiency in quantum computers, and now AI is making a large presence in clinical/healthcare. Despite the potential of AI being revolutionary in healthcare we must be cautious utilizing it. I want to highlight the advantages and disadvantages of AI to inspire and to set forth challenges for you to approach in the future. 
 The advantage of AI in healthcare is its versatile applications. You can find developments in many topics of healthcare such as drug discovery, image processing, data analysis, and mor…]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How to compute the probability of trajectories term in Stochastic Gradient Meta Reinforcement Learning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjmfk6/d_how_to_compute_the_probability_of_trajectories/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjmfk6/d_how_to_compute_the_probability_of_trajectories/"/>
        <updated>2022-03-21T21:24:55.000Z</updated>
        <summary type="html"><![CDATA[I asked this question on stats stackexchange, and it is posted here. I don't copy it here because formulas don't show up nicely on reddit.
 I am trying to implement this paper by Fallah et al (NIPs 2021) titled: On the Convergence Theory of Debiased Model-Agnostic Meta-Reinforcement Learning. As the title suggests, they propose an algorithm for meta RL that uses an stochastic approximation of the gradient. My problem is with the term that yields the probability of a given trajectory (a sequence of state-actions). I don't know how to estimate that term and the paper doesn't discuss that. I'd appreciate if anyone can share any insight on how to estimate that term.
    submitted by    /u/carlml  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI: AI discovers lethal molecules, Deepfake of Ukrainian President Volodymyr Zelenskyy, All-MLP AI model is fast and efficient, and more!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjldtr/last_week_in_ai_ai_discovers_lethal_molecules/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjldtr/last_week_in_ai_ai_discovers_lethal_molecules/"/>
        <updated>2022-03-21T20:37:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommended materials for beginners in theoretical RL]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tjld3p/recommended_materials_for_beginners_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tjld3p/recommended_materials_for_beginners_in/"/>
        <updated>2022-03-21T20:36:15.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Rich_Beautiful  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How initialize variable in RL to reproduce results]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tjl6r7/how_initialize_variable_in_rl_to_reproduce_results/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tjl6r7/how_initialize_variable_in_rl_to_reproduce_results/"/>
        <updated>2022-03-21T20:28:39.000Z</updated>
        <summary type="html"><![CDATA[I am using stabel-baselines frameworks to train the model. To build an agent environment I am using Gym environment. I want to make sure that results do not change very much when I run the same model a second time. How is it possible?
    submitted by    /u/Mariam_Dundua  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solar declination]]></title>
        <id>https://www.johndcook.com/blog/?p=100523</id>
        <link href="https://www.johndcook.com/blog/2022/03/21/solar-declination/"/>
        <updated>2022-03-21T19:28:13.000Z</updated>
        <summary type="html"><![CDATA[This post expands on a small part of the post Demystifying the Analemma by M. Tirado. Apparent solar declination given δ by δ = sin-1( sin(ε) sin(θ) ) where ε is axial tilt and θ is the angular position of a planet. See Tirado’s post for details. Here I want to unpack a couple things […]
Solar declination first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D][P] Neural Network as Frequency Offset Corrector]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjj23w/dp_neural_network_as_frequency_offset_corrector/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjj23w/dp_neural_network_as_frequency_offset_corrector/"/>
        <updated>2022-03-21T18:54:23.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone! I'm working on a project where I'm using neural networks to overcome some RF impairments in a wireless communications system. When I introduce a frequency offset to the data, my current neural network (which is a simple 2 hidden layer feed-forward NN) can not compensate. The frequency offset is basically a phase rotation on the IQ samples. The problem is this is also time-dependent such that the amount the sample is shifted is a function of time. ie: x_out[n] = x[n]e^(j2*pi*w*n) where w is the phase shift from the frequency offset, n is the discrete sample. I know my NN needs some kind of memory (so I should use an LSTM or some RNN variant), just wondering if anyone has some recommended papers or has done some work similar to this and would be willing to help, thanks!
    submitted by    /u/mtot10  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grapes.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjhhp5/grapes/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjhhp5/grapes/"/>
        <updated>2022-03-21T17:46:13.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/cookingandcraft  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coined in 2016, AIOps stands for Artificial Intelligence for IT Operations which combines big data and machine learning to automate IT operation processes. Watch this video to learn all about it.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjfnnz/coined_in_2016_aiops_stands_for_artificial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjfnnz/coined_in_2016_aiops_stands_for_artificial/"/>
        <updated>2022-03-21T16:23:57.000Z</updated>
        <summary type="html"><![CDATA[​
 https://reddit.com/link/tjfnnz/video/e1hw1trsiro81/player
    submitted by    /u/Nitorblog  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence and a Sense of Time]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjel6a/artificial_intelligence_and_a_sense_of_time/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjel6a/artificial_intelligence_and_a_sense_of_time/"/>
        <updated>2022-03-21T15:36:58.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I've been interested in pattern theory for a bit, and as far as I can tell it can be viewed as a framework for artificial intelligence. In Grenander's book "Calculus of Thoughts" David Mumford mentions that a major issue with pattern theory is the creation of generators and modalities of bonds.
 For the bonds, has anyone tried basing their properties on a sense of time? I.e. in whatever environment a program is in, it would have a clock measuring time, and would somehow equate these bonds simply with events happening close to each other in time.
 Apologies if this is a naïve/vague/nonsensical question, I'm enthusiastic but not too knowledgeable about pattern theory. Any feedback is greatly appreciated
    submitted by    /u/patterntheoryacc  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[is there some sort of Ai image generator that continues an image?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjdd0l/is_there_some_sort_of_ai_image_generator_that/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjdd0l/is_there_some_sort_of_ai_image_generator_that/"/>
        <updated>2022-03-21T14:42:03.000Z</updated>
        <summary type="html"><![CDATA[I need an Ai image generator that takes an image and generates what is beyond the border of that image.
 for example if i have an image of the front half of a dog, it fills in the other half of the dog 
 im going to use it to extend a picture of a building btw
    submitted by    /u/Bingela_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] A new way to start, stop and distribute code on cloud instances/pods - directly from any notebook, only when you need it]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tjd7ek/p_a_new_way_to_start_stop_and_distribute_code_on/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tjd7ek/p_a_new_way_to_start_stop_and_distribute_code_on/"/>
        <updated>2022-03-21T14:34:41.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone!
 With a friend, we are develpoing a tool to be able to easily start pods, distribute the code (with magics) and stop them directly from your notebook.
 Pods can be started in seconds with the specifications you need: number of instances/pods, CPU, Memory, GPU and packages
 The tool makes it easy to increase computer resources quickly, code locally and consume resources only when you need it, paralelize tasks and do parameter grid search.
 If you like it, you can visit our landing page and register for early access: telekinesis.cloud
 We are giving 10 GPU hours to our first 50 registered users!
 We hope you like it! Thank you!
 Please share feedback
    submitted by    /u/snuns90  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[League of Legends A New Dawn Trailer (Resolution increased with the help of neural networks up to 8K 60FPS)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tjcpxj/league_of_legends_a_new_dawn_trailer_resolution/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tjcpxj/league_of_legends_a_new_dawn_trailer_resolution/"/>
        <updated>2022-03-21T14:11:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stepanmetior  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The case for human-centered AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjchwc/the_case_for_humancentered_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjchwc/the_case_for_humancentered_ai/"/>
        <updated>2022-03-21T14:01:21.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bendee983  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Visualization in Python with matplotlib, Seaborn and Bokeh]]></title>
        <id>https://machinelearningmastery.com/?p=13318</id>
        <link href="https://machinelearningmastery.com/data-visualization-in-python-with-matplotlib-seaborn-and-bokeh/"/>
        <updated>2022-03-21T14:00:49.000Z</updated>
        <summary type="html"><![CDATA[Data visualization is an important aspect of all AI and machine learning applications. You can gain key insights of your […]
The post Data Visualization in Python with matplotlib, Seaborn and Bokeh appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Mehreen Saeed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[League of Legends A New Dawn Trailer (Resolution increased with the help of neural networks up to 8K 60FPS)]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tjch6j/league_of_legends_a_new_dawn_trailer_resolution/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tjch6j/league_of_legends_a_new_dawn_trailer_resolution/"/>
        <updated>2022-03-21T14:00:31.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/stepanmetior  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weekly metaverse digest #3: virtual humans, HSBC gears for the metaverse, real estate of the future, and more]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjblgg/weekly_metaverse_digest_3_virtual_humans_hsbc/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjblgg/weekly_metaverse_digest_3_virtual_humans_hsbc/"/>
        <updated>2022-03-21T13:15:52.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bent_out_of_shape_  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence Applications for 2022]]></title>
        <id>https://medium.com/p/d968ded84500</id>
        <link href="https://becominghuman.ai/artificial-intelligence-applications-for-2022-d968ded84500?source=rss----5e5bef33608a---4"/>
        <updated>2022-03-21T13:03:44.000Z</updated>
        <summary type="html"><![CDATA[The field of Artificial Intelligence (AI) continues to expand and improve by leaps and bounds. Today’s AI applications are becoming smarter…]]></summary>
        <author>
            <name>A Smith</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which trends do you think will have the biggest impact on businesses in the upcoming years?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tjaai9/which_trends_do_you_think_will_have_the_biggest/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tjaai9/which_trends_do_you_think_will_have_the_biggest/"/>
        <updated>2022-03-21T12:07:48.000Z</updated>
        <summary type="html"><![CDATA[View Poll
    submitted by    /u/futureanalytica  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Ukraine Intelligence Analysis with Computer Vision on Synthetic Aperture Radar Imagery]]></title>
        <id>http://bair.berkeley.edu/blog/2022/03/21/ukraine-sar-maers/</id>
        <link href="http://bair.berkeley.edu/blog/2022/03/21/ukraine-sar-maers/"/>
        <updated>2022-03-21T12:00:00.000Z</updated>
        <summary type="html"><![CDATA[Figure 1: Airmass measurements over Ukraine from February 18, 2022 - March 01, 2022 from the SEVIRI instrument. Data accessed via the EUMETSAT Viewer.


Satellite imagery is a critical source of information during the current invasion of Ukraine. Military strategists, journalists, and researchers use this imagery to make decisions, unveil violations of international agreements, and inform the public of the stark realities of war. With Ukraine experiencing a large amount of cloud cover and attacks often occuring during night-time, many forms of satellite imagery are hindered from seeing the ground. Synthetic aperture radar imagery penetrates cloud cover, but requires special training to interpret. Automating this tedious task would enable real-time insights, but current computer vision meth…]]></summary>
        <author>
            <name>The Berkeley Artificial Intelligence Research Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] EvoRL @ GECCO 2022: 2nd Evolutionary RL workshop @ GECCO 2022]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tj8z5w/r_evorl_gecco_2022_2nd_evolutionary_rl_workshop/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tj8z5w/r_evorl_gecco_2022_2nd_evolutionary_rl_workshop/"/>
        <updated>2022-03-21T10:49:00.000Z</updated>
        <summary type="html"><![CDATA[CALL FOR PAPERSEvoRL 2022Evolutionary Reinforcement Learning workshop at GECCO 2022, July 9-13, Boston, USA
 
 In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks. At the same time, multiple recent papers, notably work from OpenAI, have shown that evolution strategies (ES) can be competitive with standard RL algorithms on some problems while being simpler and more scalable. Similar results were obtained by researchers from Uber, this time using a gradient-free genetic algorithm (GA) to train deep neural networks on complex control tasks. Moreover, recent research in the field of evolutionary algorithms (EA) has led to the development of algorithms like Novelty Search and Quality Diversity, capable of eff…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[CfP] EvoRL @ GECCO 2022: 2nd Evolutionary RL workshop @ GECCO 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tj8yfd/cfp_evorl_gecco_2022_2nd_evolutionary_rl_workshop/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tj8yfd/cfp_evorl_gecco_2022_2nd_evolutionary_rl_workshop/"/>
        <updated>2022-03-21T10:47:39.000Z</updated>
        <summary type="html"><![CDATA[CALL FOR PAPERS
 EvoRL 2022
 Evolutionary Reinforcement Learning workshop at GECCO 2022, July 9-13, Boston, USA
 
 In recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks. At the same time, multiple recent papers, notably work from OpenAI, have shown that evolution strategies (ES) can be competitive with standard RL algorithms on some problems while being simpler and more scalable. Similar results were obtained by researchers from Uber, this time using a gradient-free genetic algorithm (GA) to train deep neural networks on complex control tasks. Moreover, recent research in the field of evolutionary algorithms (EA) has led to the development of algorithms like Novelty Search and Quality Diversity, capable of…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Link to free resources are mentioned in the description of the video!!]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tj8kmj/link_to_free_resources_are_mentioned_in_the/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tj8kmj/link_to_free_resources_are_mentioned_in_the/"/>
        <updated>2022-03-21T10:21:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/mr-minion  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chemical X NUMBUH-841]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tj87bs/chemical_x_numbuh841/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tj87bs/chemical_x_numbuh841/"/>
        <updated>2022-03-21T09:57:15.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/VIRUS-AOTOXIN  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What are some real life examples of dynamics in reinforcement learning?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tj7j78/what_are_some_real_life_examples_of_dynamics_in/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tj7j78/what_are_some_real_life_examples_of_dynamics_in/"/>
        <updated>2022-03-21T09:10:01.000Z</updated>
        <summary type="html"><![CDATA[Dynamics in reinforcement learning, that are represented by the transition function in an MDP, are meant to modelize the probability of reaching (or deriving from) the desired state. From what I understand, this probability is caused by the environement or by a malfunction within the agent(?). 
 I would like to ask if there is any real life problem modelized into an RL model with a real life transition probability? I would be super grateful if you redirect me to research papers in this axis (all I found so far are theoretical representation of dynamics)
 Thanks in advance.
    submitted by    /u/Unfinished-plans  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance testing FastAPI ML APIs with Locust | Rubik's Code]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tj6uu7/performance_testing_fastapi_ml_apis_with_locust/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tj6uu7/performance_testing_fastapi_ml_apis_with_locust/"/>
        <updated>2022-03-21T08:21:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RubiksCodeNMZ  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance testing FastAPI ML APIs with Locust | Rubik's Code]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tj6ukr/performance_testing_fastapi_ml_apis_with_locust/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tj6ukr/performance_testing_fastapi_ml_apis_with_locust/"/>
        <updated>2022-03-21T08:21:25.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/RubiksCodeNMZ  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Identifying someone's professional background through his syntax.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tj6jj5/p_identifying_someones_professional_background/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tj6jj5/p_identifying_someones_professional_background/"/>
        <updated>2022-03-21T08:00:25.000Z</updated>
        <summary type="html"><![CDATA[Hi, 
 As the title points out, I would like to identify someone's professional background (e.g. engineering, art, ...) through his syntax. 
 To do so my strategy was to find a dataset regrouping messages alongside the person's background but it doesn't exist (to my knowledge)
 Do you guys have any idea to persue? (any good dataset?, trying to find multiple datasets and merging them?, scraping LinkedIn?)
 Is it even possible to e.g. scrape LinkedIn for such information?
 Any thoughts and feedback are welcome!
 Initial strategy:
 - Either finding a dataset or scraping LinkedIn. (is it even possible?)
 - Training a NN.
 Alternative question:
 - Do you know of any other approach that would be less data intensive using for instance hand crafted features, computational linguistics and the such?
 Cheers,
    submitted by    /u/Inquation  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] AdaFamily: A family of Adam-like adaptive gradient methods]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tj6ir8/rp_adafamily_a_family_of_adamlike_adaptive/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tj6ir8/rp_adafamily_a_family_of_adamlike_adaptive/"/>
        <updated>2022-03-21T07:59:24.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/HannesF99  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Monotonic Differentiable Sorting Networks w/ animated video (ICLR 2022)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tj5la9/r_monotonic_differentiable_sorting_networks_w/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tj5la9/r_monotonic_differentiable_sorting_networks_w/"/>
        <updated>2022-03-21T06:52:41.000Z</updated>
        <summary type="html"><![CDATA[I have made an animated video (https://www.youtube.com/watch?v=Rl-sFaE1z4M) for our ICLR 2022 paper (https://arxiv.org/abs/2203.09630).
 Check it out if you are interested. I have made the video using 3b1b's manim library (https://github.com/ManimCommunity/manim).
 Feedback is always very welcome!
    submitted by    /u/Human-Career-9962  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Has anyone trained an AI to tell apart nudity in images, so it can be used to filter frames/scenes in or out of a video?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tj56i8/has_anyone_trained_an_ai_to_tell_apart_nudity_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tj56i8/has_anyone_trained_an_ai_to_tell_apart_nudity_in/"/>
        <updated>2022-03-21T06:23:17.000Z</updated>
        <summary type="html"><![CDATA[I know the question sounds funny but I mean it seriously. Is there something trained like this, that could be easily plugged to something like FFmpeg?
 Doing it on every frame would probably be too slow, but one could probably do it only on P-frames with a very small increase of error rate.
    submitted by    /u/flying-benedictus  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Switchblade Drones join the fight: U.S.-made drones will be sent to Ukraine with the additional $800 million in military assistance that Joe Biden announced on Wednesday. At least 100 Switchblade Tactical Drones will be in the package.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tj4dfd/switchblade_drones_join_the_fight_usmade_drones/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tj4dfd/switchblade_drones_join_the_fight_usmade_drones/"/>
        <updated>2022-03-21T05:27:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/dannylenwinn  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] LinkedIn open sources FastTreeSHAP Python package for interpretation of tree-based ML models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tj3f9t/n_linkedin_open_sources_fasttreeshap_python/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tj3f9t/n_linkedin_open_sources_fasttreeshap_python/"/>
        <updated>2022-03-21T04:26:08.000Z</updated>
        <summary type="html"><![CDATA[LinkedIn open sources the FastTreeSHAP Python package for efficient interpretation of tree-based ML models (XGBoost, LightGBM, sklearn random forest) using SHAPLEY. FastTreeSHAP v2 would be 2.5x faster than TreeSHAP. Let's reminder that SHAP (SHapley Additive exPlanation) values quantify the contribution of each feature to the model prediction, a bit like how each player contributes to the success of a sports team. SHAP does it by incorporating concepts from game theory and local explanations. Naively implemented, SHAP takes exponential time. LinkedIn blog post, scientific paper, and GitHub repo with IPython Notebooks.
    submitted by    /u/ClaudeCoulombe  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Using ML Interpretability Techniques for Data Analysis]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tj3dqj/d_using_ml_interpretability_techniques_for_data/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tj3dqj/d_using_ml_interpretability_techniques_for_data/"/>
        <updated>2022-03-21T04:23:31.000Z</updated>
        <summary type="html"><![CDATA[Hey, all. Hope you lot are doing alright.
 I was looking into Explainable AI and model interpretability lately, and I had an idea but am wondering whether it would constitute a valid use case.
 There is a data analysis project happening at work where we're trying to analyze data we had on hand to determine the factors that affect our KPOs and possibly derive useful actionable insights. Instead of moving forward with manually evaluating correlation and doing EDA that way, I was thinking of using variable importance measures, subpopulation analyses, and partial dependence profiles to potentially gain insights, or at the very least narrow down exploratory scope.
 I have been scrounging the internet for someone using white box models for expediting analysis tasks, but I haven't found anything that remotely fits the bill except perhaps this article: https://medium.com/swlh/how-i-used-a-machine-learning-model-to-generate-actionable-insights-3aa1dfe2ddfd
 It'd be great if you guys let me know what you think.
 Cheers,
    submitted by    /u/Impartial_Bystander  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Why is the predictive variance of Bayesian models can be interpreted as epistemic uncertainty?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tj34qs/d_why_is_the_predictive_variance_of_bayesian/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tj34qs/d_why_is_the_predictive_variance_of_bayesian/"/>
        <updated>2022-03-21T04:08:42.000Z</updated>
        <summary type="html"><![CDATA[Is there a direct link or explanation on why the predictive variance of a Bayesian model can be interpreted as epistemic uncertainty?
 Many epistemic uncertainty quantification methods, for example, MC-Dropout, Deep Ensemble, claim themselves as a Bayesian model to justify their approach for quantifying uncertainty. Let's just say their claims are valid so that the methods are truly Bayesian. But then, why are Bayesian methods supposed to estimate uncertainty well?
    submitted by    /u/swyoon_  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning", Park et al 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tj1s3b/surf_semisupervised_reward_learning_with_data/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tj1s3b/surf_semisupervised_reward_learning_with_data/"/>
        <updated>2022-03-21T02:53:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LinkedIn Researchers Open-Source ‘FastTreeSHAP’: A Python Package That Enables An Efficient Interpretation of Tree-Based Machine Learning Models]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tj1kw3/linkedin_researchers_opensource_fasttreeshap_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tj1kw3/linkedin_researchers_opensource_fasttreeshap_a/"/>
        <updated>2022-03-21T02:42:16.000Z</updated>
        <summary type="html"><![CDATA[Researchers from LinkedIn open-source the FastTreeSHAP package which is a Python module based on the paper ‘Fast TreeSHAP: Accelerating SHAP Value Computation for Trees.’ Implementing the widely-used TreeSHAP algorithm in the SHAP package allows for the efficient interpretation of tree-based machine learning models by estimating sample-level feature significance values. Its package includes two new algorithms: FastTreeSHAP v1 and FastTreeSHAP v2, both of which improve TreeSHAP’s computational efficiency by taking a different approach. 
 The empirical benchmarking tests show that FastTreeSHAP v1 is 1.5x faster than TreeSHAP while keeping memory costs the same, and FastTreeSHAP v2 is 2.5x faster while using slightly more memory. The FastTreeSHAP package fully supports parallel multi-core computing to speed up its computation.
 Continue Reading The Full Summary Article
 Paper: https://arxiv.org/pdf/2109.09847.pdf
 Github: https://github.com/linkedin/fasttreeshap
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Dealing with normalisation of data containing extreme values]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tizy60/d_dealing_with_normalisation_of_data_containing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tizy60/d_dealing_with_normalisation_of_data_containing/"/>
        <updated>2022-03-21T01:14:37.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 Imagine there’s a rainfall dataset, and there’s surely extreme values in the dataset. These extreme values are important because I would need them to predict extreme events. 
 Unfortunately, when doing normalisation, these extreme values will tend to result in almost every other values being extremely small after min-max. Any suggestions to go about this?
 This is for research
    submitted by    /u/plsendfast  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy", Yuanhan et al 2022 {Sensetime} (69m categorized images)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tizwge/bamboo_building_megascale_vision_dataset/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tizwge/bamboo_building_megascale_vision_dataset/"/>
        <updated>2022-03-21T01:11:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Equivariant Subgraph Aggregation Networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tizt3n/d_equivariant_subgraph_aggregation_networks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tizt3n/d_equivariant_subgraph_aggregation_networks/"/>
        <updated>2022-03-21T01:07:01.000Z</updated>
        <summary type="html"><![CDATA[An author interview on the Equivariant Subgraph Aggregation Networks paper. Discusses why the expressive power of GNNs is limited and a method for breaking the bottleneck of the 1-WL algorithm 
 https://youtu.be/VYZog7kbXks
    submitted by    /u/zjost85  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Modern Hopfield Networks for Return Decomposition for Delayed Rewards", Widrich et al 2021]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tizmlp/modern_hopfield_networks_for_return_decomposition/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tizmlp/modern_hopfield_networks_for_return_decomposition/"/>
        <updated>2022-03-21T00:57:59.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis + Gradio Web Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tivnnb/rp_stylenerf_a_stylebased_3daware_generator_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tivnnb/rp_stylenerf_a_stylebased_3daware_generator_for/"/>
        <updated>2022-03-20T21:41:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Problems with rainfall prediction with neural network.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tiuy88/p_problems_with_rainfall_prediction_with_neural/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tiuy88/p_problems_with_rainfall_prediction_with_neural/"/>
        <updated>2022-03-20T21:08:05.000Z</updated>
        <summary type="html"><![CDATA[So I am trying to make a neural network which would predict if it's raining or not. However I'm having problems with the neural network because it seems to not learn at all. Or at least it gives very low propability for any data which I try to predict. The dataset contains temperature, pressure, and relative humidity which are mesaured every 10 minutes. The data is from Finnish Weather forecast center (Ilmatieteenlaitos). I'm also modifying the data so that I can feed 24h of values for each rainfall value in to the neural network. Here is a photo explaining this better https://imgur.com/a/jFpsN8Q and here is the code for the project https://www.kaggle.com/code/juhosyvjrvi/rainfall-prediction/notebook 
 Thank you for any help in advance!
    submitted by    /u/Juuhis999  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Explaining the Explainable AI: A 2-Stage Approach - Link to a free online lecture by the author in comments]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/titsid/r_explaining_the_explainable_ai_a_2stage_approach/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/titsid/r_explaining_the_explainable_ai_a_2stage_approach/"/>
        <updated>2022-03-20T20:15:29.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/pinter69  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Colab to a Ploomber pipeline: ML at scale]]></title>
        <id>https://www.datasciencecentral.com/?p=56683</id>
        <link href="https://www.datasciencecentral.com/google-colab-to-a-ploomber-pipeline-ml-at-scale/"/>
        <updated>2022-03-20T18:46:30.000Z</updated>
        <summary type="html"><![CDATA[In this short blog, we’ll review the process of taking a POC data science pipeline (ML/Deep learning/NLP) that was conducted on Google Colab, and transforming it into a pipeline that can run parallel at scale and works with Git so the team can collaborate on. Background Google Colab is pretty straightforward, you can open a… Read More »Google Colab to a Ploomber pipeline: ML at scale
The post Google Colab to a Ploomber pipeline: ML at scale appeared first on Data Science Central.]]></summary>
        <author>
            <name>Michael Ido</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biases in CLIP and the Stanford HAI report]]></title>
        <id>https://www.datasciencecentral.com/?p=56692</id>
        <link href="https://www.datasciencecentral.com/biases-in-clip-and-the-stanford-hai-report/"/>
        <updated>2022-03-20T18:41:00.000Z</updated>
        <summary type="html"><![CDATA[The Stanford HAI (Human cantered artificial intelligence) report is out. I track this report every year and it always has some good insights. The report is a bit more focused on large language models and their impact – but also covers key trends The main findings are Private investment in AI soared while investment concentration… Read More »Biases in CLIP and the Stanford HAI report
The post Biases in CLIP and the Stanford HAI report appeared first on Data Science Central.]]></summary>
        <author>
            <name>ajitjaokar</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Modernize Enterprise Data and Analytics Platform (Part 1 of 4)]]></title>
        <id>https://www.datasciencecentral.com/?p=56648</id>
        <link href="https://www.datasciencecentral.com/how-to-modernize-enterprise-data-and-analytics-platform-part-1-of-4/"/>
        <updated>2022-03-20T18:33:26.000Z</updated>
        <summary type="html"><![CDATA[Data and Analytics Platform is a sub-platform of the Enterprise Digital Business Technology Platform. It contains information management and analytical capabilities. Introduction Building the digital business technology platform is a core aspect of enterprise endeavors to support its digital business transformation and therefore gain a sustainable competitive advantage. The digital business technology platform includes 5… Read More »How to Modernize Enterprise Data and Analytics Platform (Part 1 of 4)
The post How to Modernize Enterprise Data and Analytics Platform (Part 1 of 4) appeared first on Data Science Central.]]></summary>
        <author>
            <name>Alaa Mahjoub, M.Sc. Eng.</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reverse engineering Fourier conventions]]></title>
        <id>https://www.johndcook.com/blog/?p=100459</id>
        <link href="https://www.johndcook.com/blog/2022/03/20/reverse-engineering-fourier-conventions/"/>
        <updated>2022-03-20T18:26:39.000Z</updated>
        <summary type="html"><![CDATA[The most annoying thing about Fourier analysis is that there are many slightly different definitions of the Fourier transform. One time I got sufficiently annoyed that I creates a sort of Rosetta Stone of Fourier theorems under eight different conventions. Later I discovered that Mathematica supports these same eight definitions, but with slightly different notation. […]
Reverse engineering Fourier conventions first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] code to visualize attention heads]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tirbmm/d_code_to_visualize_attention_heads/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tirbmm/d_code_to_visualize_attention_heads/"/>
        <updated>2022-03-20T18:22:14.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if anyone has any code they use in python to visualize attention head maps for transformer architectures. If so are you willing to share it?
    submitted by    /u/AbjectDrink3276  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Should I go for the PhD?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tiqsqe/d_should_i_go_for_the_phd/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tiqsqe/d_should_i_go_for_the_phd/"/>
        <updated>2022-03-20T17:58:55.000Z</updated>
        <summary type="html"><![CDATA[Like many others on here, I've hit a crossroad in life and I'm unsure whether I want to do a PhD or stay in the industry, so I am looking for some advice on my particular situation. About me:
  
Interested in creating intelligent robots. As cringy as it sounds, I do believe that star-wars-like intelligent robots are possible within my lifetime and my goal is to contribute towards that
 I love love love creating novel breakthrough technology that has the potential to improve everyone's lives. I like writing papers and creating tech in equal amounts. I love seeing the impact of my work firsthand.
 At this point in time, I don't think I'll enjoy an academic career because I like doing things with a more immediate impact. Thus I think I'd like to end up in industry research.
 Graduated MSc Rob…]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[drone environment ?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tiqsq8/drone_environment/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tiqsq8/drone_environment/"/>
        <updated>2022-03-20T17:58:54.000Z</updated>
        <summary type="html"><![CDATA[Hi all.
 I need to implement a drone env to train neural network Capable of stabilizing a drone after throwing it. any suggestions for pre built envs or where to find informations on what i should consider if i want to build one on my own? I know how to use pybullet and the open ai gym interface so building one is not out of the question but a pre built one by a more experienced people would be better given the fact that I'm on tight schedule 
 Sorry for my English not a native speaker :)
    submitted by    /u/HerForFun998  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIT Researchers Developed A Machine-Learning Model To Generate Extremely Realistic Synthetic Data That Can Train Another Model For Downstream Vision Tasks]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tipnnm/mit_researchers_developed_a_machinelearning_model/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tipnnm/mit_researchers_developed_a_machinelearning_model/"/>
        <updated>2022-03-20T17:07:07.000Z</updated>
        <summary type="html"><![CDATA[Many machine learning tasks require high-quality data, such as assessing damage in a satellite that negatively affects a model’s performance. Datasets can cost millions of dollars to create if useable data exists, and even the best datasets sometimes contain biases that negatively impact a model’s performance.
 Many scientists have been working to answer an intriguing question working with synthetic data sampled from a generative model instead of real data. A generative model is a machine-learning model that requires significantly less memory to keep or share than a dataset. The range and quality of generative models have improved dramatically in recent years.
 Synthetic data has the ability to get around some of the privacy and usage rights problems that limit how actual data may be distributed. A generative model could potentially be updated to eliminate particular attributes, such as race or gender, to overcome biases in traditional datasets.
 New research by MIT Team develops a method for training a machine learning (ML) model that, rather than requiring a dataset, employs a particular form of ML model to generate exceptionally realistic synthetic data that can train another model for downstream vision tasks.
 Continue Reading
 Paper: https://openreview.net/pdf?id=qhAeZjs7dCL
 Github: https://github.com/ali-design/GenRep
 Project: https://ali-design.github.io/GenRep/
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Does RepL4NLP accept papers such as pruning and quantization?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tin2cf/d_does_repl4nlp_accept_papers_such_as_pruning_and/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tin2cf/d_does_repl4nlp_accept_papers_such_as_pruning_and/"/>
        <updated>2022-03-20T15:06:50.000Z</updated>
        <summary type="html"><![CDATA[Does RepL4NLP accept papers such as pruning and quantization? The link below gives you a list of topics if you scroll down. One of them was " Efficient learning of representations and inference: with respect to training and inference time, model size, amount of training data, etc.". I was wondering if that has anything to do with pruning and/or quantization?
 ​
 https://sites.google.com/view/repl4nlp2022/
    submitted by    /u/SiegeMemeLord  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ai for videos characters]]></title>
        <id>https://www.reddit.com/r/artificial/comments/timvbo/ai_for_videos_characters/</id>
        <link href="https://www.reddit.com/r/artificial/comments/timvbo/ai_for_videos_characters/"/>
        <updated>2022-03-20T14:58:03.000Z</updated>
        <summary type="html"><![CDATA[Is there any good ai for automating virtual characters in movies and games?
    submitted by    /u/Ok-Suspect-9855  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building An Effective Experimentation Program – 01 Introduction]]></title>
        <id>https://mlinproduction.com/?p=926</id>
        <link href="https://mlinproduction.com/experimentation-program-introduction/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=experimentation-program-introduction"/>
        <updated>2022-03-20T14:48:21.000Z</updated>
        <summary type="html"><![CDATA[Seamless. Frictionless. Elegant. Efficient.
Read More 
The post Building An Effective Experimentation Program – 01 Introduction appeared first on ML in Production.]]></summary>
        <author>
            <name>Luigi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building An Effective Experimentation Program – 01 Introduction]]></title>
        <id>https://mlinproduction.com/?p=926</id>
        <link href="https://mlinproduction.com/experimentation-program-introduction/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=experimentation-program-introduction"/>
        <updated>2022-03-20T14:48:21.000Z</updated>
        <summary type="html"><![CDATA[Seamless. Frictionless. Elegant. Efficient.
Read More 
The post Building An Effective Experimentation Program – 01 Introduction appeared first on ML in Production.]]></summary>
        <author>
            <name>Luigi</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] / [Reaearch]: Anomaly detection in massive datasets (70-100 TerraBytes)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/timcsd/discussion_reaearch_anomaly_detection_in_massive/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/timcsd/discussion_reaearch_anomaly_detection_in_massive/"/>
        <updated>2022-03-20T14:33:24.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone. I am looking for reaearch papers and ideas on how to solve this problem for my master’s thesis. Any pointers to SOTA reearch would be greatly appreciated. Let me just quickly summarize the problem at hand:
  
There are over 100k devices connected to this cloud network, where the devices track the GPS location of shipping containers. Most of the containers also have a cooling system, where it’s possible to read a couple of sensor data and also remotely control temperature/humidity in the containers.
 
My main objective is to apply some ML algo to try and predict if some device (or the integrated cooling system) is about become faulty, such that it can trigger an alarm and consequently get someone to inspect the devices due to possible malfunction.
 
The approach: Just on top of my mind, I would like to apply some unsupervised learning technique to detect if some device is behaving far from the norm. 
 
I am even more intetested in supervised learning, but it might be a little tricky to tell based on a record if it’s faulty (target variable)
 
 Any ideas? Feel free to ask if something isn’t clear.
 Training a model on 100TB of data might become a challenge in itself. I would have to apply some algo that can learn iteratively.
 Thanks!
    submitted by    /u/shotez  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained optimization with newton's method for real-life applications]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tikvxs/constrained_optimization_with_newtons_method_for/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tikvxs/constrained_optimization_with_newtons_method_for/"/>
        <updated>2022-03-20T13:19:11.000Z</updated>
        <summary type="html"><![CDATA[Hi!
 Does anyone know of any real-life applications of constrained optimization using newton's method in deep learning (specifically in minimizing the cost function)? Or if constrained optimization is even used at all when minimizing the cost function?
 I know that Newton's method can be used to find the minimum of the cost function in logistic regression. However, would there be any instances where we would add constraints to the cost function in logistic regression, therefore allowing us to use constrained optimization techniques with newton's method (such as Lagrange multipliers or interior-point methods).
    submitted by    /u/No_Pop3856  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Complex 3D Scenes from a Couple of Images!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tikejd/smooth_complex_3d_scenes_from_a_couple_of_images/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tikejd/smooth_complex_3d_scenes_from_a_couple_of_images/"/>
        <updated>2022-03-20T12:53:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Papers about 'true' utilization of pruning]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tij9t9/d_papers_about_true_utilization_of_pruning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tij9t9/d_papers_about_true_utilization_of_pruning/"/>
        <updated>2022-03-20T11:43:55.000Z</updated>
        <summary type="html"><![CDATA[Good day, my fellow researchers and engineers. 
 Most of the unstructured pruning algorithms cannot induce their theoretical performance due to the fact that we cannot ignore the zeros when the actual computation occurs.
 I recently read the paper about N:M structured sparse neural networks and I just got interested in the techniques and research about 'the actual acceleration for zeroes'.
 If we want to dig more about these kinds of sparsity acceleration ( If I'm calling it right), which paper should we look into?
 How can we ignore the zeroes, aside from saving indices of non-zero values?
    submitted by    /u/KindAd9527  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can AI help when it's "more of an art than a science"?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tiewx0/can_ai_help_when_its_more_of_an_art_than_a_science/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tiewx0/can_ai_help_when_its_more_of_an_art_than_a_science/"/>
        <updated>2022-03-20T06:24:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Representative-Job23  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flowers.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tie5w7/flowers/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tie5w7/flowers/"/>
        <updated>2022-03-20T05:33:10.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/cookingandcraft  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question about unsupervised reinforcement learning finetune process]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tidgqg/question_about_unsupervised_reinforcement/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tidgqg/question_about_unsupervised_reinforcement/"/>
        <updated>2022-03-20T04:48:32.000Z</updated>
        <summary type="html"><![CDATA[Hi, I'm trying to implement unsupervised RL algorithms, like vision-based APT.
 I trained my agent and saw some meaningful behaviors in pre-training session. but, at finetune session, the agent showed bad adjustment. I guess over-estimation was occurred. (The experiment was action repeat 8 so it looks worse than official drq implementation.)
 Orange : scratch DrQ, gray : Finetuned version after APT 2M frames training
 Orange : scratch DrQ, gray : Finetuned version after APT 2M frames training
 How can I finetune that I had already pre-trained agent well? 
 ​
 Thanks for reading.
    submitted by    /u/Spiritual_Fig3632  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Static Analyzers in Python]]></title>
        <id>https://machinelearningmastery.com/?p=13335</id>
        <link href="https://machinelearningmastery.com/static-analyzers-in-python/"/>
        <updated>2022-03-20T04:09:33.000Z</updated>
        <summary type="html"><![CDATA[Static analyzers are tools that help you check your code without really running your code. The most basic form of […]
The post Static Analyzers in Python appeared first on Machine Learning Mastery.]]></summary>
        <author>
            <name>Adrian Tam</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Find and fix bugs in ML code]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ticntt/p_find_and_fix_bugs_in_ml_code/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ticntt/p_find_and_fix_bugs_in_ml_code/"/>
        <updated>2022-03-20T03:58:36.000Z</updated>
        <summary type="html"><![CDATA[Hey everyone, it's notoriously hard to find bugs in ML models because often times they are silent. I've created a VSCode extension that can scan your Python functions and find bugs. Under the hood it utilizes a ML model that tries to understand your intent and provide suggestions. 
 Please, could you give me some feedback? I would really appreciate any help. 
 Thanks a lot in advance! 
 Website: https://tensorbox.ai 
 Video demo: https://youtu.be/MSpsCpPZokY
    submitted by    /u/CosmicRaysGalaxy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[desconected]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ti9glw/desconected/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ti9glw/desconected/"/>
        <updated>2022-03-20T00:55:38.000Z</updated>
        <summary type="html"><![CDATA[Some one knows If there's a way for not geting disconected using the S2ML generator on Google colab?
    submitted by    /u/Pale-Information3077  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation. (arXiv:2202.07191v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.07191</id>
        <link href="http://arxiv.org/abs/2202.07191"/>
        <updated>2022-03-20T00:45:06.506Z</updated>
        <summary type="html"><![CDATA[With rising male infertility, sperm head morphology classification becomes
critical for accurate and timely clinical diagnosis. Recent deep learning (DL)
morphology analysis methods achieve promising benchmark results, but leave
performance and robustness on the table by relying on limited and possibly
noisy class labels. To address this, we introduce a new DL training framework
that leverages anatomical and image priors from human sperm microscopy crops to
extract useful features without additional labeling cost. Our core idea is to
distill sperm head information with reliably-generated pseudo-masks and
unsupervised spatial prediction tasks. The predicted foreground masks from this
distillation step are then leveraged to regularize and reduce image and label
noise in the tuning stage. We evaluate our new approach on two public sperm
datasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy
and 96.5% HuSHeM accuracy).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yejia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingjing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1"&gt;Xiaomin Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yiru Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yunxia Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Danny Z. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Modelling Explicit and Implicit knowledge in RL]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ti81o2/discussion_modelling_explicit_and_implicit/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ti81o2/discussion_modelling_explicit_and_implicit/"/>
        <updated>2022-03-19T23:41:17.000Z</updated>
        <summary type="html"><![CDATA[Hi all, I recently read the YOLOR (You Only Learn One Representation) paper ( https://arxiv.org/abs/2105.04206 ). One of the key takeaways from this paper is that modelling implicit knowledge is crucial. And in order to make the model learn this implicit knowledge, simply "relaxing" the error term in cross-task learning is not enough, because then conventional loss functions won't be able to capture this implicit knowledge. So in order to tackle this problem, they model the error term for various tasks such that implicit knowledge can be learned.
 This reminded me very much of meta-RL, except I haven't seen any implementations of it where they explicitly model this per-task error and use a similar implicit+explicit knowledge learning framework.
 Since YOLOR is currently SOTA for realtime object detection and some other tasks, it seems like this was quite a significant insight and I wonder if anyone is aware of similar works in the "RL branch" of AI?
 I'd love to hear some thoughts about this or if someone can point me to related literature this would be great. In case this hasn't been looked into before, I'd love to start a PhD regarding this topic to learn more about it haha
    submitted by    /u/Chronicle112  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Agile Locomotion via Model-free Learning", Margolis et al 2022]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/ti5zsg/agile_locomotion_via_modelfree_learning_margolis/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/ti5zsg/agile_locomotion_via_modelfree_learning_margolis/"/>
        <updated>2022-03-19T21:59:11.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Very frustrated with the state of peer-reviewing in math/computational non-ML journals when it comes to ML-based approaches]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ti2rqg/d_very_frustrated_with_the_state_of_peerreviewing/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ti2rqg/d_very_frustrated_with_the_state_of_peerreviewing/"/>
        <updated>2022-03-19T19:27:48.000Z</updated>
        <summary type="html"><![CDATA[A bit of a rant here. I received a "major revision" recommendation for a paper I submitted a few months ago (was self-contained, no ML background was necessary). The field is mathematical and the journal also addresses numerical & computational aspects of it, but applying ML (especially neural net based L2 regressions) is still a bit unheard of there. As such, I made sure to dumb everything down for them and still gave the mathematical justifications, including convergence proofs.
 In short, it was about estimating certain conditional expectations having a very special structure, without resorting to heavy brute-force approaches (like nested Monte-Carlo), using L2 projections with neural networks and a special sampling scheme which was also explained in detail. In such an approach neural n…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning and phone data can improve targeting of humanitarian aid]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ti23i6/machine_learning_and_phone_data_can_improve/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ti23i6/machine_learning_and_phone_data_can_improve/"/>
        <updated>2022-03-19T18:56:23.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Jazmineco  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning + Hugging Face Gradio Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ti1yiv/rp_magma_multimodal_augmentation_of_generative/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ti1yiv/rp_magma_multimodal_augmentation_of_generative/"/>
        <updated>2022-03-19T18:49:57.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Complete Guide of Swin Transformer with Full PyTorch Implementation]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ti0u6i/d_complete_guide_of_swin_transformer_with_full/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ti0u6i/d_complete_guide_of_swin_transformer_with_full/"/>
        <updated>2022-03-19T17:58:14.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone!
 I’ve recently read Swin Transformer paper and tried to implement with PyTorch. But there’re no post that FULLY explains the nitty-gritty details of the paper with full implementation. It took me soooo long time to write this post so I wanted to share with y’all! Hope this helps someone! The implementation is based on the official implementation of Microsoft team.
 https://jasonlee-cp.github.io/paper/Swin_Transformer/#swin-transformer-architecture
    submitted by    /u/JasonTheCoders  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Conferences to publish ML tooling?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/ti0q17/d_conferences_to_publish_ml_tooling/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/ti0q17/d_conferences_to_publish_ml_tooling/"/>
        <updated>2022-03-19T17:52:55.000Z</updated>
        <summary type="html"><![CDATA[Hey, I write a lot of ML tooling and am hoping to present some of it in conferences somewhere. The main goal of publishing somewhere is to have a work that can be cited and is peer reviewed, but the real focus is on the package. Some people are weird about citing GitHub packages.
 Are there any specialized conferences for this sort of thing? Or do you just try your luck at a more niche conference?
    submitted by    /u/puppet_pals  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares: Burning Teldrassil || Clip Guided Disco Diffusion AI Art Video [4K 60 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/ti0f7x/artificial_nightmares_burning_teldrassil_clip/</id>
        <link href="https://www.reddit.com/r/artificial/comments/ti0f7x/artificial_nightmares_burning_teldrassil_clip/"/>
        <updated>2022-03-19T17:38:48.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Reading time of analog clocks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thzmw8/p_reading_time_of_analog_clocks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thzmw8/p_reading_time_of_analog_clocks/"/>
        <updated>2022-03-19T17:03:05.000Z</updated>
        <summary type="html"><![CDATA[https://github.com/akucia/analog-watch-recognition
 ​
 https://preview.redd.it/4xrti7icgdo81.jpg?width=1200&format=pjpg&auto=webp&s=b76fd4aee01ef2eda4c9bb1c739d10ff38fe3cab
    submitted by    /u/kuciu  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDPG - Neural Network output does not depend on the input]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/thz2d3/ddpg_neural_network_output_does_not_depend_on_the/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/thz2d3/ddpg_neural_network_output_does_not_depend_on_the/"/>
        <updated>2022-03-19T16:37:15.000Z</updated>
        <summary type="html"><![CDATA[Hi all!
 I am trying to use RL in order to train an agent that provides best direction to move in order to maximize the coverage of an area and I am giving reward on the basis of coverage increase at each step. I am implementing such algorithm both for 1 agent (so I am using simple DDPG) and for a set of agents (using MADDPG), but if I make the exploration noise decay, I noticed that the selected action of the model is always the same, independently on the input it receives. In fact, even during training, selected action is always a little update with respect to the one selected previously, even if the episode changes, and the inputs too. I have tried increasing/decresing the effect of noise (I am using Ornstein Uhlenbeck process) or decreasing the learning rates, but this seems to have no effect. What may be the problem of such a strange behaviour?
    submitted by    /u/cosimobr  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Code + Data for Paper (AAAI 2022): Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thyqwl/r_code_data_for_paper_aaai_2022_multiplesource/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thyqwl/r_code_data_for_paper_aaai_2022_multiplesource/"/>
        <updated>2022-03-19T16:22:25.000Z</updated>
        <summary type="html"><![CDATA[You can find the paper here:
 https://arxiv.org/abs/2201.11870
 And the code and the data here:
 https://github.com/p-karisani/CEPC
    submitted by    /u/payam_ka  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I am trying to use VQGAN+CLIP, but i can't make It work, some one more experienced knows how to fix It?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thx9mx/i_am_trying_to_use_vqganclip_but_i_cant_make_it/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thx9mx/i_am_trying_to_use_vqganclip_but_i_cant_make_it/"/>
        <updated>2022-03-19T15:13:34.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Pale-Information3077  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning with Random Forest and Dense Neural Network. [R]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thwd2z/ensemble_learning_with_random_forest_and_dense/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thwd2z/ensemble_learning_with_random_forest_and_dense/"/>
        <updated>2022-03-19T14:29:25.000Z</updated>
        <summary type="html"><![CDATA[I am working on a hospital dataset, My Y variable is the total cost of the diagnosis. I wanna predict that so it's a regression problem, Ultimately. I used random forest and Dense NN Separately and I got pretty decent scores but is it possible to ensemble random forest and neural network.
    submitted by    /u/nibras_28  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Data Scientist Weak Link in Data-drive Value Creation?]]></title>
        <id>https://www.datasciencecentral.com/?p=56662</id>
        <link href="https://www.datasciencecentral.com/data-management/"/>
        <updated>2022-03-19T14:27:01.000Z</updated>
        <summary type="html"><![CDATA[I attended an in-person customer event sponsored by Dataiku last week.  Very fun.  Man, do I miss the provocative and enlightening discussions that occur in these face-to-face customer engagements.  The icebreaker for fueling the dinner discussions was the following statement: “In the marketplace, dynamics in the job marketplace will evolve, and data-savvy subject matter experts… Read More »Is Data Scientist Weak Link in Data-drive Value Creation?
The post Is Data Scientist Weak Link in Data-drive Value Creation? appeared first on Data Science Central.]]></summary>
        <author>
            <name>Bill Schmarzo</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ghibli Howl’s Moving Castle 11x16” acrylic painting for sale! $180 plus free shipping! Message me if you’re interested.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thvg0e/ghibli_howls_moving_castle_11x16_acrylic_painting/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thvg0e/ghibli_howls_moving_castle_11x16_acrylic_painting/"/>
        <updated>2022-03-19T13:41:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/the_easel_art  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Physics and Reinforcement Learning - Discussion of Deepmind's work]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thv0en/d_physics_and_reinforcement_learning_discussion/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thv0en/d_physics_and_reinforcement_learning_discussion/"/>
        <updated>2022-03-19T13:17:25.000Z</updated>
        <summary type="html"><![CDATA[I chatted with a few experts about Deepmind's recent work on using RL for Nuclear Fusion and Plasma Control. We collected some resources and took some notes to understand the work better. Thought to share them here:
 [video] Physics and Reinforcement Learning 
  
What is fusion?
 What is plasma?
 What is a Tokamak?
 Why do we care about conrolling plasma? 
 The RL task is a continuous control task. That means that the inputs, “controller” take continuous, real values the same as would result from sliding a slider or turning a dial. Other examples of continuous control tasks are 
 There are 19 of these metaphorical “dials” controlling coils in the tokamak. The task of the RL agent, then, is to learn how best to control those 19 “dials” given observations from the sensors within the tokamak.…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How close are we to an context understanding AI?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thtxi9/how_close_are_we_to_an_context_understanding_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thtxi9/how_close_are_we_to_an_context_understanding_ai/"/>
        <updated>2022-03-19T12:13:03.000Z</updated>
        <summary type="html"><![CDATA[I am layman and just curious about the topic, how far are we to an AI which can understand the question and formulate a accurate and precise answer and not just return us the web results.
    submitted by    /u/curiosityVeil  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] What are some papers you read which helped you build an intuition of how neural networks function?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tht1qy/discussion_what_are_some_papers_you_read_which/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tht1qy/discussion_what_are_some_papers_you_read_which/"/>
        <updated>2022-03-19T11:13:58.000Z</updated>
        <summary type="html"><![CDATA[I started working through this paper on how neural networks divide and map their input space. It helped me visualize what happens in the hidden layers and how they help in better function approximation. 
 I'm looking for some suggestions on papers to read which help me build a stronger intuition on how and why neural nets work.
    submitted by    /u/theanswerisnt42  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] DeepForSpeed: A self driving car in Need For Speed Most Wanted with just a single ConvNet to play ( inspired by nvidia )]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thsx8t/p_deepforspeed_a_self_driving_car_in_need_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thsx8t/p_deepforspeed_a_self_driving_car_in_need_for/"/>
        <updated>2022-03-19T11:04:46.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/toxickettle  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Applications for using reinforcement learning to fine-tune GPT-2]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thrm7r/d_applications_for_using_reinforcement_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thrm7r/d_applications_for_using_reinforcement_learning/"/>
        <updated>2022-03-19T09:29:08.000Z</updated>
        <summary type="html"><![CDATA[I'm currently writing my Master's thesis on the merits and pitfalls of using reinforcement learning, specifically Proximal Policy Optimization, to finetune large language models. The project is inspired by the paper Fine-Tuning Language Models from Human Preferences, which uses a reward model trained on a dataset of human preferences to supply a reward signal used for fine-tuning GPT-2 to generate summaries and continuations that are optimized for human preference.
 I'm investigating the pros and cons of a more naive approach that does not require collecting a dataset of human preferences. Using the trl library, I train a BERT-classifier to distinguish between sarcastic and non-sarcastic reddit comments, and that classifier then serves as a reward model that provides a reward signal for fine-tuning GPT-2 for text generation using PPO. I have applied the same method to the task of generating negative review, by training BERT on the IMDB-dataset. This method of course leads to extensive reward hacking, but investigating how to mitigate that is part of the fun!
 I'm looking for inspiration for what other NLP tasks I could apply this to. The important constraint is that the tasks have to be something that I can train a classifier to evaluate, or otherwise evaluate algorithmically using metrics like ROUGE, because I need to supply GPT-2 with a scalar reward signal. 
 What other tasks should I try this method out on?
    submitted by    /u/Sisyfos42  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI composer> I have forced the model to generate a piece just by using dyads (a 2-note chord)! The challenge is to find notes that match and create beautiful sound. That is why I have named this piece: Love]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/thrjdb/ai_composer_i_have_forced_the_model_to_generate_a/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/thrjdb/ai_composer_i_have_forced_the_model_to_generate_a/"/>
        <updated>2022-03-19T09:22:42.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/amin_mlm  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thqd6t/ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thqd6t/ai/"/>
        <updated>2022-03-19T07:52:01.000Z</updated>
        <summary type="html"><![CDATA[I love technology but I do not like the increasing development of AI. I believe that AI has more dangers than positive impacts on humanity.
    submitted by    /u/joginderMike  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Extensions to U-nets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thp7xr/d_extensions_to_unets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thp7xr/d_extensions_to_unets/"/>
        <updated>2022-03-19T06:24:58.000Z</updated>
        <summary type="html"><![CDATA[I've been working with U-nets, and I’m curious about the possible extensions to improve the performance of U-nets. I found several so far:
 Variational U-nets: https://proceedings.neurips.cc/paper/2018/file/473447ac58e1cd7e96172575f48dca3b-Paper.pdf
 Unet++: https://arxiv.org/pdf/1912.05074.pdf
 Stacked Dilated U-nets: https://arxiv.org/pdf/2004.03466.pdf
 There seems to be more ways to expand on the traditional U-net architecture that I've yet to discover. I'd love to see other cool methods!
    submitted by    /u/2133  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The degradation of war]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thp253/the_degradation_of_war/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thp253/the_degradation_of_war/"/>
        <updated>2022-03-19T06:12:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Hacknaut  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R][P] StyleSwin: Transformer-based GAN for High-resolution Image Generation + Hugging Face Gradio Demo]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tho0ov/rp_styleswin_transformerbased_gan_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tho0ov/rp_styleswin_transformerbased_gan_for/"/>
        <updated>2022-03-19T04:59:33.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Illustrious_Row_9971  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Automatically Calibrate RGBD Cameras with PyTorch]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thnwka/p_automatically_calibrate_rgbd_cameras_with/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thnwka/p_automatically_calibrate_rgbd_cameras_with/"/>
        <updated>2022-03-19T04:51:55.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/covertBehavior  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Non-discrete sample "classification"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thmqdt/p_nondiscrete_sample_classification/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thmqdt/p_nondiscrete_sample_classification/"/>
        <updated>2022-03-19T03:38:55.000Z</updated>
        <summary type="html"><![CDATA[I'm a biologist, and I have a problem with a very common analysis done in my field. We often classify cells by unique profile proteins they express. Cells that are high in protein A but low in B may be called "Type 1", cells low in protein A but high in protein B may be called "Type 2", cells high in both A and B "Type 3", and cells low in both "Type 4".
 Sometimes this works well and cells are clearly one type or the other. But unfortunately nature doesn't care about our desire to neatly classify things, and I believe that cell identity exists on a spectrum. Protein expression isn't all or nothing, it's effectively a continuous variable. There are cases where some cells are probably actually "Type 1", some are actually "Type 2", but some meaningfully exist as "somewhere between Type 1 and…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An AI painting some colorful pitbulls]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thmhtf/an_ai_painting_some_colorful_pitbulls/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thmhtf/an_ai_painting_some_colorful_pitbulls/"/>
        <updated>2022-03-19T03:24:44.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/notrealAI  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advice on network size]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/thk4lj/advice_on_network_size/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/thk4lj/advice_on_network_size/"/>
        <updated>2022-03-19T01:11:26.000Z</updated>
        <summary type="html"><![CDATA[I'm making DQN for automatic trading. Due to the nature of the problem, I figured the model would possibly need to be quite complex, but actually I have no idea just how big it should be.
 I am feeding it with 8000 numbers representing the last prices. Due to some reasons still unknown to me, I can basically add as many layers as I want as long as they are not bigger than 4096 neurons without increasing learning speed.
 I just wanted to ask: would 20 layers be too many ? Would 5 probably not be enough ? Right now the model looks like that:
 Dense(2048, "relu")
 Dense(1024, "relu")
 Dense(512, "relu")
 Dense(256, "relu")
 Dense(128, "relu")
 Dense(1024, "relu")
 Dense(2048, "relu")
 Dense(1024, "relu")
 Dense(512, "relu")
 Dense(256, "relu")
 Dense(128, "relu")
 Dense(64, "relu")
 Dense(32, "relu")
 Dense(16, "relu")
 Dense(8, "relu")
 Dense(3, "relu")
 Any help is appreciated, really I just want to have an idea of how much would be too much or too little for this specific problem, I'm a total beginner.
    submitted by    /u/superpowers_fan  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Threat Detection for General Social Engineering Attack Using Machine Learning Techniques. (arXiv:2203.07933v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.07933</id>
        <link href="http://arxiv.org/abs/2203.07933"/>
        <updated>2022-03-19T00:42:49.469Z</updated>
        <summary type="html"><![CDATA[This paper explores the threat detection for general Social Engineering (SE)
attack using Machine Learning (ML) techniques, rather than focusing on or
limited to a specific SE attack type, e.g. email phishing. Firstly, this paper
processes and obtains more SE threat data from the previous Knowledge Graph
(KG), and then extracts different threat features and generates new datasets
corresponding with three different feature combinations. Finally, 9 types of ML
models are created and trained using the three datasets, respectively, and
their performance are compared and analyzed with 27 threat detectors and 270
times of experiments. The experimental results and analyses show that: 1) the
ML techniques are feasible in detecting general SE attacks and some ML models
are quite effective; ML-based SE threat detection is complementary with
KG-based approaches; 2) the generated datasets are usable and the SE domain
ontology proposed in previous work can dissect SE attacks and deliver the SE
threat features, allowing it to be used as a data model for future research.
Besides, more conclusions and analyses about the characteristics of different
ML detectors and the datasets are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zuoguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yimo Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongsong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Limin Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Autonomy: Self-Initiation, Adaptation and Continual Learning. (arXiv:2203.08994v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2203.08994</id>
        <link href="http://arxiv.org/abs/2203.08994"/>
        <updated>2022-03-19T00:42:49.462Z</updated>
        <summary type="html"><![CDATA[As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties,
detecting novelties, characterizing them, accommodating or adapting to them,
and gathering ground-truth training data and incrementally learning the
unknowns/novelties are critical to making the AI agent more and more
knowledgeable and powerful over time. The key challenge is how to automate the
process so that it is carried out continually on the agent's own initiative and
through its own interactions with humans, other agents and the environment just
like human on-the-job learning. This paper proposes a framework (called SOLA)
for this learning paradigm to promote the research of building autonomous and
continual learning enabled AI agents. To show feasibility, an implemented agent
is also described.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1"&gt;Sahisnu Mazumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1"&gt;Eric Robertson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigsby_S/0/1/0/all/0/1"&gt;Scott Grigsby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Framework for Gradient Leakage. (arXiv:2111.04706v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.04706</id>
        <link href="http://arxiv.org/abs/2111.04706"/>
        <updated>2022-03-19T00:42:49.456Z</updated>
        <summary type="html"><![CDATA[Federated learning is an established method for training machine learning
models without sharing training data. However, recent work has shown that it
cannot guarantee data privacy as shared gradients can still leak sensitive
information. To formalize the problem of gradient leakage, we propose a
theoretical framework that enables, for the first time, analysis of the Bayes
optimal adversary phrased as an optimization problem. We demonstrate that
existing leakage attacks can be seen as approximations of this optimal
adversary with different assumptions on the probability distributions of the
input data and gradients. Our experiments confirm the effectiveness of the
Bayes optimal adversary when it has knowledge of the underlying distribution.
Further, our experimental evaluation shows that several existing heuristic
defenses are not effective against stronger attacks, especially early in the
training process. Thus, our findings indicate that the construction of more
effective defenses and their evaluation remains an open problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1"&gt;Mislav Balunovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1"&gt;Dimitar I. Dimitrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staab_R/0/1/0/all/0/1"&gt;Robin Staab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1"&gt;Martin Vechev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction. (arXiv:2202.05146v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.05146</id>
        <link href="http://arxiv.org/abs/2202.05146"/>
        <updated>2022-03-19T00:42:49.438Z</updated>
        <summary type="html"><![CDATA[Predicting how a drug-like molecule binds to a specific protein target is a
core problem in drug discovery. An extremely fast computational binding method
would enable key applications such as fast virtual screening or drug
engineering. Existing methods are computationally expensive as they rely on
heavy candidate sampling coupled with scoring, ranking, and fine-tuning steps.
We challenge this paradigm with EquiBind, an SE(3)-equivariant geometric deep
learning model performing direct-shot prediction of both i) the receptor
binding location (blind docking) and ii) the ligand's bound pose and
orientation. EquiBind achieves significant speed-ups and better quality
compared to traditional and recent baselines. Further, we show extra
improvements when coupling it with existing fine-tuning techniques at the cost
of increased running time. Finally, we propose a novel and fast fine-tuning
model that adjusts torsion angles of a ligand's rotatable bonds based on
closed-form global minima of the von Mises angular distance to a given input
atomic point cloud, avoiding previous expensive differential evolution
strategies for energy minimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Stark_H/0/1/0/all/0/1"&gt;Hannes St&amp;#xe4;rk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ganea_O/0/1/0/all/0/1"&gt;Octavian-Eugen Ganea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Pattanaik_L/0/1/0/all/0/1"&gt;Lagnajit Pattanaik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi Jaakkola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Metric Learning in Multi-View Data: A Downstream Task Perspective. (arXiv:2106.07138v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07138</id>
        <link href="http://arxiv.org/abs/2106.07138"/>
        <updated>2022-03-19T00:42:49.431Z</updated>
        <summary type="html"><![CDATA[Self-supervised metric learning has been a successful approach for learning a
distance from an unlabeled dataset. The resulting distance is broadly useful
for improving various distance-based downstream tasks, even when no information
from downstream tasks is utilized in the metric learning stage. To gain
insights into this approach, we develop a statistical framework to
theoretically study how self-supervised metric learning can benefit downstream
tasks in the context of multi-view data. Under this framework, we show that the
target distance of metric learning satisfies several desired properties for the
downstream tasks. On the other hand, our investigation suggests the target
distance can be further improved by moderating each direction's weights. In
addition, our analysis precisely characterizes the improvement by
self-supervised metric learning on four commonly used downstream tasks: sample
identification, two-sample testing, $k$-means clustering, and $k$-nearest
neighbor classification. When the distance is estimated from an unlabeled
dataset, we establish the upper bound on distance estimation's accuracy and the
number of samples sufficient for downstream task improvement. Finally,
numerical experiments are presented to support the theoretical results in the
paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shulei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backpropagation through Time and Space: Learning Numerical Methods with Multi-Agent Reinforcement Learning. (arXiv:2203.08937v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08937</id>
        <link href="http://arxiv.org/abs/2203.08937"/>
        <updated>2022-03-19T00:42:49.424Z</updated>
        <summary type="html"><![CDATA[We introduce Backpropagation Through Time and Space (BPTTS), a method for
training a recurrent spatio-temporal neural network, that is used in a
homogeneous multi-agent reinforcement learning (MARL) setting to learn
numerical methods for hyperbolic conservation laws. We treat the numerical
schemes underlying partial differential equations (PDEs) as a Partially
Observable Markov Game (POMG) in Reinforcement Learning (RL). Similar to
numerical solvers, our agent acts at each discrete location of a computational
space for efficient and generalizable learning. To learn higher-order spatial
methods by acting on local states, the agent must discern how its actions at a
given spatiotemporal location affect the future evolution of the state. The
manifestation of this non-stationarity is addressed by BPTTS, which allows for
the flow of gradients across both space and time. The learned numerical
policies are comparable to the SOTA numerics in two settings, the Burgers'
Equation and the Euler Equations, and generalize well to other simulation
set-ups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Way_E/0/1/0/all/0/1"&gt;Elliot Way&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapilivai_D/0/1/0/all/0/1"&gt;Dheeraj S.K. Kapilivai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yiwei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lei Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework and Benchmark for Deep Batch Active Learning for Regression. (arXiv:2203.09410v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09410</id>
        <link href="http://arxiv.org/abs/2203.09410"/>
        <updated>2022-03-19T00:42:49.418Z</updated>
        <summary type="html"><![CDATA[We study the performance of different pool-based Batch Mode Deep Active
Learning (BMDAL) methods for regression on tabular data, focusing on methods
that do not require to modify the network architecture and training. Our
contributions are three-fold: First, we present a framework for constructing
BMDAL methods out of kernels, kernel transformations and selection methods,
showing that many of the most popular BMDAL methods fit into our framework.
Second, we propose new components, leading to a new BMDAL method. Third, we
introduce an open-source benchmark with 15 large tabular data sets, which we
use to compare different BMDAL methods. Our benchmark results show that a
combination of our novel components yields new state-of-the-art results in
terms of RMSE and is computationally efficient. We provide open-source code
that includes efficient implementations of all kernels, kernel transformations,
and selection methods, and can be used for reproducing our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Holzmuller_D/0/1/0/all/0/1"&gt;David Holzm&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zaverkin_V/0/1/0/all/0/1"&gt;Viktor Zaverkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kastner_J/0/1/0/all/0/1"&gt;Johannes K&amp;#xe4;stner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Steinwart_I/0/1/0/all/0/1"&gt;Ingo Steinwart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs. (arXiv:2203.09251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09251</id>
        <link href="http://arxiv.org/abs/2203.09251"/>
        <updated>2022-03-19T00:42:49.412Z</updated>
        <summary type="html"><![CDATA[In probably approximately correct (PAC) reinforcement learning (RL), an agent
is required to identify an $\epsilon$-optimal policy with probability
$1-\delta$. While minimax optimal algorithms exist for this problem, its
instance-dependent complexity remains elusive in episodic Markov decision
processes (MDPs). In this paper, we propose the first (nearly) matching upper
and lower bounds on the sample complexity of PAC RL in deterministic episodic
MDPs with finite state and action spaces. In particular, our bounds feature a
new notion of sub-optimality gap for state-action pairs that we call the
deterministic return gap. While our instance-dependent lower bound is written
as a linear program, our algorithms are very simple and do not require solving
such an optimization problem during learning. Their design and analyses employ
novel ideas, including graph-theoretical concepts such as minimum flows and
maximum cuts, which we believe to shed new light on this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tirinzoni_A/0/1/0/all/0/1"&gt;Andrea Tirinzoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Marjani_A/0/1/0/all/0/1"&gt;Aymen Al-Marjani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Emilie Kaufmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localizing Visual Sounds the Easy Way. (arXiv:2203.09324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09324</id>
        <link href="http://arxiv.org/abs/2203.09324"/>
        <updated>2022-03-19T00:42:49.404Z</updated>
        <summary type="html"><![CDATA[Unsupervised audio-visual source localization aims at localizing visible
sound sources in a video without relying on ground-truth localization for
training. Previous works often seek high audio-visual similarities for likely
positive (sounding) regions and low similarities for likely negative regions.
However, accurately distinguishing between sounding and non-sounding regions is
challenging without manual annotations. In this work, we propose a simple yet
effective approach for Easy Visual Sound Localization, namely EZ-VSL, without
relying on the construction of positive and/or negative regions during
training. Instead, we align audio and visual spaces by seeking audio-visual
representations that are aligned in, at least, one location of the associated
image, while not matching other images, at any location. We also introduce a
novel object guided localization scheme at inference time for improved
precision. Our simple and effective framework achieves state-of-the-art
performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In
particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to
83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is
available at https://github.com/stoneMo/EZ-VSL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Shentong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1"&gt;Pedro Morgado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convert, compress, correct: Three steps toward communication-efficient DNN training. (arXiv:2203.09044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09044</id>
        <link href="http://arxiv.org/abs/2203.09044"/>
        <updated>2022-03-19T00:42:49.396Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a novel algorithm, $\mathsf{CO}_3$, for
communication-efficiency distributed Deep Neural Network (DNN) training.
$\mathsf{CO}_3$ is a joint training/communication protocol, which encompasses
three processing steps for the network gradients: (i) quantization through
floating-point conversion, (ii) lossless compression, and (iii) error
correction. These three components are crucial in the implementation of
distributed DNN training over rate-constrained links. The interplay of these
three steps in processing the DNN gradients is carefully balanced to yield a
robust and high-performance scheme. The performance of the proposed scheme is
investigated through numerical evaluations over CIFAR-10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhong-Jing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1"&gt;Eduin E. Hernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Chih Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rini_S/0/1/0/all/0/1"&gt;Stefano Rini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Long-Term Reward Redistribution via Randomized Return Decomposition. (arXiv:2111.13485v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.13485</id>
        <link href="http://arxiv.org/abs/2111.13485"/>
        <updated>2022-03-19T00:42:49.378Z</updated>
        <summary type="html"><![CDATA[Many practical applications of reinforcement learning require agents to learn
from sparse and delayed rewards. It challenges the ability of agents to
attribute their actions to future outcomes. In this paper, we consider the
problem formulation of episodic reinforcement learning with trajectory
feedback. It refers to an extreme delay of reward signals, in which the agent
can only obtain one reward signal at the end of each trajectory. A popular
paradigm for this problem setting is learning with a designed auxiliary dense
reward function, namely proxy reward, instead of sparse environmental signals.
Based on this framework, this paper proposes a novel reward redistribution
algorithm, randomized return decomposition (RRD), to learn a proxy reward
function for episodic reinforcement learning. We establish a surrogate problem
by Monte-Carlo sampling that scales up least-squares-based reward
redistribution to long-horizon problems. We analyze our surrogate loss function
by connection with existing methods in the literature, which illustrates the
algorithmic properties of our approach. In experiments, we extensively evaluate
our proposed method on a variety of benchmark tasks with episodic rewards and
demonstrate substantial improvement over baseline algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhizhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1"&gt;Ruihan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jian Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer. (arXiv:2203.09293v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09293</id>
        <link href="http://arxiv.org/abs/2203.09293"/>
        <updated>2022-03-19T00:42:49.365Z</updated>
        <summary type="html"><![CDATA[Nowadays, our mobility systems are evolving into the era of intelligent
vehicles that aim to improve road safety. Due to their vulnerability,
pedestrians are the users who will benefit the most from these developments.
However, predicting their trajectory is one of the most challenging concerns.
Indeed, accurate prediction requires a good understanding of multi-agent
interactions that can be complex. Learning the underlying spatial and temporal
patterns caused by these interactions is even more of a competitive and open
problem that many researchers are tackling. In this paper, we introduce a model
called PRediction Transformer (PReTR) that extracts features from the
multi-agent scenes by employing a factorized spatio-temporal attention module.
It shows less computational needs than previously studied models with
empirically better results. Besides, previous works in motion prediction suffer
from the exposure bias problem caused by generating future sequences
conditioned on model prediction samples rather than ground-truth samples. In
order to go beyond the proposed solutions, we leverage encoder-decoder
Transformer networks for parallel decoding a set of learned object queries.
This non-autoregressive solution avoids the need for iterative conditioning and
arguably decreases training and testing computational time. We evaluate our
model on the ETH/UCY datasets, a publicly available benchmark for pedestrian
trajectory prediction. Finally, we justify our usage of the parallel decoding
technique by showing that the trajectory prediction task can be better solved
as a non-autoregressive task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1"&gt;Lina Achaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barry_T/0/1/0/all/0/1"&gt;Thierno Barry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1"&gt;Thibault Fouqueray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1"&gt;Julien Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1"&gt;Francois Aioun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1"&gt;Francois Charpillet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLM: General Language Model Pretraining with Autoregressive Blank Infilling. (arXiv:2103.10360v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10360</id>
        <link href="http://arxiv.org/abs/2103.10360"/>
        <updated>2022-03-19T00:42:49.358Z</updated>
        <summary type="html"><![CDATA[There have been various types of pretraining architectures including
autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and
encoder-decoder models (e.g., T5). However, none of the pretraining frameworks
performs the best for all tasks of three main categories including natural
language understanding (NLU), unconditional generation, and conditional
generation. We propose a General Language Model (GLM) based on autoregressive
blank infilling to address this challenge. GLM improves blank filling
pretraining by adding 2D positional encodings and allowing an arbitrary order
to predict spans, which results in performance gains over BERT and T5 on NLU
tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying
the number and lengths of blanks. On a wide range of tasks across NLU,
conditional and unconditional generation, GLM outperforms BERT, T5, and GPT
given the same model sizes and data, and achieves the best performance from a
single pretrained model with 1.25x parameters of BERT Large , demonstrating its
generalizability to different downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhengxiao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yujie Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhilin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks. (arXiv:2203.08852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08852</id>
        <link href="http://arxiv.org/abs/2203.08852"/>
        <updated>2022-03-19T00:42:49.351Z</updated>
        <summary type="html"><![CDATA[We propose a new method for spatio-temporal forecasting on arbitrarily
distributed points. Assuming that the observed system follows an unknown
partial differential equation, we derive a continuous-time model for the
dynamics of the data via the finite element method. The resulting graph neural
network estimates the instantaneous effects of the unknown dynamics on each
cell in a meshing of the spatial domain. Our model can incorporate prior
knowledge via assumptions on the form of the unknown PDE, which induce a
structural bias towards learning specific processes. Through this mechanism, we
derive a transport variant of our model from the convection equation and show
that it improves the transfer performance to higher-resolution meshes on sea
surface temperature and gas flow forecasting against baseline models
representing a selection of spatio-temporal forecasting methods. A qualitative
analysis shows that our model disentangles the data dynamics into their
constituent parts, which makes it uniquely interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1"&gt;Marten Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearest Neighbor Classifier with Margin Penalty for Active Learning. (arXiv:2203.09174v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2203.09174</id>
        <link href="http://arxiv.org/abs/2203.09174"/>
        <updated>2022-03-19T00:42:49.331Z</updated>
        <summary type="html"><![CDATA[As deep learning becomes the mainstream in the field of natural language
processing, the need for suitable active learning method are becoming
unprecedented urgent. Active Learning (AL) methods based on nearest neighbor
classifier are proposed and demonstrated superior results. However, existing
nearest neighbor classifier are not suitable for classifying mutual exclusive
classes because inter-class discrepancy cannot be assured by nearest neighbor
classifiers. As a result, informative samples in the margin area can not be
discovered and AL performance are damaged. To this end, we propose a novel
Nearest neighbor Classifier with Margin penalty for Active Learning(NCMAL).
Firstly, mandatory margin penalty are added between classes, therefore both
inter-class discrepancy and intra-class compactness are both assured. Secondly,
a novel sample selection strategy are proposed to discover informative samples
within the margin area. To demonstrate the effectiveness of the methods, we
conduct extensive experiments on for datasets with other state-of-the-art
methods. The experimental results demonstrate that our method achieves better
results with fewer annotated samples than all baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhiqiao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks. (arXiv:2203.09168v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09168</id>
        <link href="http://arxiv.org/abs/2203.09168"/>
        <updated>2022-03-19T00:42:49.321Z</updated>
        <summary type="html"><![CDATA[Capturing aleatoric uncertainty is a critical part of many machine learning
systems. In deep learning, a common approach to this end is to train a neural
network to estimate the parameters of a heteroscedastic Gaussian distribution
by maximizing the logarithm of the likelihood function under the observed data.
In this work, we examine this approach and identify potential hazards
associated with the use of log-likelihood in conjunction with gradient-based
optimizers. First, we present a synthetic example illustrating how this
approach can lead to very poor but stable parameter estimates. Second, we
identify the culprit to be the log-likelihood loss, along with certain
conditions that exacerbate the issue. Third, we present an alternative
formulation, termed $\beta$-NLL, in which each data point's contribution to the
loss is weighted by the $\beta$-exponentiated variance estimate. We show that
using an appropriate $\beta$ largely mitigates the issue in our illustrative
example. Fourth, we evaluate this approach on a range of domains and tasks and
show that it achieves considerable improvements and performs more robustly
concerning hyperparameters, both in predictive RMSE and log-likelihood
criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seitzer_M/0/1/0/all/0/1"&gt;Maximilian Seitzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_A/0/1/0/all/0/1"&gt;Arash Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antic_D/0/1/0/all/0/1"&gt;Dimitrije Antic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1"&gt;Georg Martius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near Instance Optimal Model Selection for Pure Exploration Linear Bandits. (arXiv:2109.05131v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.05131</id>
        <link href="http://arxiv.org/abs/2109.05131"/>
        <updated>2022-03-19T00:42:49.314Z</updated>
        <summary type="html"><![CDATA[We introduce the model selection problem in pure exploration linear bandits,
where the learner needs to adapt to the instance-dependent complexity measure
of the smallest hypothesis class containing the true model. We design
algorithms in both fixed confidence and fixed budget settings with near
instance optimal guarantees. The core of our algorithms is a new optimization
problem based on experimental design that leverages the geometry of the action
set to identify a near-optimal hypothesis class. Our fixed budget algorithm is
developed based on a novel selection-validation procedure, which provides a new
way to study the understudied fixed budget setting (even without the added
challenge of model selection). We adapt our algorithms, in both fixed
confidence and fixed budget settings, to problems with model misspecification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Katz_Samuels_J/0/1/0/all/0/1"&gt;Julian Katz-Samuels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09249</id>
        <link href="http://arxiv.org/abs/2203.09249"/>
        <updated>2022-03-19T00:42:49.308Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is an emerging distributed learning paradigm under
privacy constraint. Data heterogeneity is one of the main challenges in FL,
which results in slow convergence and degraded performance. Most existing
approaches only tackle the heterogeneity challenge by restricting the local
model update in client, ignoring the performance drop caused by direct global
model aggregation. Instead, we propose a data-free knowledge distillation
method to fine-tune the global model in the server (FedFTG), which relieves the
issue of direct model aggregation. Concretely, FedFTG explores the input space
of local models through a generator, and uses it to transfer the knowledge from
local models to the global model. Besides, we propose a hard sample mining
scheme to achieve effective knowledge distillation throughout the training. In
addition, we develop customized label sampling and class-level ensemble to
derive maximum utilization of knowledge, which implicitly mitigates the
distribution discrepancy across clients. Extensive experiments show that our
FedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and
can serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and
SCAFFOLD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Liang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Ling-Yu Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time and the Value of Data. (arXiv:2203.09118v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09118</id>
        <link href="http://arxiv.org/abs/2203.09118"/>
        <updated>2022-03-19T00:42:49.289Z</updated>
        <summary type="html"><![CDATA[Managers often believe that collecting more data will continually improve the
accuracy of their machine learning models. However, we argue in this paper that
when data lose relevance over time, it may be optimal to collect a limited
amount of recent data instead of keeping around an infinite supply of older
(less relevant) data. In addition, we argue that increasing the stock of data
by including older datasets may, in fact, damage the model's accuracy.
Expectedly, the model's accuracy improves by increasing the flow of data
(defined as data collection rate); however, it requires other tradeoffs in
terms of refreshing or retraining machine learning models more frequently.

Using these results, we investigate how the business value created by machine
learning models scales with data and when the stock of data establishes a
sustainable competitive advantage. We argue that data's time-dependency weakens
the barrier to entry that the stock of data creates. As a result, a competing
firm equipped with a limited (yet sufficient) amount of recent data can develop
more accurate models. This result, coupled with the fact that older datasets
may deteriorate models' accuracy, suggests that created business value doesn't
scale with the stock of available data unless the firm offloads less relevant
data from its data repository. Consequently, a firm's growth policy should
incorporate a balance between the stock of historical data and the flow of new
data.

We complement our theoretical results with an experiment. In the experiment,
we empirically measure the loss in the accuracy of a next word prediction model
trained on datasets from various time periods. Our empirical measurements
confirm the economic significance of the value decline over time. For example,
100MB of text data, after seven years, becomes as valuable as 50MB of current
data for the next word prediction task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valavi_E/0/1/0/all/0/1"&gt;Ehsan Valavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1"&gt;Joel Hestness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1"&gt;Newsha Ardalani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iansiti_M/0/1/0/all/0/1"&gt;Marco Iansiti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration. (arXiv:2107.05446v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05446</id>
        <link href="http://arxiv.org/abs/2107.05446"/>
        <updated>2022-03-19T00:42:49.282Z</updated>
        <summary type="html"><![CDATA[Source-free domain adaptation (SFDA) aims to adapt a model trained on
labelled data in a source domain to unlabelled data in a target domain without
access to the source-domain data during adaptation. Existing methods for SFDA
leverage entropy-minimization techniques which: (i) apply only to
classification; (ii) destroy model calibration; and (iii) rely on the source
model achieving a good level of feature-space class-separation in the target
domain. We address these issues for a particularly pervasive type of domain
shift called measurement shift which can be resolved by restoring the source
features rather than extracting new ones. In particular, we propose Feature
Restoration (FR) wherein we: (i) store a lightweight and flexible approximation
of the feature distribution under the source data; and (ii) adapt the
feature-extractor such that the approximate feature distribution under the
target data realigns with that saved on the source. We additionally propose a
bottom-up training scheme which boosts performance, which we call Bottom-Up
Feature Restoration (BUFR). On real and synthetic data, we demonstrate that
BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and
data efficiency, while being less reliant on the performance of the source
model in the target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1"&gt;Cian Eastwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mason_I/0/1/0/all/0/1"&gt;Ian Mason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1"&gt;Christopher K. I. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Learning on Graphs for Disease Relation Extraction. (arXiv:2203.08893v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08893</id>
        <link href="http://arxiv.org/abs/2203.08893"/>
        <updated>2022-03-19T00:42:49.275Z</updated>
        <summary type="html"><![CDATA[Objective: Disease knowledge graphs are a way to connect, organize, and
access disparate information about diseases with numerous benefits for
artificial intelligence (AI). To create knowledge graphs, it is necessary to
extract knowledge from multimodal datasets in the form of relationships between
disease concepts and normalize both concepts and relationship types. Methods:
We introduce REMAP, a multimodal approach for disease relation extraction and
classification. The REMAP machine learning approach jointly embeds a partial,
incomplete knowledge graph and a medical language dataset into a compact latent
vector space, followed by aligning the multimodal embeddings for optimal
disease relation extraction. Results: We apply REMAP approach to a disease
knowledge graph with 96,913 relations and a text dataset of 1.24 million
sentences. On a dataset annotated by human experts, REMAP improves text-based
disease relation extraction by 10.0% (accuracy) and 17.2% (F1-score) by fusing
disease knowledge graphs with text information. Further, REMAP leverages text
information to recommend new relationships in the knowledge graph,
outperforming graph-based methods by 8.4% (accuracy) and 10.4% (F1-score).
Discussion: Systematized knowledge is becoming the backbone of AI, creating
opportunities to inject semantics into AI and fully integrate it into machine
learning algorithms. While prior semantic knowledge can assist in extracting
disease relationships from text, existing methods can not fully leverage
multimodal datasets. Conclusion: REMAP is a multimodal approach for extracting
and classifying disease relationships by fusing structured knowledge and text
information. REMAP provides a flexible neural architecture to easily find,
access, and validate AI-driven relationships between disease concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yucong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Keming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Sheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianxi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1"&gt;Marinka Zitnik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Rejection Function Meets Character Recognition Tasks. (arXiv:2203.09151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09151</id>
        <link href="http://arxiv.org/abs/2203.09151"/>
        <updated>2022-03-19T00:42:49.268Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an optimal rejection method for rejecting ambiguous
samples by a rejection function. This rejection function is trained together
with a classification function under the framework of Learning-with-Rejection
(LwR). The highlights of LwR are: (1) the rejection strategy is not heuristic
but has a strong background from a machine learning theory, and (2) the
rejection function can be trained on an arbitrary feature space which is
different from the feature space for classification. The latter suggests we can
choose a feature space that is more suitable for rejection. Although the past
research on LwR focused only on its theoretical aspect, we propose to utilize
LwR for practical pattern classification tasks. Moreover, we propose to use
features from different CNN layers for classification and rejection. Our
extensive experiments of notMNIST classification and character/non-character
classification demonstrate that the proposed method achieves better performance
than traditional rejection strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiaotong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuchen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suehiro_D/0/1/0/all/0/1"&gt;Daiki Suehiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian initializations help deep variational quantum circuits escape from the barren plateau. (arXiv:2203.09376v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2203.09376</id>
        <link href="http://arxiv.org/abs/2203.09376"/>
        <updated>2022-03-19T00:42:49.262Z</updated>
        <summary type="html"><![CDATA[Variational quantum circuits have been widely employed in quantum simulation
and quantum machine learning in recent years. However, quantum circuits with
random structures have poor trainability due to the exponentially vanishing
gradient with respect to the circuit depth and the qubit number. This result
leads to a general belief that deep quantum circuits will not be feasible for
practical tasks. In this work, we propose an initialization strategy with
theoretical guarantees for the vanishing gradient problem in general deep
circuits. Specifically, we prove that under proper Gaussian initialized
parameters, the norm of the gradient decays at most polynomially when the qubit
number and the circuit depth increase. Our theoretical results hold for both
the local and the global observable cases, where the latter was believed to
have vanishing gradients even for shallow circuits. Experimental results verify
our theoretical findings in the quantum simulation and quantum chemistry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaining Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hsieh_M/0/1/0/all/0/1"&gt;Min-Hsiu Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation. (arXiv:2203.09391v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09391</id>
        <link href="http://arxiv.org/abs/2203.09391"/>
        <updated>2022-03-19T00:42:49.244Z</updated>
        <summary type="html"><![CDATA[Data Augmentation (DA) is known to improve the generalizability of deep
neural networks. Most existing DA techniques naively add a certain number of
augmented samples without considering the quality and the added computational
cost of these samples. To tackle this problem, a common strategy, adopted by
several state-of-the-art DA methods, is to adaptively generate or re-weight
augmented samples with respect to the task objective during training. However,
these adaptive DA methods: (1) are computationally expensive and not
sample-efficient, and (2) are designed merely for a specific setting. In this
work, we present a universal DA technique, called Glitter, to overcome both
issues. Glitter can be plugged into any DA method, making training
sample-efficient without sacrificing performance. From a pre-generated pool of
augmented samples, Glitter adaptively selects a subset of worst-case samples
with maximal loss, analogous to adversarial DA. Without altering the training
strategy, the task objective can be optimized on the selected subset. Our
thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three
widely used training setups including consistency training, self-distillation
and knowledge distillation reveal that Glitter is substantially faster to train
and achieves a competitive performance, compared to strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1"&gt;Ehsan Kamalloo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1"&gt;Mehdi Rezagholizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoK: Differential Privacy on Graph-Structured Data. (arXiv:2203.09205v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2203.09205</id>
        <link href="http://arxiv.org/abs/2203.09205"/>
        <updated>2022-03-19T00:42:49.237Z</updated>
        <summary type="html"><![CDATA[In this work, we study the applications of differential privacy (DP) in the
context of graph-structured data. We discuss the formulations of DP applicable
to the publication of graphs and their associated statistics as well as machine
learning on graph-based data, including graph neural networks (GNNs). The
formulation of DP in the context of graph-structured data is difficult, as
individual data points are interconnected (often non-linearly or sparsely).
This connectivity complicates the computation of individual privacy loss in
differentially private learning. The problem is exacerbated by an absence of a
single, well-established formulation of DP in graph settings. This issue
extends to the domain of GNNs, rendering private machine learning on
graph-structured data a challenging task. A lack of prior systematisation work
motivated us to study graph-based learning from a privacy perspective. In this
work, we systematise different formulations of DP on graphs, discuss challenges
and promising applications, including the GNN domain. We compare and separate
works into graph analysis tasks and graph learning tasks with GNNs. Finally, we
conclude our work with a discussion of open questions and potential directions
for further research in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_T/0/1/0/all/0/1"&gt;Tamara T. Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paetzold_J/0/1/0/all/0/1"&gt;Johannes C. Paetzold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TMS: A Temporal Multi-scale Backbone Design for Speaker Embedding. (arXiv:2203.09098v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.09098</id>
        <link href="http://arxiv.org/abs/2203.09098"/>
        <updated>2022-03-19T00:42:49.225Z</updated>
        <summary type="html"><![CDATA[Speaker embedding is an important front-end module to explore discriminative
speaker features for many speech applications where speaker information is
needed. Current SOTA backbone networks for speaker embedding are designed to
aggregate multi-scale features from an utterance with multi-branch network
architectures for speaker representation. However, naively adding many branches
of multi-scale features with the simple fully convolutional operation could not
efficiently improve the performance due to the rapid increase of model
parameters and computational complexity. Therefore, in the most current
state-of-the-art network architectures, only a few branches corresponding to a
limited number of temporal scales could be designed for speaker embeddings. To
address this problem, in this paper, we propose an effective temporal
multi-scale (TMS) model where multi-scale branches could be efficiently
designed in a speaker embedding network almost without increasing computational
costs. The new model is based on the conventional TDNN, where the network
architecture is smartly separated into two modeling operators: a
channel-modeling operator and a temporal multi-branch modeling operator. Adding
temporal multi-scale in the temporal multi-branch operator needs only a little
bit increase of the number of parameters, and thus save more computational
budget for adding more branches with large temporal scales. Moreover, in the
inference stage, we further developed a systemic re-parameterization method to
convert the TMS-based model into a single-path-based topology in order to
increase inference speed. We investigated the performance of the new TMS method
for automatic speaker verification (ASV) on in-domain and out-of-domain
conditions. Results show that the TMS-based model obtained a significant
increase in the performance over the SOTA ASV models, meanwhile, had a faster
inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiteng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jianguo Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xugang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wenhuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Di Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junhai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yantao Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1"&gt;Jianwu Dang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Learning on Graphs: A Survey. (arXiv:2203.09308v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09308</id>
        <link href="http://arxiv.org/abs/2203.09308"/>
        <updated>2022-03-19T00:42:48.923Z</updated>
        <summary type="html"><![CDATA[Graph representation learning has attracted tremendous attention due to its
remarkable performance in many real-world applications. However, prevailing
(semi-)supervised graph representation learning models for specific tasks often
suffer from label sparsity issue as data labeling is always time and resource
consuming. In light of this, few-shot learning on graphs (FSLG), which combines
the strengths of graph representation learning and few-shot learning together,
has been proposed to tackle the performance degradation in face of limited
annotated data challenge. There have been many studies working on FSLG
recently. In this paper, we comprehensively survey these work in the form of a
series of methods and applications. Specifically, we first introduce FSLG
challenges and bases, then categorize and summarize existing work of FSLG in
terms of three major graph mining tasks at different granularity levels, i.e.,
node, edge, and graph. Finally, we share our thoughts on some future research
directions of FSLG. The authors of this survey have contributed significantly
to the AI literature on FSLG over the last few years.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chuxu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kaize Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yanfang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1"&gt;Nitesh V. Chawla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Augmentation Learning. (arXiv:2203.09020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09020</id>
        <link href="http://arxiv.org/abs/2203.09020"/>
        <updated>2022-03-19T00:42:48.916Z</updated>
        <summary type="html"><![CDATA[Graph Augmentation Learning (GAL) provides outstanding solutions for graph
learning in handling incomplete data, noise data, etc. Numerous GAL methods
have been proposed for graph-based applications such as social network analysis
and traffic flow forecasting. However, the underlying reasons for the
effectiveness of these GAL methods are still unclear. As a consequence, how to
choose optimal graph augmentation strategy for a certain application scenario
is still in black box. There is a lack of systematic, comprehensive, and
experimentally validated guideline of GAL for scholars. Therefore, in this
survey, we in-depth review GAL techniques from macro (graph), meso (subgraph),
and micro (node/edge) levels. We further detailedly illustrate how GAL enhance
the data quality and the model performance. The aggregation mechanism of
augmentation strategies and graph learning models are also discussed by
different application scenarios, i.e., data-specific, model-specific, and
hybrid scenarios. To better show the outperformance of GAL, we experimentally
validate the effectiveness and adaptability of different GAL strategies in
different downstream tasks. Finally, we share our insights on several open
issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability,
and generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shuo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Huafei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1"&gt;Minh N. Dao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Feng Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D-UCaps: 3D Capsules Unet for Volumetric Image Segmentation. (arXiv:2203.08965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.08965</id>
        <link href="http://arxiv.org/abs/2203.08965"/>
        <updated>2022-03-19T00:42:48.650Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation has been so far achieving promising results with
Convolutional Neural Networks (CNNs). However, it is arguable that in
traditional CNNs, its pooling layer tends to discard important information such
as positions. Moreover, CNNs are sensitive to rotation and affine
transformation. Capsule network is a data-efficient network design proposed to
overcome such limitations by replacing pooling layers with dynamic routing and
convolutional strides, which aims to preserve the part-whole relationships.
Capsule network has shown a great performance in image recognition and natural
language processing, but applications for medical image segmentation,
particularly volumetric image segmentation, has been limited. In this work, we
propose 3D-UCaps, a 3D voxel-based Capsule network for medical volumetric image
segmentation. We build the concept of capsules into a CNN by designing a
network with two pathways: the first pathway is encoded by 3D Capsule blocks,
whereas the second pathway is decoded by 3D CNNs blocks. 3D-UCaps, therefore
inherits the merits from both Capsule network to preserve the spatial
relationship and CNNs to learn visual representation. We conducted experiments
on various datasets to demonstrate the robustness of 3D-UCaps including
iSeg-2017, LUNA16, Hippocampus, and Cardiac, where our method outperforms
previous Capsule networks and 3D-Unets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tan Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1"&gt;Binh-Son Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.07342</id>
        <link href="http://arxiv.org/abs/2110.07342"/>
        <updated>2022-03-19T00:42:48.631Z</updated>
        <summary type="html"><![CDATA[Recent methods for embodied instruction following are typically trained
end-to-end using imitation learning. This often requires the use of expert
trajectories and low-level language instructions. Such approaches assume that
neural states will integrate multimodal semantics to perform state tracking,
building spatial memory, exploration, and long-term planning. In contrast, we
propose a modular method with structured representations that (1) builds a
semantic map of the scene and (2) performs exploration with a semantic search
policy, to achieve the natural language goal. Our modular method achieves SOTA
performance (24.46 %) with a substantial (8.17 % absolute) gap from previous
work while using less data by eschewing both expert trajectories and low-level
instructions. Leveraging low-level language, however, can further increase our
performance (26.49 %). Our findings suggest that an explicit spatial memory and
a semantic search policy can provide a stronger and more general representation
for state-tracking and guidance, even in the absence of expert trajectories or
low-level instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1"&gt;So Yeon Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1"&gt;Devendra Singh Chaplot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1"&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks. (arXiv:2203.09289v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09289</id>
        <link href="http://arxiv.org/abs/2203.09289"/>
        <updated>2022-03-19T00:42:48.621Z</updated>
        <summary type="html"><![CDATA[Backdoor attacks impose a new threat in Deep Neural Networks (DNNs), where a
backdoor is inserted into the neural network by poisoning the training dataset,
misclassifying inputs that contain the adversary trigger. The major challenge
for defending against these attacks is that only the attacker knows the secret
trigger and the target class. The problem is further exacerbated by the recent
introduction of "Hidden Triggers", where the triggers are carefully fused into
the input, bypassing detection by human inspection and causing backdoor
identification through anomaly detection to fail. To defend against such
imperceptible attacks, in this work we systematically analyze how
representations, i.e., the set of neuron activations for a given DNN when using
the training data as inputs, are affected by backdoor attacks. We propose
PiDAn, an algorithm based on coherence optimization purifying the poisoned
data. Our analysis shows that representations of poisoned data and authentic
data in the target class are still embedded in different linear subspaces,
which implies that they show different coherence with some latent spaces. Based
on this observation, the proposed PiDAn algorithm learns a sample-wise weight
vector to maximize the projected coherence of weighted samples, where we
demonstrate that the learned weight vector has a natural "grouping effect" and
is distinguishable between authentic data and poisoned data. This enables the
systematic detection and mitigation of backdoor attacks. Based on our
theoretical analysis and experimental results, we demonstrate the effectiveness
of PiDAn in defending against backdoor attacks that use different settings of
poisoned samples on GTSRB and ILSVRC2012 datasets. Our PiDAn algorithm can
detect more than 90% infected classes and identify 95% poisoned samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_E/0/1/0/all/0/1"&gt;Esha Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1"&gt;Muhammad Shafique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maniatakos_M/0/1/0/all/0/1"&gt;Michail Maniatakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jabari_S/0/1/0/all/0/1"&gt;Saif Eddin Jabari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic fracture of a bicontinuously nanostructured copolymer: A deep-learning analysis of big-data-generating experiment. (arXiv:2112.01971v2 [cond-mat.mtrl-sci] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.01971</id>
        <link href="http://arxiv.org/abs/2112.01971"/>
        <updated>2022-03-19T00:42:48.614Z</updated>
        <summary type="html"><![CDATA[Here, we report measurements of detailed dynamic cohesive properties (DCPs)
beyond the dynamic fracture toughness of a bicontinuously nanostructured
copolymer, polyurea, under an extremely loading rate, from deep-learning
analyses of a dynamic big-data-generating experiment. We first describe a new
Dynamic Line-Image Shearing Interferometer (DL-ISI), which uses a streak camera
to record optical fringes of displacement-gradient vs time profile along a line
on sample's rear surface. This system enables us to detect crack initiation and
growth processes in plate-impact experiments. Then, we present a convolutional
neural network (CNN) based deep-learning framework, trained by extensive
finite-element simulations, that inversely determines the accurate DCPs from
the DL-ISI fringe images. For the measurements, plate-impact experiments were
performed on a set of samples with a mid-plane crack. A Conditional Generative
Adversarial Networks (cGAN) was employed first to reconstruct missing DL-ISI
fringes with recorded partial DL-ISI fringes. Then, the CNN and a correlation
method were applied to the fully reconstructed fringes to get the dynamic
fracture toughness, 12.1kJ/m^2, cohesive strength, 302 MPa, and maximum
cohesive separation, 80.5 um, within 0.4%, 2.7%, and 2.2% differences,
respectively. For the first time, the DCPs of polyurea have been successfully
obtained by the DL-ISI with the pre-trained CNN and correlation analyses of
cGAN-reconstructed data sets. The dynamic cohesive strength is found to be
nearly three times higher than the dynamic-failure-initiation strength. The
high dynamic fracture toughness is found to stem from both high dynamic
cohesive strength and high ductility of the dynamic cohesive separation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hanxun Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Jiao_T/0/1/0/all/0/1"&gt;Tong Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Clifton_R/0/1/0/all/0/1"&gt;Rodney J. Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyung-Suk Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orchestrated Value Mapping for Reinforcement Learning. (arXiv:2203.07171v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.07171</id>
        <link href="http://arxiv.org/abs/2203.07171"/>
        <updated>2022-03-19T00:42:48.607Z</updated>
        <summary type="html"><![CDATA[We present a general convergent class of reinforcement learning algorithms
that is founded on two distinct principles: (1) mapping value estimates to a
different space using arbitrary functions from a broad class, and (2) linearly
decomposing the reward signal into multiple channels. The first principle
enables incorporating specific properties into the value estimator that can
enhance learning. The second principle, on the other hand, allows for the value
function to be represented as a composition of multiple utility functions. This
can be leveraged for various purposes, e.g. dealing with highly varying reward
scales, incorporating a priori knowledge about the sources of reward, and
ensemble learning. Combining the two principles yields a general blueprint for
instantiating convergent algorithms by orchestrating diverse mapping functions
over multiple reward channels. This blueprint generalizes and subsumes
algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition. In
addition, our convergence proof for this general class relaxes certain required
assumptions in some of these algorithms. Based on our theory, we discuss
several interesting configurations as special cases. Finally, to illustrate the
potential of the design space that our theory opens up, we instantiate a
particular algorithm and evaluate its performance on the Atari suite.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1"&gt;Mehdi Fatemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_A/0/1/0/all/0/1"&gt;Arash Tavakoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Properties of Adversarially-Trained CNNs. (arXiv:2203.09243v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09243</id>
        <link href="http://arxiv.org/abs/2203.09243"/>
        <updated>2022-03-19T00:42:48.600Z</updated>
        <summary type="html"><![CDATA[Adversarial Training has proved to be an effective training paradigm to
enforce robustness against adversarial examples in modern neural network
architectures. Despite many efforts, explanations of the foundational
principles underpinning the effectiveness of Adversarial Training are limited
and far from being widely accepted by the Deep Learning community. In this
paper, we describe surprising properties of adversarially-trained models,
shedding light on mechanisms through which robustness against adversarial
attacks is implemented. Moreover, we highlight limitations and failure modes
affecting these models that were not discussed by prior works. We conduct
extensive analyses on a wide range of architectures and datasets, performing a
deep comparison between robust and natural models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carletti_M/0/1/0/all/0/1"&gt;Mattia Carletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terzi_M/0/1/0/all/0/1"&gt;Matteo Terzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susto_G/0/1/0/all/0/1"&gt;Gian Antonio Susto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigation of Physics-Informed Deep Learning for the Prediction of Parametric, Three-Dimensional Flow Based on Boundary Data. (arXiv:2203.09204v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09204</id>
        <link href="http://arxiv.org/abs/2203.09204"/>
        <updated>2022-03-19T00:42:48.583Z</updated>
        <summary type="html"><![CDATA[The placement of temperature sensitive and safety-critical components is
crucial in the automotive industry. It is therefore inevitable, even at the
design stage of new vehicles that these components are assessed for potential
safety issues. However, with increasing number of design proposals, risk
assessment quickly becomes expensive. We therefore present a parameterized
surrogate model for the prediction of three-dimensional flow fields in
aerothermal vehicle simulations. The proposed physics-informed neural network
(PINN) design is aimed at learning families of flow solutions according to a
geometric variation. In scope of this work, we could show that our
nondimensional, multivariate scheme can be efficiently trained to predict the
velocity and pressure distribution for different design scenarios and geometric
scales. The proposed algorithm is based on a parametric minibatch training
which enables the utilization of large datasets necessary for the
three-dimensional flow modeling. Further, we introduce a continuous resampling
algorithm that allows to operate on one static dataset. Every feature of our
methodology is tested individually and verified against conventional CFD
simulations. Finally, we apply our proposed method in context of an exemplary
real-world automotive application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heger_P/0/1/0/all/0/1"&gt;Philip Heger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Full_M/0/1/0/all/0/1"&gt;Markus Full&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilger_D/0/1/0/all/0/1"&gt;Daniel Hilger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosters_N/0/1/0/all/0/1"&gt;Norbert Hosters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity Estimation for Dark Matter Subhalos in Synthetic Gaia DR2 using Deep Learning. (arXiv:2203.08161v1 [astro-ph.GA] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.08161</id>
        <link href="http://arxiv.org/abs/2203.08161"/>
        <updated>2022-03-19T00:42:48.577Z</updated>
        <summary type="html"><![CDATA[The abundance of dark matter subhalos orbiting a host galaxy is a generic
prediction of the cosmological framework. It is a promising way to constrain
the nature of dark matter. Here we describe the challenges of detecting stars
whose phase-space distribution may be perturbed by the passage of dark matter
subhalos using a machine learning approach. The training data are three Milky
Way-like galaxies and nine synthetic Gaia DR2 surveys derived from these. We
first quantify the magnitude of the perturbations in the simulated galaxies
using an anomaly detection algorithm. We also estimate the feasibility of this
approach in the Gaia DR2-like catalogues by comparing the anomaly detection
based approach with a supervised classification. We find that a classification
algorithm optimized on about half a billion synthetic star observables exhibits
mild but nonzero sensitivity. This classification-based approach is not
sufficiently sensitive to pinpoint the exact locations of subhalos in the
simulation, as would be expected from the very limited number of subhalos in
the detectable region. The enormous size of the Gaia dataset motivates the
further development of scalable and accurate computational methods that could
be used to select potential regions of interest for dark matter searches to
ultimately constrain the Milky Way's subhalo mass function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bazarov_A/0/1/0/all/0/1"&gt;Abdullah Bazarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Benito_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a Benito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hutsi_G/0/1/0/all/0/1"&gt;Gert H&amp;#xfc;tsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kipper_R/0/1/0/all/0/1"&gt;Rain Kipper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pata_J/0/1/0/all/0/1"&gt;Joosep Pata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Poder_S/0/1/0/all/0/1"&gt;Sven P&amp;#xf5;der&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Prompted Training Enables Zero-Shot Task Generalization. (arXiv:2110.08207v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.08207</id>
        <link href="http://arxiv.org/abs/2110.08207"/>
        <updated>2022-03-19T00:42:48.570Z</updated>
        <summary type="html"><![CDATA[Large language models have recently been shown to attain reasonable zero-shot
generalization on a diverse set of tasks (Brown et al., 2020). It has been
hypothesized that this is a consequence of implicit multitask learning in
language models' pretraining (Radford et al., 2019). Can zero-shot
generalization instead be directly induced by explicit multitask learning? To
test this question at scale, we develop a system for easily mapping any natural
language tasks into a human-readable prompted form. We convert a large set of
supervised datasets, each with multiple prompts with diverse wording. These
prompted datasets allow for benchmarking the ability of a model to perform
completely held-out tasks. We fine-tune a pretrained encoder-decoder model
(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a
wide variety of tasks. The model attains strong zero-shot performance on
several standard datasets, often outperforming models up to 16x its size.
Further, our approach attains strong performance on a subset of tasks from the
BIG-bench benchmark, outperforming models up to 6x its size. All trained models
are available at https://github.com/bigscience-workshop/t-zero and all prompts
are available at https://github.com/bigscience-workshop/promptsource.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1"&gt;Victor Sanh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1"&gt;Albert Webson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1"&gt;Colin Raffel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1"&gt;Stephen H. Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1"&gt;Lintang Sutawika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1"&gt;Zaid Alyafeai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1"&gt;Antoine Chaffin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiegler_A/0/1/0/all/0/1"&gt;Arnaud Stiegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1"&gt;Teven Le Scao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raja_A/0/1/0/all/0/1"&gt;Arun Raja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1"&gt;Manan Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1"&gt;M Saiful Bari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Canwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1"&gt;Urmish Thakker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Shanya Sharma Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szczechla_E/0/1/0/all/0/1"&gt;Eliza Szczechla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taewoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1"&gt;Gunjan Chhablani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1"&gt;Nihal Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_D/0/1/0/all/0/1"&gt;Debajyoti Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jonathan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Mike Tian-Jian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Han Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manica_M/0/1/0/all/0/1"&gt;Matteo Manica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1"&gt;Zheng Xin Yong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1"&gt;Harshit Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1"&gt;Rachel Bawden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Thomas Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neeraj_T/0/1/0/all/0/1"&gt;Trishala Neeraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rozen_J/0/1/0/all/0/1"&gt;Jos Rozen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Abheesht Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1"&gt;Andrea Santilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1"&gt;Thibault Fevry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1"&gt;Jason Alan Fries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1"&gt;Ryan Teehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bers_T/0/1/0/all/0/1"&gt;Tali Bers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1"&gt;Stella Biderman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Leo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1"&gt;Thomas Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1"&gt;Alexander M. Rush&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Instance Selection with Low-Label, Supervised Learning for Outlier Detection. (arXiv:2104.12837v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12837</id>
        <link href="http://arxiv.org/abs/2104.12837"/>
        <updated>2022-03-19T00:42:48.562Z</updated>
        <summary type="html"><![CDATA[The laborious process of labeling data often bottlenecks projects that aim to
leverage the power of supervised machine learning. Active Learning (AL) has
been established as a technique to ameliorate this condition through an
iterative framework that queries a human annotator for labels of instances with
the most uncertain class assignment. Via this mechanism, AL produces a binary
classifier trained on less labeled data but with little, if any, loss in
predictive performance. Despite its advantages, AL can have difficulty with
class-imbalanced datasets and results in an inefficient labeling process. To
address these drawbacks, we investigate our unsupervised instance selection
(UNISEL) technique followed by a Random Forest (RF) classifier on 10 outlier
detection datasets under low-label conditions. These results are compared to AL
performed on the same datasets. Further, we investigate the combination of
UNISEL and AL. Results indicate that UNISEL followed by an RF performs
comparably to AL with an RF and that the combination of UNISEL and AL
demonstrates superior performance. The practical implications of these findings
in terms of time savings and generalizability afforded by UNISEL are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bradberry_T/0/1/0/all/0/1"&gt;Trent J. Bradberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hase_C/0/1/0/all/0/1"&gt;Christopher H. Hase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kent_L/0/1/0/all/0/1"&gt;LeAnna Kent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gongora_J/0/1/0/all/0/1"&gt;Joel A. G&amp;#xf3;ngora&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Markov Offline Reinforcement Learning for Healthcare. (arXiv:2203.09365v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09365</id>
        <link href="http://arxiv.org/abs/2203.09365"/>
        <updated>2022-03-19T00:42:48.544Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) tasks are typically framed as Markov Decision
Processes (MDPs), assuming that decisions are made at fixed time intervals.
However, many applications of great importance, including healthcare, do not
satisfy this assumption, yet they are commonly modelled as MDPs after an
artificial reshaping of the data. In addition, most healthcare (and similar)
problems are offline by nature, allowing for only retrospective studies. To
address both challenges, we begin by discussing the Semi-MDP (SMDP) framework,
which formally handles actions of variable timings. We next present a formal
way to apply SMDP modifications to nearly any given value-based offline RL
method. We use this theory to introduce three SMDP-based offline RL algorithms,
namely, SDQN, SDDQN, and SBCQ. We then experimentally demonstrate that these
SMDP-based algorithms learn the optimal policy in these variable-time
environments, whereas un-directed modifications of MDP modelling lead to
sub-optimal policies. Finally, we apply our new algorithms to a real-world
offline dataset pertaining to warfarin dosing for stroke prevention and
demonstrate similar results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1"&gt;Mehdi Fatemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mary Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petch_J/0/1/0/all/0/1"&gt;Jeremy Petch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_W/0/1/0/all/0/1"&gt;Walter Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Connolly_S/0/1/0/all/0/1"&gt;Stuart J. Connolly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benz_A/0/1/0/all/0/1"&gt;Alexander Benz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carnicelli_A/0/1/0/all/0/1"&gt;Anthony Carnicelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.09268</id>
        <link href="http://arxiv.org/abs/2203.09268"/>
        <updated>2022-03-19T00:42:48.535Z</updated>
        <summary type="html"><![CDATA[We present PROSUB: PROgressive SUBsampling, a deep learning based, automated
methodology that subsamples an oversampled data set (e.g. multi-channeled 3D
images) with minimal loss of information. We build upon a recent dual-network
approach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI
measurement sampling-reconstruction challenge, but suffers from deep learning
training instability, by subsampling with a hard decision boundary. PROSUB uses
the paradigm of recursive feature elimination (RFE) and progressively
subsamples measurements during deep learning training, improving optimization
stability. PROSUB also integrates a neural architecture search (NAS) paradigm,
allowing the network architecture hyperparameters to respond to the subsampling
process. We show PROSUB outperforms the winner of the MUDI MICCAI challenge,
producing large improvements >18% MSE on the MUDI challenge sub-tasks and
qualitative improvements on downstream processes useful for clinical
applications. We also show the benefits of incorporating NAS and analyze the
effect of PROSUB's components. As our method generalizes to other problems
beyond MRI measurement selection-reconstruction, our code is
https://github.com/sbb-gh/PROSUB]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Blumberg_S/0/1/0/all/0/1"&gt;Stefano B. Blumberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hongxiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grussu_F/0/1/0/all/0/1"&gt;Francesco Grussu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yukun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Figini_M/0/1/0/all/0/1"&gt;Matteo Figini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1"&gt;Daniel C. Alexander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from humans: combining imitation and deep reinforcement learning to accomplish human-level performance on a virtual foraging task. (arXiv:2203.06250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.06250</id>
        <link href="http://arxiv.org/abs/2203.06250"/>
        <updated>2022-03-19T00:42:48.529Z</updated>
        <summary type="html"><![CDATA[We develop a method to learn bio-inspired foraging policies using human data.
We conduct an experiment where humans are virtually immersed in an open field
foraging environment and are trained to collect the highest amount of rewards.
A Markov Decision Process (MDP) framework is introduced to model the human
decision dynamics. Then, Imitation Learning (IL) based on maximum likelihood
estimation is used to train Neural Networks (NN) that map human decisions to
observed states. The results show that passive imitation substantially
underperforms humans. We further refine the human-inspired policies via
Reinforcement Learning (RL), using on-policy algorithms that are more suitable
to learn from pre-trained networks. We show that the combination of IL and RL
can match human results and that good performance strongly depends on an
egocentric representation of the environment. The developed methodology can be
used to efficiently learn policies for unmanned vehicles which have to solve
missions in an open field environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giammarino_V/0/1/0/all/0/1"&gt;Vittorio Giammarino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunne_M/0/1/0/all/0/1"&gt;Matthew F Dunne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1"&gt;Kylie N Moore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasselmo_M/0/1/0/all/0/1"&gt;Michael E Hasselmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stern_C/0/1/0/all/0/1"&gt;Chantal E Stern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1"&gt;Ioannis Ch. Paschalidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Heterogeneous Graph Based Framework for Multimodal Neuroimaging Fusion Learning. (arXiv:2110.08465v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.08465</id>
        <link href="http://arxiv.org/abs/2110.08465"/>
        <updated>2022-03-19T00:42:48.522Z</updated>
        <summary type="html"><![CDATA[Here, we present a Heterogeneous Graph neural network for Multimodal
neuroimaging fusion learning (HGM). Traditional GNN-based models usually assume
the brain network is a homogeneous graph with single type of nodes and edges.
However, vast literatures have shown the heterogeneity of the human brain
especially between the two hemispheres. Homogeneous brain network is
insufficient to model the complicated brain state. Therefore, in this work we
firstly model the brain network as heterogeneous graph with multi-type nodes
(i.e., left and right hemispheric nodes) and multi-type edges (i.e., intra- and
inter-hemispheric edges). Besides, we also propose a self-supervised
pre-training strategy based on heterogeneou brain network to address the
overfitting problem due to the complex model and small sample size. Our results
on two datasets show the superiority of proposed model over other multimodal
methods for disease prediction task. Besides, ablation experiments show that
our model with pre-training strategy can alleviate the problem of limited
training sample size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1"&gt;Gen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yifan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenjin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuesong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Representation Learning with Individualization and Refinement. (arXiv:2203.09141v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09141</id>
        <link href="http://arxiv.org/abs/2203.09141"/>
        <updated>2022-03-19T00:42:48.514Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have emerged as prominent models for
representation learning on graph structured data. GNNs follow an approach of
message passing analogous to 1-dimensional Weisfeiler Lehman (1-WL) test for
graph isomorphism and consequently are limited by the distinguishing power of
1-WL. More expressive higher-order GNNs which operate on k-tuples of nodes need
increased computational resources in order to process higher-order tensors.
Instead of the WL approach, in this work, we follow the classical approach of
Individualization and Refinement (IR), a technique followed by most practical
isomorphism solvers. Individualization refers to artificially distinguishing a
node in the graph and refinement is the propagation of this information to
other nodes through message passing. We learn to adaptively select nodes to
individualize and to aggregate the resulting graphs after refinement to help
handle the complexity. Our technique lets us learn richer node embeddings while
keeping the computational complexity manageable. Theoretically, we show that
our procedure is more expressive than the 1-WL test. Experiments show that our
method outperforms prominent 1-WL GNN models as well as competitive
higher-order baselines on several benchmark synthetic and real datasets.
Furthermore, our method opens new doors for exploring the paradigm of learning
on graph structures with individualization and refinement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dupty_M/0/1/0/all/0/1"&gt;Mohammed Haroon Dupty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1"&gt;Wee Sun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Certified Robust Training with Interval Bound Propagation. (arXiv:2203.08961v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08961</id>
        <link href="http://arxiv.org/abs/2203.08961"/>
        <updated>2022-03-19T00:42:48.497Z</updated>
        <summary type="html"><![CDATA[Interval Bound Propagation (IBP) is so far the base of state-of-the-art
methods for training neural networks with certifiable robustness guarantees
when potential adversarial perturbations present, while the convergence of IBP
training remains unknown in existing literature. In this paper, we present a
theoretical analysis on the convergence of IBP training. With an
overparameterized assumption, we analyze the convergence of IBP robust
training. We show that when using IBP training to train a randomly initialized
two-layer ReLU neural network with logistic loss, gradient descent can linearly
converge to zero robust training error with a high probability if we have
sufficiently small perturbation radius and large network width.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yihan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhouxing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CKConv: Continuous Kernel Convolution For Sequential Data. (arXiv:2102.02611v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02611</id>
        <link href="http://arxiv.org/abs/2102.02611"/>
        <updated>2022-03-19T00:42:48.496Z</updated>
        <summary type="html"><![CDATA[Conventional neural architectures for sequential data present important
limitations. Recurrent networks suffer from exploding and vanishing gradients,
small effective memory horizons, and must be trained sequentially.
Convolutional networks are unable to handle sequences of unknown size and their
memory horizon must be defined a priori. In this work, we show that all these
problems can be solved by formulating convolutional kernels in CNNs as
continuous functions. The resulting Continuous Kernel Convolution (CKConv)
allows us to model arbitrarily long sequences in a parallel manner, within a
single operation, and without relying on any form of recurrence. We show that
Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art
results in multiple datasets, e.g., permuted MNIST, and, thanks to their
continuous nature, are able to handle non-uniformly sampled datasets and
irregularly-sampled data natively. CKCNNs match or perform better than neural
ODEs designed for these purposes in a faster and simpler manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1"&gt;David W. Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuzina_A/0/1/0/all/0/1"&gt;Anna Kuzina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik J. Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1"&gt;Jakub M. Tomczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1"&gt;Mark Hoogendoorn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interactive Explanatory AI System for Industrial Quality Control. (arXiv:2203.09181v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09181</id>
        <link href="http://arxiv.org/abs/2203.09181"/>
        <updated>2022-03-19T00:42:48.489Z</updated>
        <summary type="html"><![CDATA[Machine learning based image classification algorithms, such as deep neural
network approaches, will be increasingly employed in critical settings such as
quality control in industry, where transparency and comprehensibility of
decisions are crucial. Therefore, we aim to extend the defect detection task
towards an interactive human-in-the-loop approach that allows us to integrate
rich background knowledge and the inference of complex relationships going
beyond traditional purely data-driven approaches. We propose an approach for an
interactive support system for classifications in an industrial quality control
setting that combines the advantages of both (explainable) knowledge-driven and
data-driven machine learning methods, in particular inductive logic programming
and convolutional neural networks, with human expertise and control. The
resulting system can assist domain experts with decisions, provide transparent
explanations for results, and integrate feedback from users; thus reducing
workload for humans while both respecting their expertise and without removing
their agency or accountability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1"&gt;Dennis M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marz_M/0/1/0/all/0/1"&gt;Michael M&amp;#xe4;rz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheele_S/0/1/0/all/0/1"&gt;Stephan Scheele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1"&gt;Ute Schmid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranking of Communities in Multiplex Spatiotemporal Models of Brain Dynamics. (arXiv:2203.09281v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2203.09281</id>
        <link href="http://arxiv.org/abs/2203.09281"/>
        <updated>2022-03-19T00:42:48.483Z</updated>
        <summary type="html"><![CDATA[As a relatively new field, network neuroscience has tended to focus on
aggregate behaviours of the brain averaged over many successive experiments or
over long recordings in order to construct robust brain models. These models
are limited in their ability to explain dynamic state changes in the brain
which occurs spontaneously as a result of normal brain function. Hidden Markov
Models (HMMs) trained on neuroimaging time series data have since arisen as a
method to produce dynamical models that are easy to train but can be difficult
to fully parametrise or analyse. We propose an interpretation of these neural
HMMs as multiplex brain state graph models we term Hidden Markov Graph Models
(HMGMs). This interpretation allows for dynamic brain activity to be analysed
using the full repertoire of network analysis techniques. Furthermore, we
propose a general method for selecting HMM hyperparameters in the absence of
external data, based on the principle of maximum entropy, and use this to
select the number of layers in the multiplex model. We produce a new tool for
determining important communities of brain regions using a spatiotemporal
random walk-based procedure that takes advantage of the underlying Markov
structure of the model. Our analysis of real multi-subject fMRI data provides
new results that corroborate the modular processing hypothesis of the brain at
rest as well as contributing new evidence of functional overlap between and
within dynamic brain state communities. Our analysis pipeline provides a way to
characterise dynamic network activity of the brain under novel behaviours or
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wilsenach_J/0/1/0/all/0/1"&gt;James B. Wilsenach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Warnaby_C/0/1/0/all/0/1"&gt;Catherine E. Warnaby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deane_C/0/1/0/all/0/1"&gt;Charlotte M. Deane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Reinert_G/0/1/0/all/0/1"&gt;Gesine D. Reinert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Networks for Forecasting Time Series with Multiple Seasonality: A Comparative Study. (arXiv:2203.09170v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09170</id>
        <link href="http://arxiv.org/abs/2203.09170"/>
        <updated>2022-03-19T00:42:48.475Z</updated>
        <summary type="html"><![CDATA[This paper compares recurrent neural networks (RNNs) with different types of
gated cells for forecasting time series with multiple seasonality. The cells we
compare include classical long short term memory (LSTM), gated recurrent unit
(GRU), modified LSTM with dilation, and two new cells we proposed recently,
which are equipped with dilation and attention mechanisms. To model the
temporal dependencies of different scales, our RNN architecture has multiple
dilated recurrent layers stacked with hierarchical dilations. The proposed RNN
produces both point forecasts and predictive intervals (PIs) for them. An
empirical study concerning short-term electrical load forecasting for 35
European countries confirmed that the new gated cells with dilation and
attention performed best.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1"&gt;Grzegorz Dudek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smyl_S/0/1/0/all/0/1"&gt;Slawek Smyl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelka_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Pe&amp;#x142;ka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DetMatch: Two Teachers are Better Than One for Joint 2D and 3D Semi-Supervised Object Detection. (arXiv:2203.09510v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09510</id>
        <link href="http://arxiv.org/abs/2203.09510"/>
        <updated>2022-03-19T00:42:48.458Z</updated>
        <summary type="html"><![CDATA[While numerous 3D detection works leverage the complementary relationship
between RGB images and point clouds, developments in the broader framework of
semi-supervised object recognition remain uninfluenced by multi-modal fusion.
Current methods develop independent pipelines for 2D and 3D semi-supervised
learning despite the availability of paired image and point cloud frames.
Observing that the distinct characteristics of each sensor cause them to be
biased towards detecting different objects, we propose DetMatch, a flexible
framework for joint semi-supervised learning on 2D and 3D modalities. By
identifying objects detected in both sensors, our pipeline generates a cleaner,
more robust set of pseudo-labels that both demonstrates stronger performance
and stymies single-modality error propagation. Further, we leverage the richer
semantics of RGB images to rectify incorrect 3D class predictions and improve
localization of 3D boxes. Evaluating on the challenging KITTI and Waymo
datasets, we improve upon strong semi-supervised learning methods and observe
higher quality pseudo-labels. Code will be released at
https://github.com/Divadi/DetMatch]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinhyung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1"&gt;Wei Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reachability analysis of neural networks using mixed monotonicity. (arXiv:2111.07683v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.07683</id>
        <link href="http://arxiv.org/abs/2111.07683"/>
        <updated>2022-03-19T00:42:48.451Z</updated>
        <summary type="html"><![CDATA[This paper presents a new reachability analysis approach to compute interval
over-approximations of the output set of feedforward neural networks with input
uncertainty. We adapt to neural networks an existing mixed-monotonicity method
for the reachability analysis of dynamical systems and apply it to each partial
network within the main network. This ensures that the intersection of the
obtained results is the tightest interval over-approximation of the output of
each layer that can be obtained using mixed-monotonicity on any partial network
decomposition. Unlike other tools in the literature focusing on small classes
of piecewise-affine or monotone activation functions, the main strength of our
approach is its generality: it can handle neural networks with any
Lipschitz-continuous activation function. In addition, the simplicity of our
framework allows users to very easily add unimplemented activation functions,
by simply providing the function, its derivative and the global argmin and
argmax of the derivative. Our algorithm is compared to five other
interval-based tools (Interval Bound Propagation, ReluVal, Neurify, VeriNet,
CROWN) on both existing benchmarks and two sets of small and large randomly
generated networks for four activation functions (ReLU, TanH, ELU, SiLU).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Meyer_P/0/1/0/all/0/1"&gt;Pierre-Jean Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibration of P-values for calibration and for deviation of a subpopulation from the full population. (arXiv:2202.00100v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2202.00100</id>
        <link href="http://arxiv.org/abs/2202.00100"/>
        <updated>2022-03-19T00:42:48.427Z</updated>
        <summary type="html"><![CDATA[The author's recent research papers, "Cumulative deviation of a subpopulation
from the full population" and "A graphical method of cumulative differences
between two subpopulations" (both published in volume 8 of Springer's
open-access "Journal of Big Data" during 2021), propose graphical methods and
summary statistics, without extensively calibrating formal significance tests.
The summary metrics and methods can measure the calibration of probabilistic
predictions and can assess differences in responses between a subpopulation and
the full population while controlling for a covariate or score via conditioning
on it. These recently published papers construct significance tests based on
the scalar summary statistics, but only sketch how to calibrate the attained
significance levels (also known as "P-values") for the tests. The present
article reviews and synthesizes work spanning many decades in order to detail
how to calibrate the P-values. The present paper presents computationally
efficient, easily implemented numerical methods for evaluating properly
calibrated P-values, together with rigorous mathematical proofs guaranteeing
their accuracy, and illustrates and validates the methods with open-source
software and numerical examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tygert_M/0/1/0/all/0/1"&gt;Mark Tygert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning of NAS for Few-shot Learning in Medical Image Applications. (arXiv:2203.08951v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08951</id>
        <link href="http://arxiv.org/abs/2203.08951"/>
        <updated>2022-03-19T00:42:48.421Z</updated>
        <summary type="html"><![CDATA[Deep learning methods have been successful in solving tasks in machine
learning and have made breakthroughs in many sectors owing to their ability to
automatically extract features from unstructured data. However, their
performance relies on manual trial-and-error processes for selecting an
appropriate network architecture, hyperparameters for training, and
pre-/post-procedures. Even though it has been shown that network architecture
plays a critical role in learning feature representation feature from data and
the final performance, searching for the best network architecture is
computationally intensive and heavily relies on researchers' experience.
Automated machine learning (AutoML) and its advanced techniques i.e. Neural
Architecture Search (NAS) have been promoted to address those limitations. Not
only in general computer vision tasks, but NAS has also motivated various
applications in multiple areas including medical imaging. In medical imaging,
NAS has significant progress in improving the accuracy of image classification,
segmentation, reconstruction, and more. However, NAS requires the availability
of large annotated data, considerable computation resources, and pre-defined
tasks. To address such limitations, meta-learning has been adopted in the
scenarios of few-shot learning and multiple tasks. In this book chapter, we
first present a brief review of NAS by discussing well-known approaches in
search space, search strategy, and evaluation strategy. We then introduce
various NAS approaches in medical imaging with different applications such as
classification, segmentation, detection, reconstruction, etc. Meta-learning in
NAS for few-shot learning and multiple tasks is then explained. Finally, we
describe several open problems in NAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1"&gt;Viet-Khoa Vo-Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1"&gt;Kashu Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_H/0/1/0/all/0/1"&gt;Hieu Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixing Up Contrastive Learning: Self-Supervised Representation Learning for Time Series. (arXiv:2203.09270v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09270</id>
        <link href="http://arxiv.org/abs/2203.09270"/>
        <updated>2022-03-19T00:42:48.414Z</updated>
        <summary type="html"><![CDATA[The lack of labeled data is a key challenge for learning useful
representation from time series data. However, an unsupervised representation
framework that is capable of producing high quality representations could be of
great value. It is key to enabling transfer learning, which is especially
beneficial for medical applications, where there is an abundance of data but
labeling is costly and time consuming. We propose an unsupervised contrastive
learning framework that is motivated from the perspective of label smoothing.
The proposed approach uses a novel contrastive loss that naturally exploits a
data augmentation scheme in which new samples are generated by mixing two data
samples with a mixing component. The task in the proposed framework is to
predict the mixing component, which is utilized as soft targets in the loss
function. Experiments demonstrate the framework's superior performance compared
to other representation learning approaches on both univariate and multivariate
time series and illustrate its benefits for transfer learning for clinical time
series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wickstrom_K/0/1/0/all/0/1"&gt;Kristoffer Wickstr&amp;#xf8;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kampffmeyer_M/0/1/0/all/0/1"&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikalsen_K/0/1/0/all/0/1"&gt;Karl &amp;#xd8;yvind Mikalsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jenssen_R/0/1/0/all/0/1"&gt;Robert Jenssen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Graph Neural Networks. (arXiv:2102.07835v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07835</id>
        <link href="http://arxiv.org/abs/2102.07835"/>
        <updated>2022-03-19T00:42:48.398Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are a powerful architecture for tackling graph
learning tasks, yet have been shown to be oblivious to eminent substructures
such as cycles. We present TOGL, a novel layer that incorporates global
topological information of a graph using persistent homology. TOGL can be
easily integrated into any type of GNN and is strictly more expressive (in
terms the Weisfeiler--Lehman graph isomorphism test) than message-passing GNNs.
Augmenting GNNs with TOGL leads to improved predictive performance for graph
and node classification tasks, both on synthetic data sets, which can be
classified by humans using their topology but not by ordinary GNNs, and on
real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1"&gt;Edward De Brouwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1"&gt;Michael Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_Y/0/1/0/all/0/1"&gt;Yves Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Support Alignment. (arXiv:2203.08908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08908</id>
        <link href="http://arxiv.org/abs/2203.08908"/>
        <updated>2022-03-19T00:42:48.388Z</updated>
        <summary type="html"><![CDATA[We study the problem of aligning the supports of distributions. Compared to
the existing work on distribution alignment, support alignment does not require
the densities to be matched. We propose symmetric support difference as a
divergence measure to quantify the mismatch between supports. We show that
select discriminators (e.g. discriminator trained for Jensen-Shannon
divergence) are able to map support differences as support differences in their
one-dimensional output space. Following this result, our method aligns supports
by minimizing a symmetrized relaxed optimal transport cost in the discriminator
1D space via an adversarial process. Furthermore, we show that our approach can
be viewed as a limit of existing notions of alignment by increasing
transportation assignment tolerance. We quantitatively evaluate the method
across domain adaptation tasks with shifts in label distributions. Our
experiments show that the proposed method is more robust against these shifts
than other alignment-based baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1"&gt;Shangyuan Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garipov_T/0/1/0/all/0/1"&gt;Timur Garipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shiyu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1"&gt;Tommi S. Jaakkola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration in Kernel and Neural Bandits. (arXiv:2106.12034v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12034</id>
        <link href="http://arxiv.org/abs/2106.12034"/>
        <updated>2022-03-19T00:42:48.384Z</updated>
        <summary type="html"><![CDATA[We study pure exploration in bandits, where the dimension of the feature
representation can be much larger than the number of arms. To overcome the
curse of dimensionality, we propose to adaptively embed the feature
representation of each arm into a lower-dimensional space and carefully deal
with the induced model misspecification. Our approach is conceptually very
different from existing works that can either only handle low-dimensional
linear bandits or passively deal with model misspecification. We showcase the
application of our approach to two pure exploration settings that were
previously under-studied: (1) the reward function belongs to a possibly
infinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward
function is nonlinear and can be approximated by neural networks. Our main
results provide sample complexity guarantees that only depend on the effective
dimension of the feature spaces in the kernel or neural representations.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate the efficacy of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Ruoxi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1"&gt;Rebecca Willett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of speech intelligibility with DNN-based performance measures. (arXiv:2203.09148v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.09148</id>
        <link href="http://arxiv.org/abs/2203.09148"/>
        <updated>2022-03-19T00:42:48.344Z</updated>
        <summary type="html"><![CDATA[This paper presents a speech intelligibility model based on automatic speech
recognition (ASR), combining phoneme probabilities from deep neural networks
(DNN) and a performance measure that estimates the word error rate from these
probabilities. This model does not require the clean speech reference nor the
word labels during testing as the ASR decoding step, which finds the most
likely sequence of words given phoneme posterior probabilities, is omitted. The
model is evaluated via the root-mean-squared error between the predicted and
observed speech reception thresholds from eight normal-hearing listeners. The
recognition task consists of identifying noisy words from a German matrix
sentence test. The speech material was mixed with eight noise maskers covering
different modulation types, from speech-shaped stationary noise to a
single-talker masker. The prediction performance is compared to five
established models and an ASR-model using word labels. Two combinations of
features and networks were tested. Both include temporal information either at
the feature level (amplitude modulation filterbanks and a feed-forward network)
or captured by the architecture (mel-spectrograms and a time-delay deep neural
network, TDNN). The TDNN model is on par with the DNN while reducing the number
of parameters by a factor of 37; this optimization allows parallel streams on
dedicated hearing aid hardware as a forward-pass can be computed within the
10ms of each frame. The proposed model performs almost as well as the
label-based model and produces more accurate predictions than the baseline
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1"&gt;Angel Mario Castro Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spille_C/0/1/0/all/0/1"&gt;Constantin Spille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossbach_J/0/1/0/all/0/1"&gt;Jana Ro&amp;#xdf;bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollmeier_B/0/1/0/all/0/1"&gt;Birger Kollmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1"&gt;Bernd T. Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic Neural Fields: Learning Functions on Manifolds. (arXiv:2203.07967v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.07967</id>
        <link href="http://arxiv.org/abs/2203.07967"/>
        <updated>2022-03-19T00:42:48.337Z</updated>
        <summary type="html"><![CDATA[Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes & different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1"&gt;Lukas Koestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grittner_D/0/1/0/all/0/1"&gt;Daniel Grittner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1"&gt;Zorah L&amp;#xe4;hner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward the Detection of Polyglot Files. (arXiv:2203.07561v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.07561</id>
        <link href="http://arxiv.org/abs/2203.07561"/>
        <updated>2022-03-19T00:42:48.320Z</updated>
        <summary type="html"><![CDATA[Standardized file formats play a key role in the development and use of
computer software. However, it is possible to abuse standardized file formats
by creating a file that is valid in multiple file formats. The resulting
polyglot (many languages) file can confound file format identification,
allowing elements of the file to evade analysis.This is especially problematic
for malware detection systems that rely on file format identification for
feature extraction. File format identification processes that depend on file
signatures can be easily evaded thanks to flexibility in the format
specifications of certain file formats. Although work has been done to identify
file formats using more comprehensive methods than file signatures, accurate
identification of polyglot files remains an open problem. Since malware
detection systems routinely perform file format-specific feature extraction,
polyglot files need to be filtered out prior to ingestion by these systems.
Otherwise, malicious content could pass through undetected. To address the
problem of polyglot detection we assembled a data set using the mitra tool. We
then evaluated the performance of the most commonly used file identification
tool, file. Finally, we demonstrated the accuracy, precision, recall and F1
score of a range of machine and deep learning models. Malconv2 and Catboost
demonstrated the highest recall on our data set with 95.16% and 95.34%,
respectively. These models can be incorporated into a malware detector's file
processing pipeline to filter out potentially malicious polyglots before file
format-dependent feature extraction takes place.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1"&gt;Luke Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oesch_S/0/1/0/all/0/1"&gt;Sean Oesch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adkisson_M/0/1/0/all/0/1"&gt;Mary Adkisson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erwin_S/0/1/0/all/0/1"&gt;Sam Erwin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_B/0/1/0/all/0/1"&gt;Brian Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaulagain_A/0/1/0/all/0/1"&gt;Amul Chaulagain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09481</id>
        <link href="http://arxiv.org/abs/2203.09481"/>
        <updated>2022-03-19T00:42:48.287Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models are a promising new class of
generative models that are competitive with GANs on perceptual metrics. In this
paper, we explore their potential for sequentially generating video. Inspired
by recent advances in neural video compression, we use denoising diffusion
models to stochastically generate a residual to a deterministic next-frame
prediction. We compare this approach to two sequential VAE and two GAN
baselines on four datasets, where we test the generated frames for perceptual
quality and forecasting accuracy against ground truth frames. We find
significant improvements in terms of perceptual quality on all data and
improvements in terms of frame forecasting for complex high-resolution videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1"&gt;Prakhar Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1"&gt;Stephan Mandt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Dependent Anomaly Detection with Knowledge Graph Embedding Models. (arXiv:2203.09354v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09354</id>
        <link href="http://arxiv.org/abs/2203.09354"/>
        <updated>2022-03-19T00:42:48.202Z</updated>
        <summary type="html"><![CDATA[Increasing the semantic understanding and contextual awareness of machine
learning models is important for improving robustness and reducing
susceptibility to data shifts. In this work, we leverage contextual awareness
for the anomaly detection problem. Although graphed-based anomaly detection has
been widely studied, context-dependent anomaly detection is an open problem and
without much current research. We develop a general framework for converting a
context-dependent anomaly detection problem to a link prediction problem,
allowing well-established techniques from this domain to be applied. We
implement a system based on our framework that utilizes knowledge graph
embedding models and demonstrates the ability to detect outliers using context
provided by a semantic knowledge base. We show that our method can detect
context-dependent anomalies with a high degree of accuracy and show that
current object detectors can detect enough classes to provide the needed
context for good performance within our example domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaska_N/0/1/0/all/0/1"&gt;Nathan Vaska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helus_V/0/1/0/all/0/1"&gt;Victoria Helus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error estimates for physics informed neural networks approximating the Navier-Stokes equations. (arXiv:2203.09346v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2203.09346</id>
        <link href="http://arxiv.org/abs/2203.09346"/>
        <updated>2022-03-19T00:42:48.194Z</updated>
        <summary type="html"><![CDATA[We prove rigorous bounds on the errors resulting from the approximation of
the incompressible Navier-Stokes equations with (extended) physics informed
neural networks. We show that the underlying PDE residual can be made
arbitrarily small for tanh neural networks with two hidden layers. Moreover,
the total error can be estimated in terms of the training error, network size
and number of quadrature points. The theory is illustrated with numerical
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1"&gt;Tim De Ryck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jagtap_A/0/1/0/all/0/1"&gt;Ameya D. Jagtap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Siddhartha Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decomposition-Based Hybrid Ensemble CNN Framework for Improving Cross-Subject EEG Decoding Performance. (arXiv:2203.09477v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2203.09477</id>
        <link href="http://arxiv.org/abs/2203.09477"/>
        <updated>2022-03-19T00:42:48.177Z</updated>
        <summary type="html"><![CDATA[Electroencephalogram (EEG) signals are complex, non-linear, and
non-stationary in nature. However, previous studies that applied decomposition
to minimize the complexity mainly exploited the hand-engineering features,
limiting the information learned in EEG decoding. Therefore, extracting
additional primary features from different disassembled components to improve
the EEG-based recognition performance remains challenging. On the other hand,
attempts have been made to use a single model to learn the hand-engineering
features. Less work has been done to improve the generalization ability through
ensemble learning. In this work, we propose a novel decomposition-based hybrid
ensemble convolutional neural network (CNN) framework to enhance the capability
of decoding EEG signals. CNNs, in particular, automatically learn the primary
features from raw disassembled components but not handcraft features. The first
option is to fuse the obtained score before the Softmax layer and execute
back-propagation on the entire ensemble network, whereas the other is to fuse
the probability output of the Softmax layer. Moreover, a component-specific
batch normalization (CSBN) layer is employed to reduce subject variability.
Against the challenging cross-subject driver fatigue-related situation
awareness (SA) recognition task, eight models are proposed under the framework,
which all showed superior performance than the strong baselines. The
performance of different decomposition methods and ensemble modes were further
compared. Results indicated that discrete wavelet transform (DWT)-based
ensemble CNN achieves the best 82.11% among the proposed models. Our framework
can be simply extended to any CNN architecture and applied in any EEG-related
sectors, opening the possibility of extracting more preliminary information
from complex EEG data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruobin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Suganthan_P/0/1/0/all/0/1"&gt;P. N. Suganthan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning through Local Memorization. (arXiv:2111.09360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.09360</id>
        <link href="http://arxiv.org/abs/2111.09360"/>
        <updated>2022-03-19T00:42:48.171Z</updated>
        <summary type="html"><![CDATA[Federated learning allows clients to collaboratively learn statistical models
while keeping their data local. Federated learning was originally used to train
a unique global model to be served to all clients, but this approach might be
sub-optimal when clients' local data distributions are heterogeneous. In order
to tackle this limitation, recent personalized federated learning methods train
a separate model for each client while still leveraging the knowledge available
at other clients. In this work, we exploit the ability of deep neural networks
to extract high quality vectorial representations (embeddings) from non-tabular
data, e.g., images and text, to propose a personalization mechanism based on
local memorization. Personalization is obtained interpolating a pre-trained
global model with a $k$-nearest neighbors (kNN) model based on the shared
representation provided by the global model. We provide generalization bounds
for the proposed approach and we show on a suite of federated datasets that
this approach achieves significantly higher accuracy and fairness than
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marfoq_O/0/1/0/all/0/1"&gt;Othmane Marfoq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neglia_G/0/1/0/all/0/1"&gt;Giovanni Neglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1"&gt;Laetitia Kameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Richard Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Inference of Second Opinions. (arXiv:2203.08653v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.08653</id>
        <link href="http://arxiv.org/abs/2203.08653"/>
        <updated>2022-03-19T00:42:48.164Z</updated>
        <summary type="html"><![CDATA[Automated decision support systems that are able to infer second opinions
from experts can potentially facilitate a more efficient allocation of
resources; they can help decide when and from whom to seek a second opinion. In
this paper, we look at the design of this type of support systems from the
perspective of counterfactual inference. We focus on a multiclass
classification setting and first show that, if experts make predictions on
their own, the underlying causal mechanism generating their predictions needs
to satisfy a desirable set invariant property. Further, we show that, for any
causal mechanism satisfying this property, there exists an equivalent mechanism
where the predictions by each expert are generated by independent
sub-mechanisms governed by a common noise. This motivates the design of a set
invariant Gumbel-Max structural causal model where the structure of the noise
governing the sub-mechanisms underpinning the model depends on an intuitive
notion of similarity between experts which can be estimated from data.
Experiments on both synthetic and real data show that our model can be used to
infer second opinions more accurately than its non-causal counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benz_N/0/1/0/all/0/1"&gt;Nina Corvelo Benz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1"&gt;Manuel Gomez Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations. (arXiv:2107.00520v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00520</id>
        <link href="http://arxiv.org/abs/2107.00520"/>
        <updated>2022-03-19T00:42:48.158Z</updated>
        <summary type="html"><![CDATA[In many prediction problems, spurious correlations are induced by a changing
relationship between the label and a nuisance variable that is also correlated
with the covariates. For example, in classifying animals in natural images, the
background, which is a nuisance, can predict the type of animal. This
nuisance-label relationship does not always hold, and the performance of a
model trained under one such relationship may be poor on data with a different
nuisance-label relationship. To build predictive models that perform well
regardless of the nuisance-label relationship, we develop Nuisance-Randomized
Distillation (NURD). We introduce the nuisance-randomized distribution, a
distribution where the nuisance and the label are independent. Under this
distribution, we define the set of representations such that conditioning on
any member, the nuisance and the label remain independent. We prove that the
representations in this set always perform better than chance, while
representations outside of this set may not. NURD finds a representation from
this set that is most informative of the label under the nuisance-randomized
distribution, and we prove that this representation achieves the highest
performance regardless of the nuisance-label relationship. We evaluate NURD on
several tasks including chest X-ray classification where, using non-lung
patches as the nuisance, NURD produces models that predict pneumonia under
strong spurious correlations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puli_A/0/1/0/all/0/1"&gt;Aahlad Puli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lily H. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1"&gt;Eric K. Oermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Sliced Wasserstein Distances. (arXiv:2006.08812v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08812</id>
        <link href="http://arxiv.org/abs/2006.08812"/>
        <updated>2022-03-19T00:42:48.149Z</updated>
        <summary type="html"><![CDATA[While theoretically appealing, the application of the Wasserstein distance to
large-scale machine learning problems has been hampered by its prohibitive
computational cost. The sliced Wasserstein distance and its variants improve
the computational efficiency through the random projection, yet they suffer
from low accuracy if the number of projections is not sufficiently large,
because the majority of projections result in trivially small values. In this
work, we propose a new family of distance metrics, called augmented sliced
Wasserstein distances (ASWDs), constructed by first mapping samples to
higher-dimensional hypersurfaces parameterized by neural networks. It is
derived from a key observation that (random) linear projections of samples
residing on these hypersurfaces would translate to much more flexible nonlinear
projections in the original sample space, so they can capture complex
structures of the data distribution. We show that the hypersurfaces can be
optimized by gradient ascent efficiently. We provide the condition under which
the ASWD is a valid metric and show that this can be obtained by an injective
neural network architecture. Numerical results demonstrate that the ASWD
significantly outperforms other Wasserstein variants for both synthetic and
real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiongjie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunpeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow. (arXiv:2107.00204v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00204</id>
        <link href="http://arxiv.org/abs/2107.00204"/>
        <updated>2022-03-19T00:42:48.132Z</updated>
        <summary type="html"><![CDATA[For marketing, we sometimes need to recommend content for multiple pages in
sequence. Different from general sequential decision making process, the use
cases have a simpler flow where customers per seeing recommended content on
each page can only return feedback as moving forward in the process or dropping
from it until a termination state. We refer to this type of problems as
sequential decision making in linear--flow. We propose to formulate the problem
as an MDP with Bandits where Bandits are employed to model the transition
probability matrix. At recommendation time, we use Thompson sampling (TS) to
sample the transition probabilities and allocate the best series of actions
with analytical solution through exact dynamic programming. The way that we
formulate the problem allows us to leverage TS's efficiency in balancing
exploration and exploitation and Bandit's convenience in modeling actions'
incompatibility. In the simulation study, we observe the proposed MDP with
Bandits algorithm outperforms Q-learning with $\epsilon$-greedy and decreasing
$\epsilon$, independent Bandits, and interaction Bandits. We also find the
proposed algorithm's performance is the most robust to changes in the
across-page interdependence strength.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents. (arXiv:2203.09498v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2203.09498</id>
        <link href="http://arxiv.org/abs/2203.09498"/>
        <updated>2022-03-19T00:42:48.126Z</updated>
        <summary type="html"><![CDATA[Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pilarski_P/0/1/0/all/0/1"&gt;Patrick M. Pilarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Butcher_A/0/1/0/all/0/1"&gt;Andrew Butcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1"&gt;Elnaz Davoodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johanson_M/0/1/0/all/0/1"&gt;Michael Bradley Johanson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brenneis_D/0/1/0/all/0/1"&gt;Dylan J. A. Brenneis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_A/0/1/0/all/0/1"&gt;Adam S. R. Parker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acker_L/0/1/0/all/0/1"&gt;Leslie Acker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1"&gt;Matthew M. Botvinick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1"&gt;Joseph Modayil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1"&gt;Adam White&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty with UAV Search of Multiple Goal-oriented Targets. (arXiv:2203.09476v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2203.09476</id>
        <link href="http://arxiv.org/abs/2203.09476"/>
        <updated>2022-03-19T00:42:48.119Z</updated>
        <summary type="html"><![CDATA[This paper considers the complex problem of a team of UAVs searching targets
under uncertainty. The goal of the UAV team is to find all of the moving
targets as quickly as possible before they arrive at their selected goal. The
uncertainty considered is threefold: First, the UAVs do not know the targets'
locations and destinations. Second, the sensing capabilities of the UAVs are
not perfect. Third, the targets' movement model is unknown. We suggest a
real-time algorithmic framework for the UAVs, combining entropy and
stochastic-temporal belief, that aims at optimizing the probability of a quick
and successful detection of all of the targets. We have empirically evaluated
the algorithmic framework, and have shown its efficiency and significant
performance improvement compared to other solutions. Furthermore, we have
evaluated our framework using Peer Designed Agents (PDAs), which are computer
agents that simulate targets and show that our algorithmic framework
outperforms other solutions in this scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinay_M/0/1/0/all/0/1"&gt;Mor Sinay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agmon_N/0/1/0/all/0/1"&gt;Noa Agmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maksimov_O/0/1/0/all/0/1"&gt;Oleg Maksimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fux_A/0/1/0/all/0/1"&gt;Aviad Fux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kraus_S/0/1/0/all/0/1"&gt;Sarit Kraus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Graph Auto-Encoder for General Data Clustering. (arXiv:2002.08648v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08648</id>
        <link href="http://arxiv.org/abs/2002.08648"/>
        <updated>2022-03-19T00:42:48.112Z</updated>
        <summary type="html"><![CDATA[Graph-based clustering plays an important role in the clustering area. Recent
studies about graph convolution neural networks have achieved impressive
success on graph type data. However, in general clustering tasks, the graph
structure of data does not exist such that the strategy to construct a graph is
crucial for performance. Therefore, how to extend graph convolution networks
into general clustering tasks is an attractive problem. In this paper, we
propose a graph auto-encoder for general data clustering, which constructs the
graph adaptively according to the generative perspective of graphs. The
adaptive process is designed to induce the model to exploit the high-level
information behind data and utilize the non-Euclidean structure sufficiently.
We further design a novel mechanism with rigorous analysis to avoid the
collapse caused by the adaptive construction. Via combining the generative
model for network embedding and graph-based clustering, a graph auto-encoder
with a novel decoder is developed such that it performs well in weighted graph
used scenarios. Extensive experiments prove the superiority of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FourierMask: Instance Segmentation using Fourier Mapping in Implicit Neural Networks. (arXiv:2112.12535v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.12535</id>
        <link href="http://arxiv.org/abs/2112.12535"/>
        <updated>2022-03-19T00:42:48.105Z</updated>
        <summary type="html"><![CDATA[We present FourierMask, which employs Fourier series combined with implicit
neural representations to generate instance segmentation masks. We apply a
Fourier mapping (FM) to the coordinate locations and utilize the mapped
features as inputs to an implicit representation (coordinate-based multi-layer
perceptron (MLP)). FourierMask learns to predict the coefficients of the FM for
a particular instance, and therefore adapts the FM to a specific object. This
allows FourierMask to be generalized to predict instance segmentation masks
from natural images. Since implicit functions are continuous in the domain of
input coordinates, we illustrate that by sub-sampling the input pixel
coordinates, we can generate higher resolution masks during inference.
Furthermore, we train a renderer MLP (FourierRend) on the uncertain predictions
of FourierMask and illustrate that it significantly improves the quality of the
masks. FourierMask shows competitive results on the MS COCO dataset compared to
the baseline Mask R-CNN at the same output resolution and surpasses it on
higher resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riaz_H/0/1/0/all/0/1"&gt;Hamd ul Moqeet Riaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1"&gt;Nuri Benbarka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoefer_T/0/1/0/all/0/1"&gt;Timon Hoefer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GATE: Graph CCA for Temporal SElf-supervised Learning for Label-efficient fMRI Analysis. (arXiv:2203.09034v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09034</id>
        <link href="http://arxiv.org/abs/2203.09034"/>
        <updated>2022-03-19T00:42:48.090Z</updated>
        <summary type="html"><![CDATA[In this work, we focus on the challenging task, neuro-disease classification,
using functional magnetic resonance imaging (fMRI). In population graph-based
disease analysis, graph convolutional neural networks (GCNs) have achieved
remarkable success. However, these achievements are inseparable from abundant
labeled data and sensitive to spurious signals. To improve fMRI representation
learning and classification under a label-efficient setting, we propose a novel
and theory-driven self-supervised learning (SSL) framework on GCNs, namely
Graph CCA for Temporal self-supervised learning on fMRI analysis GATE.
Concretely, it is demanding to design a suitable and effective SSL strategy to
extract formation and robust features for fMRI. To this end, we investigate
several new graph augmentation strategies from fMRI dynamic functional
connectives (FC) for SSL training. Further, we leverage canonical-correlation
analysis (CCA) on different temporal embeddings and present the theoretical
implications. Consequently, this yields a novel two-step GCN learning procedure
comprised of (i) SSL on an unlabeled fMRI population graph and (ii) fine-tuning
on a small labeled fMRI dataset for a classification task. Our method is tested
on two independent fMRI datasets, demonstrating superior performance on autism
and dementia diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1"&gt;Liang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaofeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup. (arXiv:2012.15511v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15511</id>
        <link href="http://arxiv.org/abs/2012.15511"/>
        <updated>2022-03-19T00:42:48.082Z</updated>
        <summary type="html"><![CDATA[Asynchronous and parallel implementation of standard reinforcement learning
(RL) algorithms is a key enabler of the tremendous success of modern RL. Among
many asynchronous RL algorithms, arguably the most popular and effective one is
the asynchronous advantage actor-critic (A3C) algorithm. Although A3C is
becoming the workhorse of RL, its theoretical properties are still not
well-understood, including its non-asymptotic analysis and the performance gain
of parallelism (a.k.a. linear speedup). This paper revisits the A3C algorithm
and establishes its non-asymptotic convergence guarantees. Under both i.i.d.
and Markovian sampling, we establish the local convergence guarantee for A3C in
the general policy approximation case and the global convergence guarantee in
softmax policy parameterization. Under i.i.d. sampling, A3C obtains sample
complexity of $\mathcal{O}(\epsilon^{-2.5}/N)$ per worker to achieve $\epsilon$
accuracy, where $N$ is the number of workers. Compared to the best-known sample
complexity of $\mathcal{O}(\epsilon^{-2.5})$ for two-timescale AC, A3C achieves
\emph{linear speedup}, which justifies the advantage of parallelism and
asynchrony in AC algorithms theoretically for the first time. Numerical tests
on synthetic environment, OpenAI Gym environments and Atari games have been
provided to verify our theoretical analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Han Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Mingyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Labor Training. (arXiv:2006.06853v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06853</id>
        <link href="http://arxiv.org/abs/2006.06853"/>
        <updated>2022-03-19T00:42:48.075Z</updated>
        <summary type="html"><![CDATA[On-demand labor platforms aim to train a skilled workforce to serve its
incoming demand for jobs. Since limited jobs are available for training, and it
is usually not necessary to train all workers, efficient matching of training
jobs requires prioritizing fast learners over slow ones. However, the learning
rates of novice workers are unknown, resulting in a tradeoff between
exploration (learning the learning rates) and exploitation (training the best
workers). Motivated to study this tradeoff, we analyze a novel objective within
the stochastic multi-armed bandit framework. Given $K$ arms, instead of
maximizing the expected total reward from $T$ pulls (the traditional "sum"
objective), we consider the vector of cumulative rewards earned from the $K$
arms at the end of $T$ pulls and aim to maximize the expected highest
cumulative reward (the "max" objective). When rewards represent skill
increments, this corresponds to the objective of training a single highly
skilled worker from a set of novice workers, using a limited supply of training
jobs. For this objective, we show that any policy must incur an
instance-dependent asymptotic regret of $\Omega(\log T)$ (with a higher
instance-dependent constant) and a worst-case regret of
$\Omega(K^{1/3}T^{2/3})$. We then design an explore-then-commit policy
featuring exploration based on appropriately tuned confidence bounds on the
mean reward and an adaptive stopping criterion, which adapts to the problem
difficulty and achieves these bounds (up to logarithmic factors). We generalize
our algorithmic insights to the problem of maximizing the expected value of the
average cumulative reward of the top $m$ arms with the highest cumulative
rewards, corresponding to the case where multiple workers must be trained. Our
numerical experiments demonstrate the efficacy of our policies compared to
several natural alternatives in practical parameter regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozbay_E/0/1/0/all/0/1"&gt;Eren Ozbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamble_V/0/1/0/all/0/1"&gt;Vijay Kamble&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Super-Resolution With Deep Variational Autoencoders. (arXiv:2203.09445v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09445</id>
        <link href="http://arxiv.org/abs/2203.09445"/>
        <updated>2022-03-19T00:42:48.067Z</updated>
        <summary type="html"><![CDATA[Image super-resolution (SR) techniques are used to generate a high-resolution
image from a low-resolution image. Until now, deep generative models such as
autoregressive models and Generative Adversarial Networks (GANs) have proven to
be effective at modelling high-resolution images. Models based on Variational
Autoencoders (VAEs) have often been criticized for their feeble generative
performance, but with new advancements such as VDVAE (very deep VAE), there is
now strong evidence that deep VAEs have the potential to outperform current
state-of-the-art models for high-resolution image generation. In this paper, we
introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE
methodologies to improve upon image super-resolution using transfer learning on
pretrained VDVAEs. Through qualitative and quantitative evaluations, we show
that the proposed model is competitive with other state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chira_D/0/1/0/all/0/1"&gt;Darius Chira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haralampiev_I/0/1/0/all/0/1"&gt;Ilian Haralampiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1"&gt;Ole Winther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1"&gt;Andrea Dittadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1"&gt;Valentin Li&amp;#xe9;vin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Structured Abstain Problem and the Lov\'asz Hinge. (arXiv:2203.08645v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.08645</id>
        <link href="http://arxiv.org/abs/2203.08645"/>
        <updated>2022-03-19T00:42:48.051Z</updated>
        <summary type="html"><![CDATA[The Lov\'asz hinge is a convex surrogate recently proposed for structured
binary classification, in which $k$ binary predictions are made simultaneously
and the error is judged by a submodular set function. Despite its wide usage in
image segmentation and related problems, its consistency has remained open. We
resolve this open question, showing that the Lov\'asz hinge is inconsistent for
its desired target unless the set function is modular. Leveraging a recent
embedding framework, we instead derive the target loss for which the Lov\'asz
hinge is consistent. This target, which we call the structured abstain problem,
allows one to abstain on any subset of the $k$ predictions. We derive two link
functions, each of which are consistent for all submodular set functions
simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finocchiaro_J/0/1/0/all/0/1"&gt;Jessie Finocchiaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frongillo_R/0/1/0/all/0/1"&gt;Rafael Frongillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nueve_E/0/1/0/all/0/1"&gt;Enrique Nueve&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SC2: Supervised Compression for Split Computing. (arXiv:2203.08875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08875</id>
        <link href="http://arxiv.org/abs/2203.08875"/>
        <updated>2022-03-19T00:42:48.044Z</updated>
        <summary type="html"><![CDATA[Split computing distributes the execution of a neural network (e.g., for a
classification task) between a mobile device and a more powerful edge server. A
simple alternative to splitting the network is to carry out the supervised task
purely on the edge server while compressing and transmitting the full data, and
most approaches have barely outperformed this baseline. This paper proposes a
new approach for discretizing and entropy-coding intermediate feature
activations to efficiently transmit them from the mobile device to the edge
server. We show that a efficient splittable network architecture results from a
three-way tradeoff between (a) minimizing the computation on the mobile device,
(b) minimizing the size of the data to be transmitted, and (c) maximizing the
model's prediction performance. We propose an architecture based on this
tradeoff and train the splittable network and entropy model in a knowledge
distillation framework. In an extensive set of experiments involving three
vision tasks, three datasets, nine baselines, and more than 180 trained models,
we show that our approach improves supervised rate-distortion tradeoffs while
maintaining a considerably smaller encoder size. We also release sc2bench, an
installable Python package, to encourage and facilitate future studies on
supervised compression for split computing (SC2).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1"&gt;Yoshitomo Matsubara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1"&gt;Marco Levorato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1"&gt;Stephan Mandt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09347</id>
        <link href="http://arxiv.org/abs/2203.09347"/>
        <updated>2022-03-19T00:42:48.037Z</updated>
        <summary type="html"><![CDATA[In a high-dimensional regression framework, we study consequences of the
naive two-step procedure where first the dimension of the input variables is
reduced and second, the reduced input variables are used to predict the output
variable. More specifically we combine principal component analysis (PCA) with
kernel regression. In order to analyze the resulting regression errors, a novel
stability result of kernel regression with respect to the Wasserstein distance
is derived. This allows us to bound errors that occur when perturbed input data
is used to fit a kernel function. We combine the stability result with known
estimates from the literature on both principal component analysis and kernel
regression to obtain convergence rates for the two-step procedure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Eckstein_S/0/1/0/all/0/1"&gt;Stephan Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Iske_A/0/1/0/all/0/1"&gt;Armin Iske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trabs_M/0/1/0/all/0/1"&gt;Mathias Trabs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covid19 Reproduction Number: Credibility Intervals by Blockwise Proximal Monte Carlo Samplers. (arXiv:2203.09142v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09142</id>
        <link href="http://arxiv.org/abs/2203.09142"/>
        <updated>2022-03-19T00:42:48.029Z</updated>
        <summary type="html"><![CDATA[Monitoring the Covid19 pandemic constitutes a critical societal stake that
received considerable research efforts. The intensity of the pandemic on a
given territory is efficiently measured by the reproduction number, quantifying
the rate of growth of daily new infections. Recently, estimates for the time
evolution of the reproduction number were produced using an inverse problem
formulation with a nonsmooth functional minimization. While it was designed to
be robust to the limited quality of the Covid19 data (outliers, missing
counts), the procedure lacks the ability to output credibility interval based
estimates. This remains a severe limitation for practical use in actual
pandemic monitoring by epidemiologists that the present work aims to overcome
by use of Monte Carlo sampling. After interpretation of the functional into a
Bayesian framework, several sampling schemes are tailored to adjust the
nonsmooth nature of the resulting posterior distribution. The originality of
the devised algorithms stems from combining a Langevin Monte Carlo sampling
scheme with Proximal operators. Performance of the new algorithms in producing
relevant credibility intervals for the reproduction number estimates and
denoised counts are compared. Assessment is conducted on real daily new
infection counts made available by the Johns Hopkins University. The interest
of the devised monitoring tools are illustrated on Covid19 data from several
different countries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fort_G/0/1/0/all/0/1"&gt;Gersende Fort&lt;/a&gt; (IMT), &lt;a href="http://arxiv.org/find/cs/1/au:+Pascal_B/0/1/0/all/0/1"&gt;Barbara Pascal&lt;/a&gt; (CRIStAL), &lt;a href="http://arxiv.org/find/cs/1/au:+Abry_P/0/1/0/all/0/1"&gt;Patrice Abry&lt;/a&gt; (Phys-ENS), &lt;a href="http://arxiv.org/find/cs/1/au:+Pustelnik_N/0/1/0/all/0/1"&gt;Nelly Pustelnik&lt;/a&gt; (Phys-ENS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Structured Neural Networks Through Manifold Identification and Variance Reduction. (arXiv:2112.02612v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.02612</id>
        <link href="http://arxiv.org/abs/2112.02612"/>
        <updated>2022-03-19T00:42:48.023Z</updated>
        <summary type="html"><![CDATA[This paper proposes an algorithm (RMDA) for training neural networks (NNs)
with a regularization term for promoting desired structures. RMDA does not
incur computation additional to proximal SGD with momentum, and achieves
variance reduction without requiring the objective function to be of the
finite-sum form. Through the tool of manifold identification from nonlinear
optimization, we prove that after a finite number of iterations, all iterates
of RMDA possess a desired structure identical to that induced by the
regularizer at the stationary point of asymptotic convergence, even in the
presence of engineering tricks like data augmentation and dropout that
complicate the training process. Experiments on training NNs with structured
sparsity confirm that variance reduction is necessary for such an
identification, and show that RMDA thus significantly outperforms existing
methods for this task. For unstructured sparsity, RMDA also outperforms a
state-of-the-art pruning method, validating the benefits of training structured
NNs through regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zih-Syuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Ching-pei Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transframer: Arbitrary Frame Prediction with Generative Models. (arXiv:2203.09494v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09494</id>
        <link href="http://arxiv.org/abs/2203.09494"/>
        <updated>2022-03-19T00:42:48.007Z</updated>
        <summary type="html"><![CDATA[We present a general-purpose framework for image modelling and vision tasks
based on probabilistic frame prediction. Our approach unifies a broad range of
tasks, from image segmentation, to novel view synthesis and video
interpolation. We pair this framework with an architecture we term Transframer,
which uses U-Net and Transformer components to condition on annotated context
frames, and outputs sequences of sparse, compressed image features. Transframer
is the state-of-the-art on a variety of video generation benchmarks, is
competitive with the strongest models on few-shot view synthesis, and can
generate coherent 30 second videos from a single image without any explicit
geometric information. A single generalist Transframer simultaneously produces
promising results on 8 tasks, including semantic segmentation, image
classification and optical flow prediction with no task-specific architectural
components, demonstrating that multi-task computer vision can be tackled using
probabilistic image models. Our approach can in principle be applied to a wide
range of applications that require learning the conditional structure of
annotated image-formatted data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1"&gt;Charlie Nash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1"&gt;Jacob Walker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barr_I/0/1/0/all/0/1"&gt;Iain Barr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1"&gt;Peter Battaglia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13389</id>
        <link href="http://arxiv.org/abs/2103.13389"/>
        <updated>2022-03-19T00:42:47.999Z</updated>
        <summary type="html"><![CDATA[Given a large dataset for training, GANs can achieve remarkable performance
for the image synthesis task. However, training GANs in extremely low data
regimes remains a challenge, as overfitting often occurs, leading to
memorization or training divergence. In this work, we introduce SIV-GAN, an
unconditional generative model that can generate new scene compositions from a
single training image or a single video clip. We propose a two-branch
discriminator architecture, with content and layout branches designed to judge
internal content and scene layout realism separately from each other. This
discriminator design enables synthesis of visually plausible, novel
compositions of a scene, with varying content and layout, while preserving the
context of the original sample. Compared to previous single-image GANs, our
model generates more diverse, higher quality images, while not being restricted
to a single image setting. We show that SIV-GAN successfully deals with a new
challenging task of learning from a single video, for which prior GAN models
fail to achieve synthesis of both high quality and diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sushko_V/0/1/0/all/0/1"&gt;Vadim Sushko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1"&gt;Anna Khoreva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Personalized Federated Learning. (arXiv:2103.00710v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00710</id>
        <link href="http://arxiv.org/abs/2103.00710"/>
        <updated>2022-03-19T00:42:47.993Z</updated>
        <summary type="html"><![CDATA[In parallel with the rapid adoption of Artificial Intelligence (AI) empowered
by advances in AI research, there have been growing awareness and concerns of
data privacy. Recent significant developments in the data regulation landscape
have prompted a seismic shift in interest towards privacy-preserving AI. This
has contributed to the popularity of Federated Learning (FL), the leading
paradigm for the training of machine learning models on data silos in a
privacy-preserving manner. In this survey, we explore the domain of
Personalized FL (PFL) to address the fundamental challenges of FL on
heterogeneous data, a universal characteristic inherent in all real-world
datasets. We analyze the key motivations for PFL and present a unique taxonomy
of PFL techniques categorized according to the key challenges and
personalization strategies in PFL. We highlight their key ideas, challenges and
opportunities and envision promising future trajectories of research towards
new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1"&gt;Alysa Ziying Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lizhen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainability in Graph Neural Networks: An Experimental Survey. (arXiv:2203.09258v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09258</id>
        <link href="http://arxiv.org/abs/2203.09258"/>
        <updated>2022-03-19T00:42:47.851Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have been extensively developed for graph
representation learning in various application domains. However, similar to all
other neural networks models, GNNs suffer from the black-box problem as people
cannot understand the mechanism underlying them. To solve this problem, several
GNN explainability methods have been proposed to explain the decisions made by
GNNs. In this survey, we give an overview of the state-of-the-art GNN
explainability methods and how they are evaluated. Furthermore, we propose a
new evaluation metric and conduct thorough experiments to compare GNN
explainability methods on real world datasets. We also suggest future
directions for GNN explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peibo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yixing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1"&gt;Maurice Pagnucco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Mathematics of Artificial Intelligence. (arXiv:2203.08890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08890</id>
        <link href="http://arxiv.org/abs/2203.08890"/>
        <updated>2022-03-19T00:42:47.836Z</updated>
        <summary type="html"><![CDATA[We currently witness the spectacular success of artificial intelligence in
both science and public life. However, the development of a rigorous
mathematical foundation is still at an early stage. In this survey article,
which is based on an invited lecture at the International Congress of
Mathematicians 2022, we will in particular focus on the current "workhorse" of
artificial intelligence, namely deep neural networks. We will present the main
theoretical directions along with several exemplary results and discuss key
open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1"&gt;Gitta Kutyniok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Usefulness of the Fit-on-the-Test View on Evaluating Calibration of Classifiers. (arXiv:2203.08958v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08958</id>
        <link href="http://arxiv.org/abs/2203.08958"/>
        <updated>2022-03-19T00:42:47.829Z</updated>
        <summary type="html"><![CDATA[Every uncalibrated classifier has a corresponding true calibration map that
calibrates its confidence. Deviations of this idealistic map from the identity
map reveal miscalibration. Such calibration errors can be reduced with many
post-hoc calibration methods which fit some family of calibration maps on a
validation dataset. In contrast, evaluation of calibration with the expected
calibration error (ECE) on the test set does not explicitly involve fitting.
However, as we demonstrate, ECE can still be viewed as if fitting a family of
functions on the test data. This motivates the fit-on-the-test view on
evaluation: first, approximate a calibration map on the test data, and second,
quantify its distance from the identity. Exploiting this view allows us to
unlock missed opportunities: (1) use the plethora of post-hoc calibration
methods for evaluating calibration; (2) tune the number of bins in ECE with
cross-validation. Furthermore, we introduce: (3) benchmarking on pseudo-real
data where the true calibration map can be estimated very precisely; and (4)
novel calibration and evaluation methods using new calibration map families PL
and PL3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kangsepp_M/0/1/0/all/0/1"&gt;Markus K&amp;#xe4;ngsepp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valk_K/0/1/0/all/0/1"&gt;Kaspar Valk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kull_M/0/1/0/all/0/1"&gt;Meelis Kull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Normalized Density Map (SNDM) for Counting Microbiological Objects. (arXiv:2203.09474v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09474</id>
        <link href="http://arxiv.org/abs/2203.09474"/>
        <updated>2022-03-19T00:42:47.800Z</updated>
        <summary type="html"><![CDATA[The statistical properties of the density map (DM) approach to counting
microbiological objects on images are studied in detail. The DM is given by
U$^2$-Net. Two statistical methods for deep neural networks are utilized: the
bootstrap and the Monte Carlo (MC) dropout. The detailed analysis of the
uncertainties for the DM predictions leads to a deeper understanding of the DM
model's deficiencies. Based on our investigation, we propose a
self-normalization module in the network. The improved network model, called
Self-Normalized Density Map (SNDM), can correct its output density map by
itself to accurately predict the total number of objects in the image. The SNDM
architecture outperforms the original model. Moreover, both statistical
frameworks -- bootstrap and MC dropout -- have consistent statistical results
for SNDM, which were not observed in the original model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Graczyk_K/0/1/0/all/0/1"&gt;Krzysztof M. Graczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawlowski_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Paw&amp;#x142;owski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1"&gt;Sylwia Majchrowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1"&gt;Tomasz Golan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Subgraph Aggregation Networks. (arXiv:2110.02910v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.02910</id>
        <link href="http://arxiv.org/abs/2110.02910"/>
        <updated>2022-03-19T00:42:47.794Z</updated>
        <summary type="html"><![CDATA[Message-passing neural networks (MPNNs) are the leading architecture for deep
learning on graph-structured data, in large part due to their simplicity and
scalability. Unfortunately, it was shown that these architectures are limited
in their expressive power. This paper proposes a novel framework called
Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our
main observation is that while two graphs may not be distinguishable by an
MPNN, they often contain distinguishable subgraphs. Thus, we propose to
represent each graph as a set of subgraphs derived by some predefined policy,
and to process it using a suitable equivariant architecture. We develop novel
variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph
isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of
these new WL variants. We further prove that our approach increases the
expressive power of both MPNNs and more expressive architectures. Moreover, we
provide theoretical results that describe how design choices such as the
subgraph selection policy and equivariant neural architecture affect our
architecture's expressive power. To deal with the increased computational cost,
we propose a subgraph sampling scheme, which can be viewed as a stochastic
version of our framework. A comprehensive set of experiments on real and
synthetic datasets demonstrates that our framework improves the expressive
power and overall performance of popular GNN architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1"&gt;Beatrice Bevilacqua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1"&gt;Fabrizio Frasca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1"&gt;Derek Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1"&gt;Balasubramaniam Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Chen Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balamurugan_G/0/1/0/all/0/1"&gt;Gopinath Balamurugan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael M. Bronstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1"&gt;Haggai Maron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical data monitoring in the internet-services domain. (arXiv:2203.08067v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.08067</id>
        <link href="http://arxiv.org/abs/2203.08067"/>
        <updated>2022-03-19T00:42:47.787Z</updated>
        <summary type="html"><![CDATA[Large-scale monitoring, anomaly detection, and root cause analysis of metrics
are essential requirements of the internet-services industry. To address the
need to continuously monitor millions of metrics, many anomaly detection
approaches are being used on a daily basis by large internet-based companies.
However, in spite of the significant progress made to accurately and
efficiently detect anomalies in metrics, the sheer scale of the number of
metrics has meant there are still a large number of false alarms that need to
be investigated. This paper presents a framework for reliable large-scale
anomaly detection. It is significantly more accurate than existing approaches
and allows for easy interpretation of models, thus enabling practical data
monitoring in the internet-services domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galagali_N/0/1/0/all/0/1"&gt;Nikhil Galagali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Fewer Tasks through Task Interpolation. (arXiv:2106.02695v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02695</id>
        <link href="http://arxiv.org/abs/2106.02695"/>
        <updated>2022-03-19T00:42:47.771Z</updated>
        <summary type="html"><![CDATA[Meta-learning enables algorithms to quickly learn a newly encountered task
with just a few labeled examples by transferring previously learned knowledge.
However, the bottleneck of current meta-learning algorithms is the requirement
of a large number of meta-training tasks, which may not be accessible in
real-world scenarios. To address the challenge that available tasks may not
densely sample the space of tasks, we propose to augment the task set through
interpolation. By meta-learning with task interpolation (MLTI), our approach
effectively generates additional tasks by randomly sampling a pair of tasks and
interpolating the corresponding features and labels. Under both gradient-based
and metric-based meta-learning settings, our theoretical analysis shows MLTI
corresponds to a data-adaptive meta-regularization and further improves the
generalization. Empirically, in our experiments on eight datasets from diverse
domains including image recognition, pose prediction, molecule property
prediction, and medical image classification, we find that the proposed general
MLTI framework is compatible with representative meta-learning algorithms and
consistently outperforms other state-of-the-art strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Training and Inference of Graph Neural Networks with Fast Sampling and Pipelining. (arXiv:2110.08450v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.08450</id>
        <link href="http://arxiv.org/abs/2110.08450"/>
        <updated>2022-03-19T00:42:47.764Z</updated>
        <summary type="html"><![CDATA[Improving the training and inference performance of graph neural networks
(GNNs) is faced with a challenge uncommon in general neural networks: creating
mini-batches requires a lot of computation and data movement due to the
exponential growth of multi-hop graph neighborhoods along network layers. Such
a unique challenge gives rise to a diverse set of system design choices. We
argue in favor of performing mini-batch training with neighborhood sampling in
a distributed multi-GPU environment, under which we identify major performance
bottlenecks hitherto under-explored by developers: mini-batch preparation and
transfer. We present a sequence of improvements to mitigate these bottlenecks,
including a performance-engineered neighborhood sampler, a shared-memory
parallelization strategy, and the pipelining of batch transfer with GPU
computation. We also conduct an empirical analysis that supports the use of
sampling for inference, showing that test accuracies are not materially
compromised. Such an observation unifies training and inference, simplifying
model implementation. We report comprehensive experimental results with several
benchmark data sets and GNN architectures, including a demonstration that, for
the ogbn-papers100M data set, our system SALIENT achieves a speedup of 3x over
a standard PyTorch-Geometric implementation with a single GPU and a further 8x
parallel speedup with 16 GPUs. Therein, training a 3-layer GraphSAGE model with
sampling fanout (15, 10, 5) takes 2.0 seconds per epoch and inference with
fanout (20, 20, 20) takes 2.4 seconds, attaining test accuracy 64.58%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaler_T/0/1/0/all/0/1"&gt;Tim Kaler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stathas_N/0/1/0/all/0/1"&gt;Nickolas Stathas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1"&gt;Anne Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iliopoulos_A/0/1/0/all/0/1"&gt;Alexandros-Stavros Iliopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schardl_T/0/1/0/all/0/1"&gt;Tao B. Schardl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leiserson_C/0/1/0/all/0/1"&gt;Charles E. Leiserson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Spectral Bias of Convolutional Neural Tangent and Gaussian Process Kernels. (arXiv:2203.09255v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09255</id>
        <link href="http://arxiv.org/abs/2203.09255"/>
        <updated>2022-03-19T00:42:47.757Z</updated>
        <summary type="html"><![CDATA[We study the properties of various over-parametrized convolutional neural
architectures through their respective Gaussian process and neural tangent
kernels. We prove that, with normalized multi-channel input and ReLU
activation, the eigenfunctions of these kernels with the uniform measure are
formed by products of spherical harmonics, defined over the channels of the
different pixels. We next use hierarchical factorizable kernels to bound their
respective eigenvalues. We show that the eigenvalues decay polynomially,
quantify the rate of decay, and derive measures that reflect the composition of
hierarchical features in these networks. Our results provide concrete
quantitative characterization of over-parameterized convolutional network
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geifman_A/0/1/0/all/0/1"&gt;Amnon Geifman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1"&gt;Meirav Galun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1"&gt;David Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1"&gt;Ronen Basri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (arXiv:2203.08959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08959</id>
        <link href="http://arxiv.org/abs/2203.08959"/>
        <updated>2022-03-19T00:42:47.751Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a novel neural network training framework that
increases model's adversarial robustness to adversarial attacks while
maintaining high clean accuracy by combining contrastive learning (CL) with
adversarial training (AT). We propose to improve model robustness to
adversarial attacks by learning feature representations that are consistent
under both data augmentations and adversarial perturbations. We leverage
contrastive learning to improve adversarial robustness by considering an
adversarial example as another positive example, and aim to maximize the
similarity between random augmentations of data samples and their adversarial
example, while constantly updating the classification head in order to avoid a
cognitive dissociation between the classification head and the embedding space.
This dissociation is caused by the fact that CL updates the network up to the
embedding space, while freezing the classification head which is used to
generate new positive adversarial examples. We validate our method, Contrastive
Learning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it
outperforms both robust accuracy and clean accuracy over alternative supervised
and self-supervised adversarial learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahamim_A/0/1/0/all/0/1"&gt;Adir Rahamim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1"&gt;Itay Naeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Sleep Stage Classification Using CNN Generated by an Efficient Neural Architecture Search with a New Data Processing Trick. (arXiv:2110.15277v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.15277</id>
        <link href="http://arxiv.org/abs/2110.15277"/>
        <updated>2022-03-19T00:42:47.743Z</updated>
        <summary type="html"><![CDATA[With the development of automatic sleep stage classification (ASSC)
techniques, many classical methods such as k-means, decision tree, and SVM have
been used in automatic sleep stage classification. However, few methods explore
deep learning on ASSC. Meanwhile, most deep learning methods require extensive
expertise and suffer from a mass of handcrafted steps which are time-consuming
especially when dealing with multi-classification tasks. In this paper, we
propose an efficient five-sleep-stage classification method using convolutional
neural networks (CNNs) with a novel data processing trick and we design neural
architecture search (NAS) technique based on genetic algorithm (GA), NAS-G, to
search for the best CNN architecture. Firstly, we attach each kernel with an
adaptive coefficient to enhance the signal processing of the inputs. This can
enhance the propagation of informative features and suppress the propagation of
useless features in the early stage of the network. Then, we make full use of
GA's heuristic search and the advantage of no need for the gradient to search
for the best architecture of CNN. This can achieve a CNN with better
performance than a handcrafted one in a large search space at the minimum cost.
We verify the convergence of our data processing trick and compare the
performance of traditional CNNs before and after using our trick. Meanwhile, we
compare the performance between the CNN generated through NAS-G and the
traditional CNNs with our trick. The experiments demonstrate that the
convergence of CNNs with data processing trick is faster than without data
processing trick and the CNN with data processing trick generated by NAS-G
outperforms the handcrafted counterparts that use the data processing trick
too.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Ziming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slowik_A/0/1/0/all/0/1"&gt;Adam Slowik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry-Based Representations for Artificial and Biological General Intelligence. (arXiv:2203.09250v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2203.09250</id>
        <link href="http://arxiv.org/abs/2203.09250"/>
        <updated>2022-03-19T00:42:47.728Z</updated>
        <summary type="html"><![CDATA[Biological intelligence is remarkable in its ability to produce complex
behaviour in many diverse situations through data efficient, generalisable and
transferable skill acquisition. It is believed that learning "good" sensory
representations is important for enabling this, however there is little
agreement as to what a good representation should look like. In this review
article we are going to argue that symmetry transformations are a fundamental
principle that can guide our search for what makes a good representation. The
idea that there exist transformations (symmetries) that affect some aspects of
the system but not others, and their relationship to conserved quantities has
become central in modern physics, resulting in a more unified theoretical
framework and even ability to predict the existence of new particles. Recently,
symmetries have started to gain prominence in machine learning too, resulting
in more data efficient and generalisable algorithms that can mimic some of the
complex behaviours produced by biological intelligence. Finally, first
demonstrations of the importance of symmetry transformations for representation
learning in the brain are starting to arise in neuroscience. Taken together,
the overwhelming positive effect that symmetries bring to these disciplines
suggest that they may be an important general framework that determines the
structure of the universe, constrains the nature of natural tasks and
consequently shapes both biological and artificial intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Higgins_I/0/1/0/all/0/1"&gt;Irina Higgins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Racaniere_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Racani&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Rezende_D/0/1/0/all/0/1"&gt;Danilo Rezende&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Redundancy and Diversity in Cell-based Neural Architecture Search. (arXiv:2203.08887v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.08887</id>
        <link href="http://arxiv.org/abs/2203.08887"/>
        <updated>2022-03-19T00:42:47.708Z</updated>
        <summary type="html"><![CDATA[Searching for the architecture cells is a dominant paradigm in NAS. However,
little attention has been devoted to the analysis of the cell-based search
spaces even though it is highly important for the continual development of NAS.
In this work, we conduct an empirical post-hoc analysis of architectures from
the popular cell-based search spaces and find that the existing search spaces
contain a high degree of redundancy: the architecture performance is minimally
sensitive to changes at large parts of the cells, and universally adopted
designs, like the explicit search for a reduction cell, significantly increase
the complexities but have very limited impact on the performance. Across
architectures found by a diverse set of search strategies, we consistently find
that the parts of the cells that do matter for architecture performance often
follow similar and simple patterns. By explicitly constraining cells to include
these patterns, randomly sampled architectures can match or even outperform the
state of the art. These findings cast doubts into our ability to discover truly
novel architectures in the existing cell-based search spaces, and inspire our
suggestions for improvement to guide future NAS research. Code is available at
https://github.com/xingchenwan/cell-based-NAS-analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xingchen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1"&gt;Binxin Ru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Esperanca_P/0/1/0/all/0/1"&gt;Pedro M. Esperan&amp;#xe7;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning Based on OOD Detection and Task Masking. (arXiv:2203.09450v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09450</id>
        <link href="http://arxiv.org/abs/2203.09450"/>
        <updated>2022-03-19T00:42:47.701Z</updated>
        <summary type="html"><![CDATA[Existing continual learning techniques focus on either task incremental
learning (TIL) or class incremental learning (CIL) problem, but not both. CIL
and TIL differ mainly in that the task-id is provided for each test sample
during testing for TIL, but not provided for CIL. Continual learning methods
intended for one problem have limitations on the other problem. This paper
proposes a novel unified approach based on out-of-distribution (OOD) detection
and task masking, called CLOM, to solve both problems. The key novelty is that
each task is trained as an OOD detection model rather than a traditional
supervised learning model, and a task mask is trained to protect each task to
prevent forgetting. Our evaluation shows that CLOM outperforms existing
state-of-the-art baselines by large margins. The average TIL/CIL accuracy of
CLOM over six experiments is 87.6/67.9% while that of the best baselines is
only 82.4/55.0%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyuhak Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1"&gt;Sepideh Esmaeilpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Changnan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning. (arXiv:2203.09137v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09137</id>
        <link href="http://arxiv.org/abs/2203.09137"/>
        <updated>2022-03-19T00:42:47.695Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-learning (MAML) and its variants have become popular
approaches for few-shot learning. However, due to the non-convexity of deep
neural nets (DNNs) and the bi-level formulation of MAML, the theoretical
properties of MAML with DNNs remain largely unknown. In this paper, we first
prove that MAML with over-parameterized DNNs is guaranteed to converge to
global optima at a linear rate. Our convergence analysis indicates that MAML
with over-parameterized DNNs is equivalent to kernel regression with a novel
class of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then,
we propose MetaNTK-NAS, a new training-free neural architecture search (NAS)
method for few-shot learning that uses MetaNTK to rank and select
architectures. Empirically, we compare our MetaNTK-NAS with previous NAS
methods on two popular few-shot learning benchmarks, miniImageNet, and
tieredImageNet. We show that the performance of MetaNTK-NAS is comparable or
better than the state-of-the-art NAS method designed for few-shot learning
while enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NAS
makes itself more practical for many real-world tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yite Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruoyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Against Adversarial Attack in ECG Classification with Adversarial Distillation Training. (arXiv:2203.09487v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2203.09487</id>
        <link href="http://arxiv.org/abs/2203.09487"/>
        <updated>2022-03-19T00:42:47.679Z</updated>
        <summary type="html"><![CDATA[In clinics, doctors rely on electrocardiograms (ECGs) to assess severe
cardiac disorders. Owing to the development of technology and the increase in
health awareness, ECG signals are currently obtained by using medical and
commercial devices. Deep neural networks (DNNs) can be used to analyze these
signals because of their high accuracy rate. However, researchers have found
that adversarial attacks can significantly reduce the accuracy of DNNs. Studies
have been conducted to defend ECG-based DNNs against traditional adversarial
attacks, such as projected gradient descent (PGD), and smooth adversarial
perturbation (SAP) which targets ECG classification; however, to the best of
our knowledge, no study has completely explored the defense against adversarial
attacks targeting ECG classification. Thus, we did different experiments to
explore the effects of defense methods against white-box adversarial attack and
black-box adversarial attack targeting ECG classification, and we found that
some common defense methods performed well against these attacks. Besides, we
proposed a new defense method called Adversarial Distillation Training (ADT)
which comes from defensive distillation and can effectively improve the
generalization performance of DNNs. The results show that our method performed
more effectively against adversarial attacks targeting on ECG classification
than the other baseline methods, namely, adversarial training, defensive
distillation, Jacob regularization, and noise-to-signal ratio regularization.
Furthermore, we found that our method performed better against PGD attacks with
low noise levels, which means that our method has stronger robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jiahao Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Geng_S/0/1/0/all/0/1"&gt;Shijia Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zhaoji Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1"&gt;Weilun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1"&gt;Shenda Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\ell_p$ Slack Norm Support Vector Data Description. (arXiv:2203.08932v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08932</id>
        <link href="http://arxiv.org/abs/2203.08932"/>
        <updated>2022-03-19T00:42:47.672Z</updated>
        <summary type="html"><![CDATA[The support vector data description (SVDD) approach serves as a de facto
standard for one-class classification where the learning task entails inferring
the smallest hyper-sphere to enclose target objects while linearly penalising
any errors/slacks via an $\ell_1$-norm penalty term. In this study, we
generalise this modelling formalism to a general $\ell_p$-norm ($p\geq1$) slack
penalty function. By virtue of an $\ell_p$ slack norm, the proposed approach
enables formulating a non-linear cost function with respect to slacks. From a
dual problem perspective, the proposed method introduces a sparsity-inducing
dual norm into the objective function, and thus, possesses a higher capacity to
tune into the inherent sparsity of the problem for enhanced descriptive
capability. A theoretical analysis based on Rademacher complexities
characterises the generalisation performance of the proposed approach in terms
of parameter $p$ while the experimental results on several datasets confirm the
merits of the proposed method compared to other alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arashloo_S/0/1/0/all/0/1"&gt;Shervin R. Arashloo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kan Extensions in Data Science and Machine Learning. (arXiv:2203.09018v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09018</id>
        <link href="http://arxiv.org/abs/2203.09018"/>
        <updated>2022-03-19T00:42:47.666Z</updated>
        <summary type="html"><![CDATA[A common problem in data science is "use this function defined over this
small set to generate predictions over that larger set." Extrapolation,
interpolation, statistical inference and forecasting all reduce to this
problem. The Kan extension is a powerful tool in category theory that
generalizes this notion. In this work we explore several applications of Kan
extensions to data science. We begin by deriving a simple classification
algorithm as a Kan extension and experimenting with this algorithm on real
data. Next, we use the Kan extension to derive a procedure for learning
clustering algorithms from labels and explore the performance of this procedure
on real data. We then investigate how Kan extensions can be used to learn a
general mapping from datasets of labeled examples to functions and to
approximate a complex function with a simpler one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shiebler_D/0/1/0/all/0/1"&gt;Dan Shiebler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Playing with blocks: Toward re-usable deep learning models for side-channel profiled attacks. (arXiv:2203.08448v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.08448</id>
        <link href="http://arxiv.org/abs/2203.08448"/>
        <updated>2022-03-19T00:42:47.660Z</updated>
        <summary type="html"><![CDATA[This paper introduces a deep learning modular network for side-channel
analysis. Our deep learning approach features the capability to exchange part
of it (modules) with others networks. We aim to introduce reusable trained
modules into side-channel analysis instead of building architectures for each
evaluation, reducing the body of work when conducting those. Our experiments
demonstrate that our architecture feasibly assesses a side-channel evaluation
suggesting that learning transferability is possible with the network we
propose in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paguada_S/0/1/0/all/0/1"&gt;Servio Paguada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batina_L/0/1/0/all/0/1"&gt;Lejla Batina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buhan_I/0/1/0/all/0/1"&gt;Ileana Buhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armendariz_I/0/1/0/all/0/1"&gt;Igor Armendariz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic and Private Nonconvex Outlier-Robust PCA. (arXiv:2203.09276v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09276</id>
        <link href="http://arxiv.org/abs/2203.09276"/>
        <updated>2022-03-19T00:42:47.644Z</updated>
        <summary type="html"><![CDATA[We develop theoretically guaranteed stochastic methods for outlier-robust
PCA. Outlier-robust PCA seeks an underlying low-dimensional linear subspace
from a dataset that is corrupted with outliers. We are able to show that our
methods, which involve stochastic geodesic gradient descent over the
Grassmannian manifold, converge and recover an underlying subspace in various
regimes through the development of a novel convergence analysis. The main
application of this method is an effective differentially private algorithm for
outlier-robust PCA that uses a Gaussian noise mechanism within the stochastic
gradient method. Our results emphasize the advantages of the nonconvex methods
over another convex approach to solving this problem in the differentially
private setting. Experiments on synthetic and stylized data verify these
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maunu_T/0/1/0/all/0/1"&gt;Tyler Maunu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chenyu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerman_G/0/1/0/all/0/1"&gt;Gilad Lerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Low Can We Go: Trading Memory for Error in Low-Precision Training. (arXiv:2106.09686v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09686</id>
        <link href="http://arxiv.org/abs/2106.09686"/>
        <updated>2022-03-19T00:42:47.637Z</updated>
        <summary type="html"><![CDATA[Low-precision arithmetic trains deep learning models using less energy, less
memory and less time. However, we pay a price for the savings: lower precision
may yield larger round-off error and hence larger prediction error. As
applications proliferate, users must choose which precision to use to train a
new model, and chip manufacturers must decide which precisions to manufacture.
We view these precision choices as a hyperparameter tuning problem, and borrow
ideas from meta-learning to learn the tradeoff between memory and error. In
this paper, we introduce Pareto Estimation to Pick the Perfect Precision
(PEPPP). We use matrix factorization to find non-dominated configurations (the
Pareto frontier) with a limited number of network evaluations. For any given
memory budget, the precision that minimizes error is a point on this frontier.
Practitioners can use the frontier to trade memory for error and choose the
best precision for their goals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chengrun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1"&gt;Jerry Chee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1"&gt;Madeleine Udell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MolNet: A Chemically Intuitive Graph Neural Network for Prediction of Molecular Properties. (arXiv:2203.09456v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2203.09456</id>
        <link href="http://arxiv.org/abs/2203.09456"/>
        <updated>2022-03-19T00:42:47.630Z</updated>
        <summary type="html"><![CDATA[The graph neural network (GNN) has been a powerful deep-learning tool in
chemistry domain, due to its close connection with molecular graphs. Most GNN
models collect and update atom and molecule features from the fed atom (and, in
some cases, bond) features, which are basically based on the two-dimensional
(2D) graph representation of 3D molecules. Correspondingly, the adjacency
matrix, containing the information on covalent bonds, or equivalent data
structures (e.g., lists) have been the main core in the feature-updating
processes, such as graph convolution. However, the 2D-based models do not
faithfully represent 3D molecules and their physicochemical properties,
exemplified by the overlooked field effect that is a "through-space" effect,
not a "through-bond" effect. The GNN model proposed herein, denoted as MolNet,
is chemically intuitive, accommodating the 3D non-bond information in a
molecule, with a noncovalent adjacency matrix $\bf{\bar A}$, and also
bond-strength information from a weighted bond matrix $\bf{B}$. The noncovalent
atoms, not directly bonded to a given atom in a molecule, are identified within
5 $\unicode{x212B}$ of cut-off range for the construction of $\bf{\bar A}$, and
$\bf{B}$ has edge weights of 1, 1.5, 2, and 3 for single, aromatic, double, and
triple bonds, respectively. Comparative studies show that MolNet outperforms
various baseline GNN models and gives a state-of-the-art performance in the
classification task of BACE dataset and regression task of ESOL dataset. This
work suggests a future direction of deep-learning chemistry in the construction
of deep-learning models that are chemically intuitive and comparable with the
existing chemistry concepts and tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yeji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Yoonho Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jihoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eok Kyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kim_W/0/1/0/all/0/1"&gt;Won June Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Choi_I/0/1/0/all/0/1"&gt;Insung S. Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Riemannian data with Rie-SNE. (arXiv:2203.09253v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09253</id>
        <link href="http://arxiv.org/abs/2203.09253"/>
        <updated>2022-03-19T00:42:47.624Z</updated>
        <summary type="html"><![CDATA[Faithful visualizations of data residing on manifolds must take the
underlying geometry into account when producing a flat planar view of the data.
In this paper, we extend the classic stochastic neighbor embedding (SNE)
algorithm to data on general Riemannian manifolds. We replace standard Gaussian
assumptions with Riemannian diffusion counterparts and propose an efficient
approximation that only requires access to calculations of Riemannian distances
and volumes. We demonstrate that the approach also allows for mapping data from
one manifold to another, e.g. from a high-dimensional sphere to a
low-dimensional one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bergsson_A/0/1/0/all/0/1"&gt;Andri Bergsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorizing Transformers. (arXiv:2203.08913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08913</id>
        <link href="http://arxiv.org/abs/2203.08913"/>
        <updated>2022-03-19T00:42:47.618Z</updated>
        <summary type="html"><![CDATA[Language models typically need to be trained or finetuned in order to acquire
new knowledge, which involves updating their weights. We instead envision
language models that can simply read and memorize new data at inference time,
thus acquiring new knowledge immediately. In this work, we extend language
models with the ability to memorize the internal representations of past
inputs. We demonstrate that an approximate kNN lookup into a non-differentiable
memory of recent (key, value) pairs improves language modeling across various
benchmarks and tasks, including generic webtext (C4), math papers (arXiv),
books (PG-19), code (Github), as well as formal theorems (Isabelle). We show
that the performance steadily improves when we increase the size of memory up
to 262K tokens. On benchmarks including code and mathematics, we find that the
model is capable of making use of newly defined functions and theorems during
test time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuhuai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabe_M/0/1/0/all/0/1"&gt;Markus N. Rabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutchins_D/0/1/0/all/0/1"&gt;DeLesley Hutchins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szegedy_C/0/1/0/all/0/1"&gt;Christian Szegedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symbolic Learning to Optimize: Towards Interpretability and Scalability. (arXiv:2203.06578v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.06578</id>
        <link href="http://arxiv.org/abs/2203.06578"/>
        <updated>2022-03-19T00:42:47.612Z</updated>
        <summary type="html"><![CDATA[Recent studies on Learning to Optimize (L2O) suggest a promising path to
automating and accelerating the optimization procedure for complicated tasks.
Existing L2O models parameterize optimization rules by neural networks, and
learn those numerical rules via meta-training. However, they face two common
pitfalls: (1) scalability: the numerical rules represented by neural networks
create extra memory overhead for applying L2O models, and limit their
applicability to optimizing larger tasks; (2) interpretability: it is unclear
what an L2O model has learned in its black-box optimization rule, nor is it
straightforward to compare different L2O models in an explainable way. To avoid
both pitfalls, this paper proves the concept that we can "kill two birds by one
stone", by introducing the powerful tool of symbolic regression to L2O. In this
paper, we establish a holistic symbolic representation and analysis framework
for L2O, which yields a series of insights for learnable optimizers. Leveraging
our findings, we further propose a lightweight L2O model that can be
meta-trained on large-scale problems and outperformed human-designed and tuned
optimizers. Our work is set to supply a brand-new perspective to L2O research.
Codes are available at:
https://github.com/VITA-Group/Symbolic-Learning-To-Optimize.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1"&gt;Ting-Kuei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Excited state, non-adiabatic dynamics of large photoswitchable molecules using a chemically transferable machine learning potential. (arXiv:2108.04879v3 [physics.chem-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04879</id>
        <link href="http://arxiv.org/abs/2108.04879"/>
        <updated>2022-03-19T00:42:47.595Z</updated>
        <summary type="html"><![CDATA[Light-induced chemical processes are ubiquitous in nature and have widespread
technological applications. For example, photoisomerization can allow a drug
with a photo-switchable scaffold such as azobenzene to be activated with light.
In principle, photoswitches with desired photophysical properties like high
isomerization quantum yields can be identified through virtual screening with
reactive simulations. In practice, these simulations are rarely used for
screening, since they require hundreds of trajectories and expensive quantum
chemical methods to account for non-adiabatic excited state effects. Here we
introduce a diabatic artificial neural network (DANN) based on diabatic states
to accelerate such simulations for azobenzene derivatives. The network is six
orders of magnitude faster than the quantum chemistry method used for training.
DANN is transferable to azobenzene molecules outside the training set,
predicting quantum yields for unseen species that are correlated with
experiment. We use the model to virtually screen 3,100 hypothetical molecules,
and identify novel species with extremely high predicted quantum yields. The
model predictions are confirmed using high accuracy non-adiabatic dynamics. Our
results pave the way for fast and accurate virtual screening of photoactive
compounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Axelrod_S/0/1/0/all/0/1"&gt;Simon Axelrod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Shakhnovich_E/0/1/0/all/0/1"&gt;Eugene Shakhnovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gomez_Bombarelli_R/0/1/0/all/0/1"&gt;Rafael G&amp;#xf3;mez-Bombarelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting associations and meanings of objects depicted in artworks through bi-modal deep networks. (arXiv:2203.07026v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.07026</id>
        <link href="http://arxiv.org/abs/2203.07026"/>
        <updated>2022-03-19T00:42:47.588Z</updated>
        <summary type="html"><![CDATA[We present a novel bi-modal system based on deep networks to address the
problem of learning associations and simple meanings of objects depicted in
"authored" images, such as fine art paintings and drawings. Our overall system
processes both the images and associated texts in order to learn associations
between images of individual objects, their identities and the abstract
meanings they signify. Unlike past deep nets that describe depicted objects and
infer predicates, our system identifies meaning-bearing objects ("signifiers")
and their associations ("signifieds") as well as basic overall meanings for
target artworks. Our system had precision of 48% and recall of 78% with an F1
metric of 0.6 on a curated set of Dutch vanitas paintings, a genre celebrated
for its concentration on conveying a meaning of great import at the time of
their execution. We developed and tested our system on fine art paintings but
our general methods can be applied to other authored images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kell_G/0/1/0/all/0/1"&gt;Gregory Kell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1"&gt;Ryan-Rhys Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bourached_A/0/1/0/all/0/1"&gt;Anthony Bourached&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stork_D/0/1/0/all/0/1"&gt;David G. Stork&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.06717</id>
        <link href="http://arxiv.org/abs/2203.06717"/>
        <updated>2022-03-19T00:42:47.570Z</updated>
        <summary type="html"><![CDATA[We revisit large kernel design in modern convolutional neural networks
(CNNs). Inspired by recent advances of vision transformers (ViTs), in this
paper, we demonstrate that using a few large convolutional kernels instead of a
stack of small kernels could be a more powerful paradigm. We suggested five
guidelines, e.g., applying re-parameterized large depth-wise convolutions, to
design efficient high-performance large-kernel CNNs. Following the guidelines,
we propose RepLKNet, a pure CNN architecture whose kernel size is as large as
31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the
performance gap between CNNs and ViTs, e.g., achieving comparable or superior
results than Swin Transformer on ImageNet and a few typical downstream tasks,
with lower latency. RepLKNet also shows nice scalability to big data and large
models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K,
which is very competitive among the state-of-the-arts with similar model sizes.
Our study further reveals that, in contrast to small-kernel CNNs, large-kernel
CNNs have much larger effective receptive fields, and higher shape bias rather
than texture bias. Code & models at
https://github.com/megvii-research/RepLKNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xiaohan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yizhuang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jungong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Guiguang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning. (arXiv:2203.09064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09064</id>
        <link href="http://arxiv.org/abs/2203.09064"/>
        <updated>2022-03-19T00:42:47.448Z</updated>
        <summary type="html"><![CDATA[This paper presents new hierarchically cascaded transformers that can improve
data efficiency through attribute surrogates learning and spectral tokens
pooling. Vision transformers have recently been thought of as a promising
alternative to convolutional neural networks for visual recognition. But when
there is no sufficient data, it gets stuck in overfitting and shows inferior
performance. To improve data efficiency, we propose hierarchically cascaded
transformers that exploit intrinsic image structures through spectral tokens
pooling and optimize the learnable parameters through latent attribute
surrogates. The intrinsic image structure is utilized to reduce the ambiguity
between foreground content and background noise by spectral tokens pooling. And
the attribute surrogate learning scheme is designed to benefit from the rich
visual information in image-label pairs instead of simple visual concepts
assigned by their labels. Our Hierarchically Cascaded Transformers, called
HCTransformers, is built upon a self-supervised learning framework DINO and is
tested on several popular few-shot learning benchmarks.

In the inductive setting, HCTransformers surpass the DINO baseline by a large
margin of 9.7% 5-way 1-shot accuracy and 9.17% 5-way 5-shot accuracy on
miniImageNet, which demonstrates HCTransformers are efficient to extract
discriminative features. Also, HCTransformers show clear advantages over SOTA
few-shot classification methods in both 5-way 1-shot and 5-way 5-shot settings
on four popular benchmark datasets, including miniImageNet, tieredImageNet,
FC100, and CIFAR-FS. The trained weights and codes are available at
https://github.com/StomachCold/HCTransformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yangji He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weihan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hong-Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Weifeng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenqiang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09301</id>
        <link href="http://arxiv.org/abs/2203.09301"/>
        <updated>2022-03-19T00:42:47.441Z</updated>
        <summary type="html"><![CDATA[There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1"&gt;Gihyun Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed. (arXiv:2203.09179v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2203.09179</id>
        <link href="http://arxiv.org/abs/2203.09179"/>
        <updated>2022-03-19T00:42:47.435Z</updated>
        <summary type="html"><![CDATA[Gaussian process regression underpins countless academic and industrial
applications of machine learning and statistics, with maximum likelihood
estimation routinely used to select appropriate parameters for the covariance
kernel. However, it remains an open problem to establish the circumstances in
which maximum likelihood estimation is well-posed. That is, when the
predictions of the regression model are continuous (or insensitive to small
perturbations) in the training data. This article presents a rigorous proof
that the maximum likelihood estimator fails to be well-posed in Hellinger
distance in a scenario where the data are noiseless. The failure case occurs
for any Gaussian process with a stationary covariance function whose
lengthscale parameter is estimated using maximum likelihood. Although the
failure of maximum likelihood estimation is informally well-known, these
theoretical results appear to be the first of their kind, and suggest that
well-posedness may need to be assessed post-hoc, on a case-by-case basis, when
maximum likelihood estimation is used to train a Gaussian process model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Karvonen_T/0/1/0/all/0/1"&gt;Toni Karvonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Oates_C/0/1/0/all/0/1"&gt;Chris J. Oates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Intrinsic Robustness Using Label Uncertainty. (arXiv:2107.03250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03250</id>
        <link href="http://arxiv.org/abs/2107.03250"/>
        <updated>2022-03-19T00:42:47.177Z</updated>
        <summary type="html"><![CDATA[A fundamental question in adversarial machine learning is whether a robust
classifier exists for a given task. A line of research has made some progress
towards this goal by studying the concentration of measure, but we argue
standard concentration fails to fully characterize the intrinsic robustness of
a classification problem since it ignores data labels which are essential to
any classification task. Building on a novel definition of label uncertainty,
we empirically demonstrate that error regions induced by state-of-the-art
models tend to have much higher label uncertainty than randomly-selected
subsets. This observation motivates us to adapt a concentration estimation
algorithm to account for label uncertainty, resulting in more accurate
intrinsic robustness measures for benchmark image classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1"&gt;David Evans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Euler State Networks. (arXiv:2203.09382v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09382</id>
        <link href="http://arxiv.org/abs/2203.09382"/>
        <updated>2022-03-19T00:42:47.170Z</updated>
        <summary type="html"><![CDATA[Inspired by the numerical solution of ordinary differential equations, in
this paper we propose a novel Reservoir Computing (RC) model, called the Euler
State Network (EuSN). The introduced approach makes use of forward Euler
discretization and antisymmetric recurrent matrices to design reservoir
dynamics that are both stable and non-dissipative by construction.

Our mathematical analysis shows that the resulting model is biased towards
unitary effective spectral radius and zero local Lyapunov exponents,
intrinsically operating at the edge of stability. Experiments on synthetic
tasks indicate the marked superiority of the proposed approach, compared to
standard RC models, in tasks requiring long-term memorization skills.
Furthermore, results on real-world time series classification benchmarks point
out that EuSN is capable of matching (or even surpassing) the level of accuracy
of trainable Recurrent Neural Networks, while allowing up to 100-fold savings
in computation time and energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1"&gt;Claudio Gallicchio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability and Risk Bounds of Iterative Hard Thresholding. (arXiv:2203.09413v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09413</id>
        <link href="http://arxiv.org/abs/2203.09413"/>
        <updated>2022-03-19T00:42:47.161Z</updated>
        <summary type="html"><![CDATA[In this paper, we analyze the generalization performance of the Iterative
Hard Thresholding (IHT) algorithm widely used for sparse recovery problems. The
parameter estimation and sparsity recovery consistency of IHT has long been
known in compressed sensing. From the perspective of statistical learning,
another fundamental question is how well the IHT estimation would predict on
unseen data. This paper makes progress towards answering this open question by
introducing a novel sparse generalization theory for IHT under the notion of
algorithmic stability. Our theory reveals that: 1) under natural conditions on
the empirical risk function over $n$ samples of dimension $p$, IHT with
sparsity level $k$ enjoys an $\mathcal{\tilde
O}(n^{-1/2}\sqrt{k\log(n)\log(p)})$ rate of convergence in sparse excess risk;
2) a tighter $\mathcal{\tilde O}(n^{-1/2}\sqrt{\log(n)})$ bound can be
established by imposing an additional iteration stability condition on a
hypothetical IHT procedure invoked to the population risk; and 3) a fast rate
of order $\mathcal{\tilde O}\left(n^{-1}k(\log^3(n)+\log(p))\right)$ can be
derived for strongly convex risk function under proper strong-signal
conditions. The results have been substantialized to sparse linear regression
and sparse logistic regression models to demonstrate the applicability of our
theory. Preliminary numerical evidence is provided to confirm our theoretical
predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiao-Tong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive n-ary Activation Functions for Probabilistic Boolean Logic. (arXiv:2203.08977v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08977</id>
        <link href="http://arxiv.org/abs/2203.08977"/>
        <updated>2022-03-19T00:42:47.082Z</updated>
        <summary type="html"><![CDATA[Balancing model complexity against the information contained in observed data
is the central challenge to learning. In order for complexity-efficient models
to exist and be discoverable in high dimensions, we require a computational
framework that relates a credible notion of complexity to simple parameter
representations. Further, this framework must allow excess complexity to be
gradually removed via gradient-based optimization. Our n-ary, or n-argument,
activation functions fill this gap by approximating belief functions
(probabilistic Boolean logic) using logit representations of probability. Just
as Boolean logic determines the truth of a consequent claim from relationships
among a set of antecedent propositions, probabilistic formulations generalize
predictions when antecedents, truth tables, and consequents all retain
uncertainty. Our activation functions demonstrate the ability to learn
arbitrary logic, such as the binary exclusive disjunction (p xor q) and ternary
conditioned disjunction ( c ? p : q ), in a single layer using an activation
function of matching or greater arity. Further, we represent belief tables
using a basis that directly associates the number of nonzero parameters to the
effective arity of the belief function, thus capturing a concrete relationship
between logical complexity and efficient parameter representations. This opens
optimization approaches to reduce logical complexity by inducing parameter
sparsity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duersch_J/0/1/0/all/0/1"&gt;Jed A. Duersch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanach_T/0/1/0/all/0/1"&gt;Thomas A. Catanach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Niladri Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning with Positive-Negative Frame Mask for Music Representation. (arXiv:2203.09129v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2203.09129</id>
        <link href="http://arxiv.org/abs/2203.09129"/>
        <updated>2022-03-19T00:42:47.059Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning, especially contrastive learning, has made an
outstanding contribution to the development of many deep learning research
fields. Recently, researchers in the acoustic signal processing field noticed
its success and leveraged contrastive learning for better music representation.
Typically, existing approaches maximize the similarity between two distorted
audio segments sampled from the same music. In other words, they ensure a
semantic agreement at the music level. However, those coarse-grained methods
neglect some inessential or noisy elements at the frame level, which may be
detrimental to the model to learn the effective representation of music.
Towards this end, this paper proposes a novel Positive-nEgative frame mask for
Music Representation based on the contrastive learning framework, abbreviated
as PEMR. Concretely, PEMR incorporates a Positive-Negative Mask Generation
module, which leverages transformer blocks to generate frame masks on the
Log-Mel spectrogram. We can generate self-augmented negative and positive
samples by masking important components or inessential components,
respectively. We devise a novel contrastive learning objective to accommodate
both self-augmented positives/negatives sampled from the same music. We conduct
experiments on four public datasets. The experimental results of two
music-related downstream tasks, music classification, and cover song
identification, demonstrate the generalization ability and transferability of
music representation learned by PEMR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1"&gt;Dong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jieming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yudong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling Instance-Dependent Label Noise via a Universal Probabilistic Model. (arXiv:2101.05467v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05467</id>
        <link href="http://arxiv.org/abs/2101.05467"/>
        <updated>2022-03-19T00:42:47.051Z</updated>
        <summary type="html"><![CDATA[The drastic increase of data quantity often brings the severe decrease of
data quality, such as incorrect label annotations, which poses a great
challenge for robustly training Deep Neural Networks (DNNs). Existing learning
\mbox{methods} with label noise either employ ad-hoc heuristics or restrict to
specific noise assumptions. However, more general situations, such as
instance-dependent label noise, have not been fully explored, as scarce studies
focus on their label corruption process. By categorizing instances into
confusing and unconfusing instances, this paper proposes a simple yet universal
probabilistic model, which explicitly relates noisy labels to their instances.
The resultant model can be realized by DNNs, where the training procedure is
accomplished by employing an alternating optimization algorithm. Experiments on
datasets with both synthetic and real-world label noise verify that the
proposed method yields significant improvements on robustness over
state-of-the-art counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Network-based Internal Model Control design for stable nonlinear systems. (arXiv:2108.04585v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04585</id>
        <link href="http://arxiv.org/abs/2108.04585"/>
        <updated>2022-03-19T00:42:47.043Z</updated>
        <summary type="html"><![CDATA[Owing to their superior modeling capabilities, gated Recurrent Neural
Networks, such as Gated Recurrent Units (GRUs) and Long Short-Term Memory
networks (LSTMs), have become popular tools for learning dynamical systems.
This paper aims to discuss how these networks can be adopted for the synthesis
of Internal Model Control (IMC) architectures. To this end, first a gated
recurrent network is used to learn a model of the unknown input-output stable
plant. Then, a controller gated recurrent network is trained to approximate the
model inverse. The stability of these networks, ensured by means of a suitable
training procedure, allows to guarantee the input-output closed-loop stability.
The proposed scheme is able to cope with the saturation of the control
variables, and can be deployed on low-power embedded controllers, as it
requires limited online computations. The approach is then tested on the
Quadruple Tank benchmark system and compared to alternative control laws,
resulting in remarkable closed-loop performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonassi_F/0/1/0/all/0/1"&gt;Fabio Bonassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scattolini_R/0/1/0/all/0/1"&gt;Riccardo Scattolini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained style control in Transformer-based Text-to-speech Synthesis. (arXiv:2110.06306v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.06306</id>
        <link href="http://arxiv.org/abs/2110.06306"/>
        <updated>2022-03-19T00:42:47.035Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel architecture to realize fine-grained style
control on the transformer-based text-to-speech synthesis (TransformerTTS).
Specifically, we model the speaking style by extracting a time sequence of
local style tokens (LST) from the reference speech. The existing content
encoder in TransformerTTS is then replaced by our designed cross-attention
blocks for fusion and alignment between content and style. As the fusion is
performed along with the skip connection, our cross-attention block provides a
good inductive bias to gradually infuse the phoneme representation with a given
style. Additionally, we prevent the style embedding from encoding linguistic
content by randomly truncating LST during training and using wav2vec 2.0
features. Experiments show that with fine-grained style control, our system
performs better in terms of naturalness, intelligibility, and style
transferability. Our code and samples are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1"&gt;Alexander Rudnicky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-based Multi-agent Policy Optimization with Adaptive Opponent-wise Rollouts. (arXiv:2105.03363v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03363</id>
        <link href="http://arxiv.org/abs/2105.03363"/>
        <updated>2022-03-19T00:42:47.014Z</updated>
        <summary type="html"><![CDATA[This paper investigates the model-based methods in multi-agent reinforcement
learning (MARL). We specify the dynamics sample complexity and the opponent
sample complexity in MARL, and conduct a theoretic analysis of return
discrepancy upper bound. To reduce the upper bound with the intention of low
sample complexity during the whole learning process, we propose a novel
decentralized model-based MARL method, named Adaptive Opponent-wise Rollout
Policy Optimization (AORPO). In AORPO, each agent builds its multi-agent
environment model, consisting of a dynamics model and multiple opponent models,
and trains its policy with the adaptive opponent-wise rollout. We further prove
the theoretic convergence of AORPO under reasonable assumptions. Empirical
experiments on competitive and cooperative tasks demonstrate that AORPO can
achieve improved sample efficiency with comparable asymptotic performance over
the compared MARL methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xihuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jian Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes. (arXiv:2110.08059v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.08059</id>
        <link href="http://arxiv.org/abs/2110.08059"/>
        <updated>2022-03-19T00:42:47.008Z</updated>
        <summary type="html"><![CDATA[When designing Convolutional Neural Networks (CNNs), one must select the
size\break of the convolutional kernels before training. Recent works show CNNs
benefit from different kernel sizes at different layers, but exploring all
possible combinations is unfeasible in practice. A more efficient approach is
to learn the kernel size during training. However, existing works that learn
the kernel size have a limited bandwidth. These approaches scale kernels by
dilation, and thus the detail they can describe is limited. In this work, we
propose FlexConv, a novel convolutional operation with which high bandwidth
convolutional kernels of learnable kernel size can be learned at a fixed
parameter cost. FlexNets model long-term dependencies without the use of
pooling, achieve state-of-the-art performance on several sequential datasets,
outperform recent works with learned kernel sizes, and are competitive with
much deeper ResNets on image benchmark datasets. Additionally, FlexNets can be
deployed at higher resolutions than those seen during training. To avoid
aliasing, we propose a novel kernel parameterization with which the frequency
of the kernels can be analytically controlled. Our novel kernel
parameterization shows higher descriptive power and faster convergence speed
than existing parameterizations. This leads to important improvements in
classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1"&gt;David W. Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruintjes_R/0/1/0/all/0/1"&gt;Robert-Jan Bruintjes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1"&gt;Jakub M. Tomczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik J. Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1"&gt;Mark Hoogendoorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy Tensor Completion via Low-rank Tensor Ring. (arXiv:2203.08857v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.08857</id>
        <link href="http://arxiv.org/abs/2203.08857"/>
        <updated>2022-03-19T00:42:47.001Z</updated>
        <summary type="html"><![CDATA[Tensor completion is a fundamental tool for incomplete data analysis, where
the goal is to predict missing entries from partial observations. However,
existing methods often make the explicit or implicit assumption that the
observed entries are noise-free to provide a theoretical guarantee of exact
recovery of missing entries, which is quite restrictive in practice. To remedy
such drawbacks, this paper proposes a novel noisy tensor completion model,
which complements the incompetence of existing works in handling the
degeneration of high-order and noisy observations. Specifically, the tensor
ring nuclear norm (TRNN) and least-squares estimator are adopted to regularize
the underlying tensor and the observed entries, respectively. In addition, a
non-asymptotic upper bound of estimation error is provided to depict the
statistical performance of the proposed estimator. Two efficient algorithms are
developed to solve the optimization problem with convergence guarantee, one of
which is specially tailored to handle large-scale tensors by replacing the
minimization of TRNN of the original tensor equivalently with that of a much
smaller one in a heterogeneous tensor decomposition framework. Experimental
results on both synthetic and real-world data demonstrate the effectiveness and
efficiency of the proposed model in recovering noisy incomplete tensor data
compared with state-of-the-art tensor completion models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yuning Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guoxu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qibin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shengli Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Generative Models in Engineering Design: A Review. (arXiv:2110.10863v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.10863</id>
        <link href="http://arxiv.org/abs/2110.10863"/>
        <updated>2022-03-19T00:42:46.994Z</updated>
        <summary type="html"><![CDATA[Automated design synthesis has the potential to revolutionize the modern
engineering design process and improve access to highly optimized and
customized products across countless industries. Successfully adapting
generative Machine Learning to design engineering may enable such automated
design synthesis and is a research subject of great importance. We present a
review and analysis of Deep Generative Machine Learning models in engineering
design. Deep Generative Models (DGMs) typically leverage deep networks to learn
from an input dataset and synthesize new designs. Recently, DGMs such as
feedforward Neural Networks (NNs), Generative Adversarial Networks (GANs),
Variational Autoencoders (VAEs), and certain Deep Reinforcement Learning (DRL)
frameworks have shown promising results in design applications like structural
optimization, materials design, and shape synthesis. The prevalence of DGMs in
engineering design has skyrocketed since 2016. Anticipating continued growth,
we conduct a review of recent advances to benefit researchers interested in
DGMs for design. We structure our review as an exposition of the algorithms,
datasets, representation methods, and applications commonly used in the current
literature. In particular, we discuss key works that have introduced new
techniques and methods in DGMs, successfully applied DGMs to a design-related
domain, or directly supported the development of DGMs through datasets or
auxiliary methods. We further identify key challenges and limitations currently
seen in DGMs across design fields, such as design creativity, handling
constraints and objectives, and modeling both form and functional performance
simultaneously. In our discussion, we identify possible solution pathways as
key areas on which to target future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regenwetter_L/0/1/0/all/0/1"&gt;Lyle Regenwetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nobari_A/0/1/0/all/0/1"&gt;Amin Heyrani Nobari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1"&gt;Faez Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Really Need a Learnable Classifier at the End of Deep Neural Network?. (arXiv:2203.09081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09081</id>
        <link href="http://arxiv.org/abs/2203.09081"/>
        <updated>2022-03-19T00:42:46.987Z</updated>
        <summary type="html"><![CDATA[Modern deep neural networks for classification usually jointly learn a
backbone for representation and a linear classifier to output the logit of each
class. A recent study has shown a phenomenon called neural collapse that the
within-class means of features and the classifier vectors converge to the
vertices of a simplex equiangular tight frame (ETF) at the terminal phase of
training on a balanced dataset. Since the ETF geometric structure maximally
separates the pair-wise angles of all classes in the classifier, it is natural
to raise the question, why do we spend an effort to learn a classifier when we
know its optimal geometric structure? In this paper, we study the potential of
learning a neural network for classification with the classifier randomly
initialized as an ETF and fixed during training. Our analytical work based on
the layer-peeled model indicates that the feature learning with a fixed ETF
classifier naturally leads to the neural collapse state even when the dataset
is imbalanced among classes. We further show that in this case the cross
entropy (CE) loss is not necessary and can be replaced by a simple squared loss
that shares the same global optimality but enjoys a more accurate gradient and
better convergence property. Our experimental results show that our method is
able to achieve similar performances on image classification for balanced
datasets, and bring significant improvements in the long-tailed and
fine-grained classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Liang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shixiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiangtai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Vision Transformers Robust to Spurious Correlations?. (arXiv:2203.09125v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09125</id>
        <link href="http://arxiv.org/abs/2203.09125"/>
        <updated>2022-03-19T00:42:46.965Z</updated>
        <summary type="html"><![CDATA[Deep neural networks may be susceptible to learning spurious correlations
that hold on average but not in atypical test samples. As with the recent
emergence of vision transformer (ViT) models, it remains underexplored how
spurious correlations are manifested in such architectures. In this paper, we
systematically investigate the robustness of vision transformers to spurious
correlations on three challenging benchmark datasets and compare their
performance with popular CNNs. Our study reveals that when pre-trained on a
sufficiently large dataset, ViT models are more robust to spurious correlations
than CNNs. Key to their success is the ability to generalize better from the
examples where spurious correlations do not hold. Further, we perform extensive
ablations and experiments to understand the role of the self-attention
mechanism in providing robustness under spuriously correlated environments. We
hope that our work will inspire future research on further understanding the
robustness of ViT models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_S/0/1/0/all/0/1"&gt;Soumya Suvra Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yifei Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Margins for Instance Reweighting in Adversarial Training. (arXiv:2106.07904v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07904</id>
        <link href="http://arxiv.org/abs/2106.07904"/>
        <updated>2022-03-19T00:42:46.957Z</updated>
        <summary type="html"><![CDATA[Reweighting adversarial data during training has been recently shown to
improve adversarial robustness, where data closer to the current decision
boundaries are regarded as more critical and given larger weights. However,
existing methods measuring the closeness are not very reliable: they are
discrete and can take only a few values, and they are path-dependent, i.e.,
they may change given the same start and end points with different attack
paths. In this paper, we propose three types of probabilistic margin (PM),
which are continuous and path-independent, for measuring the aforementioned
closeness and reweighting adversarial data. Specifically, a PM is defined as
the difference between two estimated class-posterior probabilities, e.g., such
the probability of the true label minus the probability of the most confusing
label given some natural data. Though different PMs capture different geometric
properties, all three PMs share a negative correlation with the vulnerability
of data: data with larger/smaller PMs are safer/riskier and should have
smaller/larger weights. Experiments demonstrate that PMs are reliable
measurements and PM-based reweighting methods outperform state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer learning for cross-modal demand prediction of bike-share and public transit. (arXiv:2203.09279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09279</id>
        <link href="http://arxiv.org/abs/2203.09279"/>
        <updated>2022-03-19T00:42:46.951Z</updated>
        <summary type="html"><![CDATA[The urban transportation system is a combination of multiple transport modes,
and the interdependencies across those modes exist. This means that the travel
demand across different travel modes could be correlated as one mode may
receive demand from or create demand for another mode, not to mention natural
correlations between different demand time series due to general demand flow
patterns across the network. It is expectable that cross-modal ripple effects
become more prevalent, with Mobility as a Service. Therefore, by propagating
demand data across modes, a better demand prediction could be obtained. To this
end, this study explores various machine learning models and transfer learning
strategies for cross-modal demand prediction. The trip data of bike-share,
metro, and taxi are processed as the station-level passenger flows, and then
the proposed prediction method is tested in the large-scale case studies of
Nanjing and Chicago. The results suggest that prediction models with transfer
learning perform better than unimodal prediction models. Furthermore, stacked
Long Short-Term Memory model performs particularly well in cross-modal demand
prediction. These results verify our combined method's forecasting improvement
over existing benchmarks and demonstrate the good transferability for
cross-modal demand prediction in multiple cities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1"&gt;Mingzhuang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1"&gt;Francisco Camara Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuewu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input. (arXiv:2203.09123v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09123</id>
        <link href="http://arxiv.org/abs/2203.09123"/>
        <updated>2022-03-19T00:42:46.944Z</updated>
        <summary type="html"><![CDATA[The transferability of adversarial examples allows the deception on black-box
models, and transfer-based targeted attacks have attracted a lot of interest
due to their practical applicability. To maximize the transfer success rate,
adversarial examples should avoid overfitting to the source model, and image
augmentation is one of the primary approaches for this. However, prior works
utilize simple image transformations such as resizing, which limits input
diversity. To tackle this limitation, we propose the object-based diverse input
(ODI) method that draws an adversarial image on a 3D object and induces the
rendered image to be classified as the target class. Our motivation comes from
the humans' superior perception of an image printed on a 3D object. If the
image is clear enough, humans can recognize the image content in a variety of
viewing conditions. Likewise, if an adversarial example looks like the target
class to the model, the model should also classify the rendered image of the 3D
object as the target class. The ODI method effectively diversifies the input by
leveraging an ensemble of multiple source objects and randomizing viewing
conditions. In our experimental results on the ImageNet-Compatible dataset,
this method boosts the average targeted attack success rate from 28.3% to 47.0%
compared to the state-of-the-art methods. We also demonstrate the applicability
of the ODI method to adversarial examples on the face verification task and its
superior performance improvement. Our code is available at
https://github.com/dreamflake/ODI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Byun_J/0/1/0/all/0/1"&gt;Junyoung Byun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Seungju Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1"&gt;Myung-Joon Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hee-Seon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Changick Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk-Averse No-Regret Learning in Online Convex Games. (arXiv:2203.08957v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08957</id>
        <link href="http://arxiv.org/abs/2203.08957"/>
        <updated>2022-03-19T00:42:46.936Z</updated>
        <summary type="html"><![CDATA[We consider an online stochastic game with risk-averse agents whose goal is
to learn optimal decisions that minimize the risk of incurring significantly
high costs. Specifically, we use the Conditional Value at Risk (CVaR) as a risk
measure that the agents can estimate using bandit feedback in the form of the
cost values of only their selected actions. Since the distributions of the cost
functions depend on the actions of all agents that are generally unobservable,
they are themselves unknown and, therefore, the CVaR values of the costs are
difficult to compute. To address this challenge, we propose a new online
risk-averse learning algorithm that relies on one-point zeroth-order estimation
of the CVaR gradients computed using CVaR values that are estimated by
appropriately sampling the cost functions. We show that this algorithm achieves
sub-linear regret with high probability. We also propose two variants of this
algorithm that improve performance. The first variant relies on a new sampling
strategy that uses samples from the previous iteration to improve the
estimation accuracy of the CVaR values. The second variant employs residual
feedback that uses CVaR values from the previous iteration to reduce the
variance of the CVaR gradient estimates. We theoretically analyze the
convergence properties of these variants and illustrate their performance on an
online market problem that we model as a Cournot game.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zifan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zavlanos_M/0/1/0/all/0/1"&gt;Michael M. Zavlanos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Multi-Domain Long-Tailed Recognition, Generalization and Beyond. (arXiv:2203.09513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09513</id>
        <link href="http://arxiv.org/abs/2203.09513"/>
        <updated>2022-03-19T00:42:46.916Z</updated>
        <summary type="html"><![CDATA[Real-world data often exhibit imbalanced label distributions. Existing
studies on data imbalance focus on single-domain settings, i.e., samples are
from the same data distribution. However, natural data can originate from
distinct domains, where a minority class in one domain could have abundant
instances from other domains. We formalize the task of Multi-Domain Long-Tailed
Recognition (MDLT), which learns from multi-domain imbalanced data, addresses
label imbalance, domain shift, and divergent label distributions across
domains, and generalizes to all domain-class pairs. We first develop the
domain-class transferability graph, and show that such transferability governs
the success of learning in MDLT. We then propose BoDA, a theoretically grounded
learning strategy that tracks the upper bound of transferability statistics,
and ensures balanced alignment and calibration across imbalanced domain-class
distributions. We curate five MDLT benchmarks based on widely-used multi-domain
datasets, and compare BoDA to twenty algorithms that span different learning
strategies. Extensive and rigorous experiments verify the superior performance
of BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on
Domain Generalization benchmarks, improving generalization to unseen domains.
Code and data are available at
https://github.com/YyzHarry/multi-domain-imbalance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuzhe Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1"&gt;Dina Katabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Glimpse of Physical Layer Decision Mechanisms: Facts, Challenges, and Remedies. (arXiv:2102.07258v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07258</id>
        <link href="http://arxiv.org/abs/2102.07258"/>
        <updated>2022-03-19T00:42:46.910Z</updated>
        <summary type="html"><![CDATA[Communications are realized as a result of successive decisions at the
physical layer, from modulation selection to multi-antenna strategy, and each
decision affects the performance of the communication systems. Future
communication systems must include extensive capabilities as they will
encompass a wide variety of devices and applications. Conventional physical
layer decision mechanisms may not meet these requirements, as they are often
based on impractical and oversimplifying assumptions that result in a trade-off
between complexity and efficiency. By leveraging past experiences,
learning-driven designs are promising solutions to present a resilient decision
mechanism and enable rapid response even under exceptional circumstances. The
corresponding design solutions should evolve following the lines of
learning-driven paradigms that offer more autonomy and robustness. This
evolution must take place by considering the facts of real-world systems and
without restraining assumptions. In this paper, the common assumptions in the
physical layer are presented to highlight their discrepancies with practical
systems. As a solution, learning algorithms are examined by considering the
implementation steps and challenges. Furthermore, these issues are discussed
through a real-time case study using software-defined radio nodes to
demonstrate the potential performance improvement. A cyber-physical framework
is presented to incorporate future remedies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gecgel_S/0/1/0/all/0/1"&gt;Selen Gecgel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goztepe_C/0/1/0/all/0/1"&gt;Caner Goztepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurt_G/0/1/0/all/0/1"&gt;Gunes Karabulut Kurt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yanikomeroglu_H/0/1/0/all/0/1"&gt;Halim Yanikomeroglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding. (arXiv:2203.09175v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09175</id>
        <link href="http://arxiv.org/abs/2203.09175"/>
        <updated>2022-03-19T00:42:46.900Z</updated>
        <summary type="html"><![CDATA[Large-scale crop type classification is a task at the core of remote sensing
efforts with applications of both economic and ecological importance. Current
state-of-the-art deep learning methods are based on self-attention and use
satellite image time series (SITS) to discriminate crop types based on their
unique growth patterns. However, existing methods generalize poorly to regions
not seen during training mainly due to not being robust to temporal shifts of
the growing season caused by variations in climate. To this end, we propose
Thermal Positional Encoding (TPE) for attention-based crop classifiers. Unlike
previous positional encoding based on calendar time (e.g. day-of-year), TPE is
based on thermal time, which is obtained by accumulating daily average
temperatures over the growing season. Since crop growth is directly related to
thermal time, but not calendar time, TPE addresses the temporal shifts between
different regions to improve generalization. We propose multiple TPE
strategies, including learnable methods, to further improve results compared to
the common fixed positional encodings. We demonstrate our approach on a crop
classification task across four different European regions, where we obtain
state-of-the-art generalization results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1"&gt;Joachim Nyborg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1"&gt;Charlotte Pelletier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1"&gt;Ira Assent&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Structure Mining with Contrastive Modality Fusion for Multimedia Recommendation. (arXiv:2111.00678v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.00678</id>
        <link href="http://arxiv.org/abs/2111.00678"/>
        <updated>2022-03-19T00:42:46.892Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed growing interests in multimedia recommendation,
which aims to predict whether a user will interact with an item with multimodal
contents. Previous studies focus on modeling user-item interactions with
multimodal features included as side information. However, this scheme is not
well-designed for multimedia recommendation. Firstly, only collaborative
item-item relationships are implicitly modeled through high-order
item-user-item co-occurrences. We argue that the latent semantic item-item
structures underlying these multimodal contents could be beneficial for
learning better item representations and assist the recommender models to
comprehensively discover candidate items. Secondly, previous studies disregard
the fine-grained multimodal fusion. Although having access to multiple
modalities might allow us to capture rich information, we argue that the simple
coarse-grained fusion by linear combination or concatenation in previous work
is insufficient to fully understand content information and item
relationships.To this end, we propose a latent structure MIning with
ContRastive mOdality fusion method (MICRO for brevity). To be specific, we
devise a novel modality-aware structure learning module, which learns item-item
relationships for each modality. Based on the learned modality-aware latent
item relationships, we perform graph convolutions that explicitly inject item
affinities to modality-aware item representations. Then, we design a novel
contrastive method to fuse multimodal features. These enriched item
representations can be plugged into existing collaborative filtering methods to
make more accurate recommendations. Extensive experiments on real-world
datasets demonstrate the superiority of our method over state-of-the-art
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09096</id>
        <link href="http://arxiv.org/abs/2203.09096"/>
        <updated>2022-03-19T00:42:46.823Z</updated>
        <summary type="html"><![CDATA[The ability to predict the future trajectory of a patient is a key step
toward the development of therapeutics for complex diseases such as Alzheimer's
disease (AD). However, most machine learning approaches developed for
prediction of disease progression are either single-task or single-modality
models, which can not be directly adopted to our setting involving multi-task
learning with high dimensional images. Moreover, most of those approaches are
trained on a single dataset (i.e. cohort), which can not be generalized to
other cohorts. We propose a novel multimodal multi-task deep learning model to
predict AD progression by analyzing longitudinal clinical and neuroimaging data
from multiple cohorts. Our proposed model integrates high dimensional MRI
features from a 3D convolutional neural network with other data modalities,
including clinical and demographic information, to predict the future
trajectory of patients. Our model employs an adversarial loss to alleviate the
study-specific imaging bias, in particular the inter-study domain shifts. In
addition, a Sharpness-Aware Minimization (SAM) optimization technique is
applied to further improve model generalization. The proposed model is trained
and tested on various datasets in order to evaluate and validate the results.
Our results showed that 1) our model yields significant improvement over the
baseline models, and 2) models using extracted neuroimaging features from 3D
convolutional neural network outperform the same models when applied to
MRI-derived volumetric features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hashemifar_S/0/1/0/all/0/1"&gt;Somaye Hashemifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iriondo_C/0/1/0/all/0/1"&gt;Claudia Iriondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casey_E/0/1/0/all/0/1"&gt;Evan Casey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hejrat_M/0/1/0/all/0/1"&gt;Mohsen Hejrat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HybridNets: End-to-End Perception Network. (arXiv:2203.09035v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09035</id>
        <link href="http://arxiv.org/abs/2203.09035"/>
        <updated>2022-03-19T00:42:46.785Z</updated>
        <summary type="html"><![CDATA[End-to-end Network has become increasingly important in multi-tasking. One
prominent example of this is the growing significance of a driving perception
system in autonomous driving. This paper systematically studies an end-to-end
perception network for multi-tasking and proposes several key optimizations to
improve accuracy. First, the paper proposes efficient segmentation head and
box/class prediction networks based on weighted bidirectional feature network.
Second, the paper proposes automatically customized anchor for each level in
the weighted bidirectional feature network. Third, the paper proposes an
efficient training loss function and training strategy to balance and optimize
network. Based on these optimizations, we have developed an end-to-end
perception network to perform multi-tasking, including traffic object
detection, drivable area segmentation and lane detection simultaneously, called
HybridNets, which achieves better accuracy than prior art. In particular,
HybridNets achieves 77.3 mean Average Precision on Berkeley DeepDrive Dataset,
outperforms lane detection with 31.6 mean Intersection Over Union with 12.83
million parameters and 15.6 billion floating-point operations. In addition, it
can perform visual perception tasks in real-time and thus is a practical and
accurate solution to the multi-tasking problem. Code is available at
https://github.com/datvuthanh/HybridNets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1"&gt;Dat Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_B/0/1/0/all/0/1"&gt;Bao Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1"&gt;Hung Phan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Dimension for Deep Learning based on Hoeffding Inequality and Relative Evaluation. (arXiv:2203.09082v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09082</id>
        <link href="http://arxiv.org/abs/2203.09082"/>
        <updated>2022-03-19T00:42:46.777Z</updated>
        <summary type="html"><![CDATA[Research on the generalization ability of deep neural networks (DNNs) has
recently attracted a great deal of attention. However, due to their complex
architectures and large numbers of parameters, measuring the generalization
ability of specific DNN models remains an open challenge. In this paper, we
propose to use multiple factors to measure and rank the relative generalization
of DNNs based on a new concept of confidence dimension (CD). Furthermore, we
provide a feasible framework in our CD to theoretically calculate the upper
bound of generalization based on the conventional Vapnik-Chervonenk dimension
(VC-dimension) and Hoeffding's inequality. Experimental results on image
classification and object detection demonstrate that our CD can reflect the
relative generalization ability for different DNNs. In addition to
full-precision DNNs, we also analyze the generalization ability of binary
neural networks (BNNs), whose generalization ability remains an unsolved
problem. Our CD yields a consistent and reliable measure and ranking for both
full-precision DNNs and BNNs on all the tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Linlin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baochang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wentao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeMask: Semantically Masked Transformers for Semantic Segmentation. (arXiv:2112.12782v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.12782</id>
        <link href="http://arxiv.org/abs/2112.12782"/>
        <updated>2022-03-19T00:42:46.770Z</updated>
        <summary type="html"><![CDATA[Finetuning a pretrained backbone in the encoder part of an image transformer
network has been the traditional approach for the semantic segmentation task.
However, such an approach leaves out the semantic context that an image
provides during the encoding stage. This paper argues that incorporating
semantic information of the image into pretrained hierarchical
transformer-based backbones while finetuning improves the performance
considerably. To achieve this, we propose SeMask, a simple and effective
framework that incorporates semantic information into the encoder with the help
of a semantic attention operation. In addition, we use a lightweight semantic
decoder during training to provide supervision to the intermediate semantic
prior maps at every stage. Our experiments demonstrate that incorporating
semantic priors enhances the performance of the established hierarchical
encoders with a slight increase in the number of FLOPs. We provide empirical
proof by integrating SeMask into Swin Transformer and Mix Transformer backbones
as our encoder paired with different decoders. Our framework achieves a new
state-of-the-art of 58.22% mIoU on the ADE20K dataset and improvements of over
3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are
publicly available at
https://github.com/Picsart-AI-Research/SeMask-Segmentation .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1"&gt;Jitesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Anukriti Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orlov_N/0/1/0/all/0/1"&gt;Nikita Orlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zilong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1"&gt;Steven Walton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Non-Rigid 3D Registration. (arXiv:2203.07858v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2203.07858</id>
        <link href="http://arxiv.org/abs/2203.07858"/>
        <updated>2022-03-19T00:42:46.750Z</updated>
        <summary type="html"><![CDATA[Non-rigid registration computes an alignment between a source surface with a
target surface in a non-rigid manner. In the past decade, with the advances in
3D sensing technologies that can measure time-varying surfaces, non-rigid
registration has been applied for the acquisition of deformable shapes and has
a wide range of applications. This survey presents a comprehensive review of
non-rigid registration methods for 3D shapes, focusing on techniques related to
dynamic shape acquisition and reconstruction. In particular, we review
different approaches for representing the deformation field, and the methods
for computing the desired deformation. Both optimization-based and
learning-based methods are covered. We also review benchmarks and datasets for
evaluating non-rigid registration methods, and discuss potential future
research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1"&gt;Bailin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuxin Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyke_R/0/1/0/all/0/1"&gt;Roberto M. Dyke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Juyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.10904</id>
        <link href="http://arxiv.org/abs/2108.10904"/>
        <updated>2022-03-19T00:42:46.744Z</updated>
        <summary type="html"><![CDATA[With recent progress in joint modeling of visual and textual representations,
Vision-Language Pretraining (VLP) has achieved impressive performance on many
multimodal downstream tasks. However, the requirement for expensive annotations
including clean image captions and regional labels limits the scalability of
existing approaches, and complicates the pretraining procedure with the
introduction of multiple dataset-specific objectives. In this work, we relax
these constraints and present a minimalist pretraining framework, named Simple
Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training
complexity by exploiting large-scale weak supervision, and is trained
end-to-end with a single prefix language modeling objective. Without utilizing
extra data or task-specific customization, the resulting model significantly
outperforms previous pretraining methods and achieves new state-of-the-art
results on a wide range of discriminative and generative vision-language
benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE
(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).
Furthermore, we demonstrate that SimVLM acquires strong generalization and
transfer ability, enabling zero-shot behavior including open-ended visual
question answering and cross-modality transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zirui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiahui Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1"&gt;Adams Wei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuan Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Adversarial Robustness for Fractional Lp Threat Models. (arXiv:2203.08945v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08945</id>
        <link href="http://arxiv.org/abs/2203.08945"/>
        <updated>2022-03-19T00:42:46.719Z</updated>
        <summary type="html"><![CDATA[In recent years, researchers have extensively studied adversarial robustness
in a variety of threat models, including L_0, L_1, L_2, and L_infinity-norm
bounded adversarial attacks. However, attacks bounded by fractional L_p "norms"
(quasi-norms defined by the L_p distance with 0<p<1) have yet to be thoroughly
considered. We proactively propose a defense with several desirable properties:
it provides provable (certified) robustness, scales to ImageNet, and yields
deterministic (rather than high-probability) certified guarantees when applied
to quantized data (e.g., images). Our technique for fractional L_p robustness
constructs expressive, deep classifiers that are globally Lipschitz with
respect to the L_p^p metric, for any 0<p<1. However, our method is even more
general: we can construct classifiers which are globally Lipschitz with respect
to any metric defined as the sum of concave functions of components. Our
approach builds on a recent work, Levine and Feizi (2021), which provides a
provable defense against L_1 attacks. However, we demonstrate that our proposed
guarantees are highly non-vacuous, compared to the trivial solution of using
(Levine and Feizi, 2021) directly and applying norm inequalities. Code is
available at https://github.com/alevine0/fractionalLpRobustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1"&gt;Alexander Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed-Memory Sparse Kernels for Machine Learning. (arXiv:2203.07673v1 [cs.DC] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.07673</id>
        <link href="http://arxiv.org/abs/2203.07673"/>
        <updated>2022-03-19T00:42:46.678Z</updated>
        <summary type="html"><![CDATA[Sampled Dense Times Dense Matrix Multiplication (SDDMM) and Sparse Times
Dense Matrix Multiplication (SpMM) appear in diverse settings, such as
collaborative filtering, document clustering, and graph embedding. Frequently,
the SDDMM output becomes the input sparse matrix for a subsequent SpMM
operation. Existing work has focused on shared memory parallelization of these
primitives. While there has been extensive analysis of communication-minimizing
distributed 1.5D algorithms for SpMM, no such analysis exists for SDDMM or the
back-to-back sequence of SDDMM and SpMM, termed FusedMM. We show that
distributed memory 1.5D and 2.5D algorithms for SpMM can be converted to
algorithms for SDDMM with identical communication costs and input / output data
layouts. Further, we give two communication-eliding strategies to reduce costs
further for FusedMM kernels: either reusing the replication of an input dense
matrix for the SDDMM and SpMM in sequence, or fusing the local SDDMM and SpMM
kernels.

We benchmark FusedMM algorithms on Cori, a Cray XC40 at LBNL, using
Erdos-Renyi random matrices and large real-world sparse matrices. On 256 nodes
with 68 cores each, 1.5D FusedMM algorithms using either communication eliding
approach can save at least 30% of time spent exclusively in communication
compared to executing a distributed-memory SpMM and SDDMM kernel in sequence.
On real-world matrices with hundreds of millions of edges, all of our
algorithms exhibit at least a 10x speedup over the SpMM algorithm in PETSc. On
these matrices, our communication-eliding techniques exhibit runtimes up to 1.6
times faster than an unoptimized sequence of SDDMM and SpMM. We embed and test
the scaling of our algorithms in real-world applications, including
collaborative filtering via alternating-least-squares and inference for
attention-based graph neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bharadwaj_V/0/1/0/all/0/1"&gt;Vivek Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buluc_A/0/1/0/all/0/1"&gt;Aydin Bulu&amp;#xe7;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demmel_J/0/1/0/all/0/1"&gt;James Demmel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Constraints in First-Order Optimization: A View from Non-Smooth Dynamical Systems. (arXiv:2107.08225v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08225</id>
        <link href="http://arxiv.org/abs/2107.08225"/>
        <updated>2022-03-19T00:42:46.624Z</updated>
        <summary type="html"><![CDATA[We introduce a class of first-order methods for smooth constrained
optimization that are based on an analogy to non-smooth dynamical systems. Two
distinctive features of our approach are that (i) projections or optimizations
over the entire feasible set are avoided, in stark contrast to projected
gradient methods or the Frank-Wolfe method, and (ii) iterates are allowed to
become infeasible, which differs from active set or feasible direction methods,
where the descent motion stops as soon as a new constraint is encountered. The
resulting algorithmic procedure is simple to implement even when constraints
are nonlinear, and is suitable for large-scale constrained optimization
problems in which the feasible set fails to have a simple structure. The key
underlying idea is that constraints are expressed in terms of velocities
instead of positions, which has the algorithmic consequence that optimizations
over feasible sets at each iteration are replaced with optimizations over
local, sparse convex approximations. In particular, this means that at each
iteration only constraints that are violated are taken into account. The result
is a simplified suite of algorithms and an expanded range of possible
applications in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Muehlebach_M/0/1/0/all/0/1"&gt;Michael Muehlebach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intracranial Hemorrhage Detection Using Neural Network Based Methods With Federated Learning. (arXiv:2005.08644v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08644</id>
        <link href="http://arxiv.org/abs/2005.08644"/>
        <updated>2022-03-19T00:42:46.617Z</updated>
        <summary type="html"><![CDATA[Intracranial hemorrhage, bleeding that occurs inside the cranium, is a
serious health problem requiring rapid and often intensive medical treatment.
Such a condition is traditionally diagnosed by highly-trained specialists
analyzing computed tomography (CT) scan of the patient and identifying the
location and type of hemorrhage if one exists. We propose a neural network
approach to find and classify the condition based upon the CT scan. The model
architecture implements a time distributed convolutional network. We observed
accuracy above 92% from such an architecture, provided enough data. We propose
further extensions to our approach involving the deployment of federated
learning. This would be helpful in pooling learned parameters without violating
the inherent privacy of the data involved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_U/0/1/0/all/0/1"&gt;Utkarsh Chandra Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Anshuman Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1"&gt;Dr. K. Sree Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CYBORGS: Contrastively Bootstrapping Object Representations by Grounding in Segmentation. (arXiv:2203.09343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09343</id>
        <link href="http://arxiv.org/abs/2203.09343"/>
        <updated>2022-03-19T00:42:46.609Z</updated>
        <summary type="html"><![CDATA[Many recent approaches in contrastive learning have worked to close the gap
between pretraining on iconic images like ImageNet and pretraining on complex
scenes like COCO. This gap exists largely because commonly used random crop
augmentations obtain semantically inconsistent content in crowded scene images
of diverse objects. Previous works use preprocessing pipelines to localize
salient objects for improved cropping, but an end-to-end solution is still
elusive. In this work, we propose a framework which accomplishes this goal via
joint learning of representations and segmentation. We leverage segmentation
masks to train a model with a mask-dependent contrastive loss, and use the
partially trained model to bootstrap better masks. By iterating between these
two components, we ground the contrastive updates in segmentation information,
and simultaneously improve segmentation throughout pretraining. Experiments
show our representations transfer robustly to downstream tasks in
classification, detection and segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Example Perplexity. (arXiv:2203.08813v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08813</id>
        <link href="http://arxiv.org/abs/2203.08813"/>
        <updated>2022-03-19T00:42:46.602Z</updated>
        <summary type="html"><![CDATA[Some examples are easier for humans to classify than others. The same should
be true for deep neural networks (DNNs). We use the term example perplexity to
refer to the level of difficulty of classifying an example. In this paper, we
propose a method to measure the perplexity of an example and investigate what
factors contribute to high example perplexity. The related codes and resources
are available at https://github.com/vaynexie/Example-Perplexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Nevin L. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weiyan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1"&gt;Guanfang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao-Hui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1"&gt;Caleb Chen Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunpeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10199</id>
        <link href="http://arxiv.org/abs/2106.10199"/>
        <updated>2022-03-19T00:42:46.580Z</updated>
        <summary type="html"><![CDATA[We introduce BitFit, a sparse-finetuning method where only the bias-terms of
the model (or a subset of them) are being modified. We show that with
small-to-medium training data, applying BitFit on pre-trained BERT models is
competitive with (and sometimes better than) fine-tuning the entire model. For
larger data, the method is competitive with other sparse fine-tuning methods.
Besides their practical utility, these findings are relevant for the question
of understanding the commonly-used process of finetuning: they support the
hypothesis that finetuning is mainly about exposing knowledge induced by
language-modeling training, rather than learning new task-specific linguistic
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1"&gt;Elad Ben Zaken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation. (arXiv:2203.09516v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09516</id>
        <link href="http://arxiv.org/abs/2203.09516"/>
        <updated>2022-03-19T00:42:46.574Z</updated>
        <summary type="html"><![CDATA[Powerful priors allow us to perform inference with insufficient information.
In this paper, we propose an autoregressive prior for 3D shapes to solve
multimodal 3D tasks such as shape completion, reconstruction, and generation.
We model the distribution over 3D shapes as a non-sequential autoregressive
distribution over a discretized, low-dimensional, symbolic grid-like latent
representation of 3D shapes. This enables us to represent distributions over 3D
shapes conditioned on information from an arbitrary set of spatially anchored
query locations and thus perform shape completion in such arbitrary settings
(e.g., generating a complete chair given only a view of the back leg). We also
show that the learned autoregressive prior can be leveraged for conditional
tasks such as single-view reconstruction and language-based generation. This is
achieved by learning task-specific naive conditionals which can be approximated
by light-weight models trained on minimal paired data. We validate the
effectiveness of the proposed method using both quantitative and qualitative
evaluation and show that the proposed method outperforms the specialized
state-of-the-art methods trained for individual tasks. The project page with
code and video visualizations can be found at
https://yccyenchicheng.github.io/AutoSDF/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Paritosh Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yen-Chi Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Maneesh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1"&gt;Shubham Tulsiani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival. (arXiv:2203.09438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09438</id>
        <link href="http://arxiv.org/abs/2203.09438"/>
        <updated>2022-03-19T00:42:46.567Z</updated>
        <summary type="html"><![CDATA[To compare alternative taxi schedules and to compute them, as well as to
provide insights into an upcoming taxi trip to drivers and passengers, the
duration of a trip or its Estimated Time of Arrival (ETA) is predicted. To
reach a high prediction precision, machine learning models for ETA are state of
the art. One yet unexploited option to further increase prediction precision is
to combine multiple ETA models into an ensemble. While an increase of
prediction precision is likely, the main drawback is that the predictions made
by such an ensemble become less transparent due to the sophisticated ensemble
architecture. One option to remedy this drawback is to apply eXplainable
Artificial Intelligence (XAI). The contribution of this paper is three-fold.
First, we combine multiple machine learning models from our previous work for
ETA into a two-level ensemble model - a stacked ensemble model - which on its
own is novel; therefore, we can outperform previous state-of-the-art static
route-free ETA approaches. Second, we apply existing XAI methods to explain the
first- and second-level models of the ensemble. Third, we propose three joining
methods for combining the first-level explanations with the second-level ones.
Those joining methods enable us to explain stacked ensembles for regression
tasks. An experimental evaluation shows that the ETA models correctly learned
the importance of those input features driving the prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schleibaum_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schleibaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg P. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sester_M/0/1/0/all/0/1"&gt;Monika Sester&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2201.04584</id>
        <link href="http://arxiv.org/abs/2201.04584"/>
        <updated>2022-03-19T00:42:46.560Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of lung lesions associated with COVID-19 in CT images
requires large amount of annotated volumes. Annotations mandate expert
knowledge and are time-intensive to obtain through fully manual segmentation
methods. Additionally, lung lesions have large inter-patient variations, with
some pathologies having similar visual appearance as healthy lung tissues. This
poses a challenge when applying existing semi-automatic interactive
segmentation techniques for data labelling. To address these challenges, we
propose an efficient convolutional neural networks (CNNs) that can be learned
online while the annotator provides scribble-based interaction. To accelerate
learning from only the samples labelled through user-interactions, a
patch-based approach is used for training the network. Moreover, we use
weighted cross-entropy loss to address the class imbalance that may result from
user-interactions. During online inference, the learned network is applied to
the whole input volume using a fully convolutional approach. We compare our
proposed method with state-of-the-art using synthetic scribbles and show that
it outperforms existing methods on the task of annotating lung lesions
associated with COVID-19, achieving 16% higher Dice score while reducing
execution time by 3$\times$ and requiring 9000 lesser scribbles-based labelled
voxels. Due to the online learning aspect, our approach adapts quickly to user
input, resulting in high quality segmentation labels. Source code for ECONet is
available at: https://github.com/masadcv/ECONet-MONAILabel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Asad_M/0/1/0/all/0/1"&gt;Muhammad Asad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1"&gt;Lucas Fidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1"&gt;Tom Vercauteren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2203.09161</id>
        <link href="http://arxiv.org/abs/2203.09161"/>
        <updated>2022-03-19T00:42:46.554Z</updated>
        <summary type="html"><![CDATA[Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state of the art task
specific models. Conventional approaches to improve model performance via
creating large datasets with lots of task instances or architectural/training
changes in model may not be feasible for non-expert users. However, they can
write alternate instructions to represent an instruction task. Is
Instruction-augumentation helpful? We augment a subset of tasks in NATURAL
INSTRUCTIONS with additional instructions and find that these significantly
improve model performance (upto 35%) specially in low-data regime. Our results
indicate that an additional instruction can be equivalent to ~40 instances on
average across our evaluation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1"&gt;Ravsehaj Singh Puri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Swaroop Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1"&gt;Mihir Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1"&gt;Chitta Baral&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning-Based Long-Range Autonomous Valet Parking for Smart Cities. (arXiv:2109.11661v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.11661</id>
        <link href="http://arxiv.org/abs/2109.11661"/>
        <updated>2022-03-19T00:42:46.532Z</updated>
        <summary type="html"><![CDATA[In this paper, to reduce the congestion rate at the city center and increase
the quality of experience (QoE) of each user, the framework of long-range
autonomous valet parking (LAVP) is presented, where an Autonomous Vehicle (AV)
is deployed in the city, which can pick up, drop off users at their required
spots, and then drive to the car park out of city center autonomously. In this
framework, we aim to minimize the overall distance of the AV, while guarantee
all users are served, i.e., picking up, and dropping off users at their
required spots through optimizing the path planning of the AV and number of
serving time slots. To this end, we first propose a learning based algorithm,
which is named as Double-Layer Ant Colony Optimization (DL-ACO) algorithm to
solve the above problem in an iterative way. Then, to make the real-time
decision, while consider the dynamic environment (i.e., the AV may pick up and
drop off users from different locations), we further present a deep
reinforcement learning (DRL) based algorithm, which is known as deep Q network
(DQN). The experimental results show that the DL-ACO and DQN-based algorithms
both achieve the considerable performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khalid_M/0/1/0/all/0/1"&gt;Muhammad Khalid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Cunhua Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aslam_N/0/1/0/all/0/1"&gt;Nauman Aslam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yue Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in the Battle against Coronavirus (COVID-19): A Survey and Future Research Directions. (arXiv:2008.07343v4 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07343</id>
        <link href="http://arxiv.org/abs/2008.07343"/>
        <updated>2022-03-19T00:42:46.525Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) has been applied widely in our daily lives in a
variety of ways with numerous success stories. AI has also contributed to
dealing with the coronavirus disease (COVID-19) pandemic, which has been
happening around the globe. This paper presents a survey of AI methods being
used in various applications in the fight against the COVID-19 outbreak and
outlines the crucial role of AI research in this unprecedented battle. We touch
on areas where AI plays as an essential component, from medical image
processing, data analytics, text mining and natural language processing, the
Internet of Things, to computational biology and medicine. A summary of
COVID-19 related data sources that are available for research purposes is also
presented. Research directions on exploring the potential of AI and enhancing
its capability and power in the pandemic battle are thoroughly discussed. We
identify 13 groups of problems related to the COVID-19 pandemic and highlight
promising AI methods and tools that can be used to address these problems. It
is envisaged that this study will provide AI researchers and the wider
community with an overview of the current status of AI applications, and
motivate researchers to harness AI's potential in the fight against COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thanh Thi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quoc Viet Hung Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dung Tien Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Samuel Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eklund_P/0/1/0/all/0/1"&gt;Peter W. Eklund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_The_T/0/1/0/all/0/1"&gt;Thien Huynh-The&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thanh Tam Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1"&gt;Quoc-Viet Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1"&gt;Imran Razzak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_E/0/1/0/all/0/1"&gt;Edbert B. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phased Flight Trajectory Prediction with Deep Learning. (arXiv:2203.09033v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09033</id>
        <link href="http://arxiv.org/abs/2203.09033"/>
        <updated>2022-03-19T00:42:46.519Z</updated>
        <summary type="html"><![CDATA[The unprecedented increase of commercial airlines and private jets over the
next ten years presents a challenge for air traffic control. Precise flight
trajectory prediction is of great significance in air transportation
management, which contributes to the decision-making for safe and orderly
flights. Existing research and application mainly focus on the sequence
generation based on historical trajectories, while the aircraft-aircraft
interactions in crowded airspace especially the airspaces near busy airports
have been largely ignored. On the other hand, there are distinct
characteristics of aerodynamics for different flight phases, and the trajectory
may be affected by various uncertainties such as weather and advisories from
air traffic controllers. However, there is no literature fully considers all
these issues. Therefore, we proposed a phased flight trajectory prediction
framework. Multi-source and multi-modal datasets have been analyzed and mined
using variants of recurrent neural network (RNN) mixture. To be specific, we
first introduce spatio temporal graphs into the low-altitude airway prediction
problem, and the motion constraints of an aircraft are embedded to the
inference process for reliable forecasting results. In the en-route phase, the
dual attention mechanism is employed to adaptively extract much more important
features from overall datasets to learn the hidden patterns in dynamical
environments. The experimental results demonstrate our proposed framework can
outperform state-of-the-art methods for flight trajectory prediction for large
passenger/transport airplanes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bowen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering the building blocks of dark matter halo density profiles with neural networks. (arXiv:2203.08827v1 [astro-ph.CO])]]></title>
        <id>http://arxiv.org/abs/2203.08827</id>
        <link href="http://arxiv.org/abs/2203.08827"/>
        <updated>2022-03-19T00:42:46.511Z</updated>
        <summary type="html"><![CDATA[The density profiles of dark matter halos are typically modeled using
empirical formulae fitted to the density profiles of relaxed halo populations.
We present a neural network model that is trained to learn the mapping from the
raw density field containing each halo to the dark matter density profile. We
show that the model recovers the widely-used Navarro-Frenk-White (NFW) profile
out to the virial radius, and can additionally describe the variability in the
outer profile of the halos. The neural network architecture consists of a
supervised encoder-decoder framework, which first compresses the density inputs
into a low-dimensional latent representation, and then outputs $\rho(r)$ for
any desired value of radius $r$. The latent representation contains all the
information used by the model to predict the density profiles. This allows us
to interpret the latent representation by quantifying the mutual information
between the representation and the halos' ground-truth density profiles. A
two-dimensional representation is sufficient to accurately model the density
profiles up to the virial radius; however, a three-dimensional representation
is required to describe the outer profiles beyond the virial radius. The
additional dimension in the representation contains information about the
infalling material in the outer profiles of dark matter halos, thus discovering
the splashback boundary of halos without prior knowledge of the halos'
dynamical history.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lucie_Smith_L/0/1/0/all/0/1"&gt;Luisa Lucie-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Peiris_H/0/1/0/all/0/1"&gt;Hiranya V. Peiris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pontzen_A/0/1/0/all/0/1"&gt;Andrew Pontzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Nord_B/0/1/0/all/0/1"&gt;Brian Nord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Thiyagalingam_J/0/1/0/all/0/1"&gt;Jeyan Thiyagalingam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Piras_D/0/1/0/all/0/1"&gt;Davide Piras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Clustering and Matrix Completion for the Reconstruction of World Input-Output Tables. (arXiv:2203.08819v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.08819</id>
        <link href="http://arxiv.org/abs/2203.08819"/>
        <updated>2022-03-19T00:42:46.490Z</updated>
        <summary type="html"><![CDATA[World Input-Output (I/O) matrices provide the networks of within- and
cross-country economic relations. In the context of I/O analysis, the
methodology adopted by national statistical offices in data collection raises
the issue of obtaining reliable data in a timely fashion and it makes the
reconstruction of (part of) the I/O matrices of particular interest. In this
work, we propose a method combining hierarchical clustering and Matrix
Completion (MC) with a LASSO-like nuclear norm penalty, to impute missing
entries of a partially unknown I/O matrix. Through simulations based on
synthetic matrices we study the effectiveness of the proposed method to predict
missing values from both previous years data and current data related to
countries similar to the one for which current data are obscured. To show the
usefulness of our method, an application based on World Input-Output Database
(WIOD) tables - which are an example of industry-by-industry I/O tables - is
provided. Strong similarities in structure between WIOD and other I/O tables
are also found, which make the proposed approach easily generalizable to them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Metulini_R/0/1/0/all/0/1"&gt;Rodolfo Metulini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gnecco_G/0/1/0/all/0/1"&gt;Giorgio Gnecco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Biancalani_F/0/1/0/all/0/1"&gt;Francesco Biancalani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Riccaboni_M/0/1/0/all/0/1"&gt;Massimo Riccaboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stochastic Halpern Iteration with Variance Reduction for Stochastic Monotone Inclusion Problems. (arXiv:2203.09436v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2203.09436</id>
        <link href="http://arxiv.org/abs/2203.09436"/>
        <updated>2022-03-19T00:42:46.484Z</updated>
        <summary type="html"><![CDATA[We study stochastic monotone inclusion problems, which widely appear in
machine learning applications, including robust regression and adversarial
learning. We propose novel variants of stochastic Halpern iteration with
recursive variance reduction. In the cocoercive -- and more generally
Lipschitz-monotone -- setup, our algorithm attains $\epsilon$ norm of the
operator with $\mathcal{O}(\frac{1}{\epsilon^3})$ stochastic operator
evaluations, which significantly improves over state of the art
$\mathcal{O}(\frac{1}{\epsilon^4})$ stochastic operator evaluations required
for existing monotone inclusion solvers applied to the same problem classes. We
further show how to couple one of the proposed variants of stochastic Halpern
iteration with a scheduled restart scheme to solve stochastic monotone
inclusion problems with ${\mathcal{O}}(\frac{\log(1/\epsilon)}{\epsilon^2})$
stochastic operator evaluations under additional sharpness or strong
monotonicity assumptions. Finally, we argue via reductions between different
problem classes that our stochastic oracle complexity bounds are tight up to
logarithmic factors in terms of their $\epsilon$-dependence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xufeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Song_C/0/1/0/all/0/1"&gt;Chaobing Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Guzman_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diakonikolas_J/0/1/0/all/0/1"&gt;Jelena Diakonikolas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent-Variable Advantage-Weighted Policy Optimization for Offline RL. (arXiv:2203.08949v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08949</id>
        <link href="http://arxiv.org/abs/2203.08949"/>
        <updated>2022-03-19T00:42:46.477Z</updated>
        <summary type="html"><![CDATA[Offline reinforcement learning methods hold the promise of learning policies
from pre-collected datasets without the need to query the environment for new
transitions. This setting is particularly well-suited for continuous control
robotic applications for which online data collection based on trial-and-error
is costly and potentially unsafe. In practice, offline datasets are often
heterogeneous, i.e., collected in a variety of scenarios, such as data from
several human demonstrators or from policies that act with different purposes.
Unfortunately, such datasets can exacerbate the distribution shift between the
behavior policy underlying the data and the optimal policy to be learned,
leading to poor performance. To address this challenge, we propose to leverage
latent-variable policies that can represent a broader class of policy
distributions, leading to better adherence to the training data distribution
while maximizing reward via a policy over the latent variable. As we
empirically show on a range of simulated locomotion, navigation, and
manipulation tasks, our method referred to as latent-variable
advantage-weighted policy optimization (LAPO), improves the average performance
of the next best-performing offline reinforcement learning methods by 49% on
heterogeneous datasets, and by 8% on datasets with narrow and biased
distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghadirzadeh_A/0/1/0/all/0/1"&gt;Ali Ghadirzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tianhe Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yuan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1"&gt;Bin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling. (arXiv:2203.08810v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2203.08810</id>
        <link href="http://arxiv.org/abs/2203.08810"/>
        <updated>2022-03-19T00:42:46.470Z</updated>
        <summary type="html"><![CDATA[Speech Emotion Recognition (SER) application is frequently associated with
privacy concerns as it often acquires and transmits speech data at the
client-side to remote cloud platforms for further processing. These speech data
can reveal not only speech content and affective information but the speaker's
identity, demographic traits, and health status. Federated learning (FL) is a
distributed machine learning algorithm that coordinates clients to train a
model collaboratively without sharing local data. This algorithm shows enormous
potential for SER applications as sharing raw speech or speech features from a
user's device is vulnerable to privacy attacks. However, a major challenge in
FL is limited availability of high-quality labeled data samples. In this work,
we propose a semi-supervised federated learning framework, Semi-FedSER, that
utilizes both labeled and unlabeled data samples to address the challenge of
limited labeled data samples in FL. We show that our Semi-FedSER can generate
desired SER performance even when the local label rate l=20 using two SER
benchmark datasets: IEMOCAP and MSP-Improv.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_T/0/1/0/all/0/1"&gt;Tiantian Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shrikanth Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Deep Learning to Enhance Breast Cancer Detection on Screening Mammography. (arXiv:2203.08812v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.08812</id>
        <link href="http://arxiv.org/abs/2203.08812"/>
        <updated>2022-03-19T00:42:46.463Z</updated>
        <summary type="html"><![CDATA[A major limitation in applying deep learning to artificial intelligence (AI)
systems is the scarcity of high-quality curated datasets. We investigate strong
augmentation based self-supervised learning (SSL) techniques to address this
problem. Using breast cancer detection as an example, we first identify a
mammogram-specific transformation paradigm and then systematically compare four
recent SSL methods representing a diversity of approaches. We develop a method
to convert a pretrained model from making predictions on uniformly tiled
patches to whole images, and an attention-based pooling method that improves
the classification performance. We found that the best SSL model substantially
outperformed the baseline supervised model. The best SSL model also improved
the data efficiency of sample labeling by nearly 4-fold and was highly
transferrable from one dataset to another. SSL represents a major breakthrough
in computer vision and may help the AI for medical imaging field to shift away
from supervised learning and dependency on scarce labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Miller_J/0/1/0/all/0/1"&gt;John D. Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arasu_V/0/1/0/all/0/1"&gt;Vignesh A. Arasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pu_A/0/1/0/all/0/1"&gt;Albert X. Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Margolies_L/0/1/0/all/0/1"&gt;Laurie R. Margolies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sieh_W/0/1/0/all/0/1"&gt;Weiva Sieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electronic excited states in deep variational Monte Carlo. (arXiv:2203.09472v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2203.09472</id>
        <link href="http://arxiv.org/abs/2203.09472"/>
        <updated>2022-03-19T00:42:46.442Z</updated>
        <summary type="html"><![CDATA[Obtaining accurate ground and low-lying excited states of electronic systems
is crucial in a multitude of important applications. One ab initio method for
solving the electronic Schr\"odinger equation that scales favorably for large
systems and whose accuracy is limited only by the choice of wavefunction ansatz
employed is variational quantum Monte Carlo (QMC). The recently introduced deep
QMC approach, using a new class of ansatzes represented by deep neural
networks, has been shown to generate nearly exact ground-state solutions for
molecules containing up to a few dozen electrons, with the potential to scale
to much larger systems where other highly accurate methods are not feasible. In
this paper, we advance one such ansatz (PauliNet) to compute electronic excited
states through a simple variational procedure. We demonstrate our method on a
variety of small atoms and molecules where we consistently achieve high
accuracy for low-lying states. To highlight the method's potential for larger
systems, we show that for the benzene molecule, PauliNet is on par with
significantly more expensive high-level electronic structure methods in terms
of the excitation energy and outperforms them in terms of absolute energies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Entwistle_M/0/1/0/all/0/1"&gt;Mike Entwistle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schatzle_Z/0/1/0/all/0/1"&gt;Zeno Sch&amp;#xe4;tzle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Erdman_P/0/1/0/all/0/1"&gt;Paolo A. Erdman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hermann_J/0/1/0/all/0/1"&gt;Jan Hermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Noe_F/0/1/0/all/0/1"&gt;Frank No&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks. (arXiv:2203.09168v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09168</id>
        <link href="http://arxiv.org/abs/2203.09168"/>
        <updated>2022-03-19T00:42:46.435Z</updated>
        <summary type="html"><![CDATA[Capturing aleatoric uncertainty is a critical part of many machine learning
systems. In deep learning, a common approach to this end is to train a neural
network to estimate the parameters of a heteroscedastic Gaussian distribution
by maximizing the logarithm of the likelihood function under the observed data.
In this work, we examine this approach and identify potential hazards
associated with the use of log-likelihood in conjunction with gradient-based
optimizers. First, we present a synthetic example illustrating how this
approach can lead to very poor but stable parameter estimates. Second, we
identify the culprit to be the log-likelihood loss, along with certain
conditions that exacerbate the issue. Third, we present an alternative
formulation, termed $\beta$-NLL, in which each data point's contribution to the
loss is weighted by the $\beta$-exponentiated variance estimate. We show that
using an appropriate $\beta$ largely mitigates the issue in our illustrative
example. Fourth, we evaluate this approach on a range of domains and tasks and
show that it achieves considerable improvements and performs more robustly
concerning hyperparameters, both in predictive RMSE and log-likelihood
criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seitzer_M/0/1/0/all/0/1"&gt;Maximilian Seitzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_A/0/1/0/all/0/1"&gt;Arash Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antic_D/0/1/0/all/0/1"&gt;Dimitrije Antic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1"&gt;Georg Martius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural network processing of holographic images. (arXiv:2203.08898v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.08898</id>
        <link href="http://arxiv.org/abs/2203.08898"/>
        <updated>2022-03-19T00:42:46.428Z</updated>
        <summary type="html"><![CDATA[HOLODEC, an airborne cloud particle imager, captures holographic images of a
fixed volume of cloud to characterize the types and sizes of cloud particles,
such as water droplets and ice crystals. Cloud particle properties include
position, diameter, and shape. We present a hologram processing algorithm,
HolodecML, that utilizes a neural segmentation model, GPUs, and computational
parallelization. HolodecML is trained using synthetically generated holograms
based on a model of the instrument, and predicts masks around particles found
within reconstructed images. From these masks, the position and size of the
detected particles can be characterized in three dimensions. In order to
successfully process real holograms, we find we must apply a series of image
corrupting transformations and noise to the synthetic images used in training.

In this evaluation, HolodecML had comparable position and size estimation
performance to the standard processing method, but improved particle detection
by nearly 20\% on several thousand manually labeled HOLODEC images. However,
the improvement only occurred when image corruption was performed on the
simulated images during training, thereby mimicking non-ideal conditions in the
actual probe. The trained model also learned to differentiate artifacts and
other impurities in the HOLODEC images from the particles, even though no such
objects were present in the training data set, while the standard processing
method struggled to separate particles from artifacts. The novelty of the
training approach, which leveraged noise as a means for parameterizing
non-ideal aspects of the HOLODEC detector, could be applied in other domains
where the theoretical model is incapable of fully describing the real-world
operation of the instrument and accurate truth data required for supervised
learning cannot be obtained from real-world observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schreck_J/0/1/0/all/0/1"&gt;John S. Schreck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gantos_G/0/1/0/all/0/1"&gt;Gabrielle Gantos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hayman_M/0/1/0/all/0/1"&gt;Matthew Hayman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bensemer_A/0/1/0/all/0/1"&gt;Aaron Bensemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gagne_D/0/1/0/all/0/1"&gt;David John Gagne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QUBOs for Sorting Lists and Building Trees. (arXiv:2203.08815v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2203.08815</id>
        <link href="http://arxiv.org/abs/2203.08815"/>
        <updated>2022-03-19T00:42:46.420Z</updated>
        <summary type="html"><![CDATA[We show that the fundamental tasks of sorting lists and building search trees
or heaps can be modeled as quadratic unconstrained binary optimization problems
(QUBOs). The idea is to understand these tasks as permutation problems and to
devise QUBOs whose solutions represent appropriate permutation matrices. We
discuss how to construct such QUBOs and how to solve them using Hopfield nets
or adiabatic) quantum computing. In short, we show that neurocomputing methods
or quantum computers can solve problems usually associated with abstract data
structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1"&gt;Christian Bauckhage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerlach_T/0/1/0/all/0/1"&gt;Thore Gerlach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piatkowski_N/0/1/0/all/0/1"&gt;Nico Piatkowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Multi-Agent Reinforcement Learning with Communication. (arXiv:2203.08975v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2203.08975</id>
        <link href="http://arxiv.org/abs/2203.08975"/>
        <updated>2022-03-19T00:42:46.392Z</updated>
        <summary type="html"><![CDATA[Communication is an effective mechanism for coordinating the behavior of
multiple agents. In the field of multi-agent reinforcement learning, agents can
improve the overall learning performance and achieve their objectives by
communication. Moreover, agents can communicate various types of messages,
either to all agents or to specific agent groups, and through specific
channels. With the growing body of research work in MARL with communication
(Comm-MARL), there is lack of a systematic and structural approach to
distinguish and classify existing Comm-MARL systems. In this paper, we survey
recent works in the Comm-MARL field and consider various aspects of
communication that can play a role in the design and development of multi-agent
reinforcement learning systems. With these aspects in mind, we propose several
dimensions along which Comm-MARL systems can be analyzed, developed, and
compared.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Changxi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dastani_M/0/1/0/all/0/1"&gt;Mehdi Dastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shihan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Dependency, Data Flow, and Competitive Advantage. (arXiv:2203.09128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09128</id>
        <link href="http://arxiv.org/abs/2203.09128"/>
        <updated>2022-03-19T00:42:46.385Z</updated>
        <summary type="html"><![CDATA[Data is fundamental to machine learning-based products and services and is
considered strategic due to its externalities for businesses, governments,
non-profits, and more generally for society. It is renowned that the value of
organizations (businesses, government agencies and programs, and even
industries) scales with the volume of available data. What is often less
appreciated is that the data value in making useful organizational predictions
will range widely and is prominently a function of data characteristics and
underlying algorithms.

In this research, our goal is to study how the value of data changes over
time and how this change varies across contexts and business areas (e.g. next
word prediction in the context of history, sports, politics). We focus on data
from Reddit.com and compare the value's time-dependency across various Reddit
topics (Subreddits). We make this comparison by measuring the rate at which
user-generated text data loses its relevance to the algorithmic prediction of
conversations. We show that different subreddits have different rates of
relevance decline over time.

Relating the text topics to various business areas of interest, we argue that
competing in a business area in which data value decays rapidly alters
strategies to acquire competitive advantage. When data value decays rapidly,
access to a continuous flow of data will be more valuable than access to a
fixed stock of data. In this kind of setting, improving user engagement and
increasing user-base help creating and maintaining a competitive advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valavi_E/0/1/0/all/0/1"&gt;Ehsan Valavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1"&gt;Joel Hestness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iansiti_M/0/1/0/all/0/1"&gt;Marco Iansiti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1"&gt;Newsha Ardalani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Feng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakhani_K/0/1/0/all/0/1"&gt;Karim R. Lakhani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09116</id>
        <link href="http://arxiv.org/abs/2203.09116"/>
        <updated>2022-03-19T00:42:46.379Z</updated>
        <summary type="html"><![CDATA[This paper presents a motion data augmentation scheme incorporating motion
synthesis encouraging diversity and motion correction imposing physical
plausibility. This motion synthesis consists of our modified Variational
AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed
sampling-near-samples method generates various valid motions even with
insufficient training motion data. Our IK-based motion synthesis method allows
us to generate a variety of motions semi-automatically. Since these two schemes
generate unrealistic artifacts in the synthesized motions, our motion
correction rectifies them. This motion correction scheme consists of imitation
learning with physics simulation and subsequent motion debiasing. For this
imitation learning, we propose the PD-residual force that significantly
accelerates the training process. Furthermore, our motion debiasing
successfully offsets the motion bias induced by imitation learning to maximize
the effect of augmentation. As a result, our method outperforms previous
noise-based motion augmentation methods by a large margin on both Recurrent
Neural Network-based and Graph Convolutional Network-based human motion
prediction models. The code is available at {\rm
\url{https://github.com/meaten/MotionAug}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maeda_T/0/1/0/all/0/1"&gt;Takahiro Maeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1"&gt;Norimichi Ukita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DePS: An improved deep learning model for de novo peptide sequencing. (arXiv:2203.08820v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2203.08820</id>
        <link href="http://arxiv.org/abs/2203.08820"/>
        <updated>2022-03-19T00:42:46.372Z</updated>
        <summary type="html"><![CDATA[De novo peptide sequencing from mass spectrometry data is an important method
for protein identification. Recently, various deep learning approaches were
applied for de novo peptide sequencing and DeepNovoV2 is one of the
represetative models. In this study, we proposed an enhanced model, DePS, which
can improve the accuracy of de novo peptide sequencing even with missing signal
peaks or large number of noisy peaks in tandem mass spectrometry data. It is
showed that, for the same test set of DeepNovoV2, the DePS model achieved
excellent results of 74.22%, 74.21% and 41.68% for amino acid recall, amino
acid precision and peptide recall respectively. Furthermore, the results
suggested that DePS outperforms DeepNovoV2 on the cross species dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ge_C/0/1/0/all/0/1"&gt;Cheng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qu_J/0/1/0/all/0/1"&gt;Jia Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xie_L/0/1/0/all/0/1"&gt;Liangxu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kong_R/0/1/0/all/0/1"&gt;Ren Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shan Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Informed Neural Networks with Adaptive Localized Artificial Viscosity. (arXiv:2203.08802v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2203.08802</id>
        <link href="http://arxiv.org/abs/2203.08802"/>
        <updated>2022-03-19T00:42:46.352Z</updated>
        <summary type="html"><![CDATA[Physics-informed Neural Network (PINN) is a promising tool that has been
applied in a variety of physical phenomena described by partial differential
equations (PDE). However, it has been observed that PINNs are difficult to
train in certain "stiff" problems, which include various nonlinear hyperbolic
PDEs that display shocks in their solutions. Recent studies added a diffusion
term to the PDE, and an artificial viscosity (AV) value was manually tuned to
allow PINNs to solve these problems. In this paper, we propose three approaches
to address this problem, none of which rely on an a priori definition of the
artificial viscosity value. The first method learns a global AV value, whereas
the other two learn localized AV values around the shocks, by means of a
parametrized AV map or a residual-based AV map. We applied the proposed methods
to the inviscid Burgers equation and the Buckley-Leverett equation, the latter
being a classical problem in Petroleum Engineering. The results show that the
proposed methods are able to learn both a small AV value and the accurate shock
location and improve the approximation error over a nonadaptive global AV
alternative method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Coutinho_E/0/1/0/all/0/1"&gt;E.J.R. Coutinho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+DallAqua_M/0/1/0/all/0/1"&gt;M. Dall&amp;#x27;Aqua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+McClenny_L/0/1/0/all/0/1"&gt;L. McClenny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhong_M/0/1/0/all/0/1"&gt;M. Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Braga_Neto_U/0/1/0/all/0/1"&gt;U. Braga-Neto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gildin_E/0/1/0/all/0/1"&gt;E. Gildin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding robustness and generalization of artificial neural networks through Fourier masks. (arXiv:2203.08822v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.08822</id>
        <link href="http://arxiv.org/abs/2203.08822"/>
        <updated>2022-03-19T00:42:46.344Z</updated>
        <summary type="html"><![CDATA[Despite the enormous success of artificial neural networks (ANNs) in many
disciplines, the characterization of their computations and the origin of key
properties such as generalization and robustness remain open questions. Recent
literature suggests that robust networks with good generalization properties
tend to be biased towards processing low frequencies in images. To explore the
frequency bias hypothesis further, we develop an algorithm that allows us to
learn modulatory masks highlighting the essential input frequencies needed for
preserving a trained network's performance. We achieve this by imposing
invariance in the loss with respect to such modulations in the input
frequencies. We first use our method to test the low-frequency preference
hypothesis of adversarially trained or data-augmented networks. Our results
suggest that adversarially robust networks indeed exhibit a low-frequency bias
but we find this bias is also dependent on directions in frequency space.
However, this is not necessarily true for other types of data augmentation. Our
results also indicate that the essential frequencies in question are
effectively the ones used to achieve generalization in the first place.
Surprisingly, images seen through these modulatory masks are not recognizable
and resemble texture-like patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karantzas_N/0/1/0/all/0/1"&gt;Nikos Karantzas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besier_E/0/1/0/all/0/1"&gt;Emma Besier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caro_J/0/1/0/all/0/1"&gt;Josue Ortega Caro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1"&gt;Xaq Pitkow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1"&gt;Andreas S. Tolias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1"&gt;Ankit B. Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anselmi_F/0/1/0/all/0/1"&gt;Fabio Anselmi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Redundancy and Diversity in Cell-based Neural Architecture Search. (arXiv:2203.08887v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.08887</id>
        <link href="http://arxiv.org/abs/2203.08887"/>
        <updated>2022-03-19T00:42:46.193Z</updated>
        <summary type="html"><![CDATA[Searching for the architecture cells is a dominant paradigm in NAS. However,
little attention has been devoted to the analysis of the cell-based search
spaces even though it is highly important for the continual development of NAS.
In this work, we conduct an empirical post-hoc analysis of architectures from
the popular cell-based search spaces and find that the existing search spaces
contain a high degree of redundancy: the architecture performance is minimally
sensitive to changes at large parts of the cells, and universally adopted
designs, like the explicit search for a reduction cell, significantly increase
the complexities but have very limited impact on the performance. Across
architectures found by a diverse set of search strategies, we consistently find
that the parts of the cells that do matter for architecture performance often
follow similar and simple patterns. By explicitly constraining cells to include
these patterns, randomly sampled architectures can match or even outperform the
state of the art. These findings cast doubts into our ability to discover truly
novel architectures in the existing cell-based search spaces, and inspire our
suggestions for improvement to guide future NAS research. Code is available at
https://github.com/xingchenwan/cell-based-NAS-analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xingchen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1"&gt;Binxin Ru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Esperanca_P/0/1/0/all/0/1"&gt;Pedro M. Esperan&amp;#xe7;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural-Network-Directed Genetic Programmer for Discovery of Governing Equations. (arXiv:2203.08808v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2203.08808</id>
        <link href="http://arxiv.org/abs/2203.08808"/>
        <updated>2022-03-19T00:42:46.186Z</updated>
        <summary type="html"><![CDATA[We develop a symbolic regression framework for extracting the governing
mathematical expressions from observed data. The evolutionary approach, faiGP,
is designed to leverage the properties of a function algebra that have been
encoded into a grammar, providing a theoretical guarantee of universal
approximation and a way to minimize bloat. In this framework, the choice of
operators of the grammar may be informed by a physical theory or symmetry
considerations. Since there is currently no theory that can derive the
'constants of nature', an empirical investigation on extracting these
coefficients from an evolutionary process is of methodological interest. We
quantify the impact of different types of regularizers, including a diversity
metric adapted from studies of the transcriptome and a complexity measure, on
the performance of the framework. Our implementation, which leverages neural
networks and a genetic programmer, generates non-trivial symbolically
equivalent expressions ("Ramanujan expressions") or approximations with
potentially interesting numerical applications. To illustrate the framework, a
model of ligand-receptor binding kinetics, including an account of gene
regulation by transcription factors, and a model of the regulatory range of the
cistrome from omics data are presented. This study has important implications
on the development of data-driven methodologies for the discovery of governing
equations in experimental data derived from new sensing systems and
high-throughput screening technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1"&gt;Shahab Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gamazon_E/0/1/0/all/0/1"&gt;Eric R. Gamazon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival. (arXiv:2203.09438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09438</id>
        <link href="http://arxiv.org/abs/2203.09438"/>
        <updated>2022-03-19T00:42:46.179Z</updated>
        <summary type="html"><![CDATA[To compare alternative taxi schedules and to compute them, as well as to
provide insights into an upcoming taxi trip to drivers and passengers, the
duration of a trip or its Estimated Time of Arrival (ETA) is predicted. To
reach a high prediction precision, machine learning models for ETA are state of
the art. One yet unexploited option to further increase prediction precision is
to combine multiple ETA models into an ensemble. While an increase of
prediction precision is likely, the main drawback is that the predictions made
by such an ensemble become less transparent due to the sophisticated ensemble
architecture. One option to remedy this drawback is to apply eXplainable
Artificial Intelligence (XAI). The contribution of this paper is three-fold.
First, we combine multiple machine learning models from our previous work for
ETA into a two-level ensemble model - a stacked ensemble model - which on its
own is novel; therefore, we can outperform previous state-of-the-art static
route-free ETA approaches. Second, we apply existing XAI methods to explain the
first- and second-level models of the ensemble. Third, we propose three joining
methods for combining the first-level explanations with the second-level ones.
Those joining methods enable us to explain stacked ensembles for regression
tasks. An experimental evaluation shows that the ETA models correctly learned
the importance of those input features driving the prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schleibaum_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schleibaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg P. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sester_M/0/1/0/all/0/1"&gt;Monika Sester&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity Estimation for Dark Matter Subhalos in Synthetic Gaia DR2 using Deep Learning. (arXiv:2203.08161v1 [astro-ph.GA] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.08161</id>
        <link href="http://arxiv.org/abs/2203.08161"/>
        <updated>2022-03-19T00:42:46.172Z</updated>
        <summary type="html"><![CDATA[The abundance of dark matter subhalos orbiting a host galaxy is a generic
prediction of the cosmological framework. It is a promising way to constrain
the nature of dark matter. Here we describe the challenges of detecting stars
whose phase-space distribution may be perturbed by the passage of dark matter
subhalos using a machine learning approach. The training data are three Milky
Way-like galaxies and nine synthetic Gaia DR2 surveys derived from these. We
first quantify the magnitude of the perturbations in the simulated galaxies
using an anomaly detection algorithm. We also estimate the feasibility of this
approach in the Gaia DR2-like catalogues by comparing the anomaly detection
based approach with a supervised classification. We find that a classification
algorithm optimized on about half a billion synthetic star observables exhibits
mild but nonzero sensitivity. This classification-based approach is not
sufficiently sensitive to pinpoint the exact locations of subhalos in the
simulation, as would be expected from the very limited number of subhalos in
the detectable region. The enormous size of the Gaia dataset motivates the
further development of scalable and accurate computational methods that could
be used to select potential regions of interest for dark matter searches to
ultimately constrain the Milky Way's subhalo mass function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bazarov_A/0/1/0/all/0/1"&gt;Abdullah Bazarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Benito_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a Benito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Hutsi_G/0/1/0/all/0/1"&gt;Gert H&amp;#xfc;tsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kipper_R/0/1/0/all/0/1"&gt;Rain Kipper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pata_J/0/1/0/all/0/1"&gt;Joosep Pata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Poder_S/0/1/0/all/0/1"&gt;Sven P&amp;#xf5;der&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy Tensor Completion via Low-rank Tensor Ring. (arXiv:2203.08857v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.08857</id>
        <link href="http://arxiv.org/abs/2203.08857"/>
        <updated>2022-03-19T00:42:46.165Z</updated>
        <summary type="html"><![CDATA[Tensor completion is a fundamental tool for incomplete data analysis, where
the goal is to predict missing entries from partial observations. However,
existing methods often make the explicit or implicit assumption that the
observed entries are noise-free to provide a theoretical guarantee of exact
recovery of missing entries, which is quite restrictive in practice. To remedy
such drawbacks, this paper proposes a novel noisy tensor completion model,
which complements the incompetence of existing works in handling the
degeneration of high-order and noisy observations. Specifically, the tensor
ring nuclear norm (TRNN) and least-squares estimator are adopted to regularize
the underlying tensor and the observed entries, respectively. In addition, a
non-asymptotic upper bound of estimation error is provided to depict the
statistical performance of the proposed estimator. Two efficient algorithms are
developed to solve the optimization problem with convergence guarantee, one of
which is specially tailored to handle large-scale tensors by replacing the
minimization of TRNN of the original tensor equivalently with that of a much
smaller one in a heterogeneous tensor decomposition framework. Experimental
results on both synthetic and real-world data demonstrate the effectiveness and
efficiency of the proposed model in recovering noisy incomplete tensor data
compared with state-of-the-art tensor completion models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yuning Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guoxu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qibin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shengli Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New directions for surrogate models and differentiable programming for High Energy Physics detector simulation. (arXiv:2203.08806v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2203.08806</id>
        <link href="http://arxiv.org/abs/2203.08806"/>
        <updated>2022-03-19T00:42:46.146Z</updated>
        <summary type="html"><![CDATA[The computational cost for high energy physics detector simulation in future
experimental facilities is going to exceed the current available resources. To
overcome this challenge, new ideas on surrogate models using machine learning
methods are being explored to replace computationally expensive components.
Additionally, differentiable programming has been proposed as a complementary
approach, providing controllable and scalable simulation routines. In this
document, new and ongoing efforts for surrogate models and differential
programming applied to detector simulation are discussed in the context of the
2021 Particle Physics Community Planning Exercise (`Snowmass').]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Adelmann_A/0/1/0/all/0/1"&gt;Andreas Adelmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Hopkins_W/0/1/0/all/0/1"&gt;Walter Hopkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Kourlitis_E/0/1/0/all/0/1"&gt;Evangelos Kourlitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Kagan_M/0/1/0/all/0/1"&gt;Michael Kagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Kasieczka_G/0/1/0/all/0/1"&gt;Gregor Kasieczka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Krause_C/0/1/0/all/0/1"&gt;Claudius Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Shih_D/0/1/0/all/0/1"&gt;David Shih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Mikuni_V/0/1/0/all/0/1"&gt;Vinicius Mikuni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Nachman_B/0/1/0/all/0/1"&gt;Benjamin Nachman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Pedro_K/0/1/0/all/0/1"&gt;Kevin Pedro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Winklehner_D/0/1/0/all/0/1"&gt;Daniel Winklehner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling Instance-Dependent Label Noise via a Universal Probabilistic Model. (arXiv:2101.05467v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05467</id>
        <link href="http://arxiv.org/abs/2101.05467"/>
        <updated>2022-03-19T00:42:46.139Z</updated>
        <summary type="html"><![CDATA[The drastic increase of data quantity often brings the severe decrease of
data quality, such as incorrect label annotations, which poses a great
challenge for robustly training Deep Neural Networks (DNNs). Existing learning
\mbox{methods} with label noise either employ ad-hoc heuristics or restrict to
specific noise assumptions. However, more general situations, such as
instance-dependent label noise, have not been fully explored, as scarce studies
focus on their label corruption process. By categorizing instances into
confusing and unconfusing instances, this paper proposes a simple yet universal
probabilistic model, which explicitly relates noisy labels to their instances.
The resultant model can be realized by DNNs, where the training procedure is
accomplished by employing an alternating optimization algorithm. Experiments on
datasets with both synthetic and real-world label noise verify that the
proposed method yields significant improvements on robustness over
state-of-the-art counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09347</id>
        <link href="http://arxiv.org/abs/2203.09347"/>
        <updated>2022-03-19T00:42:46.073Z</updated>
        <summary type="html"><![CDATA[In a high-dimensional regression framework, we study consequences of the
naive two-step procedure where first the dimension of the input variables is
reduced and second, the reduced input variables are used to predict the output
variable. More specifically we combine principal component analysis (PCA) with
kernel regression. In order to analyze the resulting regression errors, a novel
stability result of kernel regression with respect to the Wasserstein distance
is derived. This allows us to bound errors that occur when perturbed input data
is used to fit a kernel function. We combine the stability result with known
estimates from the literature on both principal component analysis and kernel
regression to obtain convergence rates for the two-step procedure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Eckstein_S/0/1/0/all/0/1"&gt;Stephan Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Iske_A/0/1/0/all/0/1"&gt;Armin Iske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trabs_M/0/1/0/all/0/1"&gt;Mathias Trabs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations. (arXiv:2107.00520v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00520</id>
        <link href="http://arxiv.org/abs/2107.00520"/>
        <updated>2022-03-19T00:42:46.067Z</updated>
        <summary type="html"><![CDATA[In many prediction problems, spurious correlations are induced by a changing
relationship between the label and a nuisance variable that is also correlated
with the covariates. For example, in classifying animals in natural images, the
background, which is a nuisance, can predict the type of animal. This
nuisance-label relationship does not always hold, and the performance of a
model trained under one such relationship may be poor on data with a different
nuisance-label relationship. To build predictive models that perform well
regardless of the nuisance-label relationship, we develop Nuisance-Randomized
Distillation (NURD). We introduce the nuisance-randomized distribution, a
distribution where the nuisance and the label are independent. Under this
distribution, we define the set of representations such that conditioning on
any member, the nuisance and the label remain independent. We prove that the
representations in this set always perform better than chance, while
representations outside of this set may not. NURD finds a representation from
this set that is most informative of the label under the nuisance-randomized
distribution, and we prove that this representation achieves the highest
performance regardless of the nuisance-label relationship. We evaluate NURD on
several tasks including chest X-ray classification where, using non-lung
patches as the nuisance, NURD produces models that predict pneumonia under
strong spurious correlations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puli_A/0/1/0/all/0/1"&gt;Aahlad Puli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lily H. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1"&gt;Eric K. Oermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Labor Training. (arXiv:2006.06853v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06853</id>
        <link href="http://arxiv.org/abs/2006.06853"/>
        <updated>2022-03-19T00:42:46.060Z</updated>
        <summary type="html"><![CDATA[On-demand labor platforms aim to train a skilled workforce to serve its
incoming demand for jobs. Since limited jobs are available for training, and it
is usually not necessary to train all workers, efficient matching of training
jobs requires prioritizing fast learners over slow ones. However, the learning
rates of novice workers are unknown, resulting in a tradeoff between
exploration (learning the learning rates) and exploitation (training the best
workers). Motivated to study this tradeoff, we analyze a novel objective within
the stochastic multi-armed bandit framework. Given $K$ arms, instead of
maximizing the expected total reward from $T$ pulls (the traditional "sum"
objective), we consider the vector of cumulative rewards earned from the $K$
arms at the end of $T$ pulls and aim to maximize the expected highest
cumulative reward (the "max" objective). When rewards represent skill
increments, this corresponds to the objective of training a single highly
skilled worker from a set of novice workers, using a limited supply of training
jobs. For this objective, we show that any policy must incur an
instance-dependent asymptotic regret of $\Omega(\log T)$ (with a higher
instance-dependent constant) and a worst-case regret of
$\Omega(K^{1/3}T^{2/3})$. We then design an explore-then-commit policy
featuring exploration based on appropriately tuned confidence bounds on the
mean reward and an adaptive stopping criterion, which adapts to the problem
difficulty and achieves these bounds (up to logarithmic factors). We generalize
our algorithmic insights to the problem of maximizing the expected value of the
average cumulative reward of the top $m$ arms with the highest cumulative
rewards, corresponding to the case where multiple workers must be trained. Our
numerical experiments demonstrate the efficacy of our policies compared to
several natural alternatives in practical parameter regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozbay_E/0/1/0/all/0/1"&gt;Eren Ozbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamble_V/0/1/0/all/0/1"&gt;Vijay Kamble&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09481</id>
        <link href="http://arxiv.org/abs/2203.09481"/>
        <updated>2022-03-19T00:42:46.037Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models are a promising new class of
generative models that are competitive with GANs on perceptual metrics. In this
paper, we explore their potential for sequentially generating video. Inspired
by recent advances in neural video compression, we use denoising diffusion
models to stochastically generate a residual to a deterministic next-frame
prediction. We compare this approach to two sequential VAE and two GAN
baselines on four datasets, where we test the generated frames for perceptual
quality and forecasting accuracy against ground truth frames. We find
significant improvements in terms of perceptual quality on all data and
improvements in terms of frame forecasting for complex high-resolution videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruihan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1"&gt;Prakhar Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1"&gt;Stephan Mandt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer learning for cross-modal demand prediction of bike-share and public transit. (arXiv:2203.09279v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09279</id>
        <link href="http://arxiv.org/abs/2203.09279"/>
        <updated>2022-03-19T00:42:46.031Z</updated>
        <summary type="html"><![CDATA[The urban transportation system is a combination of multiple transport modes,
and the interdependencies across those modes exist. This means that the travel
demand across different travel modes could be correlated as one mode may
receive demand from or create demand for another mode, not to mention natural
correlations between different demand time series due to general demand flow
patterns across the network. It is expectable that cross-modal ripple effects
become more prevalent, with Mobility as a Service. Therefore, by propagating
demand data across modes, a better demand prediction could be obtained. To this
end, this study explores various machine learning models and transfer learning
strategies for cross-modal demand prediction. The trip data of bike-share,
metro, and taxi are processed as the station-level passenger flows, and then
the proposed prediction method is tested in the large-scale case studies of
Nanjing and Chicago. The results suggest that prediction models with transfer
learning perform better than unimodal prediction models. Furthermore, stacked
Long Short-Term Memory model performs particularly well in cross-modal demand
prediction. These results verify our combined method's forecasting improvement
over existing benchmarks and demonstrate the good transferability for
cross-modal demand prediction in multiple cities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1"&gt;Mingzhuang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1"&gt;Francisco Camara Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuewu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Subgraph Aggregation Networks. (arXiv:2110.02910v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.02910</id>
        <link href="http://arxiv.org/abs/2110.02910"/>
        <updated>2022-03-19T00:42:46.024Z</updated>
        <summary type="html"><![CDATA[Message-passing neural networks (MPNNs) are the leading architecture for deep
learning on graph-structured data, in large part due to their simplicity and
scalability. Unfortunately, it was shown that these architectures are limited
in their expressive power. This paper proposes a novel framework called
Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our
main observation is that while two graphs may not be distinguishable by an
MPNN, they often contain distinguishable subgraphs. Thus, we propose to
represent each graph as a set of subgraphs derived by some predefined policy,
and to process it using a suitable equivariant architecture. We develop novel
variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph
isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of
these new WL variants. We further prove that our approach increases the
expressive power of both MPNNs and more expressive architectures. Moreover, we
provide theoretical results that describe how design choices such as the
subgraph selection policy and equivariant neural architecture affect our
architecture's expressive power. To deal with the increased computational cost,
we propose a subgraph sampling scheme, which can be viewed as a stochastic
version of our framework. A comprehensive set of experiments on real and
synthetic datasets demonstrates that our framework improves the expressive
power and overall performance of popular GNN architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1"&gt;Beatrice Bevilacqua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1"&gt;Fabrizio Frasca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1"&gt;Derek Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1"&gt;Balasubramaniam Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Chen Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balamurugan_G/0/1/0/all/0/1"&gt;Gopinath Balamurugan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael M. Bronstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1"&gt;Haggai Maron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability and Risk Bounds of Iterative Hard Thresholding. (arXiv:2203.09413v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09413</id>
        <link href="http://arxiv.org/abs/2203.09413"/>
        <updated>2022-03-19T00:42:46.017Z</updated>
        <summary type="html"><![CDATA[In this paper, we analyze the generalization performance of the Iterative
Hard Thresholding (IHT) algorithm widely used for sparse recovery problems. The
parameter estimation and sparsity recovery consistency of IHT has long been
known in compressed sensing. From the perspective of statistical learning,
another fundamental question is how well the IHT estimation would predict on
unseen data. This paper makes progress towards answering this open question by
introducing a novel sparse generalization theory for IHT under the notion of
algorithmic stability. Our theory reveals that: 1) under natural conditions on
the empirical risk function over $n$ samples of dimension $p$, IHT with
sparsity level $k$ enjoys an $\mathcal{\tilde
O}(n^{-1/2}\sqrt{k\log(n)\log(p)})$ rate of convergence in sparse excess risk;
2) a tighter $\mathcal{\tilde O}(n^{-1/2}\sqrt{\log(n)})$ bound can be
established by imposing an additional iteration stability condition on a
hypothetical IHT procedure invoked to the population risk; and 3) a fast rate
of order $\mathcal{\tilde O}\left(n^{-1}k(\log^3(n)+\log(p))\right)$ can be
derived for strongly convex risk function under proper strong-signal
conditions. The results have been substantialized to sparse linear regression
and sparse logistic regression models to demonstrate the applicability of our
theory. Preliminary numerical evidence is provided to confirm our theoretical
predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiao-Tong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kan Extensions in Data Science and Machine Learning. (arXiv:2203.09018v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09018</id>
        <link href="http://arxiv.org/abs/2203.09018"/>
        <updated>2022-03-19T00:42:45.996Z</updated>
        <summary type="html"><![CDATA[A common problem in data science is "use this function defined over this
small set to generate predictions over that larger set." Extrapolation,
interpolation, statistical inference and forecasting all reduce to this
problem. The Kan extension is a powerful tool in category theory that
generalizes this notion. In this work we explore several applications of Kan
extensions to data science. We begin by deriving a simple classification
algorithm as a Kan extension and experimenting with this algorithm on real
data. Next, we use the Kan extension to derive a procedure for learning
clustering algorithms from labels and explore the performance of this procedure
on real data. We then investigate how Kan extensions can be used to learn a
general mapping from datasets of labeled examples to functions and to
approximate a complex function with a simpler one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shiebler_D/0/1/0/all/0/1"&gt;Dan Shiebler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs. (arXiv:2203.09251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09251</id>
        <link href="http://arxiv.org/abs/2203.09251"/>
        <updated>2022-03-19T00:42:45.990Z</updated>
        <summary type="html"><![CDATA[In probably approximately correct (PAC) reinforcement learning (RL), an agent
is required to identify an $\epsilon$-optimal policy with probability
$1-\delta$. While minimax optimal algorithms exist for this problem, its
instance-dependent complexity remains elusive in episodic Markov decision
processes (MDPs). In this paper, we propose the first (nearly) matching upper
and lower bounds on the sample complexity of PAC RL in deterministic episodic
MDPs with finite state and action spaces. In particular, our bounds feature a
new notion of sub-optimality gap for state-action pairs that we call the
deterministic return gap. While our instance-dependent lower bound is written
as a linear program, our algorithms are very simple and do not require solving
such an optimization problem during learning. Their design and analyses employ
novel ideas, including graph-theoretical concepts such as minimum flows and
maximum cuts, which we believe to shed new light on this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tirinzoni_A/0/1/0/all/0/1"&gt;Andrea Tirinzoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Marjani_A/0/1/0/all/0/1"&gt;Aymen Al-Marjani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1"&gt;Emilie Kaufmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Dimension for Deep Learning based on Hoeffding Inequality and Relative Evaluation. (arXiv:2203.09082v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09082</id>
        <link href="http://arxiv.org/abs/2203.09082"/>
        <updated>2022-03-19T00:42:45.983Z</updated>
        <summary type="html"><![CDATA[Research on the generalization ability of deep neural networks (DNNs) has
recently attracted a great deal of attention. However, due to their complex
architectures and large numbers of parameters, measuring the generalization
ability of specific DNN models remains an open challenge. In this paper, we
propose to use multiple factors to measure and rank the relative generalization
of DNNs based on a new concept of confidence dimension (CD). Furthermore, we
provide a feasible framework in our CD to theoretically calculate the upper
bound of generalization based on the conventional Vapnik-Chervonenk dimension
(VC-dimension) and Hoeffding's inequality. Experimental results on image
classification and object detection demonstrate that our CD can reflect the
relative generalization ability for different DNNs. In addition to
full-precision DNNs, we also analyze the generalization ability of binary
neural networks (BNNs), whose generalization ability remains an unsolved
problem. Our CD yields a consistent and reliable measure and ranking for both
full-precision DNNs and BNNs on all the tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Linlin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baochang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wentao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1"&gt;Guodong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near Instance Optimal Model Selection for Pure Exploration Linear Bandits. (arXiv:2109.05131v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.05131</id>
        <link href="http://arxiv.org/abs/2109.05131"/>
        <updated>2022-03-19T00:42:45.976Z</updated>
        <summary type="html"><![CDATA[We introduce the model selection problem in pure exploration linear bandits,
where the learner needs to adapt to the instance-dependent complexity measure
of the smallest hypothesis class containing the true model. We design
algorithms in both fixed confidence and fixed budget settings with near
instance optimal guarantees. The core of our algorithms is a new optimization
problem based on experimental design that leverages the geometry of the action
set to identify a near-optimal hypothesis class. Our fixed budget algorithm is
developed based on a novel selection-validation procedure, which provides a new
way to study the understudied fixed budget setting (even without the added
challenge of model selection). We adapt our algorithms, in both fixed
confidence and fixed budget settings, to problems with model misspecification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Katz_Samuels_J/0/1/0/all/0/1"&gt;Julian Katz-Samuels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration in Kernel and Neural Bandits. (arXiv:2106.12034v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12034</id>
        <link href="http://arxiv.org/abs/2106.12034"/>
        <updated>2022-03-19T00:42:45.969Z</updated>
        <summary type="html"><![CDATA[We study pure exploration in bandits, where the dimension of the feature
representation can be much larger than the number of arms. To overcome the
curse of dimensionality, we propose to adaptively embed the feature
representation of each arm into a lower-dimensional space and carefully deal
with the induced model misspecification. Our approach is conceptually very
different from existing works that can either only handle low-dimensional
linear bandits or passively deal with model misspecification. We showcase the
application of our approach to two pure exploration settings that were
previously under-studied: (1) the reward function belongs to a possibly
infinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward
function is nonlinear and can be approximated by neural networks. Our main
results provide sample complexity guarantees that only depend on the effective
dimension of the feature spaces in the kernel or neural representations.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate the efficacy of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Ruoxi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1"&gt;Rebecca Willett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoReL: Multi-omics Relational Learning. (arXiv:2203.08149v1 [q-bio.QM] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.08149</id>
        <link href="http://arxiv.org/abs/2203.08149"/>
        <updated>2022-03-19T00:42:45.962Z</updated>
        <summary type="html"><![CDATA[Multi-omics data analysis has the potential to discover hidden molecular
interactions, revealing potential regulatory and/or signal transduction
pathways for cellular processes of interest when studying life and disease
systems. One of critical challenges when dealing with real-world multi-omics
data is that they may manifest heterogeneous structures and data quality as
often existing data may be collected from different subjects under different
conditions for each type of omics data. We propose a novel deep Bayesian
generative model to efficiently infer a multi-partite graph encoding molecular
interactions across such heterogeneous views, using a fused Gromov-Wasserstein
(FGW) regularization between latent representations of corresponding views for
integrative analysis. With such an optimal transport regularization in the deep
Bayesian generative model, it not only allows incorporating view-specific side
information, either with graph-structured or unstructured data in different
views, but also increases the model flexibility with the distribution-based
regularization. This allows efficient alignment of heterogeneous latent
variable distributions to derive reliable interaction predictions compared to
the existing point-based graph embedding methods. Our experiments on several
real-world datasets demonstrate enhanced performance of MoReL in inferring
meaningful interactions compared to existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Hasanzadeh_A/0/1/0/all/0/1"&gt;Arman Hasanzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hajiramezanali_E/0/1/0/all/0/1"&gt;Ehsan Hajiramezanali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Duffield_N/0/1/0/all/0/1"&gt;Nick Duffield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xiaoning Qian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Inference of Second Opinions. (arXiv:2203.08653v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2203.08653</id>
        <link href="http://arxiv.org/abs/2203.08653"/>
        <updated>2022-03-19T00:42:45.945Z</updated>
        <summary type="html"><![CDATA[Automated decision support systems that are able to infer second opinions
from experts can potentially facilitate a more efficient allocation of
resources; they can help decide when and from whom to seek a second opinion. In
this paper, we look at the design of this type of support systems from the
perspective of counterfactual inference. We focus on a multiclass
classification setting and first show that, if experts make predictions on
their own, the underlying causal mechanism generating their predictions needs
to satisfy a desirable set invariant property. Further, we show that, for any
causal mechanism satisfying this property, there exists an equivalent mechanism
where the predictions by each expert are generated by independent
sub-mechanisms governed by a common noise. This motivates the design of a set
invariant Gumbel-Max structural causal model where the structure of the noise
governing the sub-mechanisms underpinning the model depends on an intuitive
notion of similarity between experts which can be estimated from data.
Experiments on both synthetic and real data show that our model can be used to
infer second opinions more accurately than its non-causal counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benz_N/0/1/0/all/0/1"&gt;Nina Corvelo Benz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1"&gt;Manuel Gomez Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Sliced Wasserstein Distances. (arXiv:2006.08812v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08812</id>
        <link href="http://arxiv.org/abs/2006.08812"/>
        <updated>2022-03-19T00:42:45.938Z</updated>
        <summary type="html"><![CDATA[While theoretically appealing, the application of the Wasserstein distance to
large-scale machine learning problems has been hampered by its prohibitive
computational cost. The sliced Wasserstein distance and its variants improve
the computational efficiency through the random projection, yet they suffer
from low accuracy if the number of projections is not sufficiently large,
because the majority of projections result in trivially small values. In this
work, we propose a new family of distance metrics, called augmented sliced
Wasserstein distances (ASWDs), constructed by first mapping samples to
higher-dimensional hypersurfaces parameterized by neural networks. It is
derived from a key observation that (random) linear projections of samples
residing on these hypersurfaces would translate to much more flexible nonlinear
projections in the original sample space, so they can capture complex
structures of the data distribution. We show that the hypersurfaces can be
optimized by gradient ascent efficiently. We provide the condition under which
the ASWD is a valid metric and show that this can be obtained by an injective
neural network architecture. Numerical results demonstrate that the ASWD
significantly outperforms other Wasserstein variants for both synthetic and
real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiongjie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunpeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning. (arXiv:2203.09137v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09137</id>
        <link href="http://arxiv.org/abs/2203.09137"/>
        <updated>2022-03-19T00:42:45.931Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-learning (MAML) and its variants have become popular
approaches for few-shot learning. However, due to the non-convexity of deep
neural nets (DNNs) and the bi-level formulation of MAML, the theoretical
properties of MAML with DNNs remain largely unknown. In this paper, we first
prove that MAML with over-parameterized DNNs is guaranteed to converge to
global optima at a linear rate. Our convergence analysis indicates that MAML
with over-parameterized DNNs is equivalent to kernel regression with a novel
class of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then,
we propose MetaNTK-NAS, a new training-free neural architecture search (NAS)
method for few-shot learning that uses MetaNTK to rank and select
architectures. Empirically, we compare our MetaNTK-NAS with previous NAS
methods on two popular few-shot learning benchmarks, miniImageNet, and
tieredImageNet. We show that the performance of MetaNTK-NAS is comparable or
better than the state-of-the-art NAS method designed for few-shot learning
while enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NAS
makes itself more practical for many real-world tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yite Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1"&gt;Ruoyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Mathematics of Artificial Intelligence. (arXiv:2203.08890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.08890</id>
        <link href="http://arxiv.org/abs/2203.08890"/>
        <updated>2022-03-19T00:42:45.520Z</updated>
        <summary type="html"><![CDATA[We currently witness the spectacular success of artificial intelligence in
both science and public life. However, the development of a rigorous
mathematical foundation is still at an early stage. In this survey article,
which is based on an invited lecture at the International Congress of
Mathematicians 2022, we will in particular focus on the current "workhorse" of
artificial intelligence, namely deep neural networks. We will present the main
theoretical directions along with several exemplary results and discuss key
open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1"&gt;Gitta Kutyniok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Graph Auto-Encoder for General Data Clustering. (arXiv:2002.08648v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08648</id>
        <link href="http://arxiv.org/abs/2002.08648"/>
        <updated>2022-03-19T00:42:45.498Z</updated>
        <summary type="html"><![CDATA[Graph-based clustering plays an important role in the clustering area. Recent
studies about graph convolution neural networks have achieved impressive
success on graph type data. However, in general clustering tasks, the graph
structure of data does not exist such that the strategy to construct a graph is
crucial for performance. Therefore, how to extend graph convolution networks
into general clustering tasks is an attractive problem. In this paper, we
propose a graph auto-encoder for general data clustering, which constructs the
graph adaptively according to the generative perspective of graphs. The
adaptive process is designed to induce the model to exploit the high-level
information behind data and utilize the non-Euclidean structure sufficiently.
We further design a novel mechanism with rigorous analysis to avoid the
collapse caused by the adaptive construction. Via combining the generative
model for network embedding and graph-based clustering, a graph auto-encoder
with a novel decoder is developed such that it performs well in weighted graph
used scenarios. Extensive experiments prove the superiority of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Generative Models in Engineering Design: A Review. (arXiv:2110.10863v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2110.10863</id>
        <link href="http://arxiv.org/abs/2110.10863"/>
        <updated>2022-03-19T00:42:45.491Z</updated>
        <summary type="html"><![CDATA[Automated design synthesis has the potential to revolutionize the modern
engineering design process and improve access to highly optimized and
customized products across countless industries. Successfully adapting
generative Machine Learning to design engineering may enable such automated
design synthesis and is a research subject of great importance. We present a
review and analysis of Deep Generative Machine Learning models in engineering
design. Deep Generative Models (DGMs) typically leverage deep networks to learn
from an input dataset and synthesize new designs. Recently, DGMs such as
feedforward Neural Networks (NNs), Generative Adversarial Networks (GANs),
Variational Autoencoders (VAEs), and certain Deep Reinforcement Learning (DRL)
frameworks have shown promising results in design applications like structural
optimization, materials design, and shape synthesis. The prevalence of DGMs in
engineering design has skyrocketed since 2016. Anticipating continued growth,
we conduct a review of recent advances to benefit researchers interested in
DGMs for design. We structure our review as an exposition of the algorithms,
datasets, representation methods, and applications commonly used in the current
literature. In particular, we discuss key works that have introduced new
techniques and methods in DGMs, successfully applied DGMs to a design-related
domain, or directly supported the development of DGMs through datasets or
auxiliary methods. We further identify key challenges and limitations currently
seen in DGMs across design fields, such as design creativity, handling
constraints and objectives, and modeling both form and functional performance
simultaneously. In our discussion, we identify possible solution pathways as
key areas on which to target future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regenwetter_L/0/1/0/all/0/1"&gt;Lyle Regenwetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nobari_A/0/1/0/all/0/1"&gt;Amin Heyrani Nobari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1"&gt;Faez Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed. (arXiv:2203.09179v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2203.09179</id>
        <link href="http://arxiv.org/abs/2203.09179"/>
        <updated>2022-03-19T00:42:45.484Z</updated>
        <summary type="html"><![CDATA[Gaussian process regression underpins countless academic and industrial
applications of machine learning and statistics, with maximum likelihood
estimation routinely used to select appropriate parameters for the covariance
kernel. However, it remains an open problem to establish the circumstances in
which maximum likelihood estimation is well-posed. That is, when the
predictions of the regression model are continuous (or insensitive to small
perturbations) in the training data. This article presents a rigorous proof
that the maximum likelihood estimator fails to be well-posed in Hellinger
distance in a scenario where the data are noiseless. The failure case occurs
for any Gaussian process with a stationary covariance function whose
lengthscale parameter is estimated using maximum likelihood. Although the
failure of maximum likelihood estimation is informally well-known, these
theoretical results appear to be the first of their kind, and suggest that
well-posedness may need to be assessed post-hoc, on a case-by-case basis, when
maximum likelihood estimation is used to train a Gaussian process model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Karvonen_T/0/1/0/all/0/1"&gt;Toni Karvonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Oates_C/0/1/0/all/0/1"&gt;Chris J. Oates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disparities in Dermatology AI Performance on a Diverse, Curated Clinical Image Set. (arXiv:2203.08807v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2203.08807</id>
        <link href="http://arxiv.org/abs/2203.08807"/>
        <updated>2022-03-19T00:42:45.477Z</updated>
        <summary type="html"><![CDATA[Access to dermatological care is a major issue, with an estimated 3 billion
people lacking access to care globally. Artificial intelligence (AI) may aid in
triaging skin diseases. However, most AI models have not been rigorously
assessed on images of diverse skin tones or uncommon diseases. To ascertain
potential biases in algorithm performance in this context, we curated the
Diverse Dermatology Images (DDI) dataset-the first publicly available, expertly
curated, and pathologically confirmed image dataset with diverse skin tones.
Using this dataset of 656 images, we show that state-of-the-art dermatology AI
models perform substantially worse on DDI, with receiver operator curve area
under the curve (ROC-AUC) dropping by 27-36 percent compared to the models'
original test results. All the models performed worse on dark skin tones and
uncommon diseases, which are represented in the DDI dataset. Additionally, we
find that dermatologists, who typically provide visual labels for AI training
and test datasets, also perform worse on images of dark skin tones and uncommon
diseases compared to ground truth biopsy annotations. Finally, fine-tuning AI
models on the well-characterized and diverse DDI images closed the performance
gap between light and dark skin tones. Moreover, algorithms fine-tuned on
diverse skin tones outperformed dermatologists on identifying malignancy on
images of dark skin tones. Our findings identify important weaknesses and
biases in dermatology AI that need to be addressed to ensure reliable
application to diverse patients and diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Daneshjou_R/0/1/0/all/0/1"&gt;Roxana Daneshjou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vodrahalli_K/0/1/0/all/0/1"&gt;Kailas Vodrahalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Novoa_R/0/1/0/all/0/1"&gt;Roberto A Novoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jenkins_M/0/1/0/all/0/1"&gt;Melissa Jenkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weixin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rotemberg_V/0/1/0/all/0/1"&gt;Veronica Rotemberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ko_J/0/1/0/all/0/1"&gt;Justin Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Swetter_S/0/1/0/all/0/1"&gt;Susan M Swetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bailey_E/0/1/0/all/0/1"&gt;Elizabeth E Bailey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gevaert_O/0/1/0/all/0/1"&gt;Olivier Gevaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukherjee_P/0/1/0/all/0/1"&gt;Pritam Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phung_M/0/1/0/all/0/1"&gt;Michelle Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yekrang_K/0/1/0/all/0/1"&gt;Kiana Yekrang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fong_B/0/1/0/all/0/1"&gt;Bradley Fong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahasrabudhe_R/0/1/0/all/0/1"&gt;Rachna Sahasrabudhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Allerup_J/0/1/0/all/0/1"&gt;Johan A. C. Allerup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Okata_Karigane_U/0/1/0/all/0/1"&gt;Utako Okata-Karigane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chiou_A/0/1/0/all/0/1"&gt;Albert Chiou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2203.09301</id>
        <link href="http://arxiv.org/abs/2203.09301"/>
        <updated>2022-03-19T00:42:45.470Z</updated>
        <summary type="html"><![CDATA[There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1"&gt;Gihyun Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixing Up Contrastive Learning: Self-Supervised Representation Learning for Time Series. (arXiv:2203.09270v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09270</id>
        <link href="http://arxiv.org/abs/2203.09270"/>
        <updated>2022-03-19T00:42:45.446Z</updated>
        <summary type="html"><![CDATA[The lack of labeled data is a key challenge for learning useful
representation from time series data. However, an unsupervised representation
framework that is capable of producing high quality representations could be of
great value. It is key to enabling transfer learning, which is especially
beneficial for medical applications, where there is an abundance of data but
labeling is costly and time consuming. We propose an unsupervised contrastive
learning framework that is motivated from the perspective of label smoothing.
The proposed approach uses a novel contrastive loss that naturally exploits a
data augmentation scheme in which new samples are generated by mixing two data
samples with a mixing component. The task in the proposed framework is to
predict the mixing component, which is utilized as soft targets in the loss
function. Experiments demonstrate the framework's superior performance compared
to other representation learning approaches on both univariate and multivariate
time series and illustrate its benefits for transfer learning for clinical time
series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wickstrom_K/0/1/0/all/0/1"&gt;Kristoffer Wickstr&amp;#xf8;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kampffmeyer_M/0/1/0/all/0/1"&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikalsen_K/0/1/0/all/0/1"&gt;Karl &amp;#xd8;yvind Mikalsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jenssen_R/0/1/0/all/0/1"&gt;Robert Jenssen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranking of Communities in Multiplex Spatiotemporal Models of Brain Dynamics. (arXiv:2203.09281v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2203.09281</id>
        <link href="http://arxiv.org/abs/2203.09281"/>
        <updated>2022-03-19T00:42:45.439Z</updated>
        <summary type="html"><![CDATA[As a relatively new field, network neuroscience has tended to focus on
aggregate behaviours of the brain averaged over many successive experiments or
over long recordings in order to construct robust brain models. These models
are limited in their ability to explain dynamic state changes in the brain
which occurs spontaneously as a result of normal brain function. Hidden Markov
Models (HMMs) trained on neuroimaging time series data have since arisen as a
method to produce dynamical models that are easy to train but can be difficult
to fully parametrise or analyse. We propose an interpretation of these neural
HMMs as multiplex brain state graph models we term Hidden Markov Graph Models
(HMGMs). This interpretation allows for dynamic brain activity to be analysed
using the full repertoire of network analysis techniques. Furthermore, we
propose a general method for selecting HMM hyperparameters in the absence of
external data, based on the principle of maximum entropy, and use this to
select the number of layers in the multiplex model. We produce a new tool for
determining important communities of brain regions using a spatiotemporal
random walk-based procedure that takes advantage of the underlying Markov
structure of the model. Our analysis of real multi-subject fMRI data provides
new results that corroborate the modular processing hypothesis of the brain at
rest as well as contributing new evidence of functional overlap between and
within dynamic brain state communities. Our analysis pipeline provides a way to
characterise dynamic network activity of the brain under novel behaviours or
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wilsenach_J/0/1/0/all/0/1"&gt;James B. Wilsenach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Warnaby_C/0/1/0/all/0/1"&gt;Catherine E. Warnaby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deane_C/0/1/0/all/0/1"&gt;Charlotte M. Deane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Reinert_G/0/1/0/all/0/1"&gt;Gesine D. Reinert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Structured Neural Networks Through Manifold Identification and Variance Reduction. (arXiv:2112.02612v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2112.02612</id>
        <link href="http://arxiv.org/abs/2112.02612"/>
        <updated>2022-03-19T00:42:45.432Z</updated>
        <summary type="html"><![CDATA[This paper proposes an algorithm (RMDA) for training neural networks (NNs)
with a regularization term for promoting desired structures. RMDA does not
incur computation additional to proximal SGD with momentum, and achieves
variance reduction without requiring the objective function to be of the
finite-sum form. Through the tool of manifold identification from nonlinear
optimization, we prove that after a finite number of iterations, all iterates
of RMDA possess a desired structure identical to that induced by the
regularizer at the stationary point of asymptotic convergence, even in the
presence of engineering tricks like data augmentation and dropout that
complicate the training process. Experiments on training NNs with structured
sparsity confirm that variance reduction is necessary for such an
identification, and show that RMDA thus significantly outperforms existing
methods for this task. For unstructured sparsity, RMDA also outperforms a
state-of-the-art pruning method, validating the benefits of training structured
NNs through regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zih-Syuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Ching-pei Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Rates for High-Dimensional Random Tessellation Forests. (arXiv:2109.10541v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2109.10541</id>
        <link href="http://arxiv.org/abs/2109.10541"/>
        <updated>2022-03-19T00:42:45.425Z</updated>
        <summary type="html"><![CDATA[Random forests are a popular class of algorithms used for regression and
classification. The original algorithm introduced by Breiman in 2001 and many
of its variants are ensembles of randomized decision trees built from
axis-aligned partitions of the feature space. One such variant, called Mondrian
forests, was proposed to handle the online setting and is the first class of
random forests for which minimax rates were obtained in arbitrary dimension.
However, the restriction to axis-aligned splits fails to capture dependencies
between features, and random forests that use oblique splits have shown
improved empirical performance for many tasks. In this work, we show that a
large class of random forests with general split directions also achieve
minimax rates in arbitrary dimension. This class includes STIT forests, a
generalization of Mondrian forests to arbitrary split directions, as well as
random forests derived from Poisson hyperplane tessellations. Crucially, our
rates adapt to sparsity in the sense that they depend only on the dimension of
the relevant feature subspace as opposed to the ambient dimension of the
feature space. This generalizes the known rates for Mondrian random forests.
These are the first results showing that random forest variants with oblique
splits can achieve minimax rates in arbitrary dimension. Our proof technique
relies on the novel application of the theory of stationary random
tessellations in stochastic geometry to statistical learning theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+OReilly_E/0/1/0/all/0/1"&gt;Eliza O&amp;#x27;Reilly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tran_N/0/1/0/all/0/1"&gt;Ngoc Mai Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Metric Learning in Multi-View Data: A Downstream Task Perspective. (arXiv:2106.07138v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07138</id>
        <link href="http://arxiv.org/abs/2106.07138"/>
        <updated>2022-03-19T00:42:45.403Z</updated>
        <summary type="html"><![CDATA[Self-supervised metric learning has been a successful approach for learning a
distance from an unlabeled dataset. The resulting distance is broadly useful
for improving various distance-based downstream tasks, even when no information
from downstream tasks is utilized in the metric learning stage. To gain
insights into this approach, we develop a statistical framework to
theoretically study how self-supervised metric learning can benefit downstream
tasks in the context of multi-view data. Under this framework, we show that the
target distance of metric learning satisfies several desired properties for the
downstream tasks. On the other hand, our investigation suggests the target
distance can be further improved by moderating each direction's weights. In
addition, our analysis precisely characterizes the improvement by
self-supervised metric learning on four commonly used downstream tasks: sample
identification, two-sample testing, $k$-means clustering, and $k$-nearest
neighbor classification. When the distance is estimated from an unlabeled
dataset, we establish the upper bound on distance estimation's accuracy and the
number of samples sufficient for downstream task improvement. Finally,
numerical experiments are presented to support the theoretical results in the
paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shulei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Graph Neural Networks. (arXiv:2102.07835v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07835</id>
        <link href="http://arxiv.org/abs/2102.07835"/>
        <updated>2022-03-19T00:42:45.397Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are a powerful architecture for tackling graph
learning tasks, yet have been shown to be oblivious to eminent substructures
such as cycles. We present TOGL, a novel layer that incorporates global
topological information of a graph using persistent homology. TOGL can be
easily integrated into any type of GNN and is strictly more expressive (in
terms the Weisfeiler--Lehman graph isomorphism test) than message-passing GNNs.
Augmenting GNNs with TOGL leads to improved predictive performance for graph
and node classification tasks, both on synthetic data sets, which can be
classified by humans using their topology but not by ordinary GNNs, and on
real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1"&gt;Edward De Brouwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1"&gt;Michael Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_Y/0/1/0/all/0/1"&gt;Yves Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Long-Term Reward Redistribution via Randomized Return Decomposition. (arXiv:2111.13485v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.13485</id>
        <link href="http://arxiv.org/abs/2111.13485"/>
        <updated>2022-03-19T00:42:45.390Z</updated>
        <summary type="html"><![CDATA[Many practical applications of reinforcement learning require agents to learn
from sparse and delayed rewards. It challenges the ability of agents to
attribute their actions to future outcomes. In this paper, we consider the
problem formulation of episodic reinforcement learning with trajectory
feedback. It refers to an extreme delay of reward signals, in which the agent
can only obtain one reward signal at the end of each trajectory. A popular
paradigm for this problem setting is learning with a designed auxiliary dense
reward function, namely proxy reward, instead of sparse environmental signals.
Based on this framework, this paper proposes a novel reward redistribution
algorithm, randomized return decomposition (RRD), to learn a proxy reward
function for episodic reinforcement learning. We establish a surrogate problem
by Monte-Carlo sampling that scales up least-squares-based reward
redistribution to long-horizon problems. We analyze our surrogate loss function
by connection with existing methods in the literature, which illustrates the
algorithmic properties of our approach. In experiments, we extensively evaluate
our proposed method on a variety of benchmark tasks with episodic rewards and
demonstrate substantial improvement over baseline algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhizhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1"&gt;Ruihan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jian Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning through Local Memorization. (arXiv:2111.09360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2111.09360</id>
        <link href="http://arxiv.org/abs/2111.09360"/>
        <updated>2022-03-19T00:42:45.371Z</updated>
        <summary type="html"><![CDATA[Federated learning allows clients to collaboratively learn statistical models
while keeping their data local. Federated learning was originally used to train
a unique global model to be served to all clients, but this approach might be
sub-optimal when clients' local data distributions are heterogeneous. In order
to tackle this limitation, recent personalized federated learning methods train
a separate model for each client while still leveraging the knowledge available
at other clients. In this work, we exploit the ability of deep neural networks
to extract high quality vectorial representations (embeddings) from non-tabular
data, e.g., images and text, to propose a personalization mechanism based on
local memorization. Personalization is obtained interpolating a pre-trained
global model with a $k$-nearest neighbors (kNN) model based on the shared
representation provided by the global model. We provide generalization bounds
for the proposed approach and we show on a suite of federated datasets that
this approach achieves significantly higher accuracy and fairness than
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marfoq_O/0/1/0/all/0/1"&gt;Othmane Marfoq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neglia_G/0/1/0/all/0/1"&gt;Giovanni Neglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kameni_L/0/1/0/all/0/1"&gt;Laetitia Kameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1"&gt;Richard Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Clustering and Matrix Completion for the Reconstruction of World Input-Output Tables. (arXiv:2203.08819v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.08819</id>
        <link href="http://arxiv.org/abs/2203.08819"/>
        <updated>2022-03-19T00:42:45.339Z</updated>
        <summary type="html"><![CDATA[World Input-Output (I/O) matrices provide the networks of within- and
cross-country economic relations. In the context of I/O analysis, the
methodology adopted by national statistical offices in data collection raises
the issue of obtaining reliable data in a timely fashion and it makes the
reconstruction of (part of) the I/O matrices of particular interest. In this
work, we propose a method combining hierarchical clustering and Matrix
Completion (MC) with a LASSO-like nuclear norm penalty, to impute missing
entries of a partially unknown I/O matrix. Through simulations based on
synthetic matrices we study the effectiveness of the proposed method to predict
missing values from both previous years data and current data related to
countries similar to the one for which current data are obscured. To show the
usefulness of our method, an application based on World Input-Output Database
(WIOD) tables - which are an example of industry-by-industry I/O tables - is
provided. Strong similarities in structure between WIOD and other I/O tables
are also found, which make the proposed approach easily generalizable to them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Metulini_R/0/1/0/all/0/1"&gt;Rodolfo Metulini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gnecco_G/0/1/0/all/0/1"&gt;Giorgio Gnecco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Biancalani_F/0/1/0/all/0/1"&gt;Francesco Biancalani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Riccaboni_M/0/1/0/all/0/1"&gt;Massimo Riccaboni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow. (arXiv:2107.00204v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00204</id>
        <link href="http://arxiv.org/abs/2107.00204"/>
        <updated>2022-03-19T00:42:45.307Z</updated>
        <summary type="html"><![CDATA[For marketing, we sometimes need to recommend content for multiple pages in
sequence. Different from general sequential decision making process, the use
cases have a simpler flow where customers per seeing recommended content on
each page can only return feedback as moving forward in the process or dropping
from it until a termination state. We refer to this type of problems as
sequential decision making in linear--flow. We propose to formulate the problem
as an MDP with Bandits where Bandits are employed to model the transition
probability matrix. At recommendation time, we use Thompson sampling (TS) to
sample the transition probabilities and allocate the best series of actions
with analytical solution through exact dynamic programming. The way that we
formulate the problem allows us to leverage TS's efficiency in balancing
exploration and exploitation and Bandit's convenience in modeling actions'
incompatibility. In the simulation study, we observe the proposed MDP with
Bandits algorithm outperforms Q-learning with $\epsilon$-greedy and decreasing
$\epsilon$, independent Bandits, and interaction Bandits. We also find the
proposed algorithm's performance is the most robust to changes in the
across-page interdependence strength.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covid19 Reproduction Number: Credibility Intervals by Blockwise Proximal Monte Carlo Samplers. (arXiv:2203.09142v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09142</id>
        <link href="http://arxiv.org/abs/2203.09142"/>
        <updated>2022-03-19T00:42:45.298Z</updated>
        <summary type="html"><![CDATA[Monitoring the Covid19 pandemic constitutes a critical societal stake that
received considerable research efforts. The intensity of the pandemic on a
given territory is efficiently measured by the reproduction number, quantifying
the rate of growth of daily new infections. Recently, estimates for the time
evolution of the reproduction number were produced using an inverse problem
formulation with a nonsmooth functional minimization. While it was designed to
be robust to the limited quality of the Covid19 data (outliers, missing
counts), the procedure lacks the ability to output credibility interval based
estimates. This remains a severe limitation for practical use in actual
pandemic monitoring by epidemiologists that the present work aims to overcome
by use of Monte Carlo sampling. After interpretation of the functional into a
Bayesian framework, several sampling schemes are tailored to adjust the
nonsmooth nature of the resulting posterior distribution. The originality of
the devised algorithms stems from combining a Langevin Monte Carlo sampling
scheme with Proximal operators. Performance of the new algorithms in producing
relevant credibility intervals for the reproduction number estimates and
denoised counts are compared. Assessment is conducted on real daily new
infection counts made available by the Johns Hopkins University. The interest
of the devised monitoring tools are illustrated on Covid19 data from several
different countries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fort_G/0/1/0/all/0/1"&gt;Gersende Fort&lt;/a&gt; (IMT), &lt;a href="http://arxiv.org/find/cs/1/au:+Pascal_B/0/1/0/all/0/1"&gt;Barbara Pascal&lt;/a&gt; (CRIStAL), &lt;a href="http://arxiv.org/find/cs/1/au:+Abry_P/0/1/0/all/0/1"&gt;Patrice Abry&lt;/a&gt; (Phys-ENS), &lt;a href="http://arxiv.org/find/cs/1/au:+Pustelnik_N/0/1/0/all/0/1"&gt;Nelly Pustelnik&lt;/a&gt; (Phys-ENS)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration. (arXiv:2107.05446v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05446</id>
        <link href="http://arxiv.org/abs/2107.05446"/>
        <updated>2022-03-19T00:42:45.291Z</updated>
        <summary type="html"><![CDATA[Source-free domain adaptation (SFDA) aims to adapt a model trained on
labelled data in a source domain to unlabelled data in a target domain without
access to the source-domain data during adaptation. Existing methods for SFDA
leverage entropy-minimization techniques which: (i) apply only to
classification; (ii) destroy model calibration; and (iii) rely on the source
model achieving a good level of feature-space class-separation in the target
domain. We address these issues for a particularly pervasive type of domain
shift called measurement shift which can be resolved by restoring the source
features rather than extracting new ones. In particular, we propose Feature
Restoration (FR) wherein we: (i) store a lightweight and flexible approximation
of the feature distribution under the source data; and (ii) adapt the
feature-extractor such that the approximate feature distribution under the
target data realigns with that saved on the source. We additionally propose a
bottom-up training scheme which boosts performance, which we call Bottom-Up
Feature Restoration (BUFR). On real and synthetic data, we demonstrate that
BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and
data efficiency, while being less reliant on the performance of the source
model in the target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1"&gt;Cian Eastwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mason_I/0/1/0/all/0/1"&gt;Ian Mason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1"&gt;Christopher K. I. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework and Benchmark for Deep Batch Active Learning for Regression. (arXiv:2203.09410v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2203.09410</id>
        <link href="http://arxiv.org/abs/2203.09410"/>
        <updated>2022-03-19T00:42:45.283Z</updated>
        <summary type="html"><![CDATA[We study the performance of different pool-based Batch Mode Deep Active
Learning (BMDAL) methods for regression on tabular data, focusing on methods
that do not require to modify the network architecture and training. Our
contributions are three-fold: First, we present a framework for constructing
BMDAL methods out of kernels, kernel transformations and selection methods,
showing that many of the most popular BMDAL methods fit into our framework.
Second, we propose new components, leading to a new BMDAL method. Third, we
introduce an open-source benchmark with 15 large tabular data sets, which we
use to compare different BMDAL methods. Our benchmark results show that a
combination of our novel components yields new state-of-the-art results in
terms of RMSE and is computationally efficient. We provide open-source code
that includes efficient implementations of all kernels, kernel transformations,
and selection methods, and can be used for reproducing our results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Holzmuller_D/0/1/0/all/0/1"&gt;David Holzm&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zaverkin_V/0/1/0/all/0/1"&gt;Viktor Zaverkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kastner_J/0/1/0/all/0/1"&gt;Johannes K&amp;#xe4;stner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Steinwart_I/0/1/0/all/0/1"&gt;Ingo Steinwart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry-Based Representations for Artificial and Biological General Intelligence. (arXiv:2203.09250v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2203.09250</id>
        <link href="http://arxiv.org/abs/2203.09250"/>
        <updated>2022-03-19T00:42:45.276Z</updated>
        <summary type="html"><![CDATA[Biological intelligence is remarkable in its ability to produce complex
behaviour in many diverse situations through data efficient, generalisable and
transferable skill acquisition. It is believed that learning "good" sensory
representations is important for enabling this, however there is little
agreement as to what a good representation should look like. In this review
article we are going to argue that symmetry transformations are a fundamental
principle that can guide our search for what makes a good representation. The
idea that there exist transformations (symmetries) that affect some aspects of
the system but not others, and their relationship to conserved quantities has
become central in modern physics, resulting in a more unified theoretical
framework and even ability to predict the existence of new particles. Recently,
symmetries have started to gain prominence in machine learning too, resulting
in more data efficient and generalisable algorithms that can mimic some of the
complex behaviours produced by biological intelligence. Finally, first
demonstrations of the importance of symmetry transformations for representation
learning in the brain are starting to arise in neuroscience. Taken together,
the overwhelming positive effect that symmetries bring to these disciplines
suggest that they may be an important general framework that determines the
structure of the universe, constrains the nature of natural tasks and
consequently shapes both biological and artificial intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Higgins_I/0/1/0/all/0/1"&gt;Irina Higgins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Racaniere_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Racani&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Rezende_D/0/1/0/all/0/1"&gt;Danilo Rezende&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Euler State Networks. (arXiv:2203.09382v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2203.09382</id>
        <link href="http://arxiv.org/abs/2203.09382"/>
        <updated>2022-03-19T00:42:45.256Z</updated>
        <summary type="html"><![CDATA[Inspired by the numerical solution of ordinary differential equations, in
this paper we propose a novel Reservoir Computing (RC) model, called the Euler
State Network (EuSN). The introduced approach makes use of forward Euler
discretization and antisymmetric recurrent matrices to design reservoir
dynamics that are both stable and non-dissipative by construction.

Our mathematical analysis shows that the resulting model is biased towards
unitary effective spectral radius and zero local Lyapunov exponents,
intrinsically operating at the edge of stability. Experiments on synthetic
tasks indicate the marked superiority of the proposed approach, compared to
standard RC models, in tasks requiring long-term memorization skills.
Furthermore, results on real-world time series classification benchmarks point
out that EuSN is capable of matching (or even surpassing) the level of accuracy
of trainable Recurrent Neural Networks, while allowing up to 100-fold savings
in computation time and energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1"&gt;Claudio Gallicchio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiability of Sparse Causal Effects using Instrumental Variables. (arXiv:2203.09380v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2203.09380</id>
        <link href="http://arxiv.org/abs/2203.09380"/>
        <updated>2022-03-19T00:42:45.249Z</updated>
        <summary type="html"><![CDATA[Exogenous heterogeneity, for example, in the form of instrumental variables
can help us learn a system's underlying causal structure and predict the
outcome of unseen intervention experiments. In this paper, we consider linear
models in which the causal effect from covariates $X$ on a response $Y$ is
sparse. We prove that the causal coefficient becomes identifiable under weak
conditions and may even be identified in models, where the number of
instruments is as small as the number of causal parents. We also develop
graphical criteria under which the identifiability holds with probability one
if the edge coefficients are sampled randomly from a distribution that is
absolutely continuous with respect to Lebesgue measure. As an estimator, we
propose spaceIV and prove that it consistently estimates the causal effect if
the model is identifiable and evaluate its performance on simulated data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pfister_N/0/1/0/all/0/1"&gt;Niklas Pfister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jonas Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Often-times data scientists work in silos. And later on in their project, they realize that “I wish I had known this ….”]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thjjfh/discussion_oftentimes_data_scientists_work_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thjjfh/discussion_oftentimes_data_scientists_work_in/"/>
        <updated>2022-03-19T00:41:06.000Z</updated>
        <summary type="html"><![CDATA[Often-times data scientists work in silos. And later on in their project, they realize that “I wish I had known this ….”
 Some of the questions that arise are: 
 ​
  
[Data] Training data is not representative of Production Data
 [Economics] Business had different success criteria than data scientist’s evaluation metrics
 [Engineering] Do not have an infrastructure in place to support models
 [Engineering] Product backend is written in Java and your pipeline is in Python
 [Legal] Need to take approval from Legal and Governance to consider data privacy, bias, and fairness
 [Stakeholders] Explainability is more important than results
  
Have you come across any such situations?
    submitted by    /u/rajg88  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I can't debug my dang code]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/thjizs/i_cant_debug_my_dang_code/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/thjizs/i_cant_debug_my_dang_code/"/>
        <updated>2022-03-19T00:40:26.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone. I'm trying to debug my code and I can't manage to find what's wrong. I'm running a Roberta model with a classifier head on a Pytorch Lightning trainer and I just get stuck after it is running the optimizer config, it's loading the train data, calling __len__(from dataset) a couple of times, loading val data, calling len again a few times and then complete freeze, what could be going on? Also it's 100% running on the gpu but python is not using any gpu in task manager so I don't think it's actually doing anything.
    submitted by    /u/DifferentMedia2536  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building games and apps entirely through natural language using OpenAI’s code-davinci model]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thhb7e/building_games_and_apps_entirely_through_natural/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thhb7e/building_games_and_apps_entirely_through_natural/"/>
        <updated>2022-03-18T22:48:30.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bperki8  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Looking for a GPU server for our university research lab]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thh51x/d_looking_for_a_gpu_server_for_our_university/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thh51x/d_looking_for_a_gpu_server_for_our_university/"/>
        <updated>2022-03-18T22:40:02.000Z</updated>
        <summary type="html"><![CDATA[Our lab is recently doing more deep learning projects and our current lab server is struggling with the increasing load. My professor tasked me with finding our lab a good GPU server for the lab. The reserved budget is 25K. Our current server have the following specifications:
 CPU: Intel Xeon Silver 4114 CPU
 RAM: 192GB
 GPU: NVIDIA Tesla V100 PCIe 32GB (one GPU)
 There are 4 phd students currently working on the server. I prefer something better than the current server. So far I found Lambda Labs and ThinkMate (based on a friend recommendation). My professor want me to select a vendor with good support and offering academic discounts. I'm looking for GPUs that are perform similar or better than Nvidia RTX 2080 Ti.
 Do you have a recommendation for any vendor that builds GPU servers (~$25K, servicing 3-5 students working mostly on CNN and RNN models), and which server configurations is best? Thank you.
    submitted by    /u/majax21  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network is training to give just one output, how can I prevent this?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thgqd7/neural_network_is_training_to_give_just_one/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thgqd7/neural_network_is_training_to_give_just_one/"/>
        <updated>2022-03-18T22:20:26.000Z</updated>
        <summary type="html"><![CDATA[Look for more info at: https://stackoverflow.com/questions/71533736/neural-network-is-training-to-give-just-one-output-how-can-i-prevent-this
    submitted by    /u/UnityPlum  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This Research Paper Explain The Compute Trends Across Three Eras Of Machine Learning]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thgj4d/this_research_paper_explain_the_compute_trends/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thgj4d/this_research_paper_explain_the_compute_trends/"/>
        <updated>2022-03-18T22:10:38.000Z</updated>
        <summary type="html"><![CDATA[The three essential components that determine the evolution of modern Machine Learning are computing, data, and algorithmic advancements (ML). The article looks at trends in the most easily quantifiable element. Before 2010, training computes expanded in lockstep with Moore’s law, doubling every two years. Since the early 2010s, when Deep Learning was first introduced, the rate of training compute has quickened, roughly doubling every six months. Late in 2015, a new trend emerged. The history of computation in ML has been divided into three eras based on these observations – the Pre-Deep Learning Era, the Deep Learning Era, and the Large-Scale Era. The article summarises the fast-growing compute requirements for training advanced ML systems.
 Trends
 The comparison is made on a dataset of 123 milestone ML systems, annotated with the computing it took to train them. Before Deep Learning took off, there was a period of slow progress. The tendency accelerated in 2010 and hasn’t slowed since. Separately, in 2015 and 2016, a new trend of large-scale models arose, expanding at a comparable rate but by two orders of magnitude faster than the preceding one.
 Continue Reading The Research Summary
 Paper: https://arxiv.org/pdf/2202.05924.pdf
 Github: https://github.com/ML-Progress/Compute-Trends
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Overview SOTA Results of NN on Various Datasets]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thgcj1/d_overview_sota_results_of_nn_on_various_datasets/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thgcj1/d_overview_sota_results_of_nn_on_various_datasets/"/>
        <updated>2022-03-18T22:01:50.000Z</updated>
        <summary type="html"><![CDATA[Hi everybody,
 I developed a new NN training approach and want to benchmark it now. I am looking for Classification/Regression tasks on various normal and image datasets, but also autoencoder-like reconstruction problems (I guess denoising is here the classic evaluation task?)
  
Are there any good websites/papers which collect current SOTA results on multiple datasets? It would be great if they also provide training times, number of parameters, etc.
 What datasets would you recommend to definitely include in such a benchmark?
  
Thanks in advance!
    submitted by    /u/arcxtriy  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] New paper on autonomous driving and multi-task: "HybridNets: End-to-End Perception Network"]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/thg7op/r_new_paper_on_autonomous_driving_and_multitask/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/thg7op/r_new_paper_on_autonomous_driving_and_multitask/"/>
        <updated>2022-03-18T21:55:34.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/xoiga123  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tokyo Researchers Hit the Lottery Ticket Theory with “Hiddenite” AI Chip - News]]></title>
        <id>https://www.reddit.com/r/artificial/comments/thebee/tokyo_researchers_hit_the_lottery_ticket_theory/</id>
        <link href="https://www.reddit.com/r/artificial/comments/thebee/tokyo_researchers_hit_the_lottery_ticket_theory/"/>
        <updated>2022-03-18T20:26:50.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/allaboutcircuits  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enable conversational chatbots for telephony using Amazon Lex and the Amazon Chime SDK]]></title>
        <id>04611bd8217a125938377ead4fa226180deae472</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/enable-conversational-chatbots-for-telephony-using-amazon-lex-and-the-amazon-chime-sdk/"/>
        <updated>2022-03-18T19:20:55.000Z</updated>
        <summary type="html"><![CDATA[Conversational AI can deliver powerful, automated, interactive experiences through voice and text. Amazon Lex is a service that combines automatic speech recognition and natural language understanding technologies, so you can build these sophisticated conversational experiences. A common application of conversational AI is found in contact centers: self-service virtual agents. We’re excited to announce that you […]]]></summary>
        <author>
            <name>Greg Herlein</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] Assessing stability of cluster label assignment over cross-validation folds]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/thbsax/research_assessing_stability_of_cluster_label/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/thbsax/research_assessing_stability_of_cluster_label/"/>
        <updated>2022-03-18T18:30:17.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone,
 I have a dataset of continuous multivariate samples. I'm clustering these samples together using an unsupervised algorithm to infer unmeasured latent states. Ideally, this would just be performed on the entire sample set. However, this label will then subsequently form part of a supervised prediction algorithm, and because the dataset isn't very large (several hundred samples), I'm almost certainly going to end up using k-fold cross validation.
 ​
 Now, the issue with using cluster labels generated from the whole dataset is that this essentially permits data leakage from the test fold back into the training fold for the supervised learning algorithm (which, for the purposes of my experiment, I don't want). So instead, the unsupervised labels will need to be generated for each fold individually.
 ​
 What I would like to do is have a measure of the relative stability/instability of label assignment for each sample across the folds. So for example, over 10 folds, ideally sample 1 would always get the label 'A' ten times out of ten... but in reality, perhaps it will get label 'A' eight times, and label 'B' twice.
 ​
 It sounds like the Bhattacharyya coefficient might be an appropriate scoring system (i.e. for each sample, calculate the square root of the product of the label proportions, and sum over all samples) - but I've never used this before, so I don't know whether it would be an appropriate use case. Does anyone have any familiarity with either this measure or this type of situation in general?
    submitted by    /u/mcflyanddie  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Democratization of large (language) models]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th8ag3/d_democratization_of_large_language_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th8ag3/d_democratization_of_large_language_models/"/>
        <updated>2022-03-18T17:38:37.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I've been thinking about how to democratize running and training of large language models, and more generally how to democratize all large models.
 I've wondered if this would be achievable with a volunteer computing project. I've found two (and only two, unfortunately) papers on the subject, so I at least know that I'm not the only one thinking about this. Those papers can be found here and here.
 That would be the end of the discussion for me, but I think the previous approaches have some flaws. Both papers use data parallelism, making training of extremely large (> 20GB) models basically impossible, because very few volunteers will have enough resources to run, much less train, the models, regardless of batch size. Systems like Hydra might be able to run things locally for models between 20 to 250GB, but above that it becomes a question of disk space.
 A more theoretical flaw is that both of the papers use a modified version of SGD, which means any model would be required to use special optimizers. I don't know how that would impact results/convergence, but it doesn't seem trivial.
 In conclusion, it seems to me that a VC system for extremely large models probably needs a novel approach that uses tensor parallelism, so that work could be more granularly distributed. It's (probably) not an easy problem, but it seems possible.
 I'm curious what this community's thoughts are, and if anyone would also be interested in working on this. Also, if anyone knows any other resources on this problem, please drop a link so I can check them out!
    submitted by    /u/Lazauya  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[‘Searching for Escher’ - an AI journey through his work]]></title>
        <id>https://www.reddit.com/r/artificial/comments/th7jfx/searching_for_escher_an_ai_journey_through_his/</id>
        <link href="https://www.reddit.com/r/artificial/comments/th7jfx/searching_for_escher_an_ai_journey_through_his/"/>
        <updated>2022-03-18T17:24:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/glenniszen  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Can I submit to ARR for resubmission and at the same time commit to a workshop]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th76i8/d_can_i_submit_to_arr_for_resubmission_and_at_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th76i8/d_can_i_submit_to_arr_for_resubmission_and_at_the/"/>
        <updated>2022-03-18T17:17:12.000Z</updated>
        <summary type="html"><![CDATA[In the 2 links below, it says you can commit to conferences such as ACL 2022 and NAACL 2022 at the same time submit to ARR for resubmission, but it doesn't say if you can do the same for workshops or conferences relating to ARR other than the 2 conferences listed above.
 In the last link, it pretty much specifies two conferences (maybe just listed as an example?) that you can commit and at the same time submit to ARR for resubmission, but for some reason doesn't directly say that you can commit to ARR related conference/workshop alongside submitting to ARR for resubmission.
 So if anybody has any info on whether I can submit to ARR for resubmission and at the same time commit to any ARR related workshop/conference, I'd appreciate it.
 https://www.2022.aclweb.org/post/acl-2022-chair-blog-post-faq
 https://aclrollingreview.org/choices/
    submitted by    /u/SiegeMemeLord  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers From Idiap Research Institute Propose ‘HyperMixer’: An MLP-Based Green AI Alternative To Transformers]]></title>
        <id>https://www.reddit.com/r/artificial/comments/th6su3/researchers_from_idiap_research_institute_propose/</id>
        <link href="https://www.reddit.com/r/artificial/comments/th6su3/researchers_from_idiap_research_institute_propose/"/>
        <updated>2022-03-18T17:12:45.000Z</updated>
        <summary type="html"><![CDATA[In recent years, transformer topologies have advanced the state-of-the-art in a wide range of natural language processing (NLP) activities. Vision transformers (ViT) are now being used more frequently in computer vision. However, because of the quadratic complexity of transformers over input length, they consume a lot of energy, which limits their research and development and industrial deployment.
 An MLP-based Green AI Replacement to Transformers presents a novel Multi-Layer Perceptron (MLP) model, HyperMixer, as an energy-efficient alternative to transformers that preserves similar inductive biases, according to a recent article published by the Idiap Research Institute in Switzerland.
 The researchers demonstrate that HyperMixer may achieve performance comparable to transformers while significantly reducing processing time, training data, and hyperparameter tuning expenses.
 HyperMixer is a new all-MLP model with inductive biases inspired by transformers. On the GLUE benchmark, HyperMixer’s performance was compared to competitors’. HyperMixer learns attention patterns similarly to transformers, as demonstrated by this ablation.
 Continue Reading The Research Summary
 Paper: https://arxiv.org/pdf/2203.03691.pdf
 https://preview.redd.it/4dbukkw5d6o81.png?width=1502&format=png&auto=webp&s=52f61392525435cf105d4bfee508a4c05eef5389
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Layer normalization in transformer blocks.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th49o4/d_layer_normalization_in_transformer_blocks/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th49o4/d_layer_normalization_in_transformer_blocks/"/>
        <updated>2022-03-18T15:42:51.000Z</updated>
        <summary type="html"><![CDATA[I was playing with the idea of trying a transformer for a sequence modelling task for the first time recently - I was interested in all the X-former variants interested in better training stability, eg. Catformer or rezero or adding gates to the residual connections.
 I found I always got late NaN losses whenever the transformer blocks contained layer normalization. Removing layer normalization or using catformer (which has no layer normalization) this didn't happen. 
 I was curious about if anyone knows why this is the case, and what current ideas are about the necessity of layer normalization to the transformer.
    submitted by    /u/WigglyHypersurface  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Capabilities for Offensive Cyber Attacks - Community Survey]]></title>
        <id>https://www.reddit.com/r/artificial/comments/th4870/ai_capabilities_for_offensive_cyber_attacks/</id>
        <link href="https://www.reddit.com/r/artificial/comments/th4870/ai_capabilities_for_offensive_cyber_attacks/"/>
        <updated>2022-03-18T15:40:55.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 I'm working on my graduate paper regarding artificial intelligence being utilized for offensive based cyber attacks. Not just focusing on Adversarial AI, rather exploring AI use cases for malicious code generation or automated vulnerability discovery and exploitation. Would any SMEs within the community be interested in participating in a Chatham house rules interview regarding this topic?
    submitted by    /u/Vicarfort  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Difference between random and unknown variable in probability]]></title>
        <id>https://www.reddit.com/r/artificial/comments/th47j7/difference_between_random_and_unknown_variable_in/</id>
        <link href="https://www.reddit.com/r/artificial/comments/th47j7/difference_between_random_and_unknown_variable_in/"/>
        <updated>2022-03-18T15:40:09.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/IMPuzzled2  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Choosing EC2 instance with two goals in mind]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th3j3i/d_choosing_ec2_instance_with_two_goals_in_mind/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th3j3i/d_choosing_ec2_instance_with_two_goals_in_mind/"/>
        <updated>2022-03-18T15:09:24.000Z</updated>
        <summary type="html"><![CDATA[I’m running train tests on data that is quite large (1.5M, 400). I need to preprocess such data with a data transformation and model-based data imputation pipeline. Then I need to run train/test fits on a neural net that needs PyTorch and GPU. With this in mind, what is the most cost-effective aws ec2 instance for large data processing that has gpu as well? Right now I’m either running it all on a p3.2xlarge, or I use a c5d.9xlarge for the data preprocessing, save that file and then move back to p3.2xlarge to run the train/test. Second option is not ideal because I’d want to run several tests quickly even when changing around feature engineering or features selected.
    submitted by    /u/Gioamorim80  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] New paper on autonomous driving and multi-task: "HybridNets: End-to-End Perception Network"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th3bug/r_new_paper_on_autonomous_driving_and_multitask/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th3bug/r_new_paper_on_autonomous_driving_and_multitask/"/>
        <updated>2022-03-18T15:00:34.000Z</updated>
        <summary type="html"><![CDATA[Hello, we are publishing our first paper as undergraduate students today. It is achieving SOTA on the BDD100K dataset (2 out of 3 tasks, at least).
 ​
 Paper: https://arxiv.org/abs/2203.09035
 Code: https://github.com/datvuthanh/HybridNets
 ​
 Network architecture:
 HybridNets architecture
 ​
 Contributions:
  
HybridNets, an end-to-end perception network, achieving outstanding results in real-time on the BDD100K dataset for 3 tasks: traffic object detection, drivable area segmentation (not SOTA), and lane line detection.
 Automatically customized anchor for each level in the weighted bidirectional feature network, on any dataset.
 An efficient training loss function and training strategy to balance and optimize multi-task networks.
  
   submitted by    /u/xoiga123  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-generated utopias]]></title>
        <id>622e6d2b6bc9aa003d5451fe</id>
        <link href="https://www.aiweirdness.com/ai-generated-utopias/"/>
        <updated>2022-03-18T14:43:29.000Z</updated>
        <summary type="html"><![CDATA[AI isn't known for being able to solve the big problems, but what about the VERY large problems, such as possible futures to strive for? I decided to find out if I could get GPT-3 to come up with new ideas for utopias.
Since GPT-3 works by predicting]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonus: Utopias that make no sense]]></title>
        <id>622e84ba6bc9aa003d54527d</id>
        <link href="https://www.aiweirdness.com/bonus-utopias-that-make-no-sense/"/>
        <updated>2022-03-18T14:43:14.000Z</updated>
        <summary type="html"><![CDATA[AI Weirdness: the strange side of machine learning]]></summary>
        <author>
            <name>Janelle Shane</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Deep Batch Active Learning for Regression]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th1ea5/r_deep_batch_active_learning_for_regression/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th1ea5/r_deep_batch_active_learning_for_regression/"/>
        <updated>2022-03-18T13:31:58.000Z</updated>
        <summary type="html"><![CDATA[(First author here.) Are you interested in improving the sample-efficiency of neural network regression through active learning? Then, our new paper might be all you need 🙂
 Paper: https://arxiv.org/abs/2203.09410
 Code: https://github.com/dholzmueller/bmdal_reg
 Short summary: Using random projections of NN gradients as features and running a simple clustering method on these works really well, is scalable and convenient to use through our code.
 Long summary: We study pool-based batch mode deep active learning (BMDAL) for regression: We start with a small labeled training data set and repeatedly select a batch of unlabeled data from a large pool set for labeling. We want to select large batches since NN training can be slow.
 ​
 Deep Batch Active Learning loop
 We propose a benchmark wi…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does publishing in ML+Healthcare papers work]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th14fz/d_how_does_publishing_in_mlhealthcare_papers_work/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th14fz/d_how_does_publishing_in_mlhealthcare_papers_work/"/>
        <updated>2022-03-18T13:17:54.000Z</updated>
        <summary type="html"><![CDATA[I have started my PhD in application of machine learning in health care. I want to understand what it takes to publish in papers like Journal of the American Medical Informatics Association (JAMIA) . Are there any patterns in the publication that one can look out for? Do they always need the best state of the art results or a novel algorithm?
    submitted by    /u/Complex_State9960  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Attention]]></title>
        <id>https://medium.com/p/f2cfb4060e9c</id>
        <link href="https://becominghuman.ai/multi-head-attention-f2cfb4060e9c?source=rss----5e5bef33608a---4"/>
        <updated>2022-03-18T13:04:06.000Z</updated>
        <summary type="html"><![CDATA[Examining a module consisting of several attention layers running in parallel.]]></summary>
        <author>
            <name>m0nads</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hopped Up: NVIDIA CEO, AI Leaders to Discuss Next Wave of AI at GTC]]></title>
        <id>https://blogs.nvidia.com/?p=55927</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/18/nvidia-ceo-jensen-huang-gtc/"/>
        <updated>2022-03-18T13:00:25.000Z</updated>
        <summary type="html"><![CDATA[NVIDIA’s GTC conference is packed with smart people and programming. The virtual gathering — which takes place from March 21-24 — sits at the intersection of some of the fastest-moving technologies of our time. It features a lineup of speakers from every corner of industry, academia and research who are ready to paint a high-definition Read article >
The post Hopped Up: NVIDIA CEO, AI Leaders to Discuss Next Wave of AI at GTC appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>Brian Caulfield</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Best labeling tool for action localization from video?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th0qp3/d_best_labeling_tool_for_action_localization_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th0qp3/d_best_labeling_tool_for_action_localization_from/"/>
        <updated>2022-03-18T12:58:54.000Z</updated>
        <summary type="html"><![CDATA[I would like to discuss what people their experiences are with labeling tools for action localization in videos. Currently I only found CVAT to be somewhat decent (but still pretty bad). Besides this there are some Github repos that publish a (not/semi) working solution, but they also don't do the trick for me.
 My biggest issue with CVAT is that I can only upload 1 video per task and that it is limited to 500MB. 
 I would like to know what other tools people have used and what your experiences are?
    submitted by    /u/FreddyShrimp  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Paper Review - Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments (Video Walkthrough)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th0j46/d_paper_review_avoiding_catastrophe_active/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th0j46/d_paper_review_avoiding_catastrophe_active/"/>
        <updated>2022-03-18T12:47:29.000Z</updated>
        <summary type="html"><![CDATA[https://youtu.be/O_dJ31T01i8
 Catastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.
 ​
 OUTLINE:
 0:00 - Introduction
 1:20 - Paper Overview
 3:15 - Catastrophic forgetting in continuous and multi-task learning
 9:30 - Dendrites in biological neurons
 16:55 - Sparse representations in biology
 18:35 - Active dendrites in deep learning
 34:15 - Experiments on multi-task learning
 39:00 - Experiments in continual learning and adaptive prototyping
 49:20 - Analyzing the inner workings of the algorithm
 53:30 - Is this the same as just training a larger network?
 59:15 - How does this relate to attention mechanisms?
 1:02:55 - Final thoughts and comments
 ​
 Paper: https://arxiv.org/abs/2201.00042
 Blog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting
    submitted by    /u/ykilcher  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Mac Studio as a computer for machine learning and data science]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/th0f2f/d_mac_studio_as_a_computer_for_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/th0f2f/d_mac_studio_as_a_computer_for_machine_learning/"/>
        <updated>2022-03-18T12:41:57.000Z</updated>
        <summary type="html"><![CDATA[I'm curious what people think of Apple's new Mac Studio as a potential workstation for machine learning and data processing tasks?
 It seems like Tensorflow can makes use of the M1 chip, but not Pytorch yet - I don't know how these compare to traditional GPU setups though? More generally, do you think the Mac Studio would be a good dev machine and good for other data-based work? I'm not sure, as it seems like the Studio is being pitched for people doing things like video editing rather than coding. Thanks!
    submitted by    /u/uio8  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What The Stanford 2022 AI Index Tells us About AI Adoption]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tgxmfd/what_the_stanford_2022_ai_index_tells_us_about_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tgxmfd/what_the_stanford_2022_ai_index_tells_us_about_ai/"/>
        <updated>2022-03-18T09:42:28.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Beautiful-Credit-868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Nordic Probabilistic AI School (ProbAI) — June 13-17, 2022]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgxlg7/n_nordic_probabilistic_ai_school_probai_june_1317/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgxlg7/n_nordic_probabilistic_ai_school_probai_june_1317/"/>
        <updated>2022-03-18T09:40:35.000Z</updated>
        <summary type="html"><![CDATA[You are welcome to apply for the Nordic Probabilistic AI School (ProbAI) 2022 being held on June 13-17 in Helsinki (Finland).
 APPLY NOW — The application deadline is March 27.
 About ProbAI 2022
 The mission of the third Nordic Probabilistic AI School (ProbAI) is to provide an inclusive education environment serving state-of-the-art expertise in machine learning and artificial intelligence. The public, students, academia and industry are welcome to join ProbAI 2022.
 ProbAI is an intermediate to advanced level "summer" school with the focus on probabilistic machine learning. Covered are topics such as probabilistic models, variational approximations, deep generative models, latent variable models, normalizing flows, neural ODEs, probabilistic programming, and much more.
 The ProbAI 2022 w…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers From The Hartree Centre, IBM, And REPROCELL Propose An Explainable Machine Learning Approach That Combines Bioinformatics And Domain Insight To Inform Precision Medicine Strategies For Inflammatory Bowel Disease]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tgu3rs/researchers_from_the_hartree_centre_ibm_and/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tgu3rs/researchers_from_the_hartree_centre_ibm_and/"/>
        <updated>2022-03-18T05:27:31.000Z</updated>
        <summary type="html"><![CDATA[A project supported by the STFC Hartree Centre Discovery Accelerator accurately predicts patient response to treatments for ulcerative colitis and Crohn’s disease.
 Artificial intelligence may soon assist more than 6 million1 individuals worldwide who suffer from inflammatory bowel disease (IBD) in selecting the optimum medication for their illness. An explainable AI pharmacogenomics methodology we created effectively predicted how patients will respond — favorably or negatively — to an IBD treatment 95% of the time, according to research published in PLOSone.
 Chronic inflammatory bowel diseases (IBDs) such as ulcerative colitis and Crohn’s disease are caused by clinical, genetic, and environmental variables such as nutrition and lifestyle. Even though all patients have the same symptoms, there is no one-size-fits-all treatment for IBD that is helpful for everybody. Choosing the optimum therapy for a patient is still a trial-and-error procedure for both the doctor and the patient.
 According to researchers at IBM Research in the UK and REPROCELL, a stem cell and fresh tissue research firm, used IBD patient data and explainable AI approaches to study treatment reactions with the help of the STFC Hartree Centre’s Discovery Accelerator. Their objective was to discover the optimum medications for IBD therapies less of a guessing game. The resulting collection of algorithms demonstrated that it was feasible to crack the IBD data black box and comprehend forecast and explain how persons with IBD could react to different medications on the market and under development.
 Continue Reading Our Research Summary
 Paper: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263248
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How did you decide a research topic?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgtxkm/d_how_did_you_decide_a_research_topic/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgtxkm/d_how_did_you_decide_a_research_topic/"/>
        <updated>2022-03-18T05:15:50.000Z</updated>
        <summary type="html"><![CDATA[Those of you who did/doing a PhD recently, how did you decide your PhD topic? My advisor seems to give me a lot of freedom, by letting me choose my own topic, but has said if it is away from his interests he can't help a lot. He also would apply for funding, once I have a clear topic, which adds even more pressure. He wants me to write a survey paper, so that both of us can know the literature in the field. I've tried looking at the research by AI companies, but most of the areas are completely crowded. 
 Which brings me to my question, how do you go about choosing your topic? Was a broad idea given to you by your advisor? Were you also expected to know everything about your field? How did you have the confidence in selecting a topic, given most of recent papers is just randomly adding/shifting/removing layers (no offense).
 Also, what did you expect/get from your advisor? Given my situation, it seems like I'll have to run the show, which makes me anxious.
    submitted by    /u/Bibbidi_Babbidi_Boo  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares : Night Elf Forrest || Clip Guided Disco Diffusion AI Art Video [4K 60 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tgr4b7/artificial_nightmares_night_elf_forrest_clip/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tgr4b7/artificial_nightmares_night_elf_forrest_clip/"/>
        <updated>2022-03-18T02:34:34.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] On the difference (or lack thereof) between Cross-Entropy Loss and KL-Divergence]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgn6ma/d_on_the_difference_or_lack_thereof_between/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgn6ma/d_on_the_difference_or_lack_thereof_between/"/>
        <updated>2022-03-17T23:17:39.000Z</updated>
        <summary type="html"><![CDATA[So cross-entropy(H(p,q)) and KL-divergence (KL(p||q)) relate to each other as follows:
 H(p,q) = KL(p||q) + H(p) and KL(p||q) = H(p,q) - H(p)
 where p is the data distribution and q is the model distribution. When p is constant (as is the case in most ML problems), minimizing H(p,q) is equivalent to minimizing KL(p||q). However, there seems to be some ambiguity about this. (One practitioner claims)[https://stats.stackexchange.com/a/409271] that there is a difference in practice, because during batch gradient descent the data distribution p' in each batch is noisy and harder learn for the model, leading to worse performance for the KL-divergence.
 I am skeptical about his claim, as H(p) is part of both cross-entropy and KL-divergence, depending on how one views them. If anything, the KL-divergence should work better because it does not directly incorporate H(p). What is your experience / your thoughts?
    submitted by    /u/optimized-adam  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning - Fawkes - facial recognition [discussion]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgn6ik/machine_learning_fawkes_facial_recognition/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgn6ik/machine_learning_fawkes_facial_recognition/"/>
        <updated>2022-03-17T23:17:30.000Z</updated>
        <summary type="html"><![CDATA[Hello to everyone,
 At the moment exist any way to avoid facial recognition from social media? Not Fawkes.... Another way because I think AI learned that the picture is modified with this software. It is possible that the software recognize this pictures as fake face? Also combine fawkes with another software as Photoshop could be a good idea to look as another user? In social media the people use Photoshop... I don't know if is still possible to recognize easy
 Thank you
    submitted by    /u/Maleficent_Camel5718  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI: Deepfakes for Politicians, AI Propaganda, Operation Safety Net, Understanding Pigs]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tgmgt5/last_week_in_ai_deepfakes_for_politicians_ai/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tgmgt5/last_week_in_ai_deepfakes_for_politicians_ai/"/>
        <updated>2022-03-17T22:44:00.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Extracting "potential user profiles" from firewall logs]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgkj6t/p_extracting_potential_user_profiles_from/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgkj6t/p_extracting_potential_user_profiles_from/"/>
        <updated>2022-03-17T21:15:53.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone.
 For a project i need to find a way to extract some "potential user profiles" from firewall logs, such as the pic i attach to this post.
 For example i would like to infer that user A belongs to the "administrative" part of the company whyle Bob belongs to the tecnician department.
 Any hints on how to do this? I taught about using k-means on some certain fields such as destination, source, protocol.
 The problem is that k-means flags different records of the same user with different clusters, and i guess this may not be a good strategy (I might try to use some kind of smoothening in which i flatten the cluster, for example if BoB has labels 0 0 0 0 0 1 1 1 1 0 0 0 0 0 then bob has cluster 0).
 ​
 https://preview.redd.it/tehf5axof0o81.png?width=1489&format=png&auto=webp&s=720ab6f792e2d409735e662d602e76ac7909e23b
    submitted by    /u/Set-New  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In A Latest Machine Learning Research, NVIDIA Researchers Propose A Novel Critically-Damped Langevin Diffusion (CLD) For Score-Based Generative Modeling]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tghxm2/in_a_latest_machine_learning_research_nvidia/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tghxm2/in_a_latest_machine_learning_research_nvidia/"/>
        <updated>2022-03-17T19:19:16.000Z</updated>
        <summary type="html"><![CDATA[A promising family of generative models has emerged: score-based generative models (SGMs) and denoising diffusion probabilistic models. SGMs have applications in image, voice, and music synthesis, image editing, super-resolution, image-to-image translation, and 3D shape generation because they provide high-quality synthesis and sample variety without requiring adversarial aims.
 SGMs use a diffusion process to progressively introduce noise to the data, changing a complicated data distribution into a tractable prior distribution for analysis. The modified data’s score function—the gradient of the log probability density—is then learned using a neural network. To synthesize new samples, the learned scores can be used to solve a stochastic differential equation (SDE). Inverting the forward diffusion corresponds to an iterative denoising process.
 Continue Reading
 Paper: https://arxiv.org/pdf/2112.07068.pdf
 Project: https://nv-tlabs.github.io/CLD-SGM/
 Code: https://github.com/nv-tlabs/CLD-SGM
 ​
 https://i.redd.it/b2wxorotuzn81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In A Latest Machine Learning Research, NVIDIA Researchers Propose A Novel Critically-Damped Langevin Diffusion (CLD) For Score-Based Generative Modeling]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tghx92/in_a_latest_machine_learning_research_nvidia/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tghx92/in_a_latest_machine_learning_research_nvidia/"/>
        <updated>2022-03-17T19:18:48.000Z</updated>
        <summary type="html"><![CDATA[A promising family of generative models has emerged: score-based generative models (SGMs) and denoising diffusion probabilistic models. SGMs have applications in image, voice, and music synthesis, image editing, super-resolution, image-to-image translation, and 3D shape generation because they provide high-quality synthesis and sample variety without requiring adversarial aims.
 SGMs use a diffusion process to progressively introduce noise to the data, changing a complicated data distribution into a tractable prior distribution for analysis. The modified data’s score function—the gradient of the log probability density—is then learned using a neural network. To synthesize new samples, the learned scores can be used to solve a stochastic differential equation (SDE). Inverting the forward diffusion corresponds to an iterative denoising process.
 Continue Reading
 Paper: https://arxiv.org/pdf/2112.07068.pdf
 Project: https://nv-tlabs.github.io/CLD-SGM/
 Code: https://github.com/nv-tlabs/CLD-SGM
 ​
 https://i.redd.it/8dl9ftuquzn81.gif
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Domain-specific pre-training of GPT? Help!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tghpun/r_domainspecific_pretraining_of_gpt_help/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tghpun/r_domainspecific_pretraining_of_gpt_help/"/>
        <updated>2022-03-17T19:09:04.000Z</updated>
        <summary type="html"><![CDATA[I am looking to adapt GPT-2 to generate dialogue utterances in the style of a certain demography/population (let's call them Ogres). However, there are no large datasets that are both 1) dialogue datasets, and 2) are generated by this target demography.
 In the absence of such data, I have been considering a few approaches for data augmentation purposes. Many of those approaches would benefit from a GPT-Ogre, which is at least capable of generating text similar to Ogres, if not necessarily dialogic.
 Approach 1
 ==========
 For this, I am considering performing additional pre-training of, say, GPT-2 on some medium-sized corpora generated by Ogres. This sounds like something that should have been done by a lot of people for a lot of different things by now, but except for some papers that have tried to do this with BERT in the Medical domain, I was not able to find any papers/GitHub repos that have done this with additional unsupervised pre-training GPT.
 It would be helpful if someone could point me to some resources around this as I feel the space of hyperparameters to figure out the best learning rate, etc. is too large, and if somebody has already done this, it would be easy to replicate it.
 Approach 2
 ==========
 There are some dialogue-specific GPT models such as DialoGPT that have been fine-tuned (in a supervised way; mind you, not pretrained in an unsupervised way). However, it is not in the Ogre style. I am wondering if it's a ridiculous idea to perform additional pre-training of a fine-tuned GPT-2 model?
    submitted by    /u/exceptaway343  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build a traceable, custom, multi-format document parsing pipeline with Amazon Textract]]></title>
        <id>0d662d62f2ad7f0ee3b80b73ba595a2756ef4510</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/build-a-traceable-custom-multi-format-document-parsing-pipeline-with-amazon-textract/"/>
        <updated>2022-03-17T18:28:10.000Z</updated>
        <summary type="html"><![CDATA[Organizational forms serve as a primary business tool across industries—from financial services, to healthcare, and more. Consider, for example, tax filing forms in the tax management industry, where new forms come out each year with largely the same information. AWS customers across sectors need to process and store information in forms as part of their […]]]></summary>
        <author>
            <name>Emily Soward</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Optimization for Architecting Hardware Accelerators]]></title>
        <id>http://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html</id>
        <link href="http://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html"/>
        <updated>2022-03-17T18:04:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Amir Yazdanbakhsh, Research Scientist and Aviral Kumar, Student Researcher, Google Research 
Advances in machine learning (ML) often come with advances in hardware and computing systems. For example, the growth of ML-based approaches in solving various problems in vision and language has led to the development of application-specific hardware accelerators (e.g., Google TPUs and Edge TPUs). While promising, standard procedures for designing accelerators customized towards a target application require manual effort to devise a reasonably accurate simulator of hardware, followed by performing many time-intensive simulations to optimize the desired objective (e.g., optimizing for low power usage or latency when running a particular application). This involves identifying the right ba…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3 Questions: How the MIT mini cheetah learns to run]]></title>
        <id>https://news.mit.edu/2022/3-questions-how-mit-mini-cheetah-learns-run-fast-0317</id>
        <link href="https://news.mit.edu/2022/3-questions-how-mit-mini-cheetah-learns-run-fast-0317"/>
        <updated>2022-03-17T17:55:00.000Z</updated>
        <summary type="html"><![CDATA[CSAIL scientists came up with a learning pipeline for the four-legged robot that learns to run entirely by trial and error in simulation.]]></summary>
        <author>
            <name>Rachel Gordon | MIT CSAIL</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handheld surgical robot can help stem fatal blood loss]]></title>
        <id>https://news.mit.edu/2022/handheld-surgical-robot-can-help-stem-fatal-blood-loss-0317</id>
        <link href="https://news.mit.edu/2022/handheld-surgical-robot-can-help-stem-fatal-blood-loss-0317"/>
        <updated>2022-03-17T17:25:00.000Z</updated>
        <summary type="html"><![CDATA[The AI-Guided Ultrasound Intervention Device is a lifesaving technology that helps a range of users deliver complex medical interventions at the point of injury.]]></summary>
        <author>
            <name>Anne McGovern | MIT Lincoln Laboratory</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] Restoring and attributing ancient texts using deep neural networks]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgeo0q/r_restoring_and_attributing_ancient_texts_using/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgeo0q/r_restoring_and_attributing_ancient_texts_using/"/>
        <updated>2022-03-17T16:52:51.000Z</updated>
        <summary type="html"><![CDATA[Ancient history relies on disciplines such as epigraphy — the study of inscribed texts known as inscriptions — for evidence of the thought, language, society and history of past civilizations. However, over the centuries, many inscriptions have been damaged to the point of illegibility, transported far from their original location and their date of writing is steeped in uncertainty.
 This work presents Ithaca, a deep neural network for the textual restoration, geographical attribution and chronological attribution of ancient Greek inscriptions.
 Paper: https://www.nature.com/articles/s41586-022-04448-z
 Code: https://github.com/deepmind/ithaca
 Online interface: https://ithaca.deepmind.com/
 Blog: https://deepmind.com/blog/article/Predicting-the-past-with-Ithaca
 Video: https://www.youtube.com/watch?v=rq0Ex_qCKeQ
    submitted by    /u/yannisassael  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic StyleGAN - A novel approach for face generation and editing]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tgelgj/semantic_stylegan_a_novel_approach_for_face/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tgelgj/semantic_stylegan_a_novel_approach_for_face/"/>
        <updated>2022-03-17T16:49:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/imapurplemango  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] PyTorch Lightning vs DeepSpeed vs FFCV vs mosaic vs ... what are they and how can you use them?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgefsw/discussion_pytorch_lightning_vs_deepspeed_vs_ffcv/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgefsw/discussion_pytorch_lightning_vs_deepspeed_vs_ffcv/"/>
        <updated>2022-03-17T16:42:41.000Z</updated>
        <summary type="html"><![CDATA[Deepspeed? FSDP? FFCV? XYZK? what do they all mean and how can you use all of them to speed up your model training? All amazing techniques developed by world-class teams and are (or are being made) accessible via PyTorch Lightning!
 If you know of other techniques you want to be integrated, please comment below!
 https://william-falcon.medium.com/pytorch-lightning-vs-deepspeed-vs-fsdp-vs-ffcv-vs-e0d6b2a95719
    submitted by    /u/waf04  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-modified short story study]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tgcffk/aimodified_short_story_study/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tgcffk/aimodified_short_story_study/"/>
        <updated>2022-03-17T15:11:59.000Z</updated>
        <summary type="html"><![CDATA[Hi all! Anybody interested in reading an AI-modified story? I created a user study where a short story of about 3000 words has been modified with NLP, and one of the various versions is shown to the reader. The system gives you a very short personality test, and asks you a few questions on what you thought of the story. There are Amazon vouchers worth 5 GBP available for those doing this now, in any country! People found not to have done the study carefully enough might be excluded. It's at https://cci.arts.ac.uk/~wnybom/cloak.html
    submitted by    /u/wnybom  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking my neural network implementation.]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tgc8yo/benchmarking_my_neural_network_implementation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tgc8yo/benchmarking_my_neural_network_implementation/"/>
        <updated>2022-03-17T15:03:40.000Z</updated>
        <summary type="html"><![CDATA[Hi, I programmed a neural network "framework" in C++ which uses CUDA. It's my own implementation, and it's probably really slow, since I'm a beginner and don't even know how oop works (it's all one big file). The project is more about understanding the math behind it, but I still want to know the algorithm's performance in relation to some widely used frameworks like TensorFlow or Keras. It does manage to keep the GPU at 80 - 90% usage for a batch sizes of around 25-50 and 32-64 neurons per layer, if that says anything.
 However, is there something like "calculated derivatives per second", as a measurement of the speed of the implementation? That would be really nice to know.
 My implementation updates the network around 13 times per second with a batch size of 50 and a topology of { 6,32,32,6 }. How slow is that?
 I tried to train the network to calculate the acceleration present in a 3 body planet system. It struggles to do so, although the adam optimizer really helped. I think it got the first digit of the acceleration close to consistently right after one night of training, but it seemed, as if the learning speed was accelerating. The topology was { 6,64,64,6 }, batch size 50 and the learning rate 0.025.
 English is not my native language, so please forgive my mistakes.
    submitted by    /u/qwedp  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Is clean real-world data important for model training?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgbs6u/discussion_is_clean_realworld_data_important_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgbs6u/discussion_is_clean_realworld_data_important_for/"/>
        <updated>2022-03-17T14:42:01.000Z</updated>
        <summary type="html"><![CDATA[Hi reddit experts,
 I know that for any machine learning and AI application, you need data in order to train your model. I would like to ask the following:
  
Where does this data usually come from? (For instance, is this data provided by the customer? Your professor? Is it always assumed that this data will be available? Can you trust the source?)
 If you don’t already have this data, will you be asked to implement a system to collect this data?
 Would the accuracy of this data be important to the model? Meaning, let’s say if we wanted to train the model of an AI temperature controller. Would it really matter if the data was just 25 degrees? Versus 25.58 degrees?
 Is it important that this data is sampled fast enough? Meaning, if I only had 1 data point per day, versus 10 data points per minute?
  
Here’s some context to why I’m asking. I’ve been a measurement and instrumentation engineer for a good part of my career. We set up measurement systems that allow customers to collect all kinds of real-world data, including temp, vibration, voltage, images, etc.
 The other day I had a debate with a friend who does AI, where he claimed that “collecting the data" was a trivial effort, because most of time he just uses the data that’s been given to him. But I challenged him, because there's lots of know-how needed in acquiring real-world physical data (for ex: which sensors to use, how to avoid noise, how to synchronize timestamps, etc.) All of this know-how ensures that the collected data is accurate, clean, and complete.
 But he just kinda brushed this off, and said that most ML/AI algorithms nowadays just look at the general trend of data, absolute accuracy doesn’t really matter, but there still needs to be some relative variability, then he can work with it. I’m having a hard time believing this is true. Can anyone shed some light on this subject?
    submitted by    /u/sharkera130  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is AI intrinsically hyped?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tgayxs/is_ai_intrinsically_hyped/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tgayxs/is_ai_intrinsically_hyped/"/>
        <updated>2022-03-17T14:02:54.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/bendee983  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Research] A great resource for Risk-Averse RL!]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tgaxy2/research_a_great_resource_for_riskaverse_rl/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tgaxy2/research_a_great_resource_for_riskaverse_rl/"/>
        <updated>2022-03-17T14:01:44.000Z</updated>
        <summary type="html"><![CDATA[https://www.cs.unh.edu/~mpetrik/tutorials/risk/
    submitted by    /u/Ok_Can2425  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[7 Best Free Datacamp Course to become a Data Scientist]]></title>
        <id>https://medium.com/p/9f967f1c5da7</id>
        <link href="https://becominghuman.ai/7-best-free-datacamp-course-to-become-a-data-scientist-9f967f1c5da7?source=rss----5e5bef33608a---4"/>
        <updated>2022-03-17T13:59:39.000Z</updated>
        <summary type="html"><![CDATA[These are my favorite free Datacamp courses to learn in-demand data skills like Python, SQL, Power BI, Tableau, Seaborn, Matplotlib, Data…]]></summary>
        <author>
            <name>javinpaul</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural network for keystroke biometrics]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tgatkc/neural_network_for_keystroke_biometrics/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tgatkc/neural_network_for_keystroke_biometrics/"/>
        <updated>2022-03-17T13:56:01.000Z</updated>
        <summary type="html"><![CDATA[Hi all,
 I apologize for the lack of technical knowledge here, as I have a biology background and am so very much out of my wheelhouse here.
 I was wondering if it was possible to just get a neural network that has already been trained to analyze keystroke biometric data and build profiles off of that information. We are having participants type out a sentence from Alice in Wonderland with a character limit of 100 characters.
 Background: I am a forensic science masters student with a biology background. My masters program has a cybercrime course where we have to conduct a research project with a digital forensic focus, and we've had very little guidance thus far. My project group decided to do a keystroke biometric comparison between English typing vs foreign language typing to see if both profiles could be tied back to one individual (eg, does switching between languages when typing affect typing manner and thus change the keystroke profile of one individual). Unfortunately, this has become a much more technical project than anticipated, and , as I mentioned, NONE of us has any real digital background. Our lecturer just sort of left us hanging and said "cool, go build and train your own neural network for this project", while my group has no idea HOW to actually do that.
 Thank you so much for any and all help! As I said, I deeply apologize for my lack of knowledge in this subject, and hope my post here is adequate.
    submitted by    /u/determinedkorra  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First visit and Every visit estimate of value of non terminal state]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tgag9j/first_visit_and_every_visit_estimate_of_value_of/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tgag9j/first_visit_and_every_visit_estimate_of_value_of/"/>
        <updated>2022-03-17T13:38:09.000Z</updated>
        <summary type="html"><![CDATA[Hi, I was going through the below question picked from reinforcement learning book (sutton & barto)
 ​
 Consider an MDP with a single non terminal state and a single action that transitions back to the non-terminal state with probability p and transitions to the terminal state with probability 1-p. Let the reward be +1 on all transitions and let gamma = 1 (discount factor). Suppose you observe one episode that last for 10 steps, with a return of 10. What are the first-visit and every visit estimators of the value of the non terminal state. 
 ​
 Hi, I am not looking for the answer but the steps that take you to the answer. I am beginner in Reinforcement learning. Please help me. Thank you.
    submitted by    /u/Tricky-Jello-7847  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agent types vs Algorithms]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tga3zm/agent_types_vs_algorithms/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tga3zm/agent_types_vs_algorithms/"/>
        <updated>2022-03-17T13:21:31.000Z</updated>
        <summary type="html"><![CDATA[Hi,
 I recently enrolled for AI in a local college, I find AI as a whole fascinating. One problem that I've run into is, the concept of user agents.
 From my understanding, user agents are the entities that try to obtain a goal, such as a human playing chess, here the human would be the user agent. The humans sensors would be their eyes and brain. The humans actuators would be their hands. Similarly an autonomous taxi could be a user agent, the taxis sensors could be cameras, speedometer, etc. It's actuators could be steering, breaks, horn, signals etc. 
 If you can tell by the latter example above, I'm currently reading AI: A modern approach(3rd edition).
 My question is; I've learned about some basic search algorithms such as DFS and BFS. I've implemented a DFS algorithm to solve a maze ie get to the exit node. What is my user agent in this program? Is the user agent the algorithm itself(DFS)?? Or do I have to create a class and put the algorithm as a function(I'm using Python).
 Thanks :)
    submitted by    /u/Adam20188  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Everyone’s a PC Gamer This GFN Thursday]]></title>
        <id>https://blogs.nvidia.com/?p=55928</id>
        <link href="https://blogs.nvidia.com/blog/2022/03/17/geforce-now-thursday-march-17/"/>
        <updated>2022-03-17T13:00:42.000Z</updated>
        <summary type="html"><![CDATA[It’s never been easier to be a PC gamer. GeForce NOW is your gateway into PC gaming. With the power of NVIDIA GeForce GPUs in the cloud, any gamer can stream titles from the top digital games stores — even on low-powered hardware. Evolve to the PC gaming ranks this GFN Thursday and get ready Read article >
The post Everyone’s a PC Gamer This GFN Thursday appeared first on NVIDIA Blog.]]></summary>
        <author>
            <name>GeForce NOW Community</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual AI: how to perform text processing/text generation in non-English languages]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tg84vw/multilingual_ai_how_to_perform_text/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tg84vw/multilingual_ai_how_to_perform_text/"/>
        <updated>2022-03-17T11:34:40.000Z</updated>
        <summary type="html"><![CDATA[Hello,
 Text processing AI has made great progress these last years but the main focus is on the English language (understandably). I think that many people are trying to do Natural Language Processing in non-English languages but are disappointed by the results. It is especially hard with text generation models like GPT-3, GPT-J, GPT-NeoX...
 In this article, I'm trying to quickly summarize what the options are today for people trying to use a multilingual AI:
 https://nlpcloud.io/multilingual-nlp-how-to-perform-nlp-in-non-english-languages.html
 If you can think of additional solutions not mentioned in this article please let me know!
    submitted by    /u/juliensalinas  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPU's and Memory [D]]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tg76ns/gpus_and_memory_d/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tg76ns/gpus_and_memory_d/"/>
        <updated>2022-03-17T10:33:52.000Z</updated>
        <summary type="html"><![CDATA[Hello! just made my first gan and am keen to try make one in 1024x1024
 I am looking to upgrade my gpu processing power by adding a RTX A4000 16g to my existing RTX 2080 8g
 will the memory stack as I have been working around some cuda memory allocation issues so far its working but Id like to know before I put down that kind of money will I be able to go past a 8g limit if I add the A4000 to my rig
 I have tried googling this question to no clear answer, feel free to link something related to this.
    submitted by    /u/Trainsmurf  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Discussion] Anyone know some Machine Learning Games Similar to Wobbledogs or Creatures 3?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tg4gpw/discussion_anyone_know_some_machine_learning/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tg4gpw/discussion_anyone_know_some_machine_learning/"/>
        <updated>2022-03-17T07:19:47.000Z</updated>
        <summary type="html"><![CDATA[I am obsessed with machine learning despite not being very smart at it myself. But specifically, I love it in games. I love interacting with it. I'm looking for more games like these:
 I play NovelAI, Creatures 3, Wobbledogs, AI Dungeon, Replika, and I was going to pick up Species: Artificial Life Real Evolution. Someone made a hook of what I think is GPT 2.7B to Crusader Kings so that you could talk to the different countries' leaders.
 Wobbledogs uses it for dog's walking. Motivation and desire are present. It also has positive / negative reinforcement.
 AI Dungeon, Replika, and NovelAI are all GPT - OpenAI GPT Da Vinci (for AID Dragon), OpenAI GPT 2.0 (of some sort, for Replika), and Fairseq / EleutherAI GPT models (for NovelAI). 
 Creatures 3 was one of the first pseudo(?) neural network games and utilizes motivation, desire, and positive / negative reinforcement.
 Species: Artificial Life Real Evolution is an abandoned game which simulated lifeforms evolving in stressful situations. They don't seem to actually be neural network, but it's close enough to be very interesting - creatures have motivations and desires but do not experience positive / negative reinforcement. There's also 60,000 of them at once... I guess it's sort of also got generational adaptation!
 PS, if anyone wants to obsess over these awesome games with me, let's do it!
    submitted by    /u/WiIdCherryPepsi  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] New paper on Tabular DL: "On Embeddings for Numerical Features in Tabular Deep Learning"]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tg4agu/r_new_paper_on_tabular_dl_on_embeddings_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tg4agu/r_new_paper_on_tabular_dl_on_embeddings_for/"/>
        <updated>2022-03-17T07:06:55.000Z</updated>
        <summary type="html"><![CDATA[Hi! We introduce our new paper "On Embeddings for Numerical Features in Tabular Deep Learning".
 Paper: https://arxiv.org/abs/2203.05556
 Code: https://github.com/Yura52/tabular-dl-num-embeddings
 TL;DR: using embeddings for numerical features (i.e. using vector representations instead of scalar values) can lead to significant profit for tabular DL models.
 Let's consider the vanilla MLP taking two numerical inputs.
 https://preview.redd.it/yb55tdw27wn81.png?width=330&format=png&auto=webp&s=a6fc53e8611baee6993aab47480f0a6a6b85e46c
 Now, here is the same MLP, but now with embeddings for numerical features:
 https://preview.redd.it/zebl8tld7wn81.png?width=368&format=png&auto=webp&s=3d20652075d0543c7d6c70f34d67140bc2c6346b
 The main contributions:
  
we show that using vector representations instead of scalar representations for numerical features can lead to significant profit for tabular DL models
 we show that MLP-like models equipped with embeddings can perform on par with Transformer-based models
 we make some progress in the "DL vs GBDT" competition
  
   submitted by    /u/Yura52  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Unsupervised Feature Selection using Supervised Algorithms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tg3nbf/p_unsupervised_feature_selection_using_supervised/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tg3nbf/p_unsupervised_feature_selection_using_supervised/"/>
        <updated>2022-03-17T06:21:31.000Z</updated>
        <summary type="html"><![CDATA[FRUFS is a Feature Relevance based Unsupervised Feature Selection approach using supervised algorithms such as XGBoost, etc. This is a new algorithm that I developed and put down in the form of a blog. Here's the link to the article. While reading the blog you will also re-discover an influential research paper in this domain on your own!
 FRUFS was evaluated on 4 different datasets under both unsupervised and supervised settings. The results are available on the blog as well as in the table below.
  
 Dataset Task All Features FRUFS Metric 
  
 MNIST Unsupervised 50.48 53.70 NMI 
  Waveform Unsupervised 38.20 39.67 NMI 
  Ionosphere Supervised 88.01 91.45 Accuracy 
  Adult Supervised 62.16 62.65 F1-score 
 
 ​
 Fig. 1 shows the feature importance of FRUFS (left) in comparison to the actual feature importance (right) on the MNIST dataset. This was done without any labels.
 Fig. 1: Visualizing feature/pixel importance of MNIST dataset
 ​
 I have also developed a scikit-learn compatible library in case you want to use FRUFS for your own project. Here's the github repo. A simple pip install FRUFS will do the trick!
 Do give the blog a read and let me know your thoughts.
    submitted by    /u/atif_hassan  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Has anybody used RecSim NG from Google for online recommender system simulation?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tg3b3c/has_anybody_used_recsim_ng_from_google_for_online/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tg3b3c/has_anybody_used_recsim_ng_from_google_for_online/"/>
        <updated>2022-03-17T05:58:27.000Z</updated>
        <summary type="html"><![CDATA[I was wondering if RecSim NG is actively used for research. Seems like there are limited resources and demos to play with.
    submitted by    /u/Few-Cap-5989  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[self-supervised RL implementation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tg2ud7/selfsupervised_rl_implementation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tg2ud7/selfsupervised_rl_implementation/"/>
        <updated>2022-03-17T05:27:21.000Z</updated>
        <summary type="html"><![CDATA[Hi, I reimplemented Behavior From the Void: Unsupervised Active Pre-Training(official implementation https://github.com/rll-research/url_benchmark)
 Because, their APT only supports vector-based environment and ICM-based implementation(that is different version from APT paper). So I reimplemented it as paper described and replemented APT shows very interesting at first(showed significant behaviors and received 20~30 episode reward constantly and deceased apt encoder error) but at actual transfer step, It shows very bad performance. Q value looks overestimated when training Q network in pretraining step. 
 Orange : scratch DrQ, gray : Finetuned version after APT 2M frames training 
 ​
 Orange : scratch DrQ, gray : Finetuned version after APT 2M frames training 
 What should I do training APT correctly? My implementation is based on DrQ and APT official implementation https://github.com/seolhokim/APT
 Really Thanks for reading.
    submitted by    /u/Spiritual_Fig3632  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Microsoft Introduces ‘PeopleLens’: An Open-Ended Artificial Intelligence System That Uses Computer Vision Algorithms To Help Young People Who Are Blind To Engage With Their Immediate Social Surroundings]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tg2lj1/microsoft_introduces_peoplelens_an_openended/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tg2lj1/microsoft_introduces_peoplelens_an_openended/"/>
        <updated>2022-03-17T05:11:52.000Z</updated>
        <summary type="html"><![CDATA[Social engagement can be challenging for children who are born blind. Despite a great desire to do so, many blind children and young people with low vision fail to engage and befriend individuals in their age group. This can be extremely difficult for the kid or adolescent, as well as their support network of family members and instructors who wish to assist them in making these crucial connections.
 Microsoft team has recently developed “PeopleLens,” an open-ended AI system that provides additional resources to people who are blind or have low vision to make sense of and interact with their local social settings. The system uses Nreal Light augmented reality glasses connected to a smartphone, allowing them to expand their existing talents and abilities.
 Quick Read: https://www.marktechpost.com/2022/03/16/microsoft-introduces-peoplelens-an-open-ended-artificial-intelligence-system-that-uses-computer-vision-algorithms-to-help-young-people-who-are-blind-to-engage-with-their-immediate-social-surround/ 
 Paper: https://www.microsoft.com/en-us/research/uploads/prod/2021/06/Morrison-Interactions2021\_PeopleLens.pdf
 Microsoft Blog: https://www.microsoft.com/en-us/research/blog/peoplelens-using-ai-to-support-social-interaction-between-children-who-are-blind-and-their-peers/ 
 https://preview.redd.it/wvol1y7nnvn81.png?width=1542&format=png&auto=webp&s=d95e16d28797ce795634902cf02a125a05839312
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Project]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tg232p/new_project/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tg232p/new_project/"/>
        <updated>2022-03-17T04:40:31.000Z</updated>
        <summary type="html"><![CDATA[I created a new project, and am currently looking for people willing to help: https://botbox.dev/ice-dragon-ai/ it's a free AI art generator
 If you are willing to help, either email me (you can find it in the link) or DM me on reddit
    submitted by    /u/Recent_Coffee_2551  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Nightmares : Call of Cthulhu || Clip Guided Disco Diffusion AI Art Video [4K 60 FPS]]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tg1l1p/artificial_nightmares_call_of_cthulhu_clip_guided/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tg1l1p/artificial_nightmares_call_of_cthulhu_clip_guided/"/>
        <updated>2022-03-17T04:10:07.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Thenamessd  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning", Hujiben et al 2021]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tfxxp9/a_review_of_the_gumbelmax_trick_and_its/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tfxxp9/a_review_of_the_gumbelmax_trick_and_its/"/>
        <updated>2022-03-17T00:53:40.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Policy improvement by planning with Gumbel", Danihelka et al 2021 {DM} (Gumbel AlphaZero/Gumbel MuZero)]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tfxv50/policy_improvement_by_planning_with_gumbel/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tfxv50/policy_improvement_by_planning_with_gumbel/"/>
        <updated>2022-03-17T00:50:04.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/gwern  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in the News (March 16th, 2022)]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfwmej/artificial_intelligence_in_the_news_march_16th/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfwmej/artificial_intelligence_in_the_news_march_16th/"/>
        <updated>2022-03-16T23:46:38.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Beautiful-Credit-868  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Do you think CXL and PCIe 5.0 will be a game changer for ML?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfw49a/d_do_you_think_cxl_and_pcie_50_will_be_a_game/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfw49a/d_do_you_think_cxl_and_pcie_50_will_be_a_game/"/>
        <updated>2022-03-16T23:21:49.000Z</updated>
        <summary type="html"><![CDATA[https://venturebeat.com/2021/11/15/astera-labs-announces-memory-acceleration-to-clear-datacenter-ai-ml-bottlenecks/ 
 Do you think this technology would allow the use of clusters to handle larger data sets, thus reducing the overall cost of doing ML?
    submitted by    /u/sawine  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NHS Introduces A New AI-Based Technology That Can Detect Heart Disease At Record Speed And With 40 Percent Higher Accuracy]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfv86g/nhs_introduces_a_new_aibased_technology_that_can/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfv86g/nhs_introduces_a_new_aibased_technology_that_can/"/>
        <updated>2022-03-16T22:40:31.000Z</updated>
        <summary type="html"><![CDATA[The NHS is now employing a cutting-edge AI program that can diagnose heart illness in just 20 SECONDS.
  
Experts claim that the computer tool replicates human abilities but with more precision.
 When the patient is in the scanner, it analyses cardiac MRI images in about 20 seconds.
 This is far faster than a human doctor would take, which may take up to 13 minutes.
 It can also identify changes in the heart’s anatomy with a 40 percent higher accuracy.
  
While the patient is in the scanner, the computer tool, which resembles human ability but with more precision and speed, can analyze cardiac MRI data in 20 seconds.
 Continue Reading The Research Summary
 https://preview.redd.it/rg4nmzlqptn81.jpg?width=6000&format=pjpg&auto=webp&s=c8f99ff5479cf9d5dfe6184073cc86664bbbba17
    submitted by    /u/No_Coffee_4638  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How does an Model-Agnostic Meta-Learning model know what tasks needs to perform?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfua0f/d_how_does_an_modelagnostic_metalearning_model/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfua0f/d_how_does_an_modelagnostic_metalearning_model/"/>
        <updated>2022-03-16T21:57:13.000Z</updated>
        <summary type="html"><![CDATA[I was reading the paper Model-Agnostic Meta-Learning and that part wasn't specified, or at least I couldn't find it.
 More specifically, a model is trained to perform n different tasks. At inference, how do people typically inform the model what tasks need to be performed? is it as simple as adding an extra input with the task to be performed, or do people use more sophisticated ways to do that? For example, the paper has an example where they train an NN on sines with different amplitudes and phases. In that case, the inputs are all identical, but the outputs shouldn't be., which in the paper they aren't.
    submitted by    /u/carlml  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finally an official MuZero implementation]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tfu624/finally_an_official_muzero_implementation/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tfu624/finally_an_official_muzero_implementation/"/>
        <updated>2022-03-16T21:52:02.000Z</updated>
        <summary type="html"><![CDATA[deepmind/mctx: Monte Carlo tree search in JAX (github.com)
    submitted by    /u/jack281291  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[R] LIME and explainable AI]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfu361/r_lime_and_explainable_ai/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfu361/r_lime_and_explainable_ai/"/>
        <updated>2022-03-16T21:48:13.000Z</updated>
        <summary type="html"><![CDATA[I am currently doing some research about explainable AI and especially LIME framework.. are there any useful resources i can start with ?? also what are possible research gaps in that area?
    submitted by    /u/Aromatic-Ad-2235  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LIME]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tftwub/lime/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tftwub/lime/"/>
        <updated>2022-03-16T21:40:24.000Z</updated>
        <summary type="html"><![CDATA[what are the best resources for studying LIME models in XAI? also what are the research gaps in this area?
    submitted by    /u/Aromatic-Ad-2235  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US Chamber Launches Commission on Artificial Intelligence to Advance U.S. Leadership]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfta12/us_chamber_launches_commission_on_artificial/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfta12/us_chamber_launches_commission_on_artificial/"/>
        <updated>2022-03-16T21:11:14.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/HotMomentumStocks  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The media has often times portrayed AI in a bad light, what are the positive applications of AI which could help us exist better in the near future that it hasn't done already?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tft92z/the_media_has_often_times_portrayed_ai_in_a_bad/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tft92z/the_media_has_often_times_portrayed_ai_in_a_bad/"/>
        <updated>2022-03-16T21:10:06.000Z</updated>
        <summary type="html"><![CDATA[Hello! This is my first post here so sorry if I don't phrase my topic of discussion well. For context, I'm doing a conceptual project about creating a more positive narrative on our future with AI focusing on one aspect of how it can benefit us in our daily lives. It will be an illustrated story to tell said narrative, not so much a research document (not smart enough for that). 
 The question basically is, what are your thoughts about the different ways and areas AI could improve our lives in which it hasn't done so already? Where is AI lacking today? What could it do more of? 
 Ideas can be grounded such as AI robots serving as caretakers for the elderly in their homes or more sci fi-like such as having AI colonize Mars for us instead of humans.
 Go wild! Would love to hear your thoughts!
    submitted by    /u/Current-Development5  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Composer: a new PyTorch library to train models ~2-4x faster with better algorithms]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tft608/p_composer_a_new_pytorch_library_to_train_models/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tft608/p_composer_a_new_pytorch_library_to_train_models/"/>
        <updated>2022-03-16T21:06:16.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/moinnadeem  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] ICPR 2022 ODeuropa Competition on Olfactory Object Recognition (ODOR)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfs8p3/n_icpr_2022_odeuropa_competition_on_olfactory/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfs8p3/n_icpr_2022_odeuropa_competition_on_olfactory/"/>
        <updated>2022-03-16T20:24:20.000Z</updated>
        <summary type="html"><![CDATA[Call for participation in the ICPR 2022 ODeuropa Competition on Olfactory Object Recognition (ODOR)! 
 Task: Smell-related object detection in artworks 
 It's the world’s first competition for the detection on olfactory objects on historical artworks. Work with a data set of >24000 object annotations in 87 categories on ~3000 images and create innovative solutions to detect a wide range of objects in the challenging domain of artworks. 
 Smell-related object detection in artworks; image source: https://www.mauritshuis.nl/en/our-collection/artworks/742-as-the-old-sing-so-pipe-the-young/
 Please visit https://odor-challenge.odeuropa.eu for further details on how to participate!
    submitted by    /u/vchristlein  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Week in AI: AI to detect problematic gambling, police surveillance of protestors and journalists, low-cost way to tune large AI models, and more!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfs6zj/last_week_in_ai_ai_to_detect_problematic_gambling/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfs6zj/last_week_in_ai_ai_to_detect_problematic_gambling/"/>
        <updated>2022-03-16T20:22:05.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/regalalgorithm  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amazon SageMaker JumpStart models and algorithms now available via API]]></title>
        <id>36f831377de824a2ca70ae5eb8f237aff18228dc</id>
        <link href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-jumpstart-models-and-algorithms-now-available-via-api/"/>
        <updated>2022-03-16T19:46:59.000Z</updated>
        <summary type="html"><![CDATA[In December 2020, AWS announced the general availability of Amazon SageMaker JumpStart, a capability of Amazon SageMaker that helps you quickly and easily get started with machine learning (ML). JumpStart provides one-click fine-tuning and deployment of a wide variety of pre-trained models across popular ML tasks, as well as a selection of end-to-end solutions that […]]]></summary>
        <author>
            <name>Vivek Madan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How artificial intelligence can help combat systemic racism]]></title>
        <id>https://news.mit.edu/2022/how-ai-can-help-combat-systemic-racism-0316</id>
        <link href="https://news.mit.edu/2022/how-ai-can-help-combat-systemic-racism-0316"/>
        <updated>2022-03-16T19:40:00.000Z</updated>
        <summary type="html"><![CDATA[MLK Visiting Professor S. Craig Watkins looks beyond algorithm bias to an AI future where models more effectively deal with systemic inequality.]]></summary>
        <author>
            <name>Scott Murray | Institute for Data, Systems, and Society</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[“Adaptive Valley” visuals generated with Disco Diffusion]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfqye3/adaptive_valley_visuals_generated_with_disco/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfqye3/adaptive_valley_visuals_generated_with_disco/"/>
        <updated>2022-03-16T19:36:17.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/Im_Will_Smith  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dividing an octave into 14 pieces]]></title>
        <id>https://www.johndcook.com/blog/?p=100171</id>
        <link href="https://www.johndcook.com/blog/2022/03/16/14-note-scale/"/>
        <updated>2022-03-16T19:07:11.000Z</updated>
        <summary type="html"><![CDATA[Keenan Pepper left a comment on my previous post saying that the DTMF tones used by touch tone phones “are actually quite close to 14 equal divisions of the octave (rather than the usual 12).” Let’s show that this is right using a little Python. import numpy as np freq = np.array([697, 770, 852, 941, […]
Dividing an octave into 14 pieces first appeared on John D. Cook.]]></summary>
        <author>
            <name>John</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will robots take our jobs?]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfpi0j/will_robots_take_our_jobs/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfpi0j/will_robots_take_our_jobs/"/>
        <updated>2022-03-16T18:57:06.000Z</updated>
        <summary type="html"><![CDATA[According to the World Economic Forum's "The Future of Jobs Report 2020", AI is expected to replace 85 million jobs worldwide by 2025. So if AI gets smarter and replace our jobs, we don’t have to work anymore but we can get whatever we need for a minimum standard of living without paying money?
    submitted by    /u/Dayoshiime18  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] How do you measure the business impact of the ML solutions in your company?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfor5y/d_how_do_you_measure_the_business_impact_of_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfor5y/d_how_do_you_measure_the_business_impact_of_the/"/>
        <updated>2022-03-16T18:31:30.000Z</updated>
        <summary type="html"><![CDATA[For those who have deployed models to production, I was wondering what are some ways to track the business impact of a model/solution once deployed.
 I haven't come across a lot of information on tracking the performance of models against business/user metrics.
 If you use analytics data, what type of data do you collect? And how do you A/B test against different models? Is there some tooling available that could help with that?
 Any help is appreciated! :)
    submitted by    /u/nathaliamdc  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] Vertical first, horizontal second. Why you should break through to production early when developing machine learning systems and how MLOps facilitates this.]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfnwsk/d_vertical_first_horizontal_second_why_you_should/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfnwsk/d_vertical_first_horizontal_second_why_you_should/"/>
        <updated>2022-03-16T17:53:37.000Z</updated>
        <summary type="html"><![CDATA[https://medium.com/p/306fa7b7a80b
 I believe a common misconception is that you only need to apply MLOps principles and tools if you are running hundreds of models. I'd argue it's not less important in a lot earlier stages of the model lifecycle.
    submitted by    /u/stiebels  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[17 Best Datacamp Courses 2022 Data Science, Python, R, ML, SQL -]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfmlm6/17_best_datacamp_courses_2022_data_science_python/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfmlm6/17_best_datacamp_courses_2022_data_science_python/"/>
        <updated>2022-03-16T16:55:47.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/sivasiriyapureddy  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[N] Live and open training of BigScience's 176B multilingual language model has just started]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfm7zb/n_live_and_open_training_of_bigsciences_176b/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfm7zb/n_live_and_open_training_of_bigsciences_176b/"/>
        <updated>2022-03-16T16:38:22.000Z</updated>
        <summary type="html"><![CDATA[The [BigScience project](https://bigscience.huggingface.co) has just started the training of its main model and the training can be followed live here: https://twitter.com/BigScienceLLM and here: https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss
 Here are more information on the model, dataset, engineering, training and hardware:
  
The model:
  
 176B parameters decoder-only architecture (GPT-like)
 70 layers - 112 attention heads per layers - hidden dimensionality of 14336 - 2048 tokens sequence length
 ALiBi positional embeddings - GeLU activation function
 Read more: 
 Blog post summarizing how the architecture, size, shape, and pre-training duration where selected: https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIT researchers have demonstrated the use of a generative machine-learning model to create synthetic data, based on real data, that can be used to train another model for image classification.]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tflyzv/mit_researchers_have_demonstrated_the_use_of_a/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tflyzv/mit_researchers_have_demonstrated_the_use_of_a/"/>
        <updated>2022-03-16T16:27:19.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/qptbook  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Composer: a new PyTorch library to train models ~2-4x faster with better algorithms]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tflvuy/p_composer_a_new_pytorch_library_to_train_models/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tflvuy/p_composer_a_new_pytorch_library_to_train_models/"/>
        <updated>2022-03-16T16:23:25.000Z</updated>
        <summary type="html"><![CDATA[Hey all!
 We're excited to release Composer (https://github.com/mosaicml/composer), an open-source library to speed up training of deep learning models by integrating better algorithms into the training process!
 Time and cost reductions across multiple model families
 Composer lets you train:
  
A ResNet-101 to 78.1% accuracy on ImageNet in 1 hour and 30 minutes ($49 on AWS), 3.5x faster and 71% cheaper than the baseline.
 A ResNet-50 to 76.51% accuracy on ImageNet in 1 hour and 14 minutes ($40 on AWS), 2.9x faster and 65% cheaper than the baseline.
 A GPT-2 to a perplexity of 24.11 on OpenWebText in 4 hours and 27 minutes ($145 on AWS), 1.7x faster and 43% cheaper than the baseline.
  
https://preview.redd.it/0bitody9qrn81.png?width=10008&format=png&auto=webp&s=d9ecdb45f6419eb49e1c2c69ee…]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Quantum Algorithms for Quantum Monte Carlo]]></title>
        <id>http://ai.googleblog.com/2022/03/hybrid-quantum-algorithms-for-quantum.html</id>
        <link href="http://ai.googleblog.com/2022/03/hybrid-quantum-algorithms-for-quantum.html"/>
        <updated>2022-03-16T16:08:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by William J. Huggins, Research Scientist, Google Quantum AI 
The intersection between the computational difficulty and practical importance of quantum chemistry challenges run on quantum computers has long been a focus for Google Quantum AI. We’ve experimentally simulated simple models of chemical bonding, high-temperature superconductivity, nanowires, and even exotic phases of matter such as time crystals on our Sycamore quantum processors. We’ve also developed algorithms suitable for the error-corrected quantum computers we aim to build, including the world’s most efficient algorithm for large-scale quantum computations of chemistry (in the usual way of formulating the problem) and a pioneering approach that allows for us to solve the same problem at an extremely high spatial res…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to fly]]></title>
        <id>https://news.mit.edu/2022/learning-fly-andrea-henshall-0316</id>
        <link href="https://news.mit.edu/2022/learning-fly-andrea-henshall-0316"/>
        <updated>2022-03-16T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[Veteran and PhD student Andrea Henshall has used MIT Open Learning to soar from the Air Force to multiple aeronautics degrees.]]></summary>
        <author>
            <name>Katherine Ouellette | MIT Open Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] INTELlinext: A LSTM and HMM-Based Solution for Next-App Prediction (collaboration with Intel)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfkum9/p_intellinext_a_lstm_and_hmmbased_solution_for/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfkum9/p_intellinext_a_lstm_and_hmmbased_solution_for/"/>
        <updated>2022-03-16T15:46:54.000Z</updated>
        <summary type="html"><![CDATA[Hello everyone, I'm excited to share our work with Intel over the past months for our undergraduate capstone. Since September, we've worked with Intel's Data Collection & Analysis team on building models for application preload to minimize loading times. 
  
INTELlinext: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection
 
Our talk at the UCSD Capstone series
 
More on the development of the data collection framework: Development of Input Libraries With Intel XLSDK to Capture Data for App Start Prediction
 
    submitted by    /u/cgorlla  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A mini-conversation with Joe Rogan's AI persona]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfkbju/a_miniconversation_with_joe_rogans_ai_persona/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfkbju/a_miniconversation_with_joe_rogans_ai_persona/"/>
        <updated>2022-03-16T15:33:26.000Z</updated>
        <summary type="html"><![CDATA[submitted by    /u/kuasha7  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] [R] Database of real life face images in 1024x1024 categorised by skin type for StyleGAN2 FID comparisons]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfjtr5/p_r_database_of_real_life_face_images_in/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfjtr5/p_r_database_of_real_life_face_images_in/"/>
        <updated>2022-03-16T15:10:45.000Z</updated>
        <summary type="html"><![CDATA[Hi!
 I am searching for a database of 500 images of each of these 6 skin types: asian, black, indian, hispanic/latino, middle eastern and white. I want to comparison some 100k images that I have produced with StyleGAN2 to verify the quality difference of skin types and want some good quality images to compare my data towards. Thank you for any advice of where I can find such. A db with good quality face images in 1024x1024 that isnt categorized with different skin types is also appreciated as I can use deepface to ceparate them
    submitted by    /u/oSunde  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is a technically principled way to compare new RL architectures that have different capacity, ruling out all possibile confounding factors?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tfjkkj/what_is_a_technically_principled_way_to_compare/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tfjkkj/what_is_a_technically_principled_way_to_compare/"/>
        <updated>2022-03-16T14:59:28.000Z</updated>
        <summary type="html"><![CDATA[I have four RL agents with different architectures whose performance I would like to test. My question, however, is: how do you know whether performance of a specific architecture is better because the architecture is actually better at OOD generalization (in case you're testing that) or because it simply has more neural networks and greater capacity?
    submitted by    /u/No_Possibility_7588  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[D] What is the current consensus on the effectiveness of Active Learning?]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfjdfp/d_what_is_the_current_consensus_on_the/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfjdfp/d_what_is_the_current_consensus_on_the/"/>
        <updated>2022-03-16T14:49:59.000Z</updated>
        <summary type="html"><![CDATA[There is a lot of research going on in Active Learning but I feel there is nothing conclusive coming out of this field. A lot of methods are struggling to beat the random sampling baseline.
 For example, they used to publish this promising paper of using drop out for uncertainty estimation:
 https://arxiv.org/pdf/1506.02142.pdf
 But people tried to use it and it was just not performing well. Since then, I am not sure if DL methods can properly bootstrap themselves and identify what they do not know.
 I feel people are just urged to publish papers and it is too easy to fake the numbers and reportedly beating baselines until the next guy tries to reproduce the results.
 Is my skepticism justified? Or are there some methods (links appreciated) that are productively used in the industry?
 Edit:
 To narrow the discussion, i mean active learning in the sense of finding the next samples to label to maximize improvement of the current model performance.
    submitted by    /u/KonArtist01  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How many trajectories to learn from in policy gradient and actor-critic methods?]]></title>
        <id>https://www.reddit.com/r/reinforcementlearning/comments/tfj4ew/how_many_trajectories_to_learn_from_in_policy/</id>
        <link href="https://www.reddit.com/r/reinforcementlearning/comments/tfj4ew/how_many_trajectories_to_learn_from_in_policy/"/>
        <updated>2022-03-16T14:37:58.000Z</updated>
        <summary type="html"><![CDATA[In the policy gradient and actor-critic methods, we collect trajectories and then update the policy network. So, how many trajectories should we collect before training the networks. Also, is there any range of learning rate we should choose from while training these network because too high learning rate can cause the problem of exploding gradients and then the networks just gives nan as the output.
    submitted by    /u/Better-Ad8608  
 [link]   [comments]]]></summary>
        <author>
            <name>Reinforcement Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi fellow AI enthusiasts/professionals! We need YOUR help to improve our AI community!]]></title>
        <id>https://www.reddit.com/r/artificial/comments/tfhhaa/hi_fellow_ai_enthusiastsprofessionals_we_need/</id>
        <link href="https://www.reddit.com/r/artificial/comments/tfhhaa/hi_fellow_ai_enthusiastsprofessionals_we_need/"/>
        <updated>2022-03-16T13:18:29.000Z</updated>
        <summary type="html"><![CDATA[If you are passionate about AI, please consider joining our community and come exchange with us! We are a community around AI called "Learn AI Together".
 I think the name says it all, but more specifically, we are a community on Discord with ~22'000 members looking for more professionals to help others, but also for anyone interested in AI that is willing to connect with people like-minded.
 We share job offers, interesting projects, events. You can have discussions regarding pretty much anything related to AI with people that will be just as excited as you are! I think it's a fantastic place to be if you are in the field or simply interested in it!
 I will also be looking for more contributors and moderators to help us if you are interested in that role, if so, please DM me on discord after joining the server, I'd be happy to chat!
 If that sounds interesting, please join here: https://www.discord.gg/learnaitogether
    submitted by    /u/OnlyProggingForFun  
 [link]   [comments]]]></summary>
        <author>
            <name>Artificial Intelligence</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-centric AI: Practical implications with the SMART Pipeline]]></title>
        <id>https://medium.com/p/9b187a1bd412</id>
        <link href="https://becominghuman.ai/data-centric-ai-practical-implications-with-the-smart-pipeline-9b187a1bd412?source=rss----5e5bef33608a---4"/>
        <updated>2022-03-16T13:14:48.000Z</updated>
        <summary type="html"><![CDATA[For context, I am a cofounder of Encord, a company building software to improve training data for computer vision.]]></summary>
        <author>
            <name>Eric Landau</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explanation video of how to do Open-QA using ORQA formulation]]></title>
        <id>https://www.reddit.com/r/neuralnetworks/comments/tfhdev/explanation_video_of_how_to_do_openqa_using_orqa/</id>
        <link href="https://www.reddit.com/r/neuralnetworks/comments/tfhdev/explanation_video_of_how_to_do_openqa_using_orqa/"/>
        <updated>2022-03-16T13:12:46.000Z</updated>
        <summary type="html"><![CDATA[Hi, in this video, I explain ORQA which uses a retriever to find the right context from the entire Wikipedia and then uses an extractive QA model to give a final answer. We discuss the task setup, architecture, and loss function.
 The video is part of 8 video series on Open domain question answering, how it is different from normal QA, the difference in loss formulations, and key papers on different Open-QA architectures.
 I will really appreciate any feedback. Thanks.
 https://www.youtube.com/watch?v=9bL2VbwZ9G8 
    submitted by    /u/infiniteakashe  
 [link]   [comments]]]></summary>
        <author>
            <name>Neural Networks, Deep Learning and Machine Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[P] Explanation video of Open Retrieval Question Answering (ORQA)]]></title>
        <id>https://www.reddit.com/r/MachineLearning/comments/tfhclu/p_explanation_video_of_open_retrieval_question/</id>
        <link href="https://www.reddit.com/r/MachineLearning/comments/tfhclu/p_explanation_video_of_open_retrieval_question/"/>
        <updated>2022-03-16T13:11:36.000Z</updated>
        <summary type="html"><![CDATA[In this video, I discuss ORQA which uses a retriever to find the right context from the entire Wikipedia and then uses an extractive QA model to give a final answer. We discuss the task setup, architecture, and loss function.
 The video is part of 8 video series on Open domain question answering, how it is different from normal QA, the difference in loss formulations, and key papers on different Open-QA architectures.
 I will really appreciate any feedback. 
 https://www.youtube.com/watch?v=9bL2VbwZ9G8
    submitted by    /u/infiniteakashe  
 [link]   [comments]]]></summary>
        <author>
            <name>Machine Learning</name>
        </author>
    </entry>
</feed>